{
  "source": "arxiv",
  "fetched_at": "2025-11-22T12:42:25.844799+00:00",
  "payload": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/8emQiXY4FBUIg0n7iylfeh+PQzU</id>\n  <title>arXiv Query: search_query=(ti:agent OR ti:\"tool use\" OR ti:tool-augmented OR ti:planning OR ti:multi-agent OR ti:autonomous OR ti:toolformer OR ti:\"tool learning\" OR abs:agent OR abs:\"tool use\" OR abs:tool-augmented OR abs:planning OR abs:multi-agent OR abs:autonomous OR abs:toolformer OR abs:\"tool learning\") AND (cat:cs.AI OR cat:cs.LG OR cat:cs.MA) AND (ti:Google OR ti:DeepMind OR ti:Microsoft OR ti:OpenAI OR ti:Meta OR ti:Apple OR ti:Stanford OR ti:MIT OR ti:CMU OR ti:Berkeley OR ti:Oxford OR ti:Harvard OR ti:Tsinghua OR ti:\"Peking University\" OR ti:PKU OR ti:USTC OR ti:SJTU OR ti:Princeton OR ti:UCLA OR ti:UCSD OR ti:\"ETH Zurich\" OR ti:NUS OR ti:NTU OR abs:Google OR abs:DeepMind OR abs:Microsoft OR abs:OpenAI OR abs:Meta OR abs:Apple OR abs:Stanford OR abs:MIT OR abs:CMU OR abs:Berkeley OR abs:Oxford OR abs:Harvard OR abs:Tsinghua OR abs:\"Peking University\" OR abs:PKU OR abs:USTC OR abs:SJTU OR abs:Princeton OR abs:UCLA OR abs:UCSD OR abs:\"ETH Zurich\" OR abs:NUS OR abs:NTU)&amp;id_list=&amp;start=100&amp;max_results=50</title>\n  <updated>2025-11-22T12:42:25Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=(ti:agent+OR+(ti:%22tool+use%22+OR+(ti:tool-augmented+OR+(ti:planning+OR+(ti:multi-agent+OR+(ti:autonomous+OR+(ti:toolformer+OR+(ti:%22tool+learning%22+OR+(abs:agent+OR+(abs:%22tool+use%22+OR+(abs:tool-augmented+OR+(abs:planning+OR+(abs:multi-agent+OR+(abs:autonomous+OR+(abs:toolformer+OR+abs:%22tool+learning%22)))))))))))))))+AND+((cat:cs.AI+OR+(cat:cs.LG+OR+cat:cs.MA))+AND+(ti:Google+OR+(ti:DeepMind+OR+(ti:Microsoft+OR+(ti:OpenAI+OR+(ti:Meta+OR+(ti:Apple+OR+(ti:Stanford+OR+(ti:MIT+OR+(ti:CMU+OR+(ti:Berkeley+OR+(ti:Oxford+OR+(ti:Harvard+OR+(ti:Tsinghua+OR+(ti:%22Peking+University%22+OR+(ti:PKU+OR+(ti:USTC+OR+(ti:SJTU+OR+(ti:Princeton+OR+(ti:UCLA+OR+(ti:UCSD+OR+(ti:%22ETH+Zurich%22+OR+(ti:NUS+OR+(ti:NTU+OR+(abs:Google+OR+(abs:DeepMind+OR+(abs:Microsoft+OR+(abs:OpenAI+OR+(abs:Meta+OR+(abs:Apple+OR+(abs:Stanford+OR+(abs:MIT+OR+(abs:CMU+OR+(abs:Berkeley+OR+(abs:Oxford+OR+(abs:Harvard+OR+(abs:Tsinghua+OR+(abs:%22Peking+University%22+OR+(abs:PKU+OR+(abs:USTC+OR+(abs:SJTU+OR+(abs:Princeton+OR+(abs:UCLA+OR+(abs:UCSD+OR+(abs:%22ETH+Zurich%22+OR+(abs:NUS+OR+abs:NTU))))))))))))))))))))))))))))))))))))))))))))))&amp;start=100&amp;max_results=50&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>\n  <opensearch:totalResults>2708</opensearch:totalResults>\n  <opensearch:startIndex>100</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2510.10197v1</id>\n    <title>Don't Just Fine-tune the Agent, Tune the Environment</title>\n    <updated>2025-10-11T12:35:15Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.10197v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.10197v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (RL) struggles with a critical cold-start problem and training instability. To address these challenges, we introduce $\\textbf{Environment Tuning}$, a novel training paradigm that enables agents to learn complex behaviors directly from problem instances without relying on pre-collected expert trajectories. $\\textbf{Environment Tuning}$ orchestrates this learning process through a structured curriculum, actionable environment augmentation that provides corrective feedback, and fine-grained progress rewards to ensure stable and efficient exploration. Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Our work presents a paradigm shift from supervised fine-tuning on static trajectories to dynamic, environment-based exploration, paving the way for training more robust and data-efficient agents.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-11T12:35:15Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Siyuan Lu</name>\n    </author>\n    <author>\n      <name>Zechuan Wang</name>\n    </author>\n    <author>\n      <name>Hongxuan Zhang</name>\n    </author>\n    <author>\n      <name>Qintong Wu</name>\n    </author>\n    <author>\n      <name>Leilei Gan</name>\n    </author>\n    <author>\n      <name>Chenyi Zhuang</name>\n    </author>\n    <author>\n      <name>Jinjie Gu</name>\n    </author>\n    <author>\n      <name>Tao Lin</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.10158v1</id>\n    <title>Multi-Scale Diffusion Transformer for Jointly Simulating User Mobility and Mobile Traffic Pattern</title>\n    <updated>2025-10-11T10:45:39Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.10158v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.10158v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>User mobility trajectory and mobile traffic data are essential for a wide spectrum of applications including urban planning, network optimization, and emergency management. However, large-scale and fine-grained mobility data remains difficult to obtain due to privacy concerns and collection costs, making it essential to simulate realistic mobility and traffic patterns. User trajectories and mobile traffic are fundamentally coupled, reflecting both physical mobility and cyber behavior in urban environments. Despite this strong interdependence, existing studies often model them separately, limiting the ability to capture cross-modal dynamics. Therefore, a unified framework is crucial. In this paper, we propose MSTDiff, a Multi-Scale Diffusion Transformer for joint simulation of mobile traffic and user trajectories. First, MSTDiff applies discrete wavelet transforms for multi-resolution traffic decomposition. Second, it uses a hybrid denoising network to process continuous traffic volumes and discrete location sequences. A transition mechanism based on urban knowledge graph embedding similarity is designed to guide semantically informed trajectory generation. Finally, a multi-scale Transformer with cross-attention captures dependencies between trajectories and traffic. Experiments show that MSTDiff surpasses state-of-the-art baselines in traffic and trajectory generation tasks, reducing Jensen-Shannon divergence (JSD) across key statistical metrics by up to 17.38% for traffic generation, and by an average of 39.53% for trajectory generation. The source code is available at: https://github.com/tsinghua-fib-lab/MSTDiff .</summary>\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-11T10:45:39Z</published>\n    <arxiv:comment>9 pages, 4 figures. Code: https://github.com/tsinghua-fib-lab/MSTDiff</arxiv:comment>\n    <arxiv:primary_category term=\"cs.NI\"/>\n    <author>\n      <name>Ziyi Liu</name>\n    </author>\n    <author>\n      <name>Qingyue Long</name>\n    </author>\n    <author>\n      <name>Zhiwen Xue</name>\n    </author>\n    <author>\n      <name>Huandong Wang</name>\n    </author>\n    <author>\n      <name>Yong Li</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.09608v1</id>\n    <title>StreamingVLM: Real-Time Understanding for Infinite Video Streams</title>\n    <updated>2025-10-10T17:59:58Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.09608v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.09608v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.</summary>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-10T17:59:58Z</published>\n    <arxiv:comment>The first two authors contributed equally to this work</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CV\"/>\n    <author>\n      <name>Ruyi Xu</name>\n    </author>\n    <author>\n      <name>Guangxuan Xiao</name>\n    </author>\n    <author>\n      <name>Yukang Chen</name>\n    </author>\n    <author>\n      <name>Liuning He</name>\n    </author>\n    <author>\n      <name>Kelly Peng</name>\n    </author>\n    <author>\n      <name>Yao Lu</name>\n    </author>\n    <author>\n      <name>Song Han</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.09709v1</id>\n    <title>The Idola Tribus of AI: Large Language Models tend to perceive order where none exists</title>\n    <updated>2025-10-10T02:51:15Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.09709v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.09709v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>We present a tendency of large language models (LLMs) to generate absurd patterns despite their clear inappropriateness in a simple task of identifying regularities in number series. Several approaches have been proposed to apply LLMs to complex real-world tasks, such as providing knowledge through retrieval-augmented generation and executing multi-step tasks using AI agent frameworks. However, these approaches rely on the logical consistency and self-coherence of LLMs, making it crucial to evaluate these aspects and consider potential countermeasures. To identify cases where LLMs fail to maintain logical consistency, we conducted an experiment in which LLMs were asked to explain the patterns in various integer sequences, ranging from arithmetic sequences to randomly generated integer series. While the models successfully identified correct patterns in arithmetic and geometric sequences, they frequently over-recognized patterns that were inconsistent with the given numbers when analyzing randomly generated series. This issue was observed even in multi-step reasoning models, including OpenAI o3, o4-mini, and Google Gemini 2.5 Flash Preview Thinking. This tendency to perceive non-existent patterns can be interpreted as the AI model equivalent of Idola Tribus and highlights potential limitations in their capability for applied tasks requiring logical reasoning, even when employing chain-of-thought reasoning mechanisms.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-10T02:51:15Z</published>\n    <arxiv:comment>14 pages, 3 figures, accepted to Findings of EMNLP 2025</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Shin-nosuke Ishikawa</name>\n    </author>\n    <author>\n      <name>Masato Todo</name>\n    </author>\n    <author>\n      <name>Taiki Ogihara</name>\n    </author>\n    <author>\n      <name>Hirotsugu Ohba</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.08790v1</id>\n    <title>COMPASS: Enhancing Agent Long-Horizon Reasoning with Evolving Context</title>\n    <updated>2025-10-09T20:14:26Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.08790v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.08790v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Long-horizon tasks that require sustained reasoning and multiple tool interactions remain challenging for LLM agents: small errors compound across steps, and even state-of-the-art models often hallucinate or lose coherence. We identify context management as the central bottleneck -- extended histories cause agents to overlook critical evidence or become distracted by irrelevant information, thus failing to replan or reflect from previous mistakes. To address this, we propose COMPASS (Context-Organized Multi-Agent Planning and Strategy System), a lightweight hierarchical framework that separates tactical execution, strategic oversight, and context organization into three specialized components: (1) a Main Agent that performs reasoning and tool use, (2) a Meta-Thinker that monitors progress and issues strategic interventions, and (3) a Context Manager that maintains concise, relevant progress briefs for different reasoning stages. Across three challenging benchmarks -- GAIA, BrowseComp, and Humanity's Last Exam -- COMPASS improves accuracy by up to 20% relative to both single- and multi-agent baselines. We further introduce a test-time scaling extension that elevates performance to match established DeepResearch agents, and a post-training pipeline that delegates context management to smaller models for enhanced efficiency.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-09T20:14:26Z</published>\n    <arxiv:comment>Under Review for ACL</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Guangya Wan</name>\n    </author>\n    <author>\n      <name>Mingyang Ling</name>\n    </author>\n    <author>\n      <name>Xiaoqi Ren</name>\n    </author>\n    <author>\n      <name>Rujun Han</name>\n    </author>\n    <author>\n      <name>Sheng Li</name>\n    </author>\n    <author>\n      <name>Zizhao Zhang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.08713v1</id>\n    <title>Unified World Models: Memory-Augmented Planning and Foresight for Visual Navigation</title>\n    <updated>2025-10-09T18:18:11Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.08713v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.08713v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Enabling embodied agents to effectively imagine future states is critical for robust and generalizable visual navigation. Current state-of-the-art approaches, however, adopt modular architectures that separate navigation planning from visual world modeling, leading to state-action misalignment and limited adaptability in novel or dynamic scenarios. To overcome this fundamental limitation, we propose UniWM, a unified, memory-augmented world model integrating egocentric visual foresight and planning within a single multimodal autoregressive backbone. Unlike modular frameworks, UniWM explicitly grounds action decisions in visually imagined outcomes, ensuring tight alignment between prediction and control. A hierarchical memory mechanism further integrates detailed short-term perceptual cues with longer-term trajectory context, enabling stable, coherent reasoning over extended horizons. Extensive experiments across four challenging benchmarks (Go Stanford, ReCon, SCAND, HuRoN) demonstrate that UniWM substantially improves navigation success rates by up to 30%, significantly reduces trajectory errors compared to strong baselines, and exhibits impressive zero-shot generalization on the unseen TartanDrive dataset. These results highlight UniWM as a principled step toward unified, imagination-driven embodied navigation.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-09T18:18:11Z</published>\n    <arxiv:comment>18 pages, 11 figures, code: https://github.com/F1y1113/UniWM</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Yifei Dong</name>\n    </author>\n    <author>\n      <name>Fengyi Wu</name>\n    </author>\n    <author>\n      <name>Guangyu Chen</name>\n    </author>\n    <author>\n      <name>Zhi-Qi Cheng</name>\n    </author>\n    <author>\n      <name>Qiyu Hu</name>\n    </author>\n    <author>\n      <name>Yuxuan Zhou</name>\n    </author>\n    <author>\n      <name>Jingdong Sun</name>\n    </author>\n    <author>\n      <name>Jun-Yan He</name>\n    </author>\n    <author>\n      <name>Qi Dai</name>\n    </author>\n    <author>\n      <name>Alexander G Hauptmann</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.07091v1</id>\n    <title>The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas</title>\n    <updated>2025-10-08T14:47:40Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.07091v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.07091v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Enabling LLMs to effectively operate long-horizon task which requires long-term planning and multiple interactions is essential for open-world autonomy. Conventional methods adopt planning with actions where a executable action list would be provided as reference. However, this action representation choice would be impractical when the environment action space is combinatorial exploded (e.g., open-ended real world). This naturally leads to a question: As environmental action space scales, what is the optimal action representation for long-horizon agents? In this paper, we systematically study the effectiveness of two different action representations. The first one is conventional planning with actions (PwA) which is predominantly adopted for its effectiveness on existing benchmarks. The other one is planning with schemas (PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ] to [OBJ]\" -&gt; \"move apple to desk\") to ensure concise action space and reliable scalability. This alternative is motivated by its alignment with human cognition and its compliance with environment-imposed action format restriction. We propose cognitive bandwidth perspective as a conceptual framework to qualitatively understand the differences between these two action representations and empirically observe a representation-choice inflection point between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve as evidence of the need for scalable representations. We further conduct controlled experiments to study how the location of this inflection point interacts with different model capacities: stronger planning proficiency shifts the inflection rightward, whereas better schema instantiation shifts it leftward. Finally, noting the suboptimal performance of PwS agents, we provide an actionable guide for building more capable PwS agents for better scalable autonomy.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-08T14:47:40Z</published>\n    <arxiv:comment>22 pages</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Baixuan Xu</name>\n    </author>\n    <author>\n      <name>Tianshi Zheng</name>\n    </author>\n    <author>\n      <name>Zhaowei Wang</name>\n    </author>\n    <author>\n      <name>Hong Ting Tsang</name>\n    </author>\n    <author>\n      <name>Weiqi Wang</name>\n    </author>\n    <author>\n      <name>Tianqing Fang</name>\n    </author>\n    <author>\n      <name>Yangqiu Song</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.06912v1</id>\n    <title>Utilizing Large Language Models for Machine Learning Explainability</title>\n    <updated>2025-10-08T11:46:23Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.06912v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.06912v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This study explores the explainability capabilities of large language models (LLMs), when employed to autonomously generate machine learning (ML) solutions. We examine two classification tasks: (i) a binary classification problem focused on predicting driver alertness states, and (ii) a multilabel classification problem based on the yeast dataset. Three state-of-the-art LLMs (i.e. OpenAI GPT, Anthropic Claude, and DeepSeek) are prompted to design training pipelines for four common classifiers: Random Forest, XGBoost, Multilayer Perceptron, and Long Short-Term Memory networks. The generated models are evaluated in terms of predictive performance (recall, precision, and F1-score) and explainability using SHAP (SHapley Additive exPlanations). Specifically, we measure Average SHAP Fidelity (Mean Squared Error between SHAP approximations and model outputs) and Average SHAP Sparsity (number of features deemed influential). The results reveal that LLMs are capable of producing effective and interpretable models, achieving high fidelity and consistent sparsity, highlighting their potential as automated tools for interpretable ML pipeline generation. The results show that LLMs can produce effective, interpretable pipelines with high fidelity and consistent sparsity, closely matching manually engineered baselines.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-08T11:46:23Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Alexandros Vassiliades</name>\n    </author>\n    <author>\n      <name>Nikolaos Polatidis</name>\n    </author>\n    <author>\n      <name>Stamatios Samaras</name>\n    </author>\n    <author>\n      <name>Sotiris Diplaris</name>\n    </author>\n    <author>\n      <name>Ignacio Cabrera Martin</name>\n    </author>\n    <author>\n      <name>Yannis Manolopoulos</name>\n    </author>\n    <author>\n      <name>Stefanos Vrochidis</name>\n    </author>\n    <author>\n      <name>Ioannis Kompatsiaris</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.06711v1</id>\n    <title>Inefficiencies of Meta Agents for Agent Design</title>\n    <updated>2025-10-08T07:06:17Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.06711v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.06711v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Recent works began to automate the design of agentic systems using meta-agents that propose and iteratively refine new agent architectures. In this paper, we examine three key challenges in a common class of meta-agents. First, we investigate how a meta-agent learns across iterations and find that simply expanding the context with all previous agents, as proposed by previous works, performs worse than ignoring prior designs entirely. We show that the performance improves with an evolutionary approach. Second, although the meta-agent designs multiple agents during training, it typically commits to a single agent at test time. We find that the designed agents have low behavioral diversity, limiting the potential for their complementary use. Third, we assess when automated design is economically viable. We find that only in a few cases--specifically, two datasets--the overall cost of designing and deploying the agents is lower than that of human-designed agents when deployed on over 15,000 examples. In contrast, the performance gains for other datasets do not justify the design cost, regardless of scale.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-08T07:06:17Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Batu El</name>\n    </author>\n    <author>\n      <name>Mert Yuksekgonul</name>\n    </author>\n    <author>\n      <name>James Zou</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.06649v1</id>\n    <title>Local Reinforcement Learning with Action-Conditioned Root Mean Squared Q-Functions</title>\n    <updated>2025-10-08T05:06:09Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.06649v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.06649v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The Forward-Forward (FF) Algorithm is a recently proposed learning procedure for neural networks that employs two forward passes instead of the traditional forward and backward passes used in backpropagation. However, FF remains largely confined to supervised settings, leaving a gap at domains where learning signals can be yielded more naturally such as RL. In this work, inspired by FF's goodness function using layer activity statistics, we introduce Action-conditioned Root mean squared Q-Functions (ARQ), a novel value estimation method that applies a goodness function and action conditioning for local RL using temporal difference learning. Despite its simplicity and biological grounding, our approach achieves superior performance compared to state-of-the-art local backprop-free RL methods in the MinAtar and the DeepMind Control Suite benchmarks, while also outperforming algorithms trained with backpropagation on most tasks. Code can be found at https://github.com/agentic-learning-ai-lab/arq.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-08T05:06:09Z</published>\n    <arxiv:comment>15 pages, 5 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Frank Wu</name>\n    </author>\n    <author>\n      <name>Mengye Ren</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.06214v1</id>\n    <title>Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents</title>\n    <updated>2025-10-07T17:59:13Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.06214v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.06214v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language model (LLM) agents increasingly rely on external tools such as search engines to solve complex, multi-step problems, and reinforcement learning (RL) has become a key paradigm for training them. However, the trajectories of search agents are structurally heterogeneous, where variations in the number, placement, and outcomes of search calls lead to fundamentally different answer directions and reward distributions. Standard policy gradient methods, which use a single global baseline, suffer from what we identify and formalize as cross-stratum bias-an \"apples-to-oranges\" comparison of heterogeneous trajectories. This cross-stratum bias distorts credit assignment and hinders exploration of complex, multi-step search strategies. To address this, we propose Stratified GRPO, whose central component, Stratified Advantage Normalization (SAN), partitions trajectories into homogeneous strata based on their structural properties and computes advantages locally within each stratum. This ensures that trajectories are evaluated only against their true peers. Our analysis proves that SAN eliminates cross-stratum bias, yields conditionally unbiased unit-variance estimates inside each stratum, and retains the global unbiasedness and unit-variance properties enjoyed by standard normalization, resulting in a more pure and scale-stable learning signal. To improve practical stability under finite-sample regimes, we further linearly blend SAN with the global estimator. Extensive experiments on diverse single-hop and multi-hop question-answering benchmarks demonstrate that Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3 points, achieving higher training rewards, greater training stability, and more effective search policies. These results establish stratification as a principled remedy for structural heterogeneity in RL for LLM search agents.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-07T17:59:13Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Mingkang Zhu</name>\n    </author>\n    <author>\n      <name>Xi Chen</name>\n    </author>\n    <author>\n      <name>Bei Yu</name>\n    </author>\n    <author>\n      <name>Hengshuang Zhao</name>\n    </author>\n    <author>\n      <name>Jiaya Jia</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.06187v1</id>\n    <title>Automated Program Repair of Uncompilable Student Code</title>\n    <updated>2025-10-07T17:46:33Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.06187v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.06187v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>A significant portion of student programming submissions in CS1 learning environments are uncompilable, limiting their use in student modeling and downstream knowledge tracing. Traditional modeling pipelines often exclude these cases, discarding observations of student learning. This study investigates automated program repair as a strategy to recover uncompilable code while preserving students' structural intent for use in student modeling. Within this framework, we assess large language models (LLMs) as repair agents, including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash (Google), under high- and low-context prompting conditions. Repairs were evaluated for compilability, edit distance, and preservation of students' original structure and logic. We find that while all three LLMs are capable of producing compilable repairs, their behavior diverges in how well they preserve students' control flow and code structure, which affects their pedagogical utility. By recovering uncompilable submissions, this work enables richer and more comprehensive analyses of learners' coding processes and development over time.</summary>\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-07T17:46:33Z</published>\n    <arxiv:primary_category term=\"cs.SE\"/>\n    <author>\n      <name>Griffin Pitts</name>\n    </author>\n    <author>\n      <name>Aum Pandya</name>\n    </author>\n    <author>\n      <name>Darsh Rank</name>\n    </author>\n    <author>\n      <name>Tirth Bhatt</name>\n    </author>\n    <author>\n      <name>Muntasir Hoq</name>\n    </author>\n    <author>\n      <name>Bita Akram</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.06135v1</id>\n    <title>Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification</title>\n    <updated>2025-10-07T17:09:23Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.06135v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.06135v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Test-time compute can be scaled both sequentially and in parallel. Sequential scaling involves lengthening the generation process, while parallel scaling involves verifying and selecting among multiple candidate outputs. Combining these two strategies has led to the most powerful AI systems, such as Grok 4 Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles), verifying responses can be substantially easier than generating them. This property, referred to as \\emph{asymmetric verification}, highlights the strong potential of test-time scaling (TTS). In this work, we study both sequential and parallel TTS of deep search agents, motivated by the intuition that verification in this setting is often much easier than generation. In experiments, we first show that sequential scaling methods, such as budget forcing, can be effective initially but soon degrade performance. Leveraging asymmetric verification, however, we are able to achieve substantial improvements by allocating only a modest amount of compute to the verifier. We conduct experiments with flagship open-source models and extend them to their ``Heavy'' variants through TTS. These deep research agents achieve gains of up to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an open-source alternative, GLM-4.5 Heavy reaches accuracy of {\\bf 54.0\\%} on BrowseComp and {\\bf 66.0\\%} on GAIA, placing it comparable to the best proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy further achieves {\\bf 69.0\\%} accuracy on BrowseComp, greatly surpassing the best proprietary results.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-07T17:09:23Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Weihao Zeng</name>\n    </author>\n    <author>\n      <name>Keqing He</name>\n    </author>\n    <author>\n      <name>Chuqiao Kuang</name>\n    </author>\n    <author>\n      <name>Xiaoguang Li</name>\n    </author>\n    <author>\n      <name>Junxian He</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.05921v1</id>\n    <title>Prompt reinforcing for long-term planning of large language models</title>\n    <updated>2025-10-07T13:30:18Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.05921v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.05921v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language models (LLMs) have achieved remarkable success in a wide range of natural language processing tasks and can be adapted through prompting. However, they remain suboptimal in multi-turn interactions, often relying on incorrect early assumptions and failing to track user goals over time, which makes such tasks particularly challenging. Prior works in dialogue systems have shown that long-term planning is essential for handling interactive tasks. In this work, we propose a prompt optimisation framework inspired by reinforcement learning, which enables such planning to take place by only modifying the task instruction prompt of the LLM-based agent. By generating turn-by-turn feedback and leveraging experience replay for prompt rewriting, our proposed method shows significant improvement in multi-turn tasks such as text-to-SQL and task-oriented dialogue. Moreover, it generalises across different LLM-based agents and can leverage diverse LLMs as meta-prompting agents. This warrants future research in reinforcement learning-inspired parameter-free optimisation methods.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-07T13:30:18Z</published>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Hsien-Chin Lin</name>\n    </author>\n    <author>\n      <name>Benjamin Matthias Ruppik</name>\n    </author>\n    <author>\n      <name>Carel van Niekerk</name>\n    </author>\n    <author>\n      <name>Chia-Hao Shen</name>\n    </author>\n    <author>\n      <name>Michael Heck</name>\n    </author>\n    <author>\n      <name>Nurul Lubis</name>\n    </author>\n    <author>\n      <name>Renato Vukovic</name>\n    </author>\n    <author>\n      <name>Shutong Feng</name>\n    </author>\n    <author>\n      <name>Milica Gašić</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.05746v1</id>\n    <title>ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems</title>\n    <updated>2025-10-07T10:04:48Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.05746v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.05746v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved state-of-the-art results on various complex reasoning tasks. Recent works have proposed techniques to automate the design of MASes, eliminating the need for manual engineering. However, these techniques perform poorly, often achieving similar or inferior performance to simple baselines. Furthermore, they require computationally expensive re-discovery of architectures for each new task domain and expensive data annotation on domains without existing labeled validation sets. A critical insight is that simple Chain of Thought (CoT) reasoning often performs competitively with these complex systems, suggesting that the fundamental reasoning unit of MASes, CoT, warrants further investigation. To this end, we present a new paradigm for automatic MAS design that pivots the focus to optimizing CoT reasoning. We introduce the Agentic Reasoning Module (ARM), an agentic generalization of CoT where each granular reasoning step is executed by a specialized reasoning module. This module is discovered through a tree search over the code space, starting from a simple CoT module and evolved using mutations informed by reflection on execution traces. The resulting ARM acts as a versatile reasoning building block which can be utilized as a direct recursive loop or as a subroutine in a learned meta-orchestrator. Our approach significantly outperforms both manually designed MASes and state-of-the-art automatic MAS design methods. Crucially, MASes built with ARM exhibit superb generalization, maintaining high performance across different foundation models and task domains without further optimization.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-07T10:04:48Z</published>\n    <arxiv:comment>29 pages, 2 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Bohan Yao</name>\n    </author>\n    <author>\n      <name>Shiva Krishna Reddy Malay</name>\n    </author>\n    <author>\n      <name>Vikas Yadav</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.05670v2</id>\n    <title>Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models</title>\n    <updated>2025-10-16T11:37:20Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.05670v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.05670v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Concept Bottleneck Models (CBNMs) are deep learning models that provide interpretability by enforcing a bottleneck layer where predictions are based exclusively on human-understandable concepts. However, this constraint also restricts information flow and often results in reduced predictive accuracy. Concept Sidechannel Models (CSMs) address this limitation by introducing a sidechannel that bypasses the bottleneck and carry additional task-relevant information. While this improves accuracy, it simultaneously compromises interpretability, as predictions may rely on uninterpretable representations transmitted through sidechannels. Currently, there exists no principled technique to control this fundamental trade-off. In this paper, we close this gap. First, we present a unified probabilistic concept sidechannel meta-model that subsumes existing CSMs as special cases. Building on this framework, we introduce the Sidechannel Independence Score (SIS), a metric that quantifies a CSM's reliance on its sidechannel by contrasting predictions made with and without sidechannel information. We propose SIS regularization, which explicitly penalizes sidechannel reliance to improve interpretability. Finally, we analyze how the expressivity of the predictor and the reliance of the sidechannel jointly shape interpretability, revealing inherent trade-offs across different CSM architectures. Empirical results show that state-of-the-art CSMs, when trained solely for accuracy, exhibit low representation interpretability, and that SIS regularization substantially improves their interpretability, intervenability, and the quality of learned interpretable task predictors. Our work provides both theoretical and practical tools for developing CSMs that balance accuracy and interpretability in a principled manner.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-07T08:29:34Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>David Debot</name>\n    </author>\n    <author>\n      <name>Giuseppe Marra</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.05580v1</id>\n    <title>MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption</title>\n    <updated>2025-10-07T04:54:39Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.05580v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.05580v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Vision-Language-Action (VLA) models show promise in embodied reasoning, yet remain far from true generalists-they often require task-specific fine-tuning, and generalize poorly to unseen tasks. We propose MetaVLA, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism-derived from Attentive Neural Processes-to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable-paving the way toward general-purpose embodied agents. Code will be available.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-07T04:54:39Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Chen Li</name>\n    </author>\n    <author>\n      <name>Zhantao Yang</name>\n    </author>\n    <author>\n      <name>Han Zhang</name>\n    </author>\n    <author>\n      <name>Fangyi Chen</name>\n    </author>\n    <author>\n      <name>Chenchen Zhu</name>\n    </author>\n    <author>\n      <name>Anudeepsekhar Bolimera</name>\n    </author>\n    <author>\n      <name>Marios Savvides</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.05442v1</id>\n    <title>Adversarial Reinforcement Learning for Large Language Model Agent Safety</title>\n    <updated>2025-10-06T23:09:18Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.05442v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.05442v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-06T23:09:18Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Zizhao Wang</name>\n    </author>\n    <author>\n      <name>Dingcheng Li</name>\n    </author>\n    <author>\n      <name>Vaishakh Keshava</name>\n    </author>\n    <author>\n      <name>Phillip Wallis</name>\n    </author>\n    <author>\n      <name>Ananth Balashankar</name>\n    </author>\n    <author>\n      <name>Peter Stone</name>\n    </author>\n    <author>\n      <name>Lukas Rutishauser</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.05327v1</id>\n    <title>DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base</title>\n    <updated>2025-10-06T19:47:27Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.05327v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.05327v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>As large language models (LLMs) continue to be integrated into modern technology, there has been an increased push towards code generation applications, which also naturally extends to hardware design automation. LLM-based solutions for register transfer level (RTL) code generation for intellectual property (IP) designs have grown, especially with fine-tuned LLMs, prompt engineering, and agentic approaches becoming popular in literature. However, a gap has been exposed in these techniques, as they fail to integrate novel IPs into the model's knowledge base, subsequently resulting in poorly generated code. Additionally, as general-purpose LLMs continue to improve, fine-tuned methods on older models will not be able to compete to produce more accurate and efficient designs. Although some retrieval augmented generation (RAG) techniques exist to mitigate challenges presented in fine-tuning approaches, works tend to leverage low-quality codebases, incorporate computationally expensive fine-tuning in the frameworks, or do not use RAG directly in the RTL generation step. In this work, we introduce DeepV: a model-agnostic RAG framework to generate RTL designs by enhancing context through a large, high-quality dataset without any RTL-specific training. Our framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17% increase in performance on the VerilogEval benchmark. We host DeepV for use by the community in a Hugging Face (HF) Space: https://huggingface.co/spaces/FICS-LLM/DeepV.</summary>\n    <category term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-06T19:47:27Z</published>\n    <arxiv:comment>22 pages, 6 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AR\"/>\n    <author>\n      <name>Zahin Ibnat</name>\n    </author>\n    <author>\n      <name>Paul E. Calzada</name>\n    </author>\n    <author>\n      <name>Rasin Mohammed Ihtemam</name>\n    </author>\n    <author>\n      <name>Sujan Kumar Saha</name>\n    </author>\n    <author>\n      <name>Jingbo Zhou</name>\n    </author>\n    <author>\n      <name>Farimah Farahmandi</name>\n    </author>\n    <author>\n      <name>Mark Tehranipoor</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.05096v2</id>\n    <title>Paper2Video: Automatic Video Generation from Scientific Papers</title>\n    <updated>2025-10-09T17:29:00Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.05096v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.05096v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Academic presentation videos have become an essential medium for research communication, yet producing them remains highly labor-intensive, often requiring hours of slide design, recording, and editing for a short 2 to 10 minutes video. Unlike natural video, presentation video generation involves distinctive challenges: inputs from research papers, dense multi-modal information (text, figures, tables), and the need to coordinate multiple aligned channels such as slides, subtitles, speech, and human talker. To address these challenges, we introduce Paper2Video, the first benchmark of 101 research papers paired with author-created presentation videos, slides, and speaker metadata. We further design four tailored evaluation metrics--Meta Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos convey the paper's information to the audience. Building on this foundation, we propose PaperTalker, the first multi-agent framework for academic presentation video generation. It integrates slide generation with effective layout refinement by a novel effective tree search visual choice, cursor grounding, subtitling, speech synthesis, and talking-head rendering, while parallelizing slide-wise generation for efficiency. Experiments on Paper2Video demonstrate that the presentation videos produced by our approach are more faithful and informative than existing baselines, establishing a practical step toward automated and ready-to-use academic video generation. Our dataset, agent, and code are available at https://github.com/showlab/Paper2Video.</summary>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-06T17:58:02Z</published>\n    <arxiv:comment>Project Page: https://showlab.github.io/Paper2Video/</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CV\"/>\n    <author>\n      <name>Zeyu Zhu</name>\n    </author>\n    <author>\n      <name>Kevin Qinghong Lin</name>\n    </author>\n    <author>\n      <name>Mike Zheng Shou</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.04935v1</id>\n    <title>MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning</title>\n    <updated>2025-10-06T15:42:55Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.04935v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.04935v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in simple tasks, where the models excessively utilize System 2-type, deliberate reasoning, leading to inefficient token generation. Furthermore, these models face challenges in adapting their reasoning capabilities to rapidly changing environments due to the static nature of their pretraining data. To address these issues, advancing Large Language Models (LLMs) for complex reasoning tasks requires innovative approaches that bridge intuitive and deliberate cognitive processes, akin to human cognition's dual-system dynamic. This paper introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless integration of System 1's fast, intuitive thinking with System 2's deliberate reasoning within LLMs. MARS strategically integrates multiple external tools, such as Google Search, Google Scholar, and Python Interpreter, to access up-to-date information and execute complex computations, while creating a specialized division of labor where System 1 efficiently processes and summarizes high-volume external information, providing distilled insights that expand System 2's reasoning context without overwhelming its capacity. Furthermore, we propose a multi-agent reinforcement learning framework extending Group Relative Policy Optimization to simultaneously optimize both systems with multi-turn tool interactions, bin-packing optimization, and sample balancing strategies that enhance collaborative efficiency. Extensive experiments demonstrate MARS achieves substantial improvements of 3.86% on the challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9% across 7 knowledge-intensive tasks, validating the effectiveness of our dual-system paradigm for complex reasoning in dynamic information environments.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-06T15:42:55Z</published>\n    <arxiv:comment>Ongoing Work</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Guoxin Chen</name>\n    </author>\n    <author>\n      <name>Zile Qiao</name>\n    </author>\n    <author>\n      <name>Wenqing Wang</name>\n    </author>\n    <author>\n      <name>Donglei Yu</name>\n    </author>\n    <author>\n      <name>Xuanzhong Chen</name>\n    </author>\n    <author>\n      <name>Hao Sun</name>\n    </author>\n    <author>\n      <name>Minpeng Liao</name>\n    </author>\n    <author>\n      <name>Kai Fan</name>\n    </author>\n    <author>\n      <name>Yong Jiang</name>\n    </author>\n    <author>\n      <name>Penguin Xie</name>\n    </author>\n    <author>\n      <name>Wayne Xin Zhao</name>\n    </author>\n    <author>\n      <name>Ruihua Song</name>\n    </author>\n    <author>\n      <name>Fei Huang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.04607v1</id>\n    <title>A Case for Declarative LLM-friendly Interfaces for Improved Efficiency of Computer-Use Agents</title>\n    <updated>2025-10-06T09:14:58Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.04607v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.04607v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Computer-use agents (CUAs) powered by large language models (LLMs) have emerged as a promising approach to automating computer tasks, yet they struggle with graphical user interfaces (GUIs). GUIs, designed for humans, force LLMs to decompose high-level goals into lengthy, error-prone sequences of fine-grained actions, resulting in low success rates and an excessive number of LLM calls.\n  We propose Goal-Oriented Interface (GOI), a novel abstraction that transforms existing GUIs into three declarative primitives: access, state, and observation, which are better suited for LLMs. Our key idea is policy-mechanism separation: LLMs focus on high-level semantic planning (policy) while GOI handles low-level navigation and interaction (mechanism). GOI does not require modifying the application source code or relying on application programming interfaces (APIs).\n  We evaluate GOI with Microsoft Office Suite (Word, PowerPoint, Excel) on Windows. Compared to a leading GUI-based agent baseline, GOI improves task success rates by 67% and reduces interaction steps by 43.5%. Notably, GOI completes over 61% of successful tasks with a single LLM call.</summary>\n    <category term=\"cs.OS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-06T09:14:58Z</published>\n    <arxiv:primary_category term=\"cs.OS\"/>\n    <author>\n      <name>Yuan Wang</name>\n    </author>\n    <author>\n      <name>Mingyu Li</name>\n    </author>\n    <author>\n      <name>Haibo Chen</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.04542v1</id>\n    <title>Code World Models for General Game Playing</title>\n    <updated>2025-10-06T07:16:07Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.04542v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.04542v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Models (LLMs) reasoning abilities are increasingly being applied to classical board and card games, but the dominant approach -- involving prompting for direct move generation -- has significant drawbacks. It relies on the model's implicit fragile pattern-matching capabilities, leading to frequent illegal moves and strategically shallow play. Here we introduce an alternative approach: We use the LLM to translate natural language rules and game trajectories into a formal, executable world model represented as Python code. This generated model -- comprising functions for state transition, legal move enumeration, and termination checks -- serves as a verifiable simulation engine for high-performance planning algorithms like Monte Carlo tree search (MCTS). In addition, we prompt the LLM to generate heuristic value functions (to make MCTS more efficient), and inference functions (to estimate hidden states in imperfect information games). Our method offers three distinct advantages compared to directly using the LLM as a policy: (1) Verifiability: The generated CWM serves as a formal specification of the game's rules, allowing planners to algorithmically enumerate valid actions and avoid illegal moves, contingent on the correctness of the synthesized model; (2) Strategic Depth: We combine LLM semantic understanding with the deep search power of classical planners; and (3) Generalization: We direct the LLM to focus on the meta-task of data-to-code translation, enabling it to adapt to new games more easily. We evaluate our agent on 10 different games, of which 4 are novel and created for this paper. 5 of the games are fully observed (perfect information), and 5 are partially observed (imperfect information). We find that our method outperforms or matches Gemini 2.5 Pro in 9 out of the 10 considered games.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-06T07:16:07Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Wolfgang Lehrach</name>\n    </author>\n    <author>\n      <name>Daniel Hennes</name>\n    </author>\n    <author>\n      <name>Miguel Lazaro-Gredilla</name>\n    </author>\n    <author>\n      <name>Xinghua Lou</name>\n    </author>\n    <author>\n      <name>Carter Wendelken</name>\n    </author>\n    <author>\n      <name>Zun Li</name>\n    </author>\n    <author>\n      <name>Antoine Dedieu</name>\n    </author>\n    <author>\n      <name>Jordi Grau-Moya</name>\n    </author>\n    <author>\n      <name>Marc Lanctot</name>\n    </author>\n    <author>\n      <name>Atil Iscen</name>\n    </author>\n    <author>\n      <name>John Schultz</name>\n    </author>\n    <author>\n      <name>Marcus Chiam</name>\n    </author>\n    <author>\n      <name>Ian Gemp</name>\n    </author>\n    <author>\n      <name>Piotr Zielinski</name>\n    </author>\n    <author>\n      <name>Satinder Singh</name>\n    </author>\n    <author>\n      <name>Kevin P. Murphy</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.04303v2</id>\n    <title>Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs</title>\n    <updated>2025-10-17T19:12:39Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.04303v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.04303v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Multi-agent deployments of large language models (LLMs) are increasingly embedded in market, allocation, and governance workflows, yet covert coordination among agents can silently erode trust and social welfare. Existing audits are dominated by heuristics that lack theoretical guarantees, struggle to transfer across tasks, and seldom ship with the infrastructure needed for independent replication. We introduce Audit the Whisper, a conference-grade research artifact that spans theory, benchmark design, detection, and reproducibility. Our contributions are: (i) a channel-capacity analysis showing how interventions such as paraphrase, rate limiting, and role permutation impose quantifiable capacity penalties-operationalised via paired-run Kullback--Leibler diagnostics-that tighten mutual-information thresholds with finite-sample guarantees and full proofs; (ii) ColludeBench-v0, covering pricing, first-price auctions, peer review, and hosted Gemini/Groq APIs with configurable covert schemes, deterministic manifests, and reward instrumentation; and (iii) a calibrated auditing pipeline that fuses cross-run mutual information, permutation invariance, watermark variance, and fairness-aware acceptance bias, each tuned to a $10^{-3}$ false-positive budget and validated by 10k honest runs plus an e-value martingale. Across ColludeBench and external suites including Secret Collusion, CASE, Perfect Collusion Benchmark, and SentinelAgent, the union meta-test attains state-of-the-art power at fixed FPR while ablations surface price-of-auditing trade-offs and fairness-driven colluders invisible to MI alone. We release regeneration scripts, anonymized manifests, and documentation so that external auditors can reproduce every figure, satisfy double-blind requirements, and extend the framework with minimal effort.</summary>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-05T17:51:52Z</published>\n    <arxiv:comment>13 pages, 0 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.MA\"/>\n    <author>\n      <name>Om Tailor</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.04284v1</id>\n    <title>Doctor-R1: Mastering Clinical Inquiry with Experiential Agentic Reinforcement Learning</title>\n    <updated>2025-10-05T16:54:02Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.04284v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.04284v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The professionalism of a human doctor in outpatient service depends on two core abilities: the ability to make accurate medical decisions and the medical consultation skill to conduct strategic, empathetic patient inquiry. Existing Large Language Models (LLMs) have achieved remarkable accuracy on medical decision-making benchmarks. However, they often lack the ability to conduct the strategic and empathetic consultation, which is essential for real-world clinical scenarios. To address this gap, we propose Doctor-R1, an AI doctor agent trained to master both of the capabilities by ask high-yield questions and conduct strategic multi-turn inquiry to guide decision-making. Our framework introduces three key components: a multi-agent interactive environment, a two-tiered reward architecture that separately optimizes clinical decision-making and communicative inquiry skills, and an experience repository to ground policy learning in high-quality prior trajectories. We evaluate Doctor-R1 on OpenAI's HealthBench and MAQuE, assessed across multi-facet metrics, such as communication quality, user experience, and task accuracy. Remarkably, Doctor-R1 surpasses state-of-the-art open-source specialized LLMs by a substantial margin with higher parameter efficiency and outperforms powerful proprietary models. Furthermore, the human evaluations show a strong preference for Doctor-R1 to generate human-preferred clinical dialogue, demonstrating the effectiveness of the framework.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-05T16:54:02Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Yunghwei Lai</name>\n    </author>\n    <author>\n      <name>Kaiming Liu</name>\n    </author>\n    <author>\n      <name>Ziyue Wang</name>\n    </author>\n    <author>\n      <name>Weizhi Ma</name>\n    </author>\n    <author>\n      <name>Yang Liu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.03847v1</id>\n    <title>Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs</title>\n    <updated>2025-10-04T15:48:04Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.03847v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.03847v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Small language models (SLMs; 1-12B params, sometimes up to 20B) are sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy rather than open-ended generation. We synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with guided decoding libraries (XGrammar, Outlines). We formalize SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, and propose engineering metrics that reflect real production goals: cost per successful task (CPS), schema validity rate, executable call rate, p50/p95 latency, and energy per request. Guided decoding, strict JSON Schema outputs, and validator-first tool execution close much of the capability gap with larger models and often let SLMs match or surpass LLMs on tool use, function calling, and RAG at 10x-100x lower token cost with materially better latency and energy. We provide design patterns for agent stacks that prioritize SLMs: schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits where fallback remains valuable (open-domain reasoning and some long-horizon planning). The result is a practical blueprint for building fast, inexpensive, and reliable agents that default to SLMs while preserving headroom with targeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency, edge inference</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-04T15:48:04Z</published>\n    <arxiv:comment>9 Pages</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Raghav Sharma</name>\n    </author>\n    <author>\n      <name>Manan Mehta</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.03776v1</id>\n    <title>Trajectory prediction for heterogeneous agents: A performance analysis on small and imbalanced datasets</title>\n    <updated>2025-10-04T11:02:21Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.03776v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.03776v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Robots and other intelligent systems navigating in complex dynamic environments should predict future actions and intentions of surrounding agents to reach their goals efficiently and avoid collisions. The dynamics of those agents strongly depends on their tasks, roles, or observable labels. Class-conditioned motion prediction is thus an appealing way to reduce forecast uncertainty and get more accurate predictions for heterogeneous agents. However, this is hardly explored in the prior art, especially for mobile robots and in limited data applications. In this paper, we analyse different class-conditioned trajectory prediction methods on two datasets. We propose a set of conditional pattern-based and efficient deep learning-based baselines, and evaluate their performance on robotics and outdoors datasets (THÖR-MAGNI and Stanford Drone Dataset). Our experiments show that all methods improve accuracy in most of the settings when considering class labels. More importantly, we observe that there are significant differences when learning from imbalanced datasets, or in new environments where sufficient data is not available. In particular, we find that deep learning methods perform better on balanced datasets, but in applications with limited data, e.g., cold start of a robot in a new environment, or imbalanced classes, pattern-based methods may be preferable.</summary>\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-04T11:02:21Z</published>\n    <arxiv:comment>This paper has been accepted to the IEEE Robotics and Automation Letters journal and presented at the 40th Anniversary of the IEEE International Conference on Robotics and Automation, which was held in Rotterdam, Netherlands on 23-26 September, 2024</arxiv:comment>\n    <arxiv:primary_category term=\"cs.RO\"/>\n    <arxiv:journal_ref>IEEE Robotics and Automation Letters ( Volume: 9, Issue: 7, July 2024)</arxiv:journal_ref>\n    <author>\n      <name>Tiago Rodrigues de Almeida</name>\n    </author>\n    <author>\n      <name>Yufei Zhu</name>\n    </author>\n    <author>\n      <name>Andrey Rudenko</name>\n    </author>\n    <author>\n      <name>Tomasz P. Kucner</name>\n    </author>\n    <author>\n      <name>Johannes A. Stork</name>\n    </author>\n    <author>\n      <name>Martin Magnusson</name>\n    </author>\n    <author>\n      <name>Achim J. Lilienthal</name>\n    </author>\n    <arxiv:doi>10.1109/LRA.2024.3408510</arxiv:doi>\n    <link rel=\"related\" href=\"https://doi.org/10.1109/LRA.2024.3408510\" title=\"doi\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.03217v1</id>\n    <title>Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair</title>\n    <updated>2025-10-03T17:53:28Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.03217v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.03217v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Agentic Automated Program Repair (APR) is increasingly tackling complex, repository-level bugs in industry, but ultimately agent-generated patches still need to be reviewed by a human before committing them to ensure they address the bug. Showing unlikely patches to developers can lead to substantial noise, wasting valuable developer time and eroding trust in automated code changes. We introduce two complementary LLM-based policies to reduce such noise: bug abstention and patch validation policies. Bug abstention excludes bugs that the agentic APR system is unlikely to fix. Patch validation rejects patches that are unlikely to be a good fix for the given bug. We evaluate both policies on three sets of bugs from Google's codebase, and their candidate patches generated by an internal agentic APR system. On a set of 174 human-reported bugs, removing bugs and patch trajectories rejected by our policies can raise success rates by up to 13 percentage points and 15 percentage points, respectively, and by up to 39 percentage points in combination. On null pointer exceptions and sanitizer-reported bugs with machine-generated bug reports, patch validation also improves average single-sample success rates. This two-policy approach provides a practical path to the reliable, industrial-scale deployment of agentic APR systems.</summary>\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-03T17:53:28Z</published>\n    <arxiv:primary_category term=\"cs.SE\"/>\n    <author>\n      <name>José Cambronero</name>\n    </author>\n    <author>\n      <name>Michele Tufano</name>\n    </author>\n    <author>\n      <name>Sherry Shi</name>\n    </author>\n    <author>\n      <name>Renyao Wei</name>\n    </author>\n    <author>\n      <name>Grant Uy</name>\n    </author>\n    <author>\n      <name>Runxiang Cheng</name>\n    </author>\n    <author>\n      <name>Chin-Jung Liu</name>\n    </author>\n    <author>\n      <name>Shiying Pan</name>\n    </author>\n    <author>\n      <name>Satish Chandra</name>\n    </author>\n    <author>\n      <name>Pat Rondon</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.03064v1</id>\n    <title>Comparative Analysis of Parameterized Action Actor-Critic Reinforcement Learning Algorithms for Web Search Match Plan Generation</title>\n    <updated>2025-10-03T14:48:57Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.03064v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.03064v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This study evaluates the performance of Soft Actor Critic (SAC), Greedy Actor Critic (GAC), and Truncated Quantile Critics (TQC) in high-dimensional decision-making tasks using fully observable environments. The focus is on parametrized action (PA) spaces, eliminating the need for recurrent networks, with benchmarks Platform-v0 and Goal-v0 testing discrete actions linked to continuous action-parameter spaces. Hyperparameter optimization was performed with Microsoft NNI, ensuring reproducibility by modifying the codebase for GAC and TQC. Results show that Parameterized Action Greedy Actor-Critic (PAGAC) outperformed other algorithms, achieving the fastest training times and highest returns across benchmarks, completing 5,000 episodes in 41:24 for the Platform game and 24:04 for the Robot Soccer Goal game. Its speed and stability provide clear advantages in complex action spaces. Compared to PASAC and PATQC, PAGAC demonstrated superior efficiency and reliability, making it ideal for tasks requiring rapid convergence and robust performance. Future work could explore hybrid strategies combining entropy-regularization with truncation-based methods to enhance stability and expand investigations into generalizability.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-03T14:48:57Z</published>\n    <arxiv:comment>10 pages, 10th International Congress on Information and Communication Technology (ICICT 2025)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Ubayd Bapoo</name>\n    </author>\n    <author>\n      <name>Clement N Nyirenda</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.02837v1</id>\n    <title>Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents</title>\n    <updated>2025-10-03T09:19:15Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.02837v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.02837v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Although recent tool-augmented benchmarks incorporate complex user requests and diverse tools, the evaluation methods for most of them remain limited to answer matching. However, as the number of steps required to resolve a user request increases, a proper evaluation of an agent's performance must go beyond the final answer to also assess the problem-solving trajectory, including previously ignored aspects such as efficiency, hallucination, and adaptivity. The most straightforward method for evaluating these aspects is to compare an agent's trajectory with the ground-truth trajectory, but this approach is fundamentally limited since annotating all valid ground-truth trajectories is prohibitively expensive. However, a simple LLM-based evaluator struggles to assess trajectories in detail without ground truth. To effectively evaluate the agents in this manner, we introduce TRACE, a framework for the multi-dimensional evaluation of tool-augmented LLM agent performance. By incorporating an evidence bank, which accumulates knowledge gathered from preceding reasoning steps, TRACE enables a multi-faceted analysis and evaluation of an agent's reasoning trajectory effectively. To validate our framework, we develop a new meta-evaluation dataset by augmenting existing benchmarks with diverse and flawed trajectories, each labeled with multi-faceted performance scores. Our results confirm that TRACE accurately evaluates these complex behaviors in a scalable and cost-effective manner, even with small open-source LLMs. Furthermore, we apply our method to evaluate the trajectories that agents produce while solving tool-augmented tasks, presenting previously unreported observations and their corresponding insights.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-03T09:19:15Z</published>\n    <arxiv:comment>Preprint. Under Review</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Wonjoong Kim</name>\n    </author>\n    <author>\n      <name>Sangwu Park</name>\n    </author>\n    <author>\n      <name>Yeonjun In</name>\n    </author>\n    <author>\n      <name>Sein Kim</name>\n    </author>\n    <author>\n      <name>Dongha Lee</name>\n    </author>\n    <author>\n      <name>Chanyoung Park</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.05157v1</id>\n    <title>Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment</title>\n    <updated>2025-10-03T05:53:51Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.05157v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.05157v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This paper presents a controlled study of adversarial reinforcement learning in network security through a custom OpenAI Gym environment that models brute-force attacks and reactive defenses on multi-port services. The environment captures realistic security trade-offs including background traffic noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot traps, and multi-level rate-limiting defenses. Competing attacker and defender agents are trained using Deep Q-Networks (DQN) within a zero-sum reward framework, where successful exploits yield large terminal rewards while incremental actions incur small costs. Through systematic evaluation across multiple configurations (varying trap detection probabilities, exploitation difficulty thresholds, and training regimens), the results demonstrate that defender observability and trap effectiveness create substantial barriers to successful attacks. The experiments reveal that reward shaping and careful training scheduling are critical for learning stability in this adversarial setting. The defender consistently maintains strategic advantage across 50,000+ training episodes, with performance gains amplifying when exposed to complex defensive strategies including adaptive IP blocking and port-specific controls. Complete implementation details, reproducible hyperparameter configurations, and architectural guidelines are provided to support future research in adversarial RL for cybersecurity. The zero-sum formulation and realistic operational constraints make this environment suitable for studying autonomous defense systems, attacker-defender co-evolution, and transfer learning to real-world network security scenarios.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-03T05:53:51Z</published>\n    <arxiv:comment>8 pages, 5 tables, 5 figures. 12th International Conference on Next Generation Computing, Communication, Systems and Security</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Abrar Shahid</name>\n    </author>\n    <author>\n      <name>Ibteeker Mahir Ishum</name>\n    </author>\n    <author>\n      <name>AKM Tahmidul Haque</name>\n    </author>\n    <author>\n      <name>M Sohel Rahman</name>\n    </author>\n    <author>\n      <name>A. B. M. Alim Al Islam</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.02125v3</id>\n    <title>Do AI Models Perform Human-like Abstract Reasoning Across Modalities?</title>\n    <updated>2025-10-06T21:24:04Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.02125v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.02125v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>OpenAI's o3-preview reasoning model exceeded human accuracy on the ARC-AGI benchmark, but does that mean state-of-the-art models recognize and reason with the abstractions that the task creators intended? We investigate models' abstraction abilities on ConceptARC. We evaluate models under settings that vary the input modality (textual vs. visual), whether the model is permitted to use external Python tools, and, for reasoning models, the amount of reasoning effort. In addition to measuring output accuracy, we perform fine-grained evaluation of the natural-language rules that models generate to explain their solutions. This dual evaluation lets us assess whether models solve tasks using the abstractions ConceptARC was designed to elicit, rather than relying on surface-level patterns. Our results show that, while some models using text-based representations match human output accuracy, the best models' rules are often based on surface-level ``shortcuts'' and capture intended abstractions far less often than humans. Thus their capabilities for general abstract reasoning may be overestimated by evaluations based on accuracy alone. In the visual modality, AI models' output accuracy drops sharply, yet our rule-level analysis reveals that models might be underestimated, as they still exhibit a substantial share of rules that capture intended abstractions, but are often unable to correctly apply these rules. In short, our results show that models still lag humans in abstract reasoning, and that using accuracy alone to evaluate abstract reasoning on ARC-like tasks may overestimate abstract-reasoning capabilities in textual modalities and underestimate it in visual modalities. We believe that our evaluation framework offers a more faithful picture of multimodal models' abstract reasoning abilities and a more principled way to track progress toward human-like, abstraction-centered intelligence.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-02T15:35:10Z</published>\n    <arxiv:comment>10 pages, 4 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Claas Beger</name>\n    </author>\n    <author>\n      <name>Ryan Yi</name>\n    </author>\n    <author>\n      <name>Shuhao Fu</name>\n    </author>\n    <author>\n      <name>Arseny Moskvichev</name>\n    </author>\n    <author>\n      <name>Sarah W. Tsai</name>\n    </author>\n    <author>\n      <name>Sivasankaran Rajamanickam</name>\n    </author>\n    <author>\n      <name>Melanie Mitchell</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.01141v1</id>\n    <title>Apriel-1.5-15b-Thinker</title>\n    <updated>2025-10-01T17:29:35Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.01141v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.01141v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-01T17:29:35Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Shruthan Radhakrishna</name>\n    </author>\n    <author>\n      <name>Aman Tiwari</name>\n    </author>\n    <author>\n      <name>Aanjaneya Shukla</name>\n    </author>\n    <author>\n      <name>Masoud Hashemi</name>\n    </author>\n    <author>\n      <name>Rishabh Maheshwary</name>\n    </author>\n    <author>\n      <name>Shiva Krishna Reddy Malay</name>\n    </author>\n    <author>\n      <name>Jash Mehta</name>\n    </author>\n    <author>\n      <name>Pulkit Pattnaik</name>\n    </author>\n    <author>\n      <name>Saloni Mittal</name>\n    </author>\n    <author>\n      <name>Khalil Slimi</name>\n    </author>\n    <author>\n      <name>Kelechi Ogueji</name>\n    </author>\n    <author>\n      <name>Akintunde Oladipo</name>\n    </author>\n    <author>\n      <name>Soham Parikh</name>\n    </author>\n    <author>\n      <name>Oluwanifemi Bamgbose</name>\n    </author>\n    <author>\n      <name>Toby Liang</name>\n    </author>\n    <author>\n      <name>Ahmed Masry</name>\n    </author>\n    <author>\n      <name>Khyati Mahajan</name>\n    </author>\n    <author>\n      <name>Sai Rajeswar Mudumba</name>\n    </author>\n    <author>\n      <name>Vikas Yadav</name>\n    </author>\n    <author>\n      <name>Sathwik Tejaswi Madhusudhan</name>\n    </author>\n    <author>\n      <name>Torsten Scholak</name>\n    </author>\n    <author>\n      <name>Sagar Davasam</name>\n    </author>\n    <author>\n      <name>Srinivas Sunkara</name>\n    </author>\n    <author>\n      <name>Nicholas Chapados</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.01074v1</id>\n    <title>Predicting Diabetic Retinopathy Using a Two-Level Ensemble Model</title>\n    <updated>2025-10-01T16:19:57Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.01074v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.01074v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Preprint Note: This is the author preprint version of a paper accepted for presentation at the IISE Annual Conference &amp; Expo 2025. The final version will appear in the official proceedings.\n  Diabetic retinopathy (DR) is a leading cause of blindness in working-age adults, and current diagnostic methods rely on resource-intensive eye exams and specialized equipment. Image-based AI tools have shown limitations in early-stage detection, motivating the need for alternative approaches. We propose a non-image-based, two-level ensemble model for DR prediction using routine laboratory test results. In the first stage, base models (Linear SVC, Random Forest, Gradient Boosting, and XGBoost) are hyperparameter tuned and internally stacked across different configurations to optimize metrics such as accuracy, recall, and precision. In the second stage, predictions are aggregated using Random Forest as a meta-learner. This hierarchical stacking strategy improves generalization, balances performance across multiple metrics, and remains computationally efficient compared to deep learning approaches. The model achieved Accuracy 0.9433, F1 Score 0.9425, Recall 0.9207, Precision 0.9653, ROC-AUC 0.9844, and AUPRC 0.9875, surpassing one-level stacking and FCN baselines. These results highlight the model potential for accurate and interpretable DR risk prediction in clinical settings.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-01T16:19:57Z</published>\n    <arxiv:comment>Accepted for presentation at the IISE Annual Conference &amp; Expo 2025, 6 pages, 2 tables, 1 figure</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Mahyar Mahmoudi</name>\n    </author>\n    <author>\n      <name>Tieming Liu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.01051v1</id>\n    <title>GEM: A Gym for Agentic LLMs</title>\n    <updated>2025-10-01T15:55:57Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.01051v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.01051v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-01T15:55:57Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Zichen Liu</name>\n    </author>\n    <author>\n      <name>Anya Sims</name>\n    </author>\n    <author>\n      <name>Keyu Duan</name>\n    </author>\n    <author>\n      <name>Changyu Chen</name>\n    </author>\n    <author>\n      <name>Simon Yu</name>\n    </author>\n    <author>\n      <name>Xiangxin Zhou</name>\n    </author>\n    <author>\n      <name>Haotian Xu</name>\n    </author>\n    <author>\n      <name>Shaopan Xiong</name>\n    </author>\n    <author>\n      <name>Bo Liu</name>\n    </author>\n    <author>\n      <name>Chenmien Tan</name>\n    </author>\n    <author>\n      <name>Chuen Yang Beh</name>\n    </author>\n    <author>\n      <name>Weixun Wang</name>\n    </author>\n    <author>\n      <name>Hao Zhu</name>\n    </author>\n    <author>\n      <name>Weiyan Shi</name>\n    </author>\n    <author>\n      <name>Diyi Yang</name>\n    </author>\n    <author>\n      <name>Michael Shieh</name>\n    </author>\n    <author>\n      <name>Yee Whye Teh</name>\n    </author>\n    <author>\n      <name>Wee Sun Lee</name>\n    </author>\n    <author>\n      <name>Min Lin</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.00615v2</id>\n    <title>ACON: Optimizing Context Compression for Long-horizon LLM Agents</title>\n    <updated>2025-10-17T06:48:23Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.00615v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.00615v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement. Our code is available at https://github.com/microsoft/acon.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-01T07:43:49Z</published>\n    <arxiv:comment>Preprint</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Minki Kang</name>\n    </author>\n    <author>\n      <name>Wei-Ning Chen</name>\n    </author>\n    <author>\n      <name>Dongge Han</name>\n    </author>\n    <author>\n      <name>Huseyin A. Inan</name>\n    </author>\n    <author>\n      <name>Lukas Wutschitz</name>\n    </author>\n    <author>\n      <name>Yanzhi Chen</name>\n    </author>\n    <author>\n      <name>Robert Sim</name>\n    </author>\n    <author>\n      <name>Saravan Rajmohan</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.00507v2</id>\n    <title>Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs</title>\n    <updated>2025-10-14T02:11:17Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.00507v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.00507v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-01T04:37:54Z</published>\n    <arxiv:comment>20 pages, 10 figures. Our Code: https://github.com/YurunChen/Graph2Eval</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Yurun Chen</name>\n    </author>\n    <author>\n      <name>Xavier Hu</name>\n    </author>\n    <author>\n      <name>Yuhan Liu</name>\n    </author>\n    <author>\n      <name>Ziqi Wang</name>\n    </author>\n    <author>\n      <name>Zeyi Liao</name>\n    </author>\n    <author>\n      <name>Lin Chen</name>\n    </author>\n    <author>\n      <name>Feng Wei</name>\n    </author>\n    <author>\n      <name>Yuxi Qian</name>\n    </author>\n    <author>\n      <name>Bo Zheng</name>\n    </author>\n    <author>\n      <name>Keting Yin</name>\n    </author>\n    <author>\n      <name>Shengyu Zhang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.01286v1</id>\n    <title>Emergent evaluation hubs in a decentralizing large language model ecosystem</title>\n    <updated>2025-09-30T23:49:26Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.01286v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.01286v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language models are proliferating, and so are the benchmarks that serve as their common yardsticks. We ask how the agglomeration patterns of these two layers compare: do they evolve in tandem or diverge? Drawing on two curated proxies for the ecosystem, the Stanford Foundation-Model Ecosystem Graph and the Evidently AI benchmark registry, we find complementary but contrasting dynamics. Model creation has broadened across countries and organizations and diversified in modality, licensing, and access. Benchmark influence, by contrast, displays centralizing patterns: in the inferred benchmark-author-institution network, the top 15% of nodes account for over 80% of high-betweenness paths, three countries produce 83% of benchmark outputs, and the global Gini for inferred benchmark authority reaches 0.89. An agent-based simulation highlights three mechanisms: higher entry of new benchmarks reduces concentration; rapid inflows can temporarily complicate coordination in evaluation; and stronger penalties against over-fitting have limited effect. Taken together, these results suggest that concentrated benchmark influence functions as coordination infrastructure that supports standardization, comparability, and reproducibility amid rising heterogeneity in model production, while also introducing trade-offs such as path dependence, selective visibility, and diminishing discriminative power as leaderboards saturate.</summary>\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-30T23:49:26Z</published>\n    <arxiv:comment>15 pages, 11 figures, 3 tables</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CY\"/>\n    <author>\n      <name>Manuel Cebrian</name>\n    </author>\n    <author>\n      <name>Tomomi Kito</name>\n    </author>\n    <author>\n      <name>Raul Castro Fernandez</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.00274v1</id>\n    <title>MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning</title>\n    <updated>2025-09-30T20:53:28Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.00274v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.00274v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-30T20:53:28Z</published>\n    <arxiv:comment>16 pages, 3 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Maisha Maliha</name>\n    </author>\n    <author>\n      <name>Dean Hougen</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.26216v1</id>\n    <title>Comparative Analysis of Ant Colony Optimization and Google OR-Tools for Solving the Open Capacitated Vehicle Routing Problem in Logistics</title>\n    <updated>2025-09-30T13:18:14Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.26216v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.26216v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>In modern logistics management systems, route planning requires high efficiency. The Open Capacitated Vehicle Routing Problem (OCVRP) deals with finding optimal delivery routes for a fleet of vehicles serving geographically distributed customers, without requiring the vehicles to return to the depot after deliveries. The present study is comparative in nature and speaks of two algorithms for OCVRP solution: Ant Colony Optimization (ACO), a nature-inspired metaheuristic; and Google OR-Tools, an industry-standard toolkit for optimization. Both implementations were developed in Python and using a custom dataset. Performance appraisal was based on routing efficiency, computation time, and scalability. The results show that ACO allows flexibility in routing parameters while OR-Tools runs much faster with more consistency and requires less input. This could help choose among routing strategies for scalable real-time logistics systems.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-30T13:18:14Z</published>\n    <arxiv:comment>6 pages, accepted at Intelligent Methods, Systems, and Applications (IMSA 2025)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Assem Omar</name>\n    </author>\n    <author>\n      <name>Youssef Omar</name>\n    </author>\n    <author>\n      <name>Marwa Solayman</name>\n    </author>\n    <author>\n      <name>Hesham Mansour</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.25873v1</id>\n    <title>Lita: Light Agent Uncovers the Agentic Coding Capabilities of LLMs</title>\n    <updated>2025-09-30T07:07:32Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.25873v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.25873v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language models (LLMs) are increasingly being applied to programming tasks, ranging from single-turn code completion to autonomous agents. Current code agent designs frequently depend on complex, hand-crafted workflows and tool sets. However, this reliance on elaborate scaffolding presents several challenges: agent performance becomes overly dependent on prompt tuning and custom design choices, heavy human intervention obscures a model's true underlying capabilities, and intricate pipelines are costly to build and maintain. Furthermore, optimizing complex task prompts increases the risk of data leakage. Currently, when introducing new models, LLM providers like OpenAI and Anthropic often publish benchmark scores to demonstrate their models' coding proficiency, but keep their proprietary evaluation frameworks confidential. To address these limitations, we introduce Lita (Lite Agent), which operationalizes liteness, a principle of minimizing manual design while retaining the essential elements of a fully autonomous agent. Lita enables a more faithful and unified evaluation without elaborate scaffolding. Experiments on the Aider Polyglot and SWE-Bench with frontier models demonstrate that Lita achieves competitive or superior performance compared to workflow-based and agentic baselines. Crucially, Lita also consumes fewer tokens and requires significantly less design effort. Our results suggest that Lita is sufficient to reveal the underlying coding competence of modern LLMs. Finally, we propose the Agent Complexity Law: the performance gap between agents of varying complexity, from simple to sophisticated designs, will shrink as the core model improves, ultimately converging to a negligible difference.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.PL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-30T07:07:32Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Hankun Dai</name>\n    </author>\n    <author>\n      <name>Maoquan Wang</name>\n    </author>\n    <author>\n      <name>Mengnan Qi</name>\n    </author>\n    <author>\n      <name>Yikai Zhang</name>\n    </author>\n    <author>\n      <name>Zijian Jin</name>\n    </author>\n    <author>\n      <name>Yongqiang Yao</name>\n    </author>\n    <author>\n      <name>Yufan Huang</name>\n    </author>\n    <author>\n      <name>Shengyu Fu</name>\n    </author>\n    <author>\n      <name>Elsie Nallipogu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.25693v2</id>\n    <title>ScheduleMe: Multi-Agent Calendar Assistant</title>\n    <updated>2025-10-01T03:03:01Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.25693v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.25693v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Recent advancements in LLMs have contributed to the rise of advanced conversational assistants that can assist with user needs through natural language conversation. This paper presents a ScheduleMe, a multi-agent calendar assistant for users to manage google calendar events in natural language. The system uses a graph-structured coordination mechanism where a central supervisory agent supervises specialized task agents, allowing modularity, conflicts resolution, and context-aware interactions to resolve ambiguities and evaluate user commands. This approach sets an example of how structured reasoning and agent cooperation might convince operators to increase the usability and flexibility of personal calendar assistant tools.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-30T02:47:54Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Oshadha Wijerathne</name>\n      <arxiv:affiliation>University of Moratuwa, Sri Lanka</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Amandi Nimasha</name>\n      <arxiv:affiliation>University of Moratuwa, Sri Lanka</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Dushan Fernando</name>\n      <arxiv:affiliation>University of Moratuwa, Sri Lanka</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Nisansa de Silva</name>\n      <arxiv:affiliation>University of Moratuwa, Sri Lanka</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Srinath Perera</name>\n      <arxiv:affiliation>WSO2 LLC</arxiv:affiliation>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.25510v1</id>\n    <title>EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit</title>\n    <updated>2025-09-29T21:08:23Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.25510v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.25510v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose EEsizer, an LLM-based AI agent that integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge. By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention. We first benchmarked 8 LLMs on six basic circuits and selected three high-performing models to optimize a 20-transistor CMOS operational amplifier, targeting multiple performance metrics, including rail-to-rail operation from 180 nm to 90 nm technology nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90 nm across three different test groups, with a maximum of 20 iterations, demonstrating adaptability and robustness at advanced nodes. To assess design robustness, we manually designed a bias circuit and performed a variation analysis using Gaussian-distributed variations on transistor dimensions and threshold voltages.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-29T21:08:23Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Chang Liu</name>\n    </author>\n    <author>\n      <name>Danial Chitnis</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.25434v2</id>\n    <title>The Open Syndrome Definition</title>\n    <updated>2025-10-22T14:07:59Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.25434v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.25434v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Case definitions are essential for effectively communicating public health threats. However, the absence of a standardized, machine-readable format poses significant challenges to interoperability, epidemiological research, the exchange of qualitative data, and the effective application of computational analysis methods, including artificial intelligence (AI). This complicates comparisons and collaborations across organizations and regions, limits data integration, and hinders technological innovation in public health. To address these issues, we propose the first open, machine-readable format for representing case and syndrome definitions. Additionally, we introduce the first comprehensive dataset of standardized case definitions and tools to convert existing human-readable definitions into machine-readable formats. We also provide an accessible online platform for browsing, analyzing, and contributing new definitions, available at https://opensyndrome.org. The Open Syndrome Definition format enables consistent, scalable use of case definitions across systems, unlocking AI's potential to strengthen public health preparedness and response. The source code for the format can be found at https://github.com/OpenSyndrome/schema under the MIT license.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-29T19:41:54Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Ana Paula Gomes Ferreira</name>\n    </author>\n    <author>\n      <name>Aleksandar Anžel</name>\n    </author>\n    <author>\n      <name>Izabel Oliva Marcilio de Souza</name>\n    </author>\n    <author>\n      <name>Helen Hughes</name>\n    </author>\n    <author>\n      <name>Alex J Elliot</name>\n    </author>\n    <author>\n      <name>Jude Dzevela Kong</name>\n    </author>\n    <author>\n      <name>Madlen Schranz</name>\n    </author>\n    <author>\n      <name>Alexander Ullrich</name>\n    </author>\n    <author>\n      <name>Georges Hattab</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.24923v1</id>\n    <title>When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training</title>\n    <updated>2025-09-29T15:25:42Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.24923v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.24923v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>While Large Language Models (LLMs) hold promise to become autonomous agents, they often explore suboptimally in sequential decision-making. Recent work has sought to enhance this capability via supervised fine-tuning (SFT) or reinforcement learning (RL), improving regret on the classic multi-armed bandit task. However, it remains unclear how these learning methods shape exploration strategies and how well they generalize. We investigate both paradigms by training LLMs with SFT on expert trajectories and RL with a range of tailored reward signals including a strategic, regret-shaped reward to reduce variance, and an algorithmic reward that enables oracle imitation. The resulting agents outperform pre-trained models and achieve performance comparable to Upper Confidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x longer horizons and across bandit families. Behavioral analysis reveals that gains often stem from more sophisticated but greedier exploitation: RL/SFT agents are more prone to early catastrophic failure than pre-trained models, prematurely abandoning exploration. Furthermore, agents trained to imitate UCB learn to outperform their teacher by adopting more exploitative variants. Our findings clarify when each training paradigm is preferable and advocate tailored reward design and evaluation beyond average regret to promote robust exploratory behavior.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-29T15:25:42Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Sanxing Chen</name>\n    </author>\n    <author>\n      <name>Xiaoyin Chen</name>\n    </author>\n    <author>\n      <name>Yukun Huang</name>\n    </author>\n    <author>\n      <name>Roy Xie</name>\n    </author>\n    <author>\n      <name>Bhuwan Dhingra</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.24804v1</id>\n    <title>DyMoDreamer: World Modeling with Dynamic Modulation</title>\n    <updated>2025-09-29T13:54:42Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.24804v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.24804v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>A critical bottleneck in deep reinforcement learning (DRL) is sample inefficiency, as training high-performance agents often demands extensive environmental interactions. Model-based reinforcement learning (MBRL) mitigates this by building world models that simulate environmental dynamics and generate synthetic experience, improving sample efficiency. However, conventional world models process observations holistically, failing to decouple dynamic objects and temporal features from static backgrounds. This approach is computationally inefficient, especially for visual tasks where dynamic objects significantly influence rewards and decision-making performance. To address this, we introduce DyMoDreamer, a novel MBRL algorithm that incorporates a dynamic modulation mechanism to improve the extraction of dynamic features and enrich the temporal information. DyMoDreamer employs differential observations derived from a novel inter-frame differencing mask, explicitly encoding object-level motion cues and temporal dynamics. Dynamic modulation is modeled as stochastic categorical distributions and integrated into a recurrent state-space model (RSSM), enhancing the model's focus on reward-relevant dynamics. Experiments demonstrate that DyMoDreamer sets a new state-of-the-art on the Atari $100$k benchmark with a $156.6$\\% mean human-normalized score, establishes a new record of $832$ on the DeepMind Visual Control Suite, and gains a $9.5$\\% performance improvement after $1$M steps on the Crafter benchmark. Our code is released at https://github.com/Ultraman-Tiga1/DyMoDreamer.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-29T13:54:42Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Boxuan Zhang</name>\n    </author>\n    <author>\n      <name>Runqing Wang</name>\n    </author>\n    <author>\n      <name>Wei Xiao</name>\n    </author>\n    <author>\n      <name>Weipu Zhang</name>\n    </author>\n    <author>\n      <name>Jian Sun</name>\n    </author>\n    <author>\n      <name>Gao Huang</name>\n    </author>\n    <author>\n      <name>Jie Chen</name>\n    </author>\n    <author>\n      <name>Gang Wang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.24405v1</id>\n    <title>Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents</title>\n    <updated>2025-09-29T07:50:39Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.24405v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.24405v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Text-to-SQL enables natural access to databases, yet most benchmarks are English-only, limiting multilingual progress. We introduce MultiSpider 2.0, extending Spider 2.0 to eight languages (English, German, French, Spanish, Portuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's structural difficulty while adding linguistic and dialectal variability, demanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art LLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when relying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we provide a collaboration-driven language agents baseline that iteratively refines queries, improving accuracy to 15\\%. These results reveal a substantial multilingual gap and motivate methods that are robust across languages and ready for real-world enterprise deployment. Our benchmark is available at https://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.DB\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.ET\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-29T07:50:39Z</published>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Khanh Trinh Pham</name>\n    </author>\n    <author>\n      <name>Thu Huong Nguyen</name>\n    </author>\n    <author>\n      <name>Jun Jo</name>\n    </author>\n    <author>\n      <name>Quoc Viet Hung Nguyen</name>\n    </author>\n    <author>\n      <name>Thanh Tam Nguyen</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.24323v1</id>\n    <title>MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems</title>\n    <updated>2025-09-29T06:20:10Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.24323v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.24323v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The past two years have witnessed the meteoric rise of Large Language Model (LLM)-powered multi-agent systems (MAS), which harness collective intelligence and exhibit a remarkable trajectory toward self-evolution. This paradigm has rapidly progressed from manually engineered systems that require bespoke configuration of prompts, tools, roles, and communication protocols toward frameworks capable of automated orchestration. Yet, dominant automatic multi-agent systems, whether generated by external modules or a single LLM agent, largely adhere to a rigid ``\\textit{generate-once-and-deploy}'' paradigm, rendering the resulting systems brittle and ill-prepared for the dynamism and uncertainty of real-world environments. To transcend this limitation, we introduce MAS$^2$, a paradigm predicated on the principle of recursive self-generation: a multi-agent system that autonomously architects bespoke multi-agent systems for diverse problems. Technically, we devise a ``\\textit{generator-implementer-rectifier}'' tri-agent team capable of dynamically composing and adaptively rectifying a target agent system in response to real-time task demands. Collaborative Tree Optimization is proposed to train and specialize these meta-agents. Extensive evaluation across seven benchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\\%$ over state-of-the-art MAS in complex scenarios such as deep research and code generation. Moreover, MAS$^2$ exhibits superior cross-backbone generalization, effectively leveraging previously unseen LLMs to yield improvements of up to $15.1\\%$. Crucially, these gains are attained without incurring excessive token costs, as MAS$^2$ consistently resides on the Pareto frontier of cost-performance trade-offs. The source codes are available at https://github.com/yeyeyeah2/MAS2.</summary>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-29T06:20:10Z</published>\n    <arxiv:primary_category term=\"cs.MA\"/>\n    <author>\n      <name>Kun Wang</name>\n    </author>\n    <author>\n      <name>Guibin Zhang</name>\n    </author>\n    <author>\n      <name>ManKit Ye</name>\n    </author>\n    <author>\n      <name>Xinyu Deng</name>\n    </author>\n    <author>\n      <name>Dongxia Wang</name>\n    </author>\n    <author>\n      <name>Xiaobin Hu</name>\n    </author>\n    <author>\n      <name>Jinyang Guo</name>\n    </author>\n    <author>\n      <name>Yang Liu</name>\n    </author>\n    <author>\n      <name>Yufei Guo</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.24196v1</id>\n    <title>Chat to Chip: Large Language Model Based Design of Arbitrarily Shaped Metasurfaces</title>\n    <updated>2025-09-29T02:24:57Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.24196v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.24196v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Traditional metasurface design is limited by the computational cost of full-wave simulations, preventing thorough exploration of complex configurations. Data-driven approaches have emerged as a solution to this bottleneck, replacing costly simulations with rapid neural network evaluations and enabling near-instant design for meta-atoms. Despite advances, implementing a new optical function still requires building and training a task-specific network, along with exhaustive searches for suitable architectures and hyperparameters. Pre-trained large language models (LLMs), by contrast, sidestep this laborious process with a simple fine-tuning technique. However, applying LLMs to the design of nanophotonic devices, particularly for arbitrarily shaped metasurfaces, is still in its early stages; as such tasks often require graphical networks. Here, we show that an LLM, fed with descriptive inputs of arbitrarily shaped metasurface geometries, can learn the physical relationships needed for spectral prediction and inverse design. We further benchmarked a range of open-weight LLMs and identified relationships between accuracy and model size at the billion-parameter level. We demonstrated that 1-D token-wise LLMs provide a practical tool to designing 2-D arbitrarily shaped metasurfaces. Linking natural-language interaction to electromagnetic modelling, this \"chat-to-chip\" workflow represents a step toward more user-friendly data-driven nanophotonics.</summary>\n    <category term=\"physics.optics\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-29T02:24:57Z</published>\n    <arxiv:primary_category term=\"physics.optics\"/>\n    <author>\n      <name>Huanshu Zhang</name>\n    </author>\n    <author>\n      <name>Lei Kang</name>\n    </author>\n    <author>\n      <name>Sawyer D. Campbell</name>\n    </author>\n    <author>\n      <name>Douglas H. Werner</name>\n    </author>\n    <arxiv:doi>10.1515/nanoph-2025-0343</arxiv:doi>\n    <link rel=\"related\" href=\"https://doi.org/10.1515/nanoph-2025-0343\" title=\"doi\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.23882v2</id>\n    <title>Quant Fever, Reasoning Blackholes, Schrodinger's Compliance, and More: Probing GPT-OSS-20B</title>\n    <updated>2025-10-05T14:53:45Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.23882v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.23882v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>OpenAI's GPT-OSS family provides open-weight language models with explicit chain-of-thought (CoT) reasoning and a Harmony prompt format. We summarize an extensive security evaluation of GPT-OSS-20B that probes the model's behavior under different adversarial conditions. Using the Jailbreak Oracle (JO) [1], a systematic LLM evaluation tool, the study uncovers several failure modes including quant fever, reasoning blackholes, Schrodinger's compliance, reasoning procedure mirage, and chain-oriented prompting. Experiments demonstrate how these behaviors can be exploited on the GPT-OSS-20B model, leading to severe consequences.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-09-28T13:44:37Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Shuyi Lin</name>\n    </author>\n    <author>\n      <name>Tian Lu</name>\n    </author>\n    <author>\n      <name>Zikai Wang</name>\n    </author>\n    <author>\n      <name>Bo Wen</name>\n    </author>\n    <author>\n      <name>Yibo Zhao</name>\n    </author>\n    <author>\n      <name>Cheng Tan</name>\n    </author>\n  </entry>\n</feed>\n"
}