{
  "source": "arxiv",
  "fetched_at": "2025-11-22T12:42:24.250042+00:00",
  "payload": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/F1iEZVchWjWo5h+07crQdIe3iLg</id>\n  <title>arXiv Query: search_query=(ti:agent OR ti:\"tool use\" OR ti:tool-augmented OR ti:planning OR ti:multi-agent OR ti:autonomous OR ti:toolformer OR ti:\"tool learning\" OR abs:agent OR abs:\"tool use\" OR abs:tool-augmented OR abs:planning OR abs:multi-agent OR abs:autonomous OR abs:toolformer OR abs:\"tool learning\") AND (cat:cs.AI OR cat:cs.LG OR cat:cs.MA) AND (ti:Google OR ti:DeepMind OR ti:Microsoft OR ti:OpenAI OR ti:Meta OR ti:Apple OR ti:Stanford OR ti:MIT OR ti:CMU OR ti:Berkeley OR ti:Oxford OR ti:Harvard OR ti:Tsinghua OR ti:\"Peking University\" OR ti:PKU OR ti:USTC OR ti:SJTU OR ti:Princeton OR ti:UCLA OR ti:UCSD OR ti:\"ETH Zurich\" OR ti:NUS OR ti:NTU OR abs:Google OR abs:DeepMind OR abs:Microsoft OR abs:OpenAI OR abs:Meta OR abs:Apple OR abs:Stanford OR abs:MIT OR abs:CMU OR abs:Berkeley OR abs:Oxford OR abs:Harvard OR abs:Tsinghua OR abs:\"Peking University\" OR abs:PKU OR abs:USTC OR abs:SJTU OR abs:Princeton OR abs:UCLA OR abs:UCSD OR abs:\"ETH Zurich\" OR abs:NUS OR abs:NTU)&amp;id_list=&amp;start=50&amp;max_results=50</title>\n  <updated>2025-11-22T12:42:23Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=(ti:agent+OR+(ti:%22tool+use%22+OR+(ti:tool-augmented+OR+(ti:planning+OR+(ti:multi-agent+OR+(ti:autonomous+OR+(ti:toolformer+OR+(ti:%22tool+learning%22+OR+(abs:agent+OR+(abs:%22tool+use%22+OR+(abs:tool-augmented+OR+(abs:planning+OR+(abs:multi-agent+OR+(abs:autonomous+OR+(abs:toolformer+OR+abs:%22tool+learning%22)))))))))))))))+AND+((cat:cs.AI+OR+(cat:cs.LG+OR+cat:cs.MA))+AND+(ti:Google+OR+(ti:DeepMind+OR+(ti:Microsoft+OR+(ti:OpenAI+OR+(ti:Meta+OR+(ti:Apple+OR+(ti:Stanford+OR+(ti:MIT+OR+(ti:CMU+OR+(ti:Berkeley+OR+(ti:Oxford+OR+(ti:Harvard+OR+(ti:Tsinghua+OR+(ti:%22Peking+University%22+OR+(ti:PKU+OR+(ti:USTC+OR+(ti:SJTU+OR+(ti:Princeton+OR+(ti:UCLA+OR+(ti:UCSD+OR+(ti:%22ETH+Zurich%22+OR+(ti:NUS+OR+(ti:NTU+OR+(abs:Google+OR+(abs:DeepMind+OR+(abs:Microsoft+OR+(abs:OpenAI+OR+(abs:Meta+OR+(abs:Apple+OR+(abs:Stanford+OR+(abs:MIT+OR+(abs:CMU+OR+(abs:Berkeley+OR+(abs:Oxford+OR+(abs:Harvard+OR+(abs:Tsinghua+OR+(abs:%22Peking+University%22+OR+(abs:PKU+OR+(abs:USTC+OR+(abs:SJTU+OR+(abs:Princeton+OR+(abs:UCLA+OR+(abs:UCSD+OR+(abs:%22ETH+Zurich%22+OR+(abs:NUS+OR+abs:NTU))))))))))))))))))))))))))))))))))))))))))))))&amp;start=50&amp;max_results=50&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>\n  <opensearch:totalResults>2708</opensearch:totalResults>\n  <opensearch:startIndex>50</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2510.24986v1</id>\n    <title>Epileptic Seizure Detection and Prediction from EEG Data: A Machine Learning Approach with Clinical Validation</title>\n    <updated>2025-10-28T21:28:18Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.24986v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.24986v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>In recent years, machine learning has become an increasingly powerful tool for supporting seizure detection and monitoring in epilepsy care. Traditional approaches focus on identifying seizures only after they begin, which limits the opportunity for early intervention and proactive treatment. In this study, we propose a novel approach that integrates both real-time seizure detection and prediction, aiming to capture subtle temporal patterns in EEG data that may indicate an upcoming seizure. Our approach was evaluated using the CHB-MIT Scalp EEG Database, which includes 969 hours of recordings and 173 seizures collected from 23 pediatric and young adult patients with drug-resistant epilepsy. To support seizure detection, we implemented a range of supervised machine learning algorithms, including K-Nearest Neighbors, Logistic Regression, Random Forest, and Support Vector Machine. The Logistic Regression achieved 90.9% detection accuracy with 89.6% recall, demonstrating balanced performance suitable for clinical screening. Random Forest and Support Vector Machine models achieved higher accuracy (94.0%) but with 0% recall, failing to detect any seizures, illustrating that accuracy alone is insufficient for evaluating medical ML models with class imbalance. For seizure prediction, we employed Long Short-Term Memory (LSTM) networks, which use deep learning to model temporal dependencies in EEG data. The LSTM model achieved 89.26% prediction accuracy. These results highlight the potential of developing accessible, real-time monitoring tools that not only detect seizures as traditionally done, but also predict them before they occur. This ability to predict seizures marks a significant shift from reactive seizure management to a more proactive approach, allowing patients to anticipate seizures and take precautionary measures to reduce the risk of injury or other complications.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-28T21:28:18Z</published>\n    <arxiv:comment>9 pages, 3 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Ria Jayanti</name>\n    </author>\n    <author>\n      <name>Tanish Jain</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.24699v1</id>\n    <title>AgentFold: Long-Horizon Web Agents with Proactive Context Management</title>\n    <updated>2025-10-28T17:51:50Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.24699v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.24699v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-28T17:51:50Z</published>\n    <arxiv:comment>26 pages, 9 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Rui Ye</name>\n    </author>\n    <author>\n      <name>Zhongwang Zhang</name>\n    </author>\n    <author>\n      <name>Kuan Li</name>\n    </author>\n    <author>\n      <name>Huifeng Yin</name>\n    </author>\n    <author>\n      <name>Zhengwei Tao</name>\n    </author>\n    <author>\n      <name>Yida Zhao</name>\n    </author>\n    <author>\n      <name>Liangcai Su</name>\n    </author>\n    <author>\n      <name>Liwen Zhang</name>\n    </author>\n    <author>\n      <name>Zile Qiao</name>\n    </author>\n    <author>\n      <name>Xinyu Wang</name>\n    </author>\n    <author>\n      <name>Pengjun Xie</name>\n    </author>\n    <author>\n      <name>Fei Huang</name>\n    </author>\n    <author>\n      <name>Siheng Chen</name>\n    </author>\n    <author>\n      <name>Jingren Zhou</name>\n    </author>\n    <author>\n      <name>Yong Jiang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.24645v2</id>\n    <title>FunReason-MT Technical Report: Advanced Data Synthesis Solution for Real-world Multi-Turn Tool-use</title>\n    <updated>2025-11-16T17:39:19Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.24645v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.24645v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Function calling (FC) empowers large language models (LLMs) and autonomous agents to interface with external tools, a critical capability for solving complex, real-world problems. As this ability becomes increasingly central to advanced AI systems, the need for high-quality, multi-turn training data to develop and refine it cannot be overstated. Existing data synthesis methods, such as random environment sampling or multi-agent role-playing, are not powerful enough to generate high-quality data in real-world environments. Practical challenges come in three folds: targeted data synthesis, hard query construction, and multi-turn logical dependency. To address these structural deficiencies, we present FunReason-MT, a novel data synthesis framework for real-world multi-turn tool use. FunReason-MT resolves the complexity barrier in multi-turn FC data by employing 1) Environment-API Graph Interactions to gather varied high-quality trajectories with targeted tool, 2) Advanced Tool-Query Synthesis to simplify hard query construction, and 3) Guided Iterative Chain for sophisticated CoT generation. Evaluations on Berkeley Function-Calling Leaderboard (BFCLv3) demonstrate the power of our framework: a 4B model built upon FunReason-MT generated data achieves state-of-the-art performance among comparable-sized models. Further performance improvements on BFCLv4 confirm that FunReason-MT provides a reliable and robust source for agentic learning.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-28T17:15:26Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Zengzhuang Xu</name>\n    </author>\n    <author>\n      <name>Bingguang Hao</name>\n    </author>\n    <author>\n      <name>Zechuan Wang</name>\n    </author>\n    <author>\n      <name>Yuntao Wen</name>\n    </author>\n    <author>\n      <name>Xinyi Xu</name>\n    </author>\n    <author>\n      <name>Yang Liu</name>\n    </author>\n    <author>\n      <name>Long Chen</name>\n    </author>\n    <author>\n      <name>Dong Wang</name>\n    </author>\n    <author>\n      <name>Maolin Wang</name>\n    </author>\n    <author>\n      <name>Tong Zhao</name>\n    </author>\n    <author>\n      <name>Yicheng Chen</name>\n    </author>\n    <author>\n      <name>Cunyin Peng</name>\n    </author>\n    <author>\n      <name>Jinjie Gu</name>\n    </author>\n    <author>\n      <name>Leilei Gan</name>\n    </author>\n    <author>\n      <name>Xiangyu Zhao</name>\n    </author>\n    <author>\n      <name>Chenyi Zhuang</name>\n    </author>\n    <author>\n      <name>Shi Gu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.23870v1</id>\n    <title>OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning</title>\n    <updated>2025-10-27T21:22:41Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.23870v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.23870v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge 2025, a bilingual benchmark requiring complex reasoning such as arithmetic, commonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding the second-best system by more than 6% in execution accuracy (EX), with 55.0% in English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA). Our system follows an agentic framework with two components: Planner agent that generates stepwise natural language plans, and SQL agent that converts these plans into executable SQL. Since SQL agent reliably adheres to the plan, our refinements focus on the planner. Unlike prior methods that rely on multiple sub-agents for planning and suffer from orchestration overhead, we introduce a feedback-guided meta-prompting strategy to refine a single planner. Failure cases from a held-out set are clustered with human input, and an LLM distills them into corrective guidelines that are integrated into the planner's system prompt, improving generalization without added complexity. For the multilingual scenario, to address transliteration and entity mismatch issues, we incorporate entity-linking guidelines that generate alternative surface forms for entities and explicitly include them in the plan. Finally, we enhance reliability through plan diversification: multiple candidate plans are generated for each query, with the SQL agent producing a query for each plan, and final output selected via majority voting over their executions.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-27T21:22:41Z</published>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Marianne Menglin Liu</name>\n    </author>\n    <author>\n      <name>Sai Ashish Somayajula</name>\n    </author>\n    <author>\n      <name>Syed Fahad Allam Shah</name>\n    </author>\n    <author>\n      <name>Sujith Ravi</name>\n    </author>\n    <author>\n      <name>Dan Roth</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.22977v1</id>\n    <title>The Reasoning Trap: How Enhancing LLM Reasoning Amplifies Tool Hallucination</title>\n    <updated>2025-10-27T03:58:29Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.22977v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.22977v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Enhancing the reasoning capabilities of Large Language Models (LLMs) is a key strategy for building Agents that \"think then act.\" However, recent observations, like OpenAI's o3, suggest a paradox: stronger reasoning often coincides with increased hallucination, yet no prior work has systematically examined whether reasoning enhancement itself causes tool hallucination. To address this gap, we pose the central question: Does strengthening reasoning increase tool hallucination? To answer this, we introduce SimpleToolHalluBench, a diagnostic benchmark measuring tool hallucination in two failure modes: (i) no tool available, and (ii) only distractor tools available. Through controlled experiments, we establish three key findings. First, we demonstrate a causal relationship: progressively enhancing reasoning through RL increases tool hallucination proportionally with task performance gains. Second, this effect transcends overfitting - training on non-tool tasks (e.g., mathematics) still amplifies subsequent tool hallucination. Third, the effect is method-agnostic, appearing when reasoning is instilled via supervised fine-tuning and when it is merely elicited at inference by switching from direct answers to step-by-step thinking. We also evaluate mitigation strategies including Prompt Engineering and Direct Preference Optimization (DPO), revealing a fundamental reliability-capability trade-off: reducing hallucination consistently degrades utility. Mechanistically, Reasoning RL disproportionately collapses tool-reliability-related representations, and hallucinations surface as amplified divergences concentrated in late-layer residual streams. These findings reveal that current reasoning enhancement methods inherently amplify tool hallucination, highlighting the need for new training objectives that jointly optimize for capability and reliability.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-27T03:58:29Z</published>\n    <arxiv:comment>18 pages, 5 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Chenlong Yin</name>\n    </author>\n    <author>\n      <name>Zeyang Sha</name>\n    </author>\n    <author>\n      <name>Shiwen Cui</name>\n    </author>\n    <author>\n      <name>Changhua Meng</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.22940v4</id>\n    <title>Generating Auxiliary Tasks with Reinforcement Learning</title>\n    <updated>2025-11-03T23:55:55Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.22940v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.22940v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Auxiliary Learning (AL) is a form of multi-task learning in which a model trains on auxiliary tasks to boost performance on a primary objective. While AL has improved generalization across domains such as navigation, image classification, and NLP, it often depends on human-labeled auxiliary tasks that are costly to design and require domain expertise. Meta-learning approaches mitigate this by learning to generate auxiliary tasks, but typically rely on gradient based bi-level optimization, adding substantial computational and implementation overhead. We propose RL-AUX, a reinforcement-learning (RL) framework that dynamically creates auxiliary tasks by assigning auxiliary labels to each training example, rewarding the agent whenever its selections improve the performance on the primary task. We also explore learning per-example weights for the auxiliary loss. On CIFAR-100 grouped into 20 superclasses, our RL method outperforms human-labeled auxiliary tasks and matches the performance of a prominent bi-level optimization baseline. We present similarly strong results on other classification datasets. These results suggest RL is a viable path to generating effective auxiliary tasks.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-27T02:51:51Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Judah Goldfeder</name>\n    </author>\n    <author>\n      <name>Matthew So</name>\n    </author>\n    <author>\n      <name>Hod Lipson</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.22781v2</id>\n    <title>Agentic Meta-Orchestrator for Multi-task Copilots</title>\n    <updated>2025-11-05T05:01:21Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.22781v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.22781v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Microsoft Copilot suites serve as the universal entry point for various agents skilled in handling important tasks, ranging from assisting a customer with product purchases to detecting vulnerabilities in corporate programming code. Each agent can be powered by language models, software engineering operations, such as database retrieval, and internal \\&amp; external knowledge. The repertoire of a copilot can expand dynamically with new agents. This requires a robust orchestrator that can distribute tasks from user prompts to the right agents. In this work, we propose an Agentic Meta-orchestrator (AMO) for handling multiple tasks and scalable agents in copilot services, which can provide both natural language and action responses. We will also demonstrate the planning that leverages meta-learning, i.e., a trained decision tree model for deciding the best inference strategy among various agents/models. We showcase the effectiveness of our AMO through two production use cases: Microsoft 365 (M365) E-Commerce Copilot and code compliance copilot. M365 E-Commerce Copilot advertises Microsoft products to external customers to promote sales success. The M365 E-Commerce Copilot provides up-to-date product information and connects to multiple agents, such as relational databases and human customer support. The code compliance copilot scans the internal DevOps code to detect known and new compliance issues in pull requests (PR).</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-26T18:13:04Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <arxiv:journal_ref>ICDM RARA workshop 2025</arxiv:journal_ref>\n    <author>\n      <name>Xiaofeng Zhu</name>\n    </author>\n    <author>\n      <name>Yunshen Zhou</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.22654v1</id>\n    <title>UCB-type Algorithm for Budget-Constrained Expert Learning</title>\n    <updated>2025-10-26T12:36:17Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.22654v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.22654v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>In many modern applications, a system must dynamically choose between several adaptive learning algorithms that are trained online. Examples include model selection in streaming environments, switching between trading strategies in finance, and orchestrating multiple contextual bandit or reinforcement learning agents. At each round, a learner must select one predictor among $K$ adaptive experts to make a prediction, while being able to update at most $M \\le K$ of them under a fixed training budget.\n  We address this problem in the \\emph{stochastic setting} and introduce \\algname{M-LCB}, a computationally efficient UCB-style meta-algorithm that provides \\emph{anytime regret guarantees}. Its confidence intervals are built directly from realized losses, require no additional optimization, and seamlessly reflect the convergence properties of the underlying experts. If each expert achieves internal regret $\\tilde O(T^α)$, then \\algname{M-LCB} ensures overall regret bounded by $\\tilde O\\!\\Bigl(\\sqrt{\\tfrac{KT}{M}} \\;+\\; (K/M)^{1-α}\\,T^α\\Bigr)$.\n  To our knowledge, this is the first result establishing regret guarantees when multiple adaptive experts are trained simultaneously under per-round budget constraints. We illustrate the framework with two representative cases: (i) parametric models trained online with stochastic losses, and (ii) experts that are themselves multi-armed bandit algorithms. These examples highlight how \\algname{M-LCB} extends the classical bandit paradigm to the more realistic scenario of coordinating stateful, self-learning experts under limited resources.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-26T12:36:17Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Ilgam Latypov</name>\n    </author>\n    <author>\n      <name>Alexandra Suvorikova</name>\n    </author>\n    <author>\n      <name>Alexey Kroshnin</name>\n    </author>\n    <author>\n      <name>Alexander Gasnikov</name>\n    </author>\n    <author>\n      <name>Yuriy Dorn</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.22048v1</id>\n    <title>PF$Δ$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations</title>\n    <updated>2025-10-24T22:09:09Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.22048v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.22048v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Power flow (PF) calculations are the backbone of real-time grid operations, across workflows such as contingency analysis (where repeated PF evaluations assess grid security under outages) and topology optimization (which involves PF-based searches over combinatorially large action spaces). Running these calculations at operational timescales or across large evaluation spaces remains a major computational bottleneck. Additionally, growing uncertainty in power system operations from the integration of renewables and climate-induced extreme weather also calls for tools that can accurately and efficiently simulate a wide range of scenarios and operating conditions. Machine learning methods offer a potential speedup over traditional solvers, but their performance has not been systematically assessed on benchmarks that capture real-world variability. This paper introduces PF$Δ$, a benchmark dataset for power flow that captures diverse variations in load, generation, and topology. PF$Δ$ contains 859,800 solved power flow instances spanning six different bus system sizes, capturing three types of contingency scenarios (N , N -1, and N -2), and including close-to-infeasible cases near steady-state voltage stability limits. We evaluate traditional solvers and GNN-based methods, highlighting key areas where existing approaches struggle, and identifying open problems for future research. Our dataset is available at https://huggingface.co/datasets/pfdelta/pfdelta/tree/main and our code with data generation scripts and model implementations is at https://github.com/MOSSLab-MIT/pfdelta.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-24T22:09:09Z</published>\n    <arxiv:comment>31 pages, 14 figures. Accepted at NeurIPS 2025</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <arxiv:journal_ref>NeurIPS 2025</arxiv:journal_ref>\n    <author>\n      <name>Ana K. Rivera</name>\n    </author>\n    <author>\n      <name>Anvita Bhagavathula</name>\n    </author>\n    <author>\n      <name>Alvaro Carbonero</name>\n    </author>\n    <author>\n      <name>Priya Donti</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.22039v1</id>\n    <title>Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability</title>\n    <updated>2025-10-24T21:45:56Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.22039v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.22039v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Learning a compact representation of history is critical for planning and generalization in partially observable environments. While meta-reinforcement learning (RL) agents can attain near Bayes-optimal policies, they often fail to learn the compact, interpretable Bayes-optimal belief states. This representational inefficiency potentially limits the agent's adaptability and generalization capacity. Inspired by predictive coding in neuroscience--which suggests that the brain predicts sensory inputs as a neural implementation of Bayesian inference--and by auxiliary predictive objectives in deep RL, we investigate whether integrating self-supervised predictive coding modules into meta-RL can facilitate learning of Bayes-optimal representations. Through state machine simulation, we show that meta-RL with predictive modules consistently generates more interpretable representations that better approximate Bayes-optimal belief states compared to conventional meta-RL across a wide variety of tasks, even when both achieve optimal policies. In challenging tasks requiring active information seeking, only meta-RL with predictive modules successfully learns optimal representations and policies, whereas conventional meta-RL struggles with inadequate representation learning. Finally, we demonstrate that better representation learning leads to improved generalization. Our results strongly suggest the role of predictive learning as a guiding principle for effective representation learning in agents navigating partial observability.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"q-bio.NC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-24T21:45:56Z</published>\n    <arxiv:comment>Accepted to Annual Conference on Neural Information Processing Systems (NeurIPS) 2025</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Po-Chen Kuo</name>\n    </author>\n    <author>\n      <name>Han Hou</name>\n    </author>\n    <author>\n      <name>Will Dabney</name>\n    </author>\n    <author>\n      <name>Edgar Y. Walker</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.21557v1</id>\n    <title>Co-Sight: Enhancing LLM-Based Agents via Conflict-Aware Meta-Verification and Trustworthy Reasoning with Structured Facts</title>\n    <updated>2025-10-24T15:14:14Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.21557v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.21557v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Long-horizon reasoning in LLM-based agents often fails not from generative weakness but from insufficient verification of intermediate reasoning. Co-Sight addresses this challenge by turning reasoning into a falsifiable and auditable process through two complementary mechanisms: Conflict-Aware Meta-Verification (CAMV) and Trustworthy Reasoning with Structured Facts (TRSF). CAMV reformulates verification as conflict identification and targeted falsification, allocating computation only to disagreement hotspots among expert agents rather than to full reasoning chains. This bounds verification cost to the number of inconsistencies and improves efficiency and reliability. TRSF continuously organizes, validates, and synchronizes evidence across agents through a structured facts module. By maintaining verified, traceable, and auditable knowledge, it ensures that all reasoning is grounded in consistent, source-verified information and supports transparent verification throughout the reasoning process. Together, TRSF and CAMV form a closed verification loop, where TRSF supplies structured facts and CAMV selectively falsifies or reinforces them, yielding transparent and trustworthy reasoning. Empirically, Co-Sight achieves state-of-the-art accuracy on GAIA (84.4%) and Humanity's Last Exam (35.5%), and strong results on Chinese-SimpleQA (93.8%). Ablation studies confirm that the synergy between structured factual grounding and conflict-aware verification drives these improvements. Co-Sight thus offers a scalable paradigm for reliable long-horizon reasoning in LLM-based agents. Code is available at https://github.com/ZTE-AICloud/Co-Sight/tree/cosight2.0_benchmarks.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-24T15:14:14Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Hongwei Zhang</name>\n    </author>\n    <author>\n      <name>Ji Lu</name>\n    </author>\n    <author>\n      <name>Shiqing Jiang</name>\n    </author>\n    <author>\n      <name>Chenxiang Zhu</name>\n    </author>\n    <author>\n      <name>Li Xie</name>\n    </author>\n    <author>\n      <name>Chen Zhong</name>\n    </author>\n    <author>\n      <name>Haoran Chen</name>\n    </author>\n    <author>\n      <name>Yurui Zhu</name>\n    </author>\n    <author>\n      <name>Yongsheng Du</name>\n    </author>\n    <author>\n      <name>Yanqin Gao</name>\n    </author>\n    <author>\n      <name>Lingjun Huang</name>\n    </author>\n    <author>\n      <name>Baoli Wang</name>\n    </author>\n    <author>\n      <name>Fang Tan</name>\n    </author>\n    <author>\n      <name>Peng Zou</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.21427v1</id>\n    <title>Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems</title>\n    <updated>2025-10-24T13:06:43Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.21427v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.21427v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large-scale networked systems, such as traffic, power, and wireless grids, challenge reinforcement-learning agents with both scale and environment shifts. To address these challenges, we propose GSAC (Generalizable and Scalable Actor-Critic), a framework that couples causal representation learning with meta actor-critic learning to achieve both scalability and domain generalization. Each agent first learns a sparse local causal mask that provably identifies the minimal neighborhood variables influencing its dynamics, yielding exponentially tight approximately compact representations (ACRs) of state and domain factors. These ACRs bound the error of truncating value functions to $κ$-hop neighborhoods, enabling efficient learning on graphs. A meta actor-critic then trains a shared policy across multiple source domains while conditioning on the compact domain factors; at test time, a few trajectories suffice to estimate the new domain factor and deploy the adapted policy. We establish finite-sample guarantees on causal recovery, actor-critic convergence, and adaptation gap, and show that GSAC adapts rapidly and significantly outperforms learning-from-scratch and conventional adaptation baselines.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-24T13:06:43Z</published>\n    <arxiv:comment>NeurIPS 2025 (Spotlight)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Hao Liang</name>\n    </author>\n    <author>\n      <name>Shuqing Shi</name>\n    </author>\n    <author>\n      <name>Yudi Zhang</name>\n    </author>\n    <author>\n      <name>Biwei Huang</name>\n    </author>\n    <author>\n      <name>Yali Du</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.01884v2</id>\n    <title>CudaForge: An Agent Framework with Hardware Feedback for CUDA Kernel Optimization</title>\n    <updated>2025-11-05T02:10:35Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.01884v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.01884v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Developing efficient CUDA kernels is increasingly critical for AI applications such as large-scale LLM training. However, manual kernel design is both costly and time-consuming, motivating automatic approaches that leverage LLMs for code generation. Existing methods for automatic kernel generation, however, often produce low-efficiency kernels, incur high computational overhead, and fail to generalize across settings. In this work, we propose CudaForge, a training-free multi-agent workflow for CUDA kernel generation and optimization. Our workflow is inspired by the iterative workflow of human experts, which contains steps such as developing initial kernels, testing correctness, analyzing hardware feedback, and iterative improvement. More specifically, CudaForge employs two LLM agents: a Coder and a Judge, that iteratively generate, correct, and optimize CUDA kernels, while integrating hardware feedback such as Nsight Compute (NCU) metrics. In extensive evaluations, we show that CudaForge, by leveraging base models like OpenAI-o3, achieves 97.6\\% correctness of generated kernels and an average 1.68$\\times$ speedup over PyTorch baselines, substantially surpassing state-of-the-art models including OpenAI-o3 and Kevin on KernelBench.Beyond accuracy and speed, CudaForge demonstrates strong generalization across GPUs (A100, RTX 6000, 4090, 3090) and base models (OpenAI-o3, GPT-5, gpt-oss-120B, Claude-Sonnet-4, QwQ-32B), while maintaining high efficiency. In particular, generating an optimized kernel takes about 26.5 minutes on one RTX6000 and incurs about \\$ 0.3 API cost, which is significantly cheaper than existing agentic work that costs 6 H100 hours and \\$ 5 API cost per kernel. Our results highlight that multi-agent, training-free workflows can enable cost-effective, generalizable, and high-performance CUDA kernel optimization. Code available at https://github.com/OptimAI-Lab/CudaForge</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-23T22:52:00Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Zijian Zhang</name>\n    </author>\n    <author>\n      <name>Rong Wang</name>\n    </author>\n    <author>\n      <name>Shiyang Li</name>\n    </author>\n    <author>\n      <name>Yuebo Luo</name>\n    </author>\n    <author>\n      <name>Mingyi Hong</name>\n    </author>\n    <author>\n      <name>Caiwen Ding</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.20579v1</id>\n    <title>Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</title>\n    <updated>2025-10-23T14:05:56Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.20579v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.20579v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Most video reasoning models only generate textual reasoning traces without indicating when and where key evidence appears. Recent models such as OpenAI-o3 have sparked wide interest in evidence-centered reasoning for images, yet extending this ability to videos is more challenging, as it requires joint temporal tracking and spatial localization across dynamic scenes. We introduce Open-o3 Video, a non-agent framework that integrates explicit spatio-temporal evidence into video reasoning, and carefully collect training data and design training strategies to address the aforementioned challenges. The model highlights key timestamps, objects, and bounding boxes alongside its answers, allowing reasoning to be grounded in concrete visual observations. To enable this functionality, we first curate and build two high-quality datasets, STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructed temporal and spatial annotations, since most existing datasets offer either temporal spans for videos or spatial boxes on images, lacking unified spatio-temporal supervision and reasoning traces. Then, we adopt a cold-start reinforcement learning strategy with multiple specially designed rewards that jointly encourage answer accuracy, temporal alignment, and spatial precision. On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance, raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistent improvements are also observed on a broad range of video understanding benchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyond accuracy, the reasoning traces produced by Open-o3 Video also provide valuable signals for test-time scaling, enabling confidence-aware verification and improving answer reliability.</summary>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MM\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-23T14:05:56Z</published>\n    <arxiv:primary_category term=\"cs.CV\"/>\n    <author>\n      <name>Jiahao Meng</name>\n    </author>\n    <author>\n      <name>Xiangtai Li</name>\n    </author>\n    <author>\n      <name>Haochen Wang</name>\n    </author>\n    <author>\n      <name>Yue Tan</name>\n    </author>\n    <author>\n      <name>Tao Zhang</name>\n    </author>\n    <author>\n      <name>Lingdong Kong</name>\n    </author>\n    <author>\n      <name>Yunhai Tong</name>\n    </author>\n    <author>\n      <name>Anran Wang</name>\n    </author>\n    <author>\n      <name>Zhiyang Teng</name>\n    </author>\n    <author>\n      <name>Yujing Wang</name>\n    </author>\n    <author>\n      <name>Zhuochen Wang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.20342v1</id>\n    <title>Teaching Language Models to Reason with Tools</title>\n    <updated>2025-10-23T08:41:44Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.20342v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.20342v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large reasoning models (LRMs) like OpenAI-o1 have shown impressive capabilities in natural language reasoning. However, these models frequently demonstrate inefficiencies or inaccuracies when tackling complex mathematical operations. While integrating computational tools such as Code Interpreters (CIs) offers a promising solution, it introduces a critical challenge: a conflict between the model's internal, probabilistic reasoning and the external, deterministic knowledge provided by the CI, which often leads models to unproductive deliberation. To overcome this, we introduce CoRT (Code-Optimized Reasoning Training), a post-training framework designed to teach LRMs to effectively utilize CIs. We propose \\emph{Hint-Engineering}, a new data synthesis strategy that strategically injects diverse hints at optimal points within reasoning paths. This approach generates high-quality, code-integrated reasoning data specifically tailored to optimize LRM-CI interaction. Using this method, we have synthesized 30 high-quality samples to post-train models ranging from 1.5B to 32B parameters through supervised fine-tuning. CoRT further refines the multi-round interleaving of external CI usage and internal thinking by employing rejection sampling and reinforcement learning. Our experimental evaluations demonstrate CoRT's effectiveness, yielding absolute improvements of 4\\% and 8\\% on DeepSeek-R1-Distill-Qwen-32B and DeepSeek-R1-Distill-Qwen-1.5B, respectively, across five challenging mathematical reasoning datasets. Moreover, CoRT significantly enhances efficiency, reducing token usage by approximately 30\\% for the 32B model and 50\\% for the 1.5B model compared to pure natural language reasoning baselines. The models and code are available at: https://github.com/ChengpengLi1003/CoRT.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-23T08:41:44Z</published>\n    <arxiv:comment>NIPS2025 Accepted</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Chengpeng Li</name>\n    </author>\n    <author>\n      <name>Zhengyang Tang</name>\n    </author>\n    <author>\n      <name>Ziniu Li</name>\n    </author>\n    <author>\n      <name>Mingfeng Xue</name>\n    </author>\n    <author>\n      <name>Keqin Bao</name>\n    </author>\n    <author>\n      <name>Tian Ding</name>\n    </author>\n    <author>\n      <name>Ruoyu Sun</name>\n    </author>\n    <author>\n      <name>Benyou Wang</name>\n    </author>\n    <author>\n      <name>Xiang Wang</name>\n    </author>\n    <author>\n      <name>Junyang Lin</name>\n    </author>\n    <author>\n      <name>Dayiheng Liu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.20205v1</id>\n    <title>Merge and Conquer: Evolutionarily Optimizing AI for 2048</title>\n    <updated>2025-10-23T04:45:05Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.20205v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.20205v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Optimizing artificial intelligence (AI) for dynamic environments remains a fundamental challenge in machine learning research. In this paper, we examine evolutionary training methods for optimizing AI to solve the game 2048, a 2D sliding puzzle. 2048, with its mix of strategic gameplay and stochastic elements, presents an ideal playground for studying decision-making, long-term planning, and dynamic adaptation. We implemented two distinct systems: a two-agent metaprompting system where a \"thinker\" large language model (LLM) agent refines gameplay strategies for an \"executor\" LLM agent, and a single-agent system based on refining a value function for a limited Monte Carlo Tree Search. We also experimented with rollback features to avoid performance degradation. Our results demonstrate the potential of evolutionary refinement techniques in improving AI performance in non-deterministic environments. The single-agent system achieved substantial improvements, with an average increase of 473.2 points per cycle, and with clear upward trends (correlation $ρ$=0.607) across training cycles. The LLM's understanding of the game grew as well, shown in its development of increasingly advanced strategies. Conversely, the two-agent system did not garner much improvement, highlighting the inherent limits of meta-prompting.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-23T04:45:05Z</published>\n    <arxiv:comment>9 pages, 5 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Maggie Bai</name>\n    </author>\n    <author>\n      <name>Ava Kim Cohen</name>\n    </author>\n    <author>\n      <name>Eleanor Koss</name>\n    </author>\n    <author>\n      <name>Charlie Lichtenbaum</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.20176v2</id>\n    <title>Mixture-of-Minds: Multi-Agent Reinforcement Learning for Table Understanding</title>\n    <updated>2025-10-24T15:36:31Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.20176v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.20176v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Understanding and reasoning over tables is a critical capability for many real-world applications. Large language models (LLMs) have shown promise on this task, but current approaches remain limited. Fine-tuning based methods strengthen language reasoning; yet they are prone to arithmetic errors and hallucination. In contrast, tool-based methods enable precise table manipulation but rely on rigid schemas and lack semantic understanding. These complementary drawbacks highlight the need for approaches that integrate robust reasoning with reliable table processing. In this work, we propose Mixture-of-Minds, a multi-agent framework that decomposes table reasoning into three specialized roles: planning, coding, and answering. This design enables each agent to focus on a specific aspect of the task while leveraging code execution for precise table manipulation. Building on this workflow, we introduce a self-improvement training framework that employs Monte Carlo Tree Search (MCTS) rollouts to generate pseudo-gold trajectories and optimize agents with reinforcement learning (RL). Extensive experiments show that Mixture-of-Minds delivers substantial gains, reaching 62.13% on TableBench and surpassing OpenAI-o4-mini-high. These results demonstrate the promise of combining structured multi-agent workflows with RL to advance table understanding.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-23T03:51:17Z</published>\n    <arxiv:comment>18 pages, 4 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Yuhang Zhou</name>\n    </author>\n    <author>\n      <name>Mingrui Zhang</name>\n    </author>\n    <author>\n      <name>Ke Li</name>\n    </author>\n    <author>\n      <name>Mingyi Wang</name>\n    </author>\n    <author>\n      <name>Qiao Liu</name>\n    </author>\n    <author>\n      <name>Qifei Wang</name>\n    </author>\n    <author>\n      <name>Jiayi Liu</name>\n    </author>\n    <author>\n      <name>Fei Liu</name>\n    </author>\n    <author>\n      <name>Serena Li</name>\n    </author>\n    <author>\n      <name>Weiwei Li</name>\n    </author>\n    <author>\n      <name>Mingze Gao</name>\n    </author>\n    <author>\n      <name>Abhishek Kumar</name>\n    </author>\n    <author>\n      <name>Xiangjun Fan</name>\n    </author>\n    <author>\n      <name>Zhuokai Zhao</name>\n    </author>\n    <author>\n      <name>Lizhu Zhang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.19897v1</id>\n    <title>Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation</title>\n    <updated>2025-10-22T17:58:03Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.19897v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.19897v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>We investigate how agents built on pretrained large language models can learn target classification functions from labeled examples without parameter updates. While conventional approaches like fine-tuning are often costly, inflexible, and opaque, we propose a memory-augmented framework that leverages both labeled data and LLM-generated critiques. Our framework uses episodic memory to store instance-level critiques-capturing specific past experiences-and semantic memory to distill these into reusable, task-level guidance. Across a diverse set of tasks, incorporating critiques yields up to a 24.8 percent accuracy improvement over retrieval-based (RAG-style) baselines that rely only on labels. Through extensive empirical evaluation, we uncover distinct behavioral differences between OpenAI and opensource models, particularly in how they handle fact-oriented versus preference-based data. To interpret how models respond to different representations of supervision encoded in memory, we introduce a novel metric, suggestibility. This helps explain observed behaviors and illuminates how model characteristics and memory strategies jointly shape learning dynamics. Our findings highlight the promise of memory-driven, reflective learning for building more adaptive and interpretable LLM agents.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-22T17:58:03Z</published>\n    <arxiv:comment>11 pages</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Jackson Hassell</name>\n    </author>\n    <author>\n      <name>Dan Zhang</name>\n    </author>\n    <author>\n      <name>Hannah Kim</name>\n    </author>\n    <author>\n      <name>Tom Mitchell</name>\n    </author>\n    <author>\n      <name>Estevam Hruschka</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.19792v1</id>\n    <title>On Controlled Change: Generative AI's Impact on Professional Authority in Journalism</title>\n    <updated>2025-10-22T17:27:32Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.19792v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.19792v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Using (generative) artificial intelligence tools and systems in journalism is expected to increase journalists' production rates, transform newsrooms' economic models, and further personalize the audience's news consumption practices. Since its release in 2022, OpenAI's ChatGPT and other large language models have raised the alarms inside news organizations, not only for bringing new challenges to news reporting and fact-checking but also for what these technologies would mean for journalists' professional authority in journalism. This paper examines how journalists in Dutch media manage the integration of AI technologies into their daily routines. Drawing from 13 interviews with editors, journalists, and innovation managers in different news outlets and media companies, we propose the concept of controlled change. as a heuristic to explain how journalists are proactively setting guidelines, experimenting with AI tools, and identifying their limitations and capabilities. Using professional authority as a theoretical framework, we argue that journalists anticipate and integrate AI technologies in a supervised manner and identify three primary mechanisms through which journalists manage this integration: (1) developing adaptive guidelines that align AI use with ethical codes, (2) experimenting with AI technologies to determine their necessity and fit, and (3) critically assessing the capabilities and limitations of AI systems.</summary>\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-22T17:27:32Z</published>\n    <arxiv:primary_category term=\"cs.CY\"/>\n    <author>\n      <name>Tomás Dodds</name>\n    </author>\n    <author>\n      <name>Wang Ngai Yeung</name>\n    </author>\n    <author>\n      <name>Claudia Mellado</name>\n    </author>\n    <author>\n      <name>Mathias-Felipe de Lima-Santos</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.19732v1</id>\n    <title>Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning</title>\n    <updated>2025-10-22T16:24:47Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.19732v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.19732v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>To enable embodied agents to operate effectively over extended timeframes, it is crucial to develop models that form and access memories to stay contextualized in their environment. In the current paradigm of training transformer-based policies for embodied sequential decision-making tasks, visual inputs often overwhelm the context limits of transformers, while humans can maintain and utilize a lifetime of experience compressed as memories. Significant compression is possible in principle, as much of the input is irrelevant and can be abstracted. However, existing approaches predominantly focus on either recurrent models with fixed-size memory or transformers with full-context reliance. In this work, we propose Memo, a transformer-based architecture and training recipe for reinforcement learning (RL) on memory-intensive, long-horizon tasks. Memo incorporates the creation and retrieval of memory by interleaving periodic summarization tokens with the inputs of a model during training. We demonstrate Memo's effectiveness on a gridworld meta-RL benchmark and a multi-object navigation task in photo-realistic indoor settings. Memo outperforms naive long-context transformer baselines while being more compute and storage efficient. Additionally, Memo generalizes better to longer contexts at inference time and remains robust in streaming settings, where historical context must be truncated to fit inference constraints.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-22T16:24:47Z</published>\n    <arxiv:comment>Accepted for Spotlight Presentation at NeurIPS 2025</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Gunshi Gupta</name>\n    </author>\n    <author>\n      <name>Karmesh Yadav</name>\n    </author>\n    <author>\n      <name>Zsolt Kira</name>\n    </author>\n    <author>\n      <name>Yarin Gal</name>\n    </author>\n    <author>\n      <name>Rahaf Aljundi</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.18783v1</id>\n    <title>Enhancing Fractional Gradient Descent with Learned Optimizers</title>\n    <updated>2025-10-21T16:33:20Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.18783v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.18783v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Fractional Gradient Descent (FGD) offers a novel and promising way to accelerate optimization by incorporating fractional calculus into machine learning. Although FGD has shown encouraging initial results across various optimization tasks, it faces significant challenges with convergence behavior and hyperparameter selection. Moreover, the impact of its hyperparameters is not fully understood, and scheduling them is particularly difficult in non-convex settings such as neural network training. To address these issues, we propose a novel approach called Learning to Optimize Caputo Fractional Gradient Descent (L2O-CFGD), which meta-learns how to dynamically tune the hyperparameters of Caputo FGD (CFGD). Our method's meta-learned schedule outperforms CFGD with static hyperparameters found through an extensive search and, in some tasks, achieves performance comparable to a fully black-box meta-learned optimizer. L2O-CFGD can thus serve as a powerful tool for researchers to identify high-performing hyperparameters and gain insights on how to leverage the history-dependence of the fractional differential in optimization.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"math.OC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-21T16:33:20Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Jan Sobotka</name>\n    </author>\n    <author>\n      <name>Petr Šimánek</name>\n    </author>\n    <author>\n      <name>Pavel Kordík</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.18488v1</id>\n    <title>AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification</title>\n    <updated>2025-10-21T10:11:33Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.18488v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.18488v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>On-device virtual assistants like Siri and Google Assistant are increasingly pivotal, yet their capabilities are hamstrung by a reliance on rigid, developer-dependent APIs. GUI agents offer a powerful, API-independent alternative, but their adoption is hindered by the perception of poor performance, as even the best models (e.g. Qwen3-VL-235B) scores are capped at around 60% on benchmarks like AndroidControl, far from viability for real-world use. Our research reveals that issue lies not only with the models but with the benchmarks themselves. We identified notable shortcomings in AndroidControl, including ambiguities and factual errors, which systematically underrates agent capabilities. To address this critical oversight, we enhanced AndroidControl into AndroidControl-Curated, a refined version of the benchmark improved through a rigorous purification pipeline. On this enhanced benchmark, state-of-the-art models achieve success rates nearing 75% on complex tasks (15% improvement), reflecting that on-device GUI agents are actually closer to practical deployment than previously thought. We introduce our new SOTA model, Magma-R1- 3B, post-trained on just 2.4k curated samples using 60 hours of an H20 GPU (approximately $60). Despite being 200 times smaller in parameters, this model delivers performance comparable to Qwen3- VL-235B. We release both AndroidControl-Curated benchmark and Magma-R1 model to the research community, encouraging adoption of this enhanced benchmark to better reflect model capabilities and accelerate the development of robust, on-device virtual assistants.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-21T10:11:33Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Ho Fai Leung</name>\n    </author>\n    <author>\n      <name>Xiaoyan Xi</name>\n    </author>\n    <author>\n      <name>Fei Zuo</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.17947v2</id>\n    <title>PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</title>\n    <updated>2025-10-22T01:18:53Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.17947v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.17947v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Models (LLMs) are improving at an exceptional rate. With the advent of agentic workflows, multi-turn dialogue has become the de facto mode of interaction with LLMs for completing long and complex tasks. While LLM capabilities continue to improve, they remain increasingly susceptible to jailbreaking, especially in multi-turn scenarios where harmful intent can be subtly injected across the conversation to produce nefarious outcomes. While single-turn attacks have been extensively explored, adaptability, efficiency and effectiveness continue to remain key challenges for their multi-turn counterparts. To address these gaps, we present PLAGUE, a novel plug-and-play framework for designing multi-turn attacks inspired by lifelong-learning agents. PLAGUE dissects the lifetime of a multi-turn attack into three carefully designed phases (Primer, Planner and Finisher) that enable a systematic and information-rich exploration of the multi-turn attack family. Evaluations show that red-teaming agents designed using PLAGUE achieve state-of-the-art jailbreaking results, improving attack success rates (ASR) by more than 30% across leading models in a lesser or comparable query budget. Particularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on OpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered highly resistant to jailbreaks in safety literature. Our work offers tools and insights to understand the importance of plan initialization, context optimization and lifelong learning in crafting multi-turn attacks for a comprehensive model vulnerability evaluation.</summary>\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-20T17:37:03Z</published>\n    <arxiv:comment>First two authors have equal author contributions</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CR\"/>\n    <author>\n      <name>Neeladri Bhuiya</name>\n    </author>\n    <author>\n      <name>Madhav Aggarwal</name>\n    </author>\n    <author>\n      <name>Diptanshu Purwar</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.17198v1</id>\n    <title>From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh</title>\n    <updated>2025-10-20T06:20:59Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.17198v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.17198v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The great rivers of Bangladesh, arteries of commerce and sustenance, are also agents of relentless destruction. Each year, they swallow whole villages and vast tracts of farmland, erasing communities from the map and displacing thousands of families. To track this slow-motion catastrophe has, until now, been a Herculean task for human analysts. Here we show how a powerful general-purpose vision model, the Segment Anything Model (SAM), can be adapted to this task with remarkable precision. To do this, we assembled a new dataset - a digital chronicle of loss compiled from historical Google Earth imagery of Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially, this dataset is the first to include manually annotated data on the settlements that have vanished beneath the water. Our method first uses a simple color-channel analysis to provide a rough segmentation of land and water, and then fine-tunes SAM's mask decoder to recognize the subtle signatures of riverbank erosion. The resulting model demonstrates a keen eye for this destructive process, achieving a mean Intersection over Union of 86.30% and a Dice score of 92.60% - a performance that significantly surpasses traditional methods and off-the-shelf deep learning models. This work delivers three key contributions: the first annotated dataset of disappeared settlements in Bangladesh due to river erosion; a specialized AI model fine-tuned for this critical task; and a method for quantifying land loss with compelling visual evidence. Together, these tools provide a powerful new lens through which policymakers and disaster management agencies can monitor erosion, anticipate its trajectory, and ultimately protect the vulnerable communities in its path.</summary>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-20T06:20:59Z</published>\n    <arxiv:comment>Submitted to the International Conference on Data and Applied Analytics (IDAA 2025). 15 pages, 5 figures, 4 tables</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CV\"/>\n    <author>\n      <name>M Saifuzzaman Rafat</name>\n    </author>\n    <author>\n      <name>Mohd Ruhul Ameen</name>\n    </author>\n    <author>\n      <name>Akif Islam</name>\n    </author>\n    <author>\n      <name>Abu Saleh Musa Miah</name>\n    </author>\n    <author>\n      <name>Jungpil Shin</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.18892v1</id>\n    <title>When Models Can't Follow: Testing Instruction Adherence Across 256 LLMs</title>\n    <updated>2025-10-18T16:33:15Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.18892v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.18892v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Despite widespread deployment of Large Language Models, systematic evaluation of instruction-following capabilities remains challenging. While comprehensive benchmarks exist, focused assessments that quickly diagnose specific instruction adherence patterns are valuable. As newer models may be trained on existing benchmarks, novel evaluation approaches are needed to assess genuine capabilities rather than memorized performance. This paper presents a streamlined evaluation framework using twenty carefully designed prompts to assess LLM instruction-following across diverse task categories. We demonstrate this framework through a large-scale empirical study conducted on October 14, 2025, testing 256 verified working models from 331 available via OpenRouter. To ensure methodological rigor and prevent selection bias, we first verified each model's basic functionality before inclusion. Unlike large-scale benchmarks requiring extensive computational resources, our approach offers a practical diagnostic tool researchers and practitioners can readily apply. Our methodology builds upon verifiable instructions while introducing a compact test suite balancing comprehensiveness with efficiency. Each prompt targets distinct aspects of instruction following, including format compliance, content constraints, logical sequencing, and multi-step task execution. We evaluate models from major providers (OpenAI, Anthropic, Google, Meta, Mistral) and emerging implementations (Qwen, DeepSeek, community models), providing comparative performance analysis. Our findings reveal consistent failure modes and identify specific instruction types posing particular challenges. This work contributes both a practical evaluation tool and one of the most comprehensive empirical analyses of instruction-following capabilities across the contemporary LLM landscape.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-18T16:33:15Z</published>\n    <arxiv:comment>21 pages, 3 figures, 5 tables. Comprehensive evaluation of 256 LLMs on instruction-following tasks</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Richard J. Young</name>\n    </author>\n    <author>\n      <name>Brandon Gillins</name>\n    </author>\n    <author>\n      <name>Alice M. Matthews</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.16276v1</id>\n    <title>What Limits Agentic Systems Efficiency?</title>\n    <updated>2025-10-18T00:21:45Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.16276v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.16276v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated strong reasoning capabilities. To further enhance LLM capabilities, recent agentic systems, such as Deep Research, incorporate web interactions into LLM reasoning to mitigate uncertainties and reduce potential errors. However, existing research predominantly focuses on reasoning performance, often neglecting the efficiency of agentic systems. In this work, we present a comprehensive empirical study that identifies efficiency bottlenecks in web-interactive agentic systems. We decompose end-to-end latency into two primary components: LLM API latency and web environment latency. We conduct a comprehensive empirical study across 15 models and 5 providers to demonstrate high variability in API-based agentic systems. We observe that web environment latency can contribute as much as 53.7% to the overall latency in a web-based agentic system. To improve latency, we propose SpecCache, a caching framework augmented with speculative execution that can reduce web environment overhead. Extensive evaluations on two standard benchmarks show that our approach improves the cache hit rate by up to 58x compared to a random caching strategy, while reducing web environment overhead by up to 3.2x, without degrading agentic system performance.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-18T00:21:45Z</published>\n    <arxiv:comment>27 pages, 15 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Song Bian</name>\n    </author>\n    <author>\n      <name>Minghao Yan</name>\n    </author>\n    <author>\n      <name>Anand Jayarajan</name>\n    </author>\n    <author>\n      <name>Gennady Pekhimenko</name>\n    </author>\n    <author>\n      <name>Shivaram Venkataraman</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.16234v1</id>\n    <title>ScholarEval: Research Idea Evaluation Grounded in Literature</title>\n    <updated>2025-10-17T21:55:07Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.16234v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.16234v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>As AI tools become increasingly common for research ideation, robust evaluation is critical to ensure the validity and usefulness of generated ideas. We introduce ScholarEval, a retrieval augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness - the empirical validity of proposed methods based on existing literature, and contribution - the degree of advancement made by the idea across different dimensions relative to prior research. To evaluate ScholarEval, we introduce ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas and reviews, comprised of 117 ideas across four disciplines: artificial intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows that ScholarEval achieves significantly higher coverage of points mentioned in the human expert annotated rubrics in ScholarIdeas compared to all baselines. Furthermore, ScholarEval is consistently preferred over our strongest baseline o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI, in terms of evaluation actionability, depth, and evidence support. Our large-scale user study also shows that ScholarEval significantly outperforms deep research in literature engagement, idea refinement, and usefulness. We openly release our code, dataset, and ScholarEval tool for the community to use and build on.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-17T21:55:07Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Hanane Nour Moussa</name>\n    </author>\n    <author>\n      <name>Patrick Queiroz Da Silva</name>\n    </author>\n    <author>\n      <name>Daniel Adu-Ampratwum</name>\n    </author>\n    <author>\n      <name>Alyson East</name>\n    </author>\n    <author>\n      <name>Zitong Lu</name>\n    </author>\n    <author>\n      <name>Nikki Puccetti</name>\n    </author>\n    <author>\n      <name>Mingyi Xue</name>\n    </author>\n    <author>\n      <name>Huan Sun</name>\n    </author>\n    <author>\n      <name>Bodhisattwa Prasad Majumder</name>\n    </author>\n    <author>\n      <name>Sachin Kumar</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.04683v1</id>\n    <title>AI-Powered Citation Auditing: A Zero-Assumption Protocol for Systematic Reference Verification in Academic Research</title>\n    <updated>2025-10-17T16:53:03Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.04683v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.04683v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Academic citation integrity faces persistent challenges, with research indicating 20% of citations contain errors and manual verification requiring months of expert time. This paper presents a novel AI-powered methodology for systematic, comprehensive reference auditing using agentic AI with tool-use capabilities. We develop a zero-assumption verification protocol that independently validates every reference against multiple academic databases (Semantic Scholar, Google Scholar, CrossRef) without assuming any citation is correct. The methodology was validated across 30 academic documents (2,581 references) spanning undergraduate projects to doctoral theses and peer-reviewed publications. Results demonstrate 91.7% average verification rate on published PLOS papers, with successful detection of fabricated references, retracted articles, orphan citations, and predatory journals. Time efficiency improved dramatically: 90-minute audits for 916-reference doctoral theses versus months of manual review. The system achieved &lt;0.5% false positive rate while identifying critical issues manual review might miss. This work establishes the first validated AI-agent methodology for academic citation integrity, demonstrating practical applicability for supervisors, students, and institutional quality assurance.</summary>\n    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-17T16:53:03Z</published>\n    <arxiv:comment>10 pages, 1 table. Code and validation data available at https://github.com/leonjvr/ai-citation-auditor</arxiv:comment>\n    <arxiv:primary_category term=\"cs.DL\"/>\n    <author>\n      <name>L. J. Janse van Rensburg</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.15772v1</id>\n    <title>Self-evolving expertise in complex non-verifiable subject domains: dialogue as implicit meta-RL</title>\n    <updated>2025-10-17T15:59:44Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.15772v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.15772v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>So-called `wicked problems', those involving complex multi-dimensional settings, non-verifiable outcomes, heterogeneous impacts and a lack of single objectively correct answers, have plagued humans throughout history. Modern examples include decisions over justice frameworks, solving environmental pollution, planning for pandemic resilience and food security. The use of state-of-the-art artificial intelligence systems (notably Large Language Model-based agents) collaborating with humans on solving such problems is being actively explored. While the abilities of LLMs can be improved by, for example, fine-tuning, hand-crafted system prompts and scaffolding with external tools, LLMs lack endogenous mechanisms to develop expertise through experience in such settings. This work address this gap with Dialectica, a framework where agents engage in structured dialogue on defined topics, augmented by memory, self-reflection, and policy-constrained context editing. Formally, discussion is viewed as an implicit meta-reinforcement learning process. The `dialogue-trained' agents are evaluated post-hoc using judged pairwise comparisons of elicited responses. Across two model architectures (locally run Qwen3:30b and OpenAI's o4-mini) results show that enabling reflection-based context editing during discussion produces agents which dominate their baseline counterparts on Elo scores, normalized Bradley-Terry-Davidson ability, and AlphaRank mass. The predicted signatures of learning are observed qualitatively in statement and reflection logs, where reflections identify weaknesses and reliably shape subsequent statements. Agreement between quantitative and qualitative evidence supports dialogue-driven context evolution as a practical path to targeted expertise amplification in open non-verifiable domains.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-17T15:59:44Z</published>\n    <arxiv:comment>50 pages, 4 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Richard M. Bailey</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.15620v1</id>\n    <title>GRATING: Low-Latency and Memory-Efficient Semantic Selection on Device</title>\n    <updated>2025-10-17T13:06:09Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.15620v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.15620v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Semantic top-K selection with cross-encoder rerankers underpins of on-device AI services, such as retrieval-augmented generation, agent memory, and personalized recommendation. However, its latency and memory demands dominate end-to-end budgets on edge hardware. Revisiting the objective of top-K selection, we reveal that only relative rankings matter, not exact per-candidate scores. We further observe sequence-level sparsity: relative rankings stabilize early in intermediate layers, allowing pruning opportunities prior to completing full inference.\n  Building on this insight, we propose monolithic forwarding and develop a training-free inference system, GRATING. By maintaining a global view of all candidates, it reduces latency through progressive cluster pruning. It also bounds peak memory usage by strategically overlapping I/O with computation via dual-layer sliding window and chunked execution. We evaluate GRATING against state-of-the-art baselines on rerankers from 0.6B to 8B parameters across Apple M2 and RTX 5070. GRATING consistently reduces latency by up to 89.0% and peak memory by up to 94.9% in microbenchmarks, without any loss in precision. Across three real-world on-device AI applications, GRATING lowers latency by 11.6%-51.0% and peak memory by 18.6%-77.8%, demonstrating substantial improvements in efficiency and deployability.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-17T13:06:09Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Jiahao Zhou</name>\n    </author>\n    <author>\n      <name>Chengliang Lin</name>\n    </author>\n    <author>\n      <name>Dingji Li</name>\n    </author>\n    <author>\n      <name>Mingkai Dong</name>\n    </author>\n    <author>\n      <name>Haibo Chen</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.15280v1</id>\n    <title>Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition</title>\n    <updated>2025-10-17T03:40:26Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.15280v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.15280v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the landscape of scientific research. Beyond accelerating tasks such as hypothesis generation, experimental design, and result interpretation, they prompt a more fundamental question: Are FMs merely enhancing existing scientific methodologies, or are they redefining the way science is conducted? In this paper, we argue that FMs are catalyzing a transition toward a new scientific paradigm. We introduce a three-stage framework to describe this evolution: (1) Meta-Scientific Integration, where FMs enhance workflows within traditional paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active collaborators in problem formulation, reasoning, and discovery; and (3) Autonomous Scientific Discovery, where FMs operate as independent agents capable of generating new scientific knowledge with minimal human intervention. Through this lens, we review current applications and emerging capabilities of FMs across existing scientific paradigms. We further identify risks and future directions for FM-enabled scientific discovery. This position paper aims to support the scientific community in understanding the transformative role of FMs and to foster reflection on the future of scientific discovery. Our project is available at https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-17T03:40:26Z</published>\n    <arxiv:comment>NeurIPS 2025</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Fan Liu</name>\n    </author>\n    <author>\n      <name>Jindong Han</name>\n    </author>\n    <author>\n      <name>Tengfei Lyu</name>\n    </author>\n    <author>\n      <name>Weijia Zhang</name>\n    </author>\n    <author>\n      <name>Zhe-Rui Yang</name>\n    </author>\n    <author>\n      <name>Lu Dai</name>\n    </author>\n    <author>\n      <name>Cancheng Liu</name>\n    </author>\n    <author>\n      <name>Hao Liu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.15218v2</id>\n    <title>Machine Learning for Early Detection of Meningitis: Stacked Ensemble Learning with EHR Data</title>\n    <updated>2025-10-20T03:37:32Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.15218v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.15218v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>We utilized a cohort of 214 meningitis patients and 46,303 non-meningitis patients from the MIMIC-III database. After extensive data preprocessing, which included ICD-based cohort selection, one-hot encoding of coding, and a two-stage feature selection process (for both the training set and the testing sets), clinically relevant features such as gender and high-risk ICD codes (including subarachnoid hemorrhage, secondary malignant neoplasm of the brain, and generalized epilepsy) are selected. Overall, these clinically reasonable and temporally adherent features provided excellent modeling performance. Three models (Random Forest, LightGBM, and Deep Neural Networks (DNN) are trained as base models for Ensemble Learning. Base model outputs are aggregated and stacked into a meta model (Logistic Regression) that uses the base model outputs as input values in training. Ultimately, soldier outputs (AUC of Testing Set 1: 0.9637, AUC of Testing Set 2: 0.9472) are obtained through ensemble learning.\n  We created a challenging condition for diagnosing meningitis, simulating a real-world ER (Emergency Room) scenario to enhance clinical use in real-world applications. While directly deploying a diagnostic tool that clinicians can use is challenging, this paper paves the way for a potential future AI-driven diagnostic approach for meningitis using Ensemble Learning.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-17T00:56:47Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Han Ouyang</name>\n    </author>\n    <author>\n      <name>Jesse Hamilton</name>\n    </author>\n    <author>\n      <name>Saeed Amal</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.15061v2</id>\n    <title>Antislop: A Comprehensive Framework for Identifying and Eliminating Repetitive Patterns in Language Models</title>\n    <updated>2025-10-21T21:42:07Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.15061v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.15061v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Widespread LLM adoption has introduced characteristic repetitive phraseology, termed \"slop,\" which degrades output quality and makes AI-generated text immediately recognizable. We present Antislop, a comprehensive framework providing tools to both detect and eliminate these overused patterns. Our approach combines three innovations: (1) The Antislop Sampler, which uses backtracking to suppress unwanted strings at inference time without destroying vocabulary; (2) An automated pipeline that profiles model-specific slop against human baselines and generates training data; (3) Final Token Preference Optimization (FTPO), a novel fine-tuning method that operates on individual tokens, surgically adjusting logits wherever a banned pattern has appeared in an inference trace. We demonstrate that some slop patterns appear over 1,000x more frequently in LLM output than human text. The Antislop Sampler successfully suppresses 8,000+ patterns while maintaining quality, whereas token banning becomes unusable at just 2,000. Most importantly, FTPO achieves 90% slop reduction while maintaining or improving performance in cross-domain evals including GSM8K, MMLU, and creative writing tasks. In contrast, DPO suffers significant degradation in writing quality and lexical diversity despite achieving weaker suppression. We release all code and results under MIT license: https://github.com/sam-paech/auto-antislop.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-16T18:22:22Z</published>\n    <arxiv:comment>11 pages + appendices, 16 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Samuel Paech</name>\n    </author>\n    <author>\n      <name>Allen Roush</name>\n    </author>\n    <author>\n      <name>Judah Goldfeder</name>\n    </author>\n    <author>\n      <name>Ravid Shwartz-Ziv</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.16047v1</id>\n    <title>Algorithms for dynamic scheduling in manufacturing, towards digital factories Improving Deadline Feasibility and Responsiveness via Temporal Networks</title>\n    <updated>2025-10-16T17:28:25Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.16047v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.16047v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Modern manufacturing systems must meet hard delivery deadlines while coping with stochastic task durations caused by process noise, equipment variability, and human intervention. Traditional deterministic schedules break down when reality deviates from nominal plans, triggering costly last-minute repairs. This thesis combines offline constraint-programming (CP) optimisation with online temporal-network execution to create schedules that remain feasible under worst-case uncertainty. First, we build a CP model of the flexible job-shop with per-job deadline tasks and insert an optimal buffer $Δ^*$ to obtain a fully pro-active baseline. We then translate the resulting plan into a Simple Temporal Network with Uncertainty (STNU) and verify dynamic controllability, which guarantees that a real-time dispatcher can retime activities for every bounded duration realisation without violating resource or deadline constraints. Extensive Monte-Carlo simulations on the open Kacem~1--4 benchmark suite show that our hybrid approach eliminates 100\\% of deadline violations observed in state-of-the-art meta-heuristic schedules, while adding only 3--5\\% makespan overhead. Scalability experiments confirm that CP solve-times and STNU checks remain sub-second on medium-size instances. The work demonstrates how temporal-network reasoning can bridge the gap between proactive buffering and dynamic robustness, moving industry a step closer to truly digital, self-correcting factories.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-16T17:28:25Z</published>\n    <arxiv:comment>8 pages 2 column, 11 figures. Bachelor's thesis</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Ioan Hedea</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.14900v1</id>\n    <title>Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates</title>\n    <updated>2025-10-16T17:17:00Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.14900v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.14900v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-16T17:17:00Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Wen-Kwang Tsao</name>\n    </author>\n    <author>\n      <name>Yao-Ching Yu</name>\n    </author>\n    <author>\n      <name>Chien-Ming Huang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.14808v1</id>\n    <title>Agentic NL2SQL to Reduce Computational Costs</title>\n    <updated>2025-10-16T15:42:28Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.14808v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.14808v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Translating natural language queries into SQL queries (NL2SQL or Text-to-SQL) has recently been empowered by large language models (LLMs). Using LLMs to perform NL2SQL methods on a large collection of SQL databases necessitates processing large quantities of meta-information about the databases, which in turn results in lengthy prompts with many tokens and high processing costs. To address this challenge, we introduce Datalake Agent, an agentic system designed to enable an LLM to solve NL2SQL tasks more efficiently. Instead of utilizing direct solvers for NL2SQL that call the LLM once with all meta-information in the prompt, the Datalake Agent employs an interactive loop to reduce the utilized meta-information. Within the loop, the LLM is used in a reasoning framework that selectively requests only the necessary information to solve a table question answering task. We evaluate the Datalake Agent on a collection of 23 databases with 100 table question answering tasks. The Datalake Agent reduces the tokens used by the LLM by up to 87\\% and thus allows for substantial cost reductions while maintaining competitive performance.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-16T15:42:28Z</published>\n    <arxiv:comment>Accepted at the NeurIPS 2025 Workshop on Efficient Reasoning. 10 pages, 11 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Dominik Jehle</name>\n    </author>\n    <author>\n      <name>Lennart Purucker</name>\n    </author>\n    <author>\n      <name>Frank Hutter</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.14669v1</id>\n    <title>Machine Learning and Public Health: Identifying and Mitigating Algorithmic Bias through a Systematic Review</title>\n    <updated>2025-10-16T13:24:11Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.14669v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.14669v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Machine learning (ML) promises to revolutionize public health through improved surveillance, risk stratification, and resource allocation. However, without systematic attention to algorithmic bias, ML may inadvertently reinforce existing health disparities. We present a systematic literature review of algorithmic bias identification, discussion, and reporting in Dutch public health ML research from 2021 to 2025. To this end, we developed the Risk of Algorithmic Bias Assessment Tool (RABAT) by integrating elements from established frameworks (Cochrane Risk of Bias, PROBAST, Microsoft Responsible AI checklist) and applied it to 35 peer-reviewed studies. Our analysis reveals pervasive gaps: although data sampling and missing data practices are well documented, most studies omit explicit fairness framing, subgroup analyses, and transparent discussion of potential harms. In response, we introduce a four-stage fairness-oriented framework called ACAR (Awareness, Conceptualization, Application, Reporting), with guiding questions derived from our systematic literature review to help researchers address fairness across the ML lifecycle. We conclude with actionable recommendations for public health ML practitioners to consistently consider algorithmic bias and foster transparency, ensuring that algorithmic innovations advance health equity rather than undermine it.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-16T13:24:11Z</published>\n    <arxiv:comment>Extended version of the paper accepted at the AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025), including an appendix. 10 pages, 2 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Sara Altamirano</name>\n    </author>\n    <author>\n      <name>Arjan Vreeken</name>\n    </author>\n    <author>\n      <name>Sennay Ghebreab</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.14150v2</id>\n    <title>CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization</title>\n    <updated>2025-11-10T14:12:41Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.14150v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.14150v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>In this work, we introduce CodeEvolve, an open-source evolutionary coding agent that unites Large Language Models (LLMs) with genetic algorithms to solve complex computational problems. Our framework adapts powerful evolutionary concepts to the LLM domain, building upon recent methods for generalized scientific discovery. CodeEvolve employs an island-based genetic algorithm to maintain population diversity and increase throughput, introduces a novel inspiration-based crossover mechanism that leverages the LLMs context window to combine features from successful solutions, and implements meta-prompting strategies for dynamic exploration of the solution space. We conduct a rigorous evaluation of CodeEvolve on a subset of the mathematical benchmarks used to evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that our method surpasses AlphaEvolve's performance on several challenging problems. To foster collaboration and accelerate progress, we release our complete framework as an open-source repository.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-15T22:58:06Z</published>\n    <arxiv:comment>11 pages, 9 figures, 2 tables</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Henrique Assumpção</name>\n    </author>\n    <author>\n      <name>Diego Ferreira</name>\n    </author>\n    <author>\n      <name>Leandro Campos</name>\n    </author>\n    <author>\n      <name>Fabricio Murai</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.13400v1</id>\n    <title>From Minimal Existence to Human Definition: The CES-IMU-HSG Theoretical Framework</title>\n    <updated>2025-10-15T10:56:09Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.13400v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.13400v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This study presents an inter-universal mathematical-logical framework constructed upon the minimal axiom Cogito, ergo sum (CES), integrating the Intermediate Meta-Universe (IMU) and the Hierarchical State Grid (HSG). The CES defines existence as a reflexive correspondence --'to be' and 'to be sayable'--and positions any formal system, including ZFC or HoTT, as an attachable extension atop this minimal structure. The IMU functions as a registry of axiomatic dependencies that connect heterogeneous theories, employing the Institution-theoretic framework to ensure coherent inter-theoretical linkages. The HSG concretizes these ideas through categorical construction, defined by three orthogonal axes: the state-depth axis, the mapping-hierarchy axis, and the temporal axis incorporating the principle of 'no future reference.' Through these, the identity of 'definition = state' is formally established as a categorical property. Extending this structure to biological systems, the neural system is implemented as a 0-3D complex of neuron-function fields on the HSG, while its categorical extensions via fiberization over the material base enable the parallel integration of multiple physiological universes-neural, endocrine, learning, genetic, and input/output systems-into a coherent adjoint ensemble. Within this framework, human behavior and cognition emerge as temporal compositions of inter-universal algorithms constrained by the material base. Finally, by contrasting human cognition, which relies on external CES, with machine existence, this study introduces the concept of internal CES, wherein a machine grounds its own logic upon the factuality of its operation. This internal self-axiomatization establishes a continuous bridge between philosophical ontology and engineering implementation, providing a new foundation for the autonomous and self-defining existence of artificial intelligence.</summary>\n    <category term=\"cs.NE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-15T10:56:09Z</published>\n    <arxiv:comment>57 pages, 2 figures, 4 tables, in English, in Japanese</arxiv:comment>\n    <arxiv:primary_category term=\"cs.NE\"/>\n    <author>\n      <name>Kei Itoh</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.13397v2</id>\n    <title>Assessing the robustness of heterogeneous treatment effects in survival analysis under informative censoring</title>\n    <updated>2025-10-28T12:46:53Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.13397v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.13397v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Dropout is common in clinical studies, with up to half of patients leaving early due to side effects or other reasons. When dropout is informative (i.e., dependent on survival time), it introduces censoring bias, because of which treatment effect estimates are also biased. In this paper, we propose an assumption-lean framework to assess the robustness of conditional average treatment effect (CATE) estimates in survival analysis when facing censoring bias. Unlike existing works that rely on strong assumptions, such as non-informative censoring, to obtain point estimation, we use partial identification to derive informative bounds on the CATE. Thereby, our framework helps to identify patient subgroups where treatment is effective despite informative censoring. We further develop a novel meta-learner that estimates the bounds using arbitrary machine learning models and with favorable theoretical properties, including double robustness and quasi-oracle efficiency. We demonstrate the practical value of our meta-learner through numerical experiments and in an application to a cancer drug trial. Together, our framework offers a practical tool for assessing the robustness of estimated treatment effects in the presence of censoring and thus promotes the reliable use of survival data for evidence generation in medicine and epidemiology.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-15T10:51:17Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Yuxin Wang</name>\n    </author>\n    <author>\n      <name>Dennis Frauen</name>\n    </author>\n    <author>\n      <name>Jonas Schweisthal</name>\n    </author>\n    <author>\n      <name>Maresa Schröder</name>\n    </author>\n    <author>\n      <name>Stefan Feuerriegel</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.13050v1</id>\n    <title>An Operational Deep Learning System for Satellite-Based High-Resolution Global Nowcasting</title>\n    <updated>2025-10-15T00:11:03Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.13050v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.13050v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Precipitation nowcasting, which predicts rainfall up to a few hours ahead, is a critical tool for vulnerable communities in the Global South frequently exposed to intense, rapidly developing storms. Timely forecasts provide a crucial window to protect lives and livelihoods. Traditional numerical weather prediction (NWP) methods suffer from high latency, low spatial and temporal resolution, and significant gaps in accuracy across the world. Recent machine learning-based nowcasting methods, common in the Global North, cannot be extended to the Global South due to extremely sparse radar coverage. We present Global MetNet, an operational global machine learning nowcasting model. It leverages the Global Precipitation Mission's CORRA dataset, geostationary satellite data, and global NWP data to predict precipitation for the next 12 hours. The model operates at a high resolution of approximately 0.05° (~5km) spatially and 15 minutes temporally. Global MetNet significantly outperforms industry-standard hourly forecasts and achieves significantly higher skill, making forecasts useful over a much larger area of the world than previously available. Our model demonstrates better skill in data-sparse regions than even the best high-resolution NWP models achieve in the US. Validated using ground radar and satellite data, it shows significant improvements across key metrics like the critical success index and fractions skill score for all precipitation rates and lead times. Crucially, our model generates forecasts in under a minute, making it readily deployable for real-time applications. It is already deployed for millions of users on Google Search. This work represents a key step in reducing global disparities in forecast quality and integrating sparse, high-resolution satellite observations into weather forecasting.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"physics.ao-ph\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-15T00:11:03Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Shreya Agrawal</name>\n    </author>\n    <author>\n      <name>Mohammed Alewi Hassen</name>\n    </author>\n    <author>\n      <name>Emmanuel Asiedu Brempong</name>\n    </author>\n    <author>\n      <name>Boris Babenko</name>\n    </author>\n    <author>\n      <name>Fred Zyda</name>\n    </author>\n    <author>\n      <name>Olivia Graham</name>\n    </author>\n    <author>\n      <name>Di Li</name>\n    </author>\n    <author>\n      <name>Samier Merchant</name>\n    </author>\n    <author>\n      <name>Santiago Hincapie Potes</name>\n    </author>\n    <author>\n      <name>Tyler Russell</name>\n    </author>\n    <author>\n      <name>Danny Cheresnick</name>\n    </author>\n    <author>\n      <name>Aditya Prakash Kakkirala</name>\n    </author>\n    <author>\n      <name>Stephan Rasp</name>\n    </author>\n    <author>\n      <name>Avinatan Hassidim</name>\n    </author>\n    <author>\n      <name>Yossi Matias</name>\n    </author>\n    <author>\n      <name>Nal Kalchbrenner</name>\n    </author>\n    <author>\n      <name>Pramod Gupta</name>\n    </author>\n    <author>\n      <name>Jason Hickey</name>\n    </author>\n    <author>\n      <name>Aaron Bell</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.12712v3</id>\n    <title>Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning</title>\n    <updated>2025-10-24T23:29:20Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.12712v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.12712v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Multimodal Large Language Models (MLLMs) are increasingly applied in real-world scenarios where user-provided images are often imperfect, requiring active image manipulations such as cropping, editing, or enhancement to uncover salient visual cues. Beyond static visual perception, MLLMs must also think with images: dynamically transforming visual content and integrating it with other tools to solve complex tasks. However, this shift from treating vision as passive context to a manipulable cognitive workspace remains underexplored. Most existing benchmarks still follow a think about images paradigm, where images are regarded as static inputs. To address this gap, we introduce VisualToolBench, a visual tool-use reasoning benchmark that rigorously evaluates MLLMs' ability to perceive, transform, and reason across complex visual-textual tasks under the think-with-images paradigm. VisualToolBench comprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse domains, each paired with detailed rubrics to enable systematic evaluation. Our evaluation shows that current MLLMs struggle with tasks requiring effective integration of vision and general-purpose tools. Even the strongest model (GPT-5-think) reaches only 18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI models benefiting from diverse image manipulations while Gemini-2.5-pro shows no improvement. By introducing the first benchmark centered on think with images, VisualToolBench offers critical insights for advancing visual intelligence in MLLMs.</summary>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-14T16:50:49Z</published>\n    <arxiv:primary_category term=\"cs.CV\"/>\n    <author>\n      <name>Xingang Guo</name>\n    </author>\n    <author>\n      <name>Utkarsh Tyagi</name>\n    </author>\n    <author>\n      <name>Advait Gosai</name>\n    </author>\n    <author>\n      <name>Paula Vergara</name>\n    </author>\n    <author>\n      <name>Jayeon Park</name>\n    </author>\n    <author>\n      <name>Ernesto Gabriel Hernández Montoya</name>\n    </author>\n    <author>\n      <name>Chen Bo Calvin Zhang</name>\n    </author>\n    <author>\n      <name>Bin Hu</name>\n    </author>\n    <author>\n      <name>Yunzhong He</name>\n    </author>\n    <author>\n      <name>Bing Liu</name>\n    </author>\n    <author>\n      <name>Rakshith Sharma Srinivasa</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.12864v1</id>\n    <title>From Literal to Liberal: A Meta-Prompting Framework for Eliciting Human-Aligned Exception Handling in Large Language Models</title>\n    <updated>2025-10-14T16:42:52Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.12864v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.12864v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Models (LLMs) are increasingly being deployed as the reasoning engines for agentic AI systems, yet they exhibit a critical flaw: a rigid adherence to explicit rules that leads to decisions misaligned with human common sense and intent. This \"rule-rigidity\" is a significant barrier to building trustworthy autonomous agents. While prior work has shown that supervised fine-tuning (SFT) with human explanations can mitigate this issue, SFT is computationally expensive and inaccessible to many practitioners. To address this gap, we introduce the Rule-Intent Distinction (RID) Framework, a novel, low-compute meta-prompting technique designed to elicit human-aligned exception handling in LLMs in a zero-shot manner. The RID framework provides the model with a structured cognitive schema for deconstructing tasks, classifying rules, weighing conflicting outcomes, and justifying its final decision. We evaluated the RID framework against baseline and Chain-of-Thought (CoT) prompting on a custom benchmark of 20 scenarios requiring nuanced judgment across diverse domains. Our human-verified results demonstrate that the RID framework significantly improves performance, achieving a 95% Human Alignment Score (HAS), compared to 80% for the baseline and 75% for CoT. Furthermore, it consistently produces higher-quality, intent-driven reasoning. This work presents a practical, accessible, and effective method for steering LLMs from literal instruction-following to liberal, goal-oriented reasoning, paving the way for more reliable and pragmatic AI agents.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-14T16:42:52Z</published>\n    <arxiv:comment>13 pages. Code and data are available at https://github.com/strongSoda/LITERAL-TO-LIBERAL</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Imran Khan</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.12624v1</id>\n    <title>Learning-To-Measure: In-context Active Feature Acquisition</title>\n    <updated>2025-10-14T15:23:32Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.12624v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.12624v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Active feature acquisition (AFA) is a sequential decision-making problem where the goal is to improve model performance for test instances by adaptively selecting which features to acquire. In practice, AFA methods often learn from retrospective data with systematic missingness in the features and limited task-specific labels. Most prior work addresses acquisition for a single predetermined task, limiting scalability. To address this limitation, we formalize the meta-AFA problem, where the goal is to learn acquisition policies across various tasks. We introduce Learning-to-Measure (L2M), which consists of i) reliable uncertainty quantification over unseen tasks, and ii) an uncertainty-guided greedy feature acquisition agent that maximizes conditional mutual information. We demonstrate a sequence-modeling or autoregressive pre-training approach that underpins reliable uncertainty quantification for tasks with arbitrary missingness. L2M operates directly on datasets with retrospective missingness and performs the meta-AFA task in-context, eliminating per-task retraining. Across synthetic and real-world tabular benchmarks, L2M matches or surpasses task-specific baselines, particularly under scarce labels and high missingness.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-14T15:23:32Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Yuta Kobayashi</name>\n    </author>\n    <author>\n      <name>Zilin Jing</name>\n    </author>\n    <author>\n      <name>Jiayu Yao</name>\n    </author>\n    <author>\n      <name>Hongseok Namkoong</name>\n    </author>\n    <author>\n      <name>Shalmali Joshi</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.12246v1</id>\n    <title>PromptFlow: Training Prompts Like Neural Networks</title>\n    <updated>2025-10-14T07:56:12Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.12246v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.12246v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Models (LLMs) have demonstrated profound impact on Natural Language Processing (NLP) tasks. However, their effective deployment across diverse domains often require domain-specific adaptation strategies, as generic models may underperform when faced with specialized data distributions. Recent advances in prompt engineering (PE) offer a promising alternative to extensive retraining by refining input instructions to align LLM outputs with task objectives. This paradigm has emerged as a rapid and versatile approach for model fine-tuning. Despite its potential, manual prompt design remains labor-intensive and heavily depends on specialized expertise, often requiring iterative human effort to achieve optimal formulations. To address this limitation, automated prompt engineering methodologies have been developed to systematically generate task-specific prompts. However, current implementations predominantly employ static update rules and lack mechanisms for dynamic strategy selection, resulting in suboptimal adaptation to varying NLP task requirements. Furthermore, most methods treat and update the whole prompts at each step, without considering editing prompt sections at a finer granularity. At last, in particular, the problem of how to recycle experience in LLM is still underexplored. To this end, we propose the PromptFlow, a modular training framework inspired by TensorFlow, which integrates meta-prompts, operators, optimization, and evaluator. Our framework can be equipped with the latest optimization methods and autonomously explores optimal prompt refinement trajectories through gradient-based meta-learning, requiring minimal task-specific training data. Specifically, we devise a reinforcement learning method to recycle experience for LLM in the PE process. Finally, we conduct extensive experiments on various datasets, and demonstrate the effectiveness of PromptFlow.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-14T07:56:12Z</published>\n    <arxiv:comment>Comments: 18 pages, 14 figures, conference submission, appendix included</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Jingyi Wang</name>\n    </author>\n    <author>\n      <name>Hongyuan Zhu</name>\n    </author>\n    <author>\n      <name>Ye Niu</name>\n    </author>\n    <author>\n      <name>Yunhui Deng</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.12194v1</id>\n    <title>ResearStudio: A Human-Intervenable Framework for Building Controllable Deep-Research Agents</title>\n    <updated>2025-10-14T06:40:11Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.12194v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.12194v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Current deep-research agents run in a ''fire-and-forget'' mode: once started, they give users no way to fix errors or add expert knowledge during execution. We present ResearStudio, the first open-source framework that places real-time human control at its core. The system follows a Collaborative Workshop design. A hierarchical Planner-Executor writes every step to a live ''plan-as-document,'' a fast communication layer streams each action, file change, and tool call to a web interface. At any moment, the user can pause the run, edit the plan or code, run custom commands, and resume -- switching smoothly between AI-led, human-assisted and human-led, AI-assisted modes. In fully autonomous mode, ResearStudio achieves state-of-the-art results on the GAIA benchmark, surpassing systems like OpenAI's DeepResearch and Manus. These results show that strong automated performance and fine-grained human control can coexist. The full code, protocol, and evaluation scripts are available at https://github.com/ResearAI/ResearStudio. We will continue to update the repository to encourage further work on safe and controllable research agents. Our live demo is publicly accessible at http://ai-researcher.net:3000/. We support the development of DeepScientist, which can be accessed at https://github.com/ResearAI/DeepScientist.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-14T06:40:11Z</published>\n    <arxiv:comment>EMNLP 2025 Demo, Oral</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Linyi Yang</name>\n    </author>\n    <author>\n      <name>Yixuan Weng</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.11558v1</id>\n    <title>Zero Data Retention in LLM-based Enterprise AI Assistants: A Comparative Study of Market Leading Agentic AI Products</title>\n    <updated>2025-10-13T16:00:34Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.11558v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.11558v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Governance of data, compliance, and business privacy matters, particularly for healthcare and finance businesses. Since the recent emergence of AI enterprise AI assistants enhancing business productivity, safeguarding private data and compliance is now a priority. With the implementation of AI assistants across the enterprise, the zero data retention can be achieved by implementing zero data retention policies by Large Language Model businesses like Open AI and Anthropic and Meta. In this work, we explore zero data retention policies for the Enterprise apps of large language models (LLMs). Our key contribution is defining the architectural, compliance, and usability trade-offs of such systems in parallel. In this research work, we examine the development of commercial AI assistants with two industry leaders and market titans in this arena - Salesforce and Microsoft. Both of these companies used distinct technical architecture to support zero data retention policies. Salesforce AgentForce and Microsoft Copilot are among the leading AI assistants providing much-needed push to business productivity in customer care. The purpose of this paper is to analyze the technical architecture and deployment of zero data retention policy by consuming applications as well as big language models service providers like Open Ai, Anthropic, and Meta.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-13T16:00:34Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Komal Gupta</name>\n    </author>\n    <author>\n      <name>Aditya Shrivastava</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.10930v1</id>\n    <title>Evaluating Language Models' Evaluations of Games</title>\n    <updated>2025-10-13T02:45:37Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.10930v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.10930v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Reasoning is not just about solving problems -- it is also about evaluating which problems are worth solving at all. Evaluations of artificial intelligence (AI) systems primarily focused on problem solving, historically by studying how models play games such as chess and Go. In this paper, we advocate for a new paradigm that assesses AI systems' evaluation of games. First, we introduce a formalism for evaluating such evaluations. We then leverage a large-scale dataset of over $100$ novel board games and over 450 human judgments to compare evaluations produced by modern language and reasoning models against those of people and symbolic computational agents. We consider two kinds of evaluative queries: assessing the payoff (or fairness) and the funness of games. These queries span two dimensions relevant to the design of evaluations of AI evaluations: how complex a query is to compute and how difficult a query is to quantify. Our results show that reasoning models are generally more aligned to people in their evaluations of games than non-reasoning language models. However, we observe a non-monotonic relationship: as models get closer to game-theoretic optimal, their fit to human data weakens. We also observe more \"jaggedness\" across models for assessing funness, in line with the greater difficulty of quantifying this query. Across queries and games, reasoning models show highly variable and unpredictable resource usage when assessing queries, pointing to the importance of imbuing more resource-rational meta-reasoning in language and reasoning models.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-13T02:45:37Z</published>\n    <arxiv:comment>Pre-print</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Katherine M. Collins</name>\n    </author>\n    <author>\n      <name>Cedegao E. Zhang</name>\n    </author>\n    <author>\n      <name>Graham Todd</name>\n    </author>\n    <author>\n      <name>Lance Ying</name>\n    </author>\n    <author>\n      <name>Mauricio Barba da Costa</name>\n    </author>\n    <author>\n      <name>Ryan Liu</name>\n    </author>\n    <author>\n      <name>Prafull Sharma</name>\n    </author>\n    <author>\n      <name>Adrian Weller</name>\n    </author>\n    <author>\n      <name>Ionatan Kuperwajs</name>\n    </author>\n    <author>\n      <name>Lionel Wong</name>\n    </author>\n    <author>\n      <name>Joshua B. Tenenbaum</name>\n    </author>\n    <author>\n      <name>Thomas L. Griffiths</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.10813v1</id>\n    <title>LLMs as Strategic Agents: Beliefs, Best Response Behavior, and Emergent Heuristics</title>\n    <updated>2025-10-12T21:40:29Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.10813v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.10813v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Models (LLMs) are increasingly applied to domains that require reasoning about other agents' behavior, such as negotiation, policy design, and market simulation, yet existing research has mostly evaluated their adherence to equilibrium play or their exhibited depth of reasoning. Whether they display genuine strategic thinking, understood as the coherent formation of beliefs about other agents, evaluation of possible actions, and choice based on those beliefs, remains unexplored. We develop a framework to identify this ability by disentangling beliefs, evaluation, and choice in static, complete-information games, and apply it across a series of non-cooperative environments. By jointly analyzing models' revealed choices and reasoning traces, and introducing a new context-free game to rule out imitation from memorization, we show that current frontier models exhibit belief-coherent best-response behavior at targeted reasoning depths. When unconstrained, they self-limit their depth of reasoning and form differentiated conjectures about human and synthetic opponents, revealing an emergent form of meta-reasoning. Under increasing complexity, explicit recursion gives way to internally generated heuristic rules of choice that are stable, model-specific, and distinct from known human biases. These findings indicate that belief coherence, meta-reasoning, and novel heuristic formation can emerge jointly from language modeling objectives, providing a structured basis for the study of strategic cognition in artificial agents.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.GT\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-12T21:40:29Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Enric Junque de Fortuny</name>\n    </author>\n    <author>\n      <name>Veronica Roberta Cappelli</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2510.10570v1</id>\n    <title>Multitask Learning with Learned Task Relationships</title>\n    <updated>2025-10-12T12:42:19Z</updated>\n    <link href=\"https://arxiv.org/abs/2510.10570v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2510.10570v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Classical consensus-based strategies for federated and decentralized learning are statistically suboptimal in the presence of heterogeneous local data or task distributions. As a result, in recent years, there has been growing interest in multitask or personalized strategies, which allow individual agents to benefit from one another in pursuing locally optimal models without enforcing consensus. Existing strategies require either precise prior knowledge of the underlying task relationships or are fully non-parametric and instead rely on meta-learning or proximal constructions. In this work, we introduce an algorithmic framework that strikes a balance between these extremes. By modeling task relationships through a Gaussian Markov Random Field with an unknown precision matrix, we develop a strategy that jointly learns both the task relationships and the local models, allowing agents to self-organize in a way consistent with their individual data distributions. Our theoretical analysis quantifies the quality of the learned relationship, and our numerical experiments demonstrate its practical effectiveness.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.DC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-10-12T12:42:19Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Zirui Wan</name>\n    </author>\n    <author>\n      <name>Stefan Vlaski</name>\n    </author>\n  </entry>\n</feed>\n"
}