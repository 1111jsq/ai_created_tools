{
  "source": "arxiv",
  "fetched_at": "2025-11-20T09:28:15.151114+00:00",
  "payload": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/8rg+9R9hrFLtrjmf9fso9x5Jt4M</id>\n  <title>arXiv Query: search_query=(ti:agent OR ti:\"tool use\" OR ti:tool-augmented OR ti:planning OR ti:multi-agent OR ti:autonomous OR ti:toolformer OR ti:\"tool learning\" OR abs:agent OR abs:\"tool use\" OR abs:tool-augmented OR abs:planning OR abs:multi-agent OR abs:autonomous OR abs:toolformer OR abs:\"tool learning\") AND (cat:cs.AI OR cat:cs.LG OR cat:cs.MA) AND (ti:Google OR ti:DeepMind OR ti:Microsoft OR ti:OpenAI OR ti:Meta OR ti:Apple OR ti:Stanford OR ti:MIT OR ti:CMU OR ti:Berkeley OR ti:Oxford OR ti:Harvard OR ti:Tsinghua OR ti:\"Peking University\" OR ti:PKU OR ti:USTC OR ti:SJTU OR ti:Princeton OR ti:UCLA OR ti:UCSD OR ti:\"ETH Zurich\" OR ti:NUS OR ti:NTU OR abs:Google OR abs:DeepMind OR abs:Microsoft OR abs:OpenAI OR abs:Meta OR abs:Apple OR abs:Stanford OR abs:MIT OR abs:CMU OR abs:Berkeley OR abs:Oxford OR abs:Harvard OR abs:Tsinghua OR abs:\"Peking University\" OR abs:PKU OR abs:USTC OR abs:SJTU OR abs:Princeton OR abs:UCLA OR abs:UCSD OR abs:\"ETH Zurich\" OR abs:NUS OR abs:NTU) AND submittedDate:\"202511010000 TO 202511162359\"&amp;id_list=&amp;start=0&amp;max_results=50</title>\n  <updated>2025-11-20T09:28:14Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=(ti:agent+OR+(ti:%22tool+use%22+OR+(ti:tool-augmented+OR+(ti:planning+OR+(ti:multi-agent+OR+(ti:autonomous+OR+(ti:toolformer+OR+(ti:%22tool+learning%22+OR+(abs:agent+OR+(abs:%22tool+use%22+OR+(abs:tool-augmented+OR+(abs:planning+OR+(abs:multi-agent+OR+(abs:autonomous+OR+(abs:toolformer+OR+abs:%22tool+learning%22)))))))))))))))+AND+((cat:cs.AI+OR+(cat:cs.LG+OR+cat:cs.MA))+AND+((ti:Google+OR+(ti:DeepMind+OR+(ti:Microsoft+OR+(ti:OpenAI+OR+(ti:Meta+OR+(ti:Apple+OR+(ti:Stanford+OR+(ti:MIT+OR+(ti:CMU+OR+(ti:Berkeley+OR+(ti:Oxford+OR+(ti:Harvard+OR+(ti:Tsinghua+OR+(ti:%22Peking+University%22+OR+(ti:PKU+OR+(ti:USTC+OR+(ti:SJTU+OR+(ti:Princeton+OR+(ti:UCLA+OR+(ti:UCSD+OR+(ti:%22ETH+Zurich%22+OR+(ti:NUS+OR+(ti:NTU+OR+(abs:Google+OR+(abs:DeepMind+OR+(abs:Microsoft+OR+(abs:OpenAI+OR+(abs:Meta+OR+(abs:Apple+OR+(abs:Stanford+OR+(abs:MIT+OR+(abs:CMU+OR+(abs:Berkeley+OR+(abs:Oxford+OR+(abs:Harvard+OR+(abs:Tsinghua+OR+(abs:%22Peking+University%22+OR+(abs:PKU+OR+(abs:USTC+OR+(abs:SJTU+OR+(abs:Princeton+OR+(abs:UCLA+OR+(abs:UCSD+OR+(abs:%22ETH+Zurich%22+OR+(abs:NUS+OR+abs:NTU)))))))))))))))))))))))))))))))))))))))))))))+AND+submittedDate:%22202511010000+TO+202511162359%22))&amp;start=0&amp;max_results=50&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>50</opensearch:itemsPerPage>\n  <opensearch:totalResults>27</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2511.12712v1</id>\n    <title>Adaptive Focus Memory for Language Models</title>\n    <updated>2025-11-16T17:52:32Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.12712v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.12712v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, but their behavior is still bottlenecked by fixed context windows and naive memory strategies. Replaying the full conversation at every turn is simple but expensive, while static summarization or recency-only heuristics often erase safety-critical user details. We present Adaptive Focus Memory (AFM), a dynamic context manager that assigns each past message one of three fidelity levels -- FULL, COMPRESSED, or PLACEHOLDER -- based on semantic similarity to the current query, half-life recency weighting, and importance classification. AFM packs messages chronologically under a strict token budget, preferring high fidelity for the most relevant turns while aiming to preserve a cheap trace of the dialogue. In a safety-oriented benchmark involving a user with a severe peanut allergy planning a trip to Thailand, AFM retains the allergy across both short and medium-length conversations, matches the safety performance of naive replay, and cuts average token usage by 66% relative to a replay baseline. We release a modular Python implementation of AFM designed for OpenAI-compatible APIs and offline operation, enabling practitioners to reduce inference cost without sacrificing safety or factual continuity in the evaluated scenario.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-16T17:52:32Z</published>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Christopher Cruz</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.12635v1</id>\n    <title>LLM4SCREENLIT: Recommendations on Assessing the Performance of Large Language Models for Screening Literature in Systematic Reviews</title>\n    <updated>2025-11-16T15:04:50Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.12635v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.12635v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Context: Large language models (LLMs) are released faster than users' ability to evaluate them rigorously. When LLMs underpin research, such as identifying relevant literature for systematic reviews (SRs), robust empirical assessment is essential. Objective: We identify and discuss key challenges in assessing LLM performance for selecting relevant literature, identify good (evaluation) practices, and propose recommendations. Method: Using a recent large-scale study as an example, we identify problems with the use of traditional metrics for assessing the performance of Gen-AI tools for identifying relevant literature in SRs. We analyzed 27 additional papers investigating this issue, extracted the performance metrics, and found both good practices and widespread problems, especially with the use and reporting of performance metrics for SR screening. Results: Major weaknesses included: i) a failure to use metrics that are robust to imbalanced data and do not directly indicate whether results are better than chance, e.g., the use of Accuracy, ii) a failure to consider the impact of lost evidence when making claims concerning workload savings, and iii) pervasive failure to report the full confusion matrix (or performance metrics from which it can be reconstructed) which is essential for future meta-analyses. On the positive side, we extract good (evaluation) practices on which our recommendations for researchers and practitioners, as well as policymakers, are built. Conclusions: SR screening evaluations should prioritize lost evidence/recall alongside chance-anchored and cost-sensitive Weighted MCC (WMCC) metric, report complete confusion matrices, treat unclassifiable outputs as referred-back positives for assessment, adopt leakage-aware designs with non-LLM baselines and open artifacts, and ground conclusions in cost-benefit analysis where FNs carry higher penalties than FPs.</summary>\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-16T15:04:50Z</published>\n    <arxiv:comment>19 pages, 4 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.SE\"/>\n    <author>\n      <name>Lech Madeyski</name>\n    </author>\n    <author>\n      <name>Barbara Kitchenham</name>\n    </author>\n    <author>\n      <name>Martin Shepperd</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.12319v1</id>\n    <title>Decision and Gender Biases in Large Language Models: A Behavioral Economic Perspective</title>\n    <updated>2025-11-15T18:38:17Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.12319v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.12319v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language models (LLMs) increasingly mediate economic and organisational processes, from automated customer support and recruitment to investment advice and policy analysis. These systems are often assumed to embody rational decision making free from human error; yet they are trained on human language corpora that may embed cognitive and social biases. This study investigates whether advanced LLMs behave as rational agents or whether they reproduce human behavioural tendencies when faced with classic decision problems. Using two canonical experiments in behavioural economics, the ultimatum game and a gambling game, we elicit decisions from two state of the art models, Google Gemma7B and Qwen, under neutral and gender conditioned prompts. We estimate parameters of inequity aversion and loss-aversion and compare them with human benchmarks. The models display attenuated but persistent deviations from rationality, including moderate fairness concerns, mild loss aversion, and subtle gender conditioned differences.</summary>\n    <category term=\"econ.GN\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-15T18:38:17Z</published>\n    <arxiv:primary_category term=\"econ.GN\"/>\n    <author>\n      <name>Luca Corazzini</name>\n    </author>\n    <author>\n      <name>Elisa Deriu</name>\n    </author>\n    <author>\n      <name>Marco Guerzoni</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.11519v1</id>\n    <title>Experience-Guided Adaptation of Inference-Time Reasoning Strategies</title>\n    <updated>2025-11-14T17:45:28Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.11519v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.11519v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-14T17:45:28Z</published>\n    <arxiv:comment>29 pages, 5 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Adam Stein</name>\n    </author>\n    <author>\n      <name>Matthew Trager</name>\n    </author>\n    <author>\n      <name>Benjamin Bowman</name>\n    </author>\n    <author>\n      <name>Michael Kleinman</name>\n    </author>\n    <author>\n      <name>Aditya Chattopadhyay</name>\n    </author>\n    <author>\n      <name>Wei Xia</name>\n    </author>\n    <author>\n      <name>Stefano Soatto</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.11083v3</id>\n    <title>Efficient Reinforcement Learning for Zero-Shot Coordination in Evolving Games</title>\n    <updated>2025-11-18T10:20:13Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.11083v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.11083v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Zero-shot coordination(ZSC), a key challenge in multi-agent game theory, has become a hot topic in reinforcement learning (RL) research recently, especially in complex evolving games. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators from a diverse, potentially evolving, pool of partners that are not seen before without any fine-tuning. Population-based training, which approximates such an evolving partner pool, has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient RL training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi cooperative game and confirms its superiority.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-14T08:59:22Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Bingyu Hui</name>\n    </author>\n    <author>\n      <name>Lebin Yu</name>\n    </author>\n    <author>\n      <name>Quanming Yao</name>\n    </author>\n    <author>\n      <name>Yunpeng Qu</name>\n    </author>\n    <author>\n      <name>Xudong Zhang</name>\n    </author>\n    <author>\n      <name>Jian Wang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.10949v1</id>\n    <title>Exposing Weak Links in Multi-Agent Systems under Adversarial Prompting</title>\n    <updated>2025-11-14T04:22:49Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.10949v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.10949v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>LLM-based agents are increasingly deployed in multi-agent systems (MAS). As these systems move toward real-world applications, their security becomes paramount. Existing research largely evaluates single-agent security, leaving a critical gap in understanding the vulnerabilities introduced by multi-agent design. However, existing systems fall short due to lack of unified frameworks and metrics focusing on unique rejection modes in MAS. We present SafeAgents, a unified and extensible framework for fine-grained security assessment of MAS. SafeAgents systematically exposes how design choices such as plan construction strategies, inter-agent context sharing, and fallback behaviors affect susceptibility to adversarial prompting. We introduce Dharma, a diagnostic measure that helps identify weak links within multi-agent pipelines. Using SafeAgents, we conduct a comprehensive study across five widely adopted multi-agent architectures (centralized, decentralized, and hybrid variants) on four datasets spanning web tasks, tool use, and code generation. Our findings reveal that common design patterns carry significant vulnerabilities. For example, centralized systems that delegate only atomic instructions to sub-agents obscure harmful objectives, reducing robustness. Our results highlight the need for security-aware design in MAS. Link to code is https://github.com/microsoft/SafeAgents</summary>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-14T04:22:49Z</published>\n    <arxiv:comment>10 pages, 3 figures. Code available at https://github.com/microsoft/SafeAgents</arxiv:comment>\n    <arxiv:primary_category term=\"cs.MA\"/>\n    <author>\n      <name>Nirmit Arora</name>\n    </author>\n    <author>\n      <name>Sathvik Joel</name>\n    </author>\n    <author>\n      <name>Ishan Kavathekar</name>\n    </author>\n    <author>\n      <name> Palak</name>\n    </author>\n    <author>\n      <name>Rohan Gandhi</name>\n    </author>\n    <author>\n      <name>Yash Pandya</name>\n    </author>\n    <author>\n      <name>Tanuja Ganu</name>\n    </author>\n    <author>\n      <name>Aditya Kanade</name>\n    </author>\n    <author>\n      <name>Akshay Nambi</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.10002v2</id>\n    <title>PustakAI: Curriculum-Aligned and Interactive Textbooks Using Large Language Models</title>\n    <updated>2025-11-14T07:47:21Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.10002v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.10002v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Models (LLMs) have demonstrated remarkable capabilities in understanding and generating human-like content. This has revolutionized various sectors such as healthcare, software development, and education. In education, LLMs offer potential for personalized and interactive learning experiences, especially in regions with limited teaching resources. However, adapting these models effectively to curriculum-specific content, such as the National Council of Educational Research and Training (NCERT) syllabus in India, presents unique challenges in terms of accuracy, alignment, and pedagogical relevance. In this paper, we present the framework \"PustakAI\"\\footnote{Pustak means `book' in many Indian languages.} for the design and evaluation of a novel question-answering dataset \"NCERT-QA\" aligned with the NCERT curriculum for English and Science subjects of grades 6 to 8. We classify the curated QA pairs as Factoid, Inferential, and Others (evaluative and reasoning). We evaluate the dataset with various prompting techniques, such as meta-prompt, few-shot, and CoT-style prompting, using diverse evaluation metrics to understand which approach aligns more efficiently with the structure and demands of the curriculum. Along with the usability of the dataset, we analyze the strengths and limitations of current open-source LLMs (Gemma3:1b, Llama3.2:3b, and Nemotron-mini:4b) and high-end LLMs (Llama-4-Scout-17B and Deepseek-r1-70B) as AI-based learning tools in formal education systems.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-13T06:12:12Z</published>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Shivam Sharma</name>\n      <arxiv:affiliation>CSIS Department, BITS Pilani K K Birla Goa Campus, India</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Riya Naik</name>\n      <arxiv:affiliation>CSIS Department, BITS Pilani K K Birla Goa Campus, India</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Tejas Gawas</name>\n      <arxiv:affiliation>CSIS Department, BITS Pilani K K Birla Goa Campus, India</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Heramb Patil</name>\n      <arxiv:affiliation>CSIS Department, BITS Pilani K K Birla Goa Campus, India</arxiv:affiliation>\n    </author>\n    <author>\n      <name>Kunal Korgaonkar</name>\n      <arxiv:affiliation>CSIS Department, BITS Pilani K K Birla Goa Campus, India</arxiv:affiliation>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.09450v1</id>\n    <title>How does the Performance of the Data-driven Traffic Flow Forecasting Models deteriorate with Increasing Forecasting Horizon? An Extensive Approach Considering Statistical, Machine Learning and Deep Learning Models</title>\n    <updated>2025-11-12T16:06:35Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.09450v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.09450v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>With rapid urbanization in recent decades, traffic congestion has intensified due to increased movement of people and goods. As planning shifts from demand-based to supply-oriented strategies, Intelligent Transportation Systems (ITS) have become essential for managing traffic within existing infrastructure. A core ITS function is traffic forecasting, enabling proactive measures like ramp metering, signal control, and dynamic routing through platforms such as Google Maps. This study assesses the performance of statistical, machine learning (ML), and deep learning (DL) models in forecasting traffic speed and flow using real-world data from California's Harbor Freeway, sourced from the Caltrans Performance Measurement System (PeMS). Each model was evaluated over 20 forecasting windows (up to 1 hour 40 minutes) using RMSE, MAE, and R-Square metrics. Results show ANFIS-GP performs best at early windows with RMSE of 0.038, MAE of 0.0276, and R-Square of 0.9983, while Bi-LSTM is more robust for medium-term prediction due to its capacity to model long-range temporal dependencies, achieving RMSE of 0.1863, MAE of 0.0833, and R-Square of 0.987 at a forecasting of 20. The degradation in model performance was quantified using logarithmic transformation, with slope values used to measure robustness. Among DL models, Bi-LSTM had the flattest slope (0.0454 RMSE, 0.0545 MAE for flow), whereas ANFIS-GP had 0.1058 for RMSE and 0.1037 for flow MAE. The study concludes by identifying hybrid models as a promising future direction.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-12T16:06:35Z</published>\n    <arxiv:comment>6,227 words text + 2*250 (2 tables) = 6,727 words</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Amanta Sherfenaz</name>\n    </author>\n    <author>\n      <name>Nazmul Haque</name>\n    </author>\n    <author>\n      <name>Protiva Sadhukhan Prova</name>\n    </author>\n    <author>\n      <name>Md Asif Raihan</name>\n    </author>\n    <author>\n      <name>Md. Hadiuzzaman</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.11671v1</id>\n    <title>Evaluation of LLM-based Explanations for a Learning Analytics Dashboard</title>\n    <updated>2025-11-11T19:36:40Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.11671v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.11671v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Learning Analytics Dashboards can be a powerful tool to support self-regulated learning in Digital Learning Environments and promote development of meta-cognitive skills, such as reflection. However, their effectiveness can be affected by the interpretability of the data they provide. To assist in the interpretation, we employ a large language model to generate verbal explanations of the data in the dashboard and evaluate it against a standalone dashboard and explanations provided by human teachers in an expert study with university level educators (N=12). We find that the LLM-based explanations of the skill state presented in the dashboard, as well as general recommendations on how to proceed with learning within the course are significantly more favored compared to the other conditions. This indicates that using LLMs for interpretation purposes can enhance the learning experience for learners while maintaining the pedagogical standards approved by teachers.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-11T19:36:40Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Alina Deriyeva</name>\n    </author>\n    <author>\n      <name>Benjamin Paassen</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.07904v2</id>\n    <title>Test-driven Reinforcement Learning</title>\n    <updated>2025-11-15T04:28:51Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.07904v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.07904v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-11T06:58:52Z</published>\n    <arxiv:comment>AAAI 2026 oral</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Zhao Yu</name>\n    </author>\n    <author>\n      <name>Xiuping Wu</name>\n    </author>\n    <author>\n      <name>Liangjun Ke</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.10680v1</id>\n    <title>LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices</title>\n    <updated>2025-11-11T03:55:06Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.10680v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.10680v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-11T03:55:06Z</published>\n    <arxiv:comment>27 pages, in French language. 10 tables, 26 references. Submitted to Energy and AI</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Jean-Philippe Lignier</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.07685v1</id>\n    <title>ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</title>\n    <updated>2025-11-10T23:07:14Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.07685v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.07685v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-10T23:07:14Z</published>\n    <arxiv:comment>27 pages, 21 figures, pre-print</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Manasi Sharma</name>\n    </author>\n    <author>\n      <name>Chen Bo Calvin Zhang</name>\n    </author>\n    <author>\n      <name>Chaithanya Bandi</name>\n    </author>\n    <author>\n      <name>Clinton Wang</name>\n    </author>\n    <author>\n      <name>Ankit Aich</name>\n    </author>\n    <author>\n      <name>Huy Nghiem</name>\n    </author>\n    <author>\n      <name>Tahseen Rabbani</name>\n    </author>\n    <author>\n      <name>Ye Htet</name>\n    </author>\n    <author>\n      <name>Brian Jang</name>\n    </author>\n    <author>\n      <name>Sumana Basu</name>\n    </author>\n    <author>\n      <name>Aishwarya Balwani</name>\n    </author>\n    <author>\n      <name>Denis Peskoff</name>\n    </author>\n    <author>\n      <name>Marcos Ayestaran</name>\n    </author>\n    <author>\n      <name>Sean M. Hendryx</name>\n    </author>\n    <author>\n      <name>Brad Kenstler</name>\n    </author>\n    <author>\n      <name>Bing Liu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.07090v4</id>\n    <title>Green AI: A systematic review and meta-analysis of its definitions, lifecycle models, hardware and measurement attempts</title>\n    <updated>2025-11-13T11:33:02Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.07090v4\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.07090v4\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Across the Artificial Intelligence (AI) lifecycle - from hardware to development, deployment, and reuse - burdens span energy, carbon, water, and embodied impacts. Cloud provider tools improve transparency but remain heterogeneous and often omit water and value chain effects, limiting comparability and reproducibility. Addressing these multi dimensional burdens requires a lifecycle approach linking phase explicit mapping with system levers (hardware, placement, energy mix, cooling, scheduling) and calibrated measurement across facility, system, device, and workload levels. This article (i) establishes a unified, operational definition of Green AI distinct from Sustainable AI; (ii) formalizes a five phase lifecycle mapped to Life Cycle Assessment (LCA) stages, making energy, carbon, water, and embodied impacts first class; (iii) specifies governance via Plan Do Check Act (PDCA) cycles with decision gateways; (iv) systematizes hardware and system level strategies across the edge cloud continuum to reduce embodied burdens; and (v) defines a calibrated measurement framework combining estimator models with direct metering to enable reproducible, provider agnostic comparisons. Combining definition, lifecycle processes, hardware strategies, and calibrated measurement, this article offers actionable, evidence based guidance for researchers, practitioners, and policymakers.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-10T13:26:06Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Marcel Rojahn</name>\n    </author>\n    <author>\n      <name>Marcus Grum</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.04481v1</id>\n    <title>Promoting Sustainable Web Agents: Benchmarking and Estimating Energy Consumption through Empirical and Theoretical Analysis</title>\n    <updated>2025-11-06T15:59:59Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.04481v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.04481v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Web agents, like OpenAI's Operator and Google's Project Mariner, are powerful agentic systems pushing the boundaries of Large Language Models (LLM). They can autonomously interact with the internet at the user's behest, such as navigating websites, filling search masks, and comparing price lists. Though web agent research is thriving, induced sustainability issues remain largely unexplored. To highlight the urgency of this issue, we provide an initial exploration of the energy and $CO_2$ cost associated with web agents from both a theoretical -via estimation- and an empirical perspective -by benchmarking. Our results show how different philosophies in web agent creation can severely impact the associated expended energy, and that more energy consumed does not necessarily equate to better results. We highlight a lack of transparency regarding disclosing model parameters and processes used for some web agents as a limiting factor when estimating energy consumption. Our work contributes towards a change in thinking of how we evaluate web agents, advocating for dedicated metrics measuring energy consumption in benchmarks.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-06T15:59:59Z</published>\n    <arxiv:comment>Accepted by AAAI 2026 AISI</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Lars Krupp</name>\n    </author>\n    <author>\n      <name>Daniel Geißler</name>\n    </author>\n    <author>\n      <name>Vishal Banwari</name>\n    </author>\n    <author>\n      <name>Paul Lukowicz</name>\n    </author>\n    <author>\n      <name>Jakob Karolus</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.03958v1</id>\n    <title>Multi-Agent Collaborative Framework For Math Problem Generation</title>\n    <updated>2025-11-06T01:24:07Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.03958v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.03958v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Automatic question generation (AQG) for mathematics education remains an elusive goal for Intelligent Tutoring Systems and educators. While pre-trained transformer-based language models have significantly advanced natural language generation, they often struggle to precisely control problem complexity and cognitive demands. In this paper, we introduce a collaborative multi-agent framework as a novel method of incorporating inference-time computation into AQG. This approach leverages multiple agents that iteratively refine generated question-answer pairs to better balance complexity and cognitive demand. We evaluate the generated questions on five meta-evaluation criteria: relevance, importance, clarity, difficulty matching, answerability, to assess the system's ability to control the required complexity and quality of the questions. Preliminary evaluations show that this collaborative multi-agent framework elevates the quality of generated educational content by fostering a more nuanced balance between cognitive challenge and clarity. These promising outcomes suggest that integrating collaborative multi-agent workflows can yield more controlled, pedagogically valuable content that can help advance automated educational content generation and adaptive learning environments.</summary>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-06T01:24:07Z</published>\n    <arxiv:comment>Published in the Proceedings of the 18th International Conference on Educational Data Mining, 6 pages, 5 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.MA\"/>\n    <arxiv:journal_ref>Kia Karbasi, Kevin Hong, Mohammad Amin Samadi, &amp; Gregory Pottie. (2025). Multi-Agent Collaborative Framework For Math Problem Generation. Proceedings of the 18th International Conference on Educational Data Mining, 613--618</arxiv:journal_ref>\n    <author>\n      <name>Kia Karbasi</name>\n    </author>\n    <author>\n      <name>Kevin Hong</name>\n    </author>\n    <author>\n      <name>Mohammad Amin Samadi</name>\n    </author>\n    <author>\n      <name>Gregory Pottie</name>\n    </author>\n    <arxiv:doi>10.5281/zenodo.15870246</arxiv:doi>\n    <link rel=\"related\" href=\"https://doi.org/10.5281/zenodo.15870246\" title=\"doi\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.03690v1</id>\n    <title>The OpenHands Software Agent SDK: A Composable and Extensible Foundation for Production Agents</title>\n    <updated>2025-11-05T18:16:44Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.03690v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.03690v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Agents are now used widely in the process of software development, but building production-ready software engineering agents is a complex task. Deploying software agents effectively requires flexibility in implementation and experimentation, reliable and secure execution, and interfaces for users to interact with agents. In this paper, we present the OpenHands Software Agent SDK, a toolkit for implementing software development agents that satisfy these desiderata. This toolkit is a complete architectural redesign of the agent components of the popular OpenHands framework for software development agents, which has 64k+ GitHub stars. To achieve flexibility, we design a simple interface for implementing agents that requires only a few lines of code in the default case, but is easily extensible to more complex, full-featured agents with features such as custom tools, memory management, and more. For security and reliability, it delivers seamless local-to-remote execution portability, integrated REST/WebSocket services. For interaction with human users, it can connect directly to a variety of interfaces, such as visual workspaces (VS Code, VNC, browser), command-line interfaces, and APIs. Compared with existing SDKs from OpenAI, Claude, and Google, OpenHands uniquely integrates native sandboxed execution, lifecycle control, model-agnostic multi-LLM routing, and built-in security analysis. Empirical results on SWE-Bench Verified and GAIA benchmarks demonstrate strong performance. Put together, these elements allow the OpenHands Software Agent SDK to provide a practical foundation for prototyping, unlocking new classes of custom applications, and reliably deploying agents at scale.</summary>\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-05T18:16:44Z</published>\n    <arxiv:primary_category term=\"cs.SE\"/>\n    <author>\n      <name>Xingyao Wang</name>\n    </author>\n    <author>\n      <name>Simon Rosenberg</name>\n    </author>\n    <author>\n      <name>Juan Michelini</name>\n    </author>\n    <author>\n      <name>Calvin Smith</name>\n    </author>\n    <author>\n      <name>Hoang Tran</name>\n    </author>\n    <author>\n      <name>Engel Nyst</name>\n    </author>\n    <author>\n      <name>Rohit Malhotra</name>\n    </author>\n    <author>\n      <name>Xuhui Zhou</name>\n    </author>\n    <author>\n      <name>Valerie Chen</name>\n    </author>\n    <author>\n      <name>Robert Brennan</name>\n    </author>\n    <author>\n      <name>Graham Neubig</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.03497v1</id>\n    <title>ROSBag MCP Server: Analyzing Robot Data with LLMs for Agentic Embodied AI Applications</title>\n    <updated>2025-11-05T14:27:58Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.03497v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.03497v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Agentic AI systems and Physical or Embodied AI systems have been two key research verticals at the forefront of Artificial Intelligence and Robotics, with Model Context Protocol (MCP) increasingly becoming a key component and enabler of agentic applications. However, the literature at the intersection of these verticals, i.e., Agentic Embodied AI, remains scarce. This paper introduces an MCP server for analyzing ROS and ROS 2 bags, allowing for analyzing, visualizing and processing robot data with natural language through LLMs and VLMs. We describe specific tooling built with robotics domain knowledge, with our initial release focused on mobile robotics and supporting natively the analysis of trajectories, laser scan data, transforms, or time series data. This is in addition to providing an interface to standard ROS 2 CLI tools (\"ros2 bag list\" or \"ros2 bag info\"), as well as the ability to filter bags with a subset of topics or trimmed in time. Coupled with the MCP server, we provide a lightweight UI that allows the benchmarking of the tooling with different LLMs, both proprietary (Anthropic, OpenAI) and open-source (through Groq). Our experimental results include the analysis of tool calling capabilities of eight different state-of-the-art LLM/VLM models, both proprietary and open-source, large and small. Our experiments indicate that there is a large divide in tool calling capabilities, with Kimi K2 and Claude Sonnet 4 demonstrating clearly superior performance. We also conclude that there are multiple factors affecting the success rates, from the tool description schema to the number of arguments, as well as the number of tools available to the models. The code is available with a permissive license at https://github.com/binabik-ai/mcp-rosbags.</summary>\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.SE\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-05T14:27:58Z</published>\n    <arxiv:primary_category term=\"cs.RO\"/>\n    <author>\n      <name>Lei Fu</name>\n    </author>\n    <author>\n      <name>Sahar Salimpour</name>\n    </author>\n    <author>\n      <name>Leonardo Militano</name>\n    </author>\n    <author>\n      <name>Harry Edelman</name>\n    </author>\n    <author>\n      <name>Jorge Peña Queralta</name>\n    </author>\n    <author>\n      <name>Giovanni Toffetti</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.03434v1</id>\n    <title>Inter-Agent Trust Models: A Comparative Study of Brief, Claim, Proof, Stake, Reputation and Constraint in Agentic Web Protocol Design-A2A, AP2, ERC-8004, and Beyond</title>\n    <updated>2025-11-05T12:50:06Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.03434v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.03434v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>As the \"agentic web\" takes shape-billions of AI agents (often LLM-powered) autonomously transacting and collaborating-trust shifts from human oversight to protocol design. In 2025, several inter-agent protocols crystallized this shift, including Google's Agent-to-Agent (A2A), Agent Payments Protocol (AP2), and Ethereum's ERC-8004 \"Trustless Agents,\" yet their underlying trust assumptions remain under-examined. This paper presents a comparative study of trust models in inter-agent protocol design: Brief (self- or third-party verifiable claims), Claim (self-proclaimed capabilities and identity, e.g. AgentCard), Proof (cryptographic verification, including zero-knowledge proofs and trusted execution environment attestations), Stake (bonded collateral with slashing and insurance), Reputation (crowd feedback and graph-based trust signals), and Constraint (sandboxing and capability bounding). For each, we analyze assumptions, attack surfaces, and design trade-offs, with particular emphasis on LLM-specific fragilities-prompt injection, sycophancy/nudge-susceptibility, hallucination, deception, and misalignment-that render purely reputational or claim-only approaches brittle. Our findings indicate no single mechanism suffices. We argue for trustless-by-default architectures anchored in Proof and Stake to gate high-impact actions, augmented by Brief for identity and discovery and Reputation overlays for flexibility and social signals. We comparatively evaluate A2A, AP2, ERC-8004 and related historical variations in academic research under metrics spanning security, privacy, latency/cost, and social robustness (Sybil/collusion/whitewashing resistance). We conclude with hybrid trust model recommendations that mitigate reputation gaming and misinformed LLM behavior, and we distill actionable design guidelines for safer, interoperable, and scalable agent economies.</summary>\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.NI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.SI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-05T12:50:06Z</published>\n    <arxiv:comment>Submitted to AAAI 2026 Workshop on Trust and Control in Agentic AI (TrustAgent)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.HC\"/>\n    <author>\n      <name>Botao 'Amber' Hu</name>\n    </author>\n    <author>\n      <name>Helena Rong</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.02781v1</id>\n    <title>Measuring AI Diffusion: A Population-Normalized Metric for Tracking Global AI Usage</title>\n    <updated>2025-11-04T18:03:51Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.02781v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.02781v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Measuring global AI diffusion remains challenging due to a lack of population-normalized, cross-country usage data. We introduce AI User Share, a novel indicator that estimates the share of each country's working-age population actively using AI tools. Built from anonymized Microsoft telemetry and adjusted for device access and mobile scaling, this metric spans 147 economies and provides consistent, real-time insight into global AI diffusion. We find wide variation in adoption, with a strong correlation between AI User Share and GDP. High uptake is concentrated in developed economies, though usage among internet-connected populations in lower-income countries reveals substantial latent demand. We also detect sharp increases in usage following major product launches, such as DeepSeek in early 2025. While the metric's reliance solely on Microsoft telemetry introduces potential biases related to this user base, it offers an important new lens into how AI is spreading globally. AI User Share enables timely benchmarking that can inform data-driven AI policy.</summary>\n    <category term=\"cs.CY\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-04T18:03:51Z</published>\n    <arxiv:comment>18 pages, 6 figures, 2 tables. Also available at https://aka.ms/AI_Diffusion_Technical_Report</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CY\"/>\n    <author>\n      <name>Amit Misra</name>\n    </author>\n    <author>\n      <name>Jane Wang</name>\n    </author>\n    <author>\n      <name>Scott McCullers</name>\n    </author>\n    <author>\n      <name>Kevin White</name>\n    </author>\n    <author>\n      <name>Juan Lavista Ferres</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.02606v1</id>\n    <title>A Multi-Agent Psychological Simulation System for Human Behavior Modeling</title>\n    <updated>2025-11-04T14:28:03Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.02606v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.02606v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Training and education in human-centered fields require authentic practice, yet realistic simulations of human behavior have remained limited. We present a multi-agent psychological simulation system that models internal cognitive-affective processes to generate believable human behaviors. In contrast to black-box neural models, this system is grounded in established psychological theories (e.g., self-efficacy, mindset, social constructivism) and explicitly simulates an ``inner parliament'' of agents corresponding to key psychological factors. These agents deliberate and interact to determine the system's output behavior, enabling unprecedented transparency and alignment with human psychology. We describe the system's architecture and theoretical foundations, illustrate its use in teacher training and research, and discuss how it embodies principles of social learning, cognitive apprenticeship, deliberate practice, and meta-cognition.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-04T14:28:03Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Xiangen Hu</name>\n    </author>\n    <author>\n      <name>Jiarui Tong</name>\n    </author>\n    <author>\n      <name>Sheng Xu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.02560v1</id>\n    <title>SigmaCollab: An Application-Driven Dataset for Physically Situated Collaboration</title>\n    <updated>2025-11-04T13:30:15Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.02560v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.02560v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>We introduce SigmaCollab, a dataset enabling research on physically situated human-AI collaboration. The dataset consists of a set of 85 sessions in which untrained participants were guided by a mixed-reality assistive AI agent in performing procedural tasks in the physical world. SigmaCollab includes a set of rich, multimodal data streams, such as the participant and system audio, egocentric camera views from the head-mounted device, depth maps, head, hand and gaze tracking information, as well as additional annotations performed post-hoc. While the dataset is relatively small in size (~ 14 hours), its application-driven and interactive nature brings to the fore novel research challenges for human-AI collaboration, and provides more realistic testing grounds for various AI models operating in this space. In future work, we plan to use the dataset to construct a set of benchmarks for physically situated collaboration in mixed-reality task assistive scenarios. SigmaCollab is available at https://github.com/microsoft/SigmaCollab.</summary>\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-04T13:30:15Z</published>\n    <arxiv:primary_category term=\"cs.HC\"/>\n    <author>\n      <name>Dan Bohus</name>\n    </author>\n    <author>\n      <name>Sean Andrist</name>\n    </author>\n    <author>\n      <name>Ann Paradiso</name>\n    </author>\n    <author>\n      <name>Nick Saw</name>\n    </author>\n    <author>\n      <name>Tim Schoonbeek</name>\n    </author>\n    <author>\n      <name>Maia Stiber</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.02303v1</id>\n    <title>Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation</title>\n    <updated>2025-11-04T06:37:31Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.02303v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.02303v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large Language Models (LLMs) trained with reinforcement learning and verifiable rewards have achieved strong results on complex reasoning tasks. Recent work extends this paradigm to a multi-agent setting, where a meta-thinking agent proposes plans and monitors progress while a reasoning agent executes subtasks through sequential conversational turns. Despite promising performance, we identify a critical limitation: lazy agent behavior, in which one agent dominates while the other contributes little, undermining collaboration and collapsing the setup to an ineffective single agent. In this paper, we first provide a theoretical analysis showing why lazy behavior naturally arises in multi-agent reasoning. We then introduce a stable and efficient method for measuring causal influence, helping mitigate this issue. Finally, as collaboration intensifies, the reasoning agent risks getting lost in multi-turn interactions and trapped by previous noisy responses. To counter this, we propose a verifiable reward mechanism that encourages deliberation by allowing the reasoning agent to discard noisy outputs, consolidate instructions, and restart its reasoning process when necessary. Extensive experiments demonstrate that our framework alleviates lazy agent behavior and unlocks the full potential of multi-agent framework for complex reasoning tasks.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-04T06:37:31Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Zhiwei Zhang</name>\n    </author>\n    <author>\n      <name>Xiaomin Li</name>\n    </author>\n    <author>\n      <name>Yudi Lin</name>\n    </author>\n    <author>\n      <name>Hui Liu</name>\n    </author>\n    <author>\n      <name>Ramraj Chandradevan</name>\n    </author>\n    <author>\n      <name>Linlin Wu</name>\n    </author>\n    <author>\n      <name>Minhua Lin</name>\n    </author>\n    <author>\n      <name>Fali Wang</name>\n    </author>\n    <author>\n      <name>Xianfeng Tang</name>\n    </author>\n    <author>\n      <name>Qi He</name>\n    </author>\n    <author>\n      <name>Suhang Wang</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.01999v1</id>\n    <title>TRACE: Textual Reasoning for Affordance Coordinate Extraction</title>\n    <updated>2025-11-03T19:13:26Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.01999v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.01999v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Vision-Language Models (VLMs) struggle to translate high-level instructions into the precise spatial affordances required for robotic manipulation. While visual Chain-of-Thought (CoT) methods exist, they are often computationally intensive. In this work, we introduce TRACE (Textual Reasoning for Affordance Coordinate Extraction), a novel methodology that integrates a textual Chain of Reasoning (CoR) into the affordance prediction process. We use this methodology to create the TRACE dataset, a large-scale collection created via an autonomous pipeline that pairs instructions with explicit textual rationales. By fine-tuning a VLM on this data, our model learns to externalize its spatial reasoning before acting. Our experiments show that our TRACE-tuned model achieves state-of-the-art performance, reaching 48.1% accuracy on the primary Where2Place (W2P) benchmark (a 9.6% relative improvement) and 55.0% on the more challenging W2P(h) subset. Crucially, an ablation study demonstrates that performance scales directly with the amount of reasoning data used, confirming the CoR's effectiveness. Furthermore, analysis of the model's attention maps reveals an interpretable reasoning process where focus shifts dynamically across reasoning steps. This work shows that training VLMs to generate a textual CoR is an effective and robust strategy for enhancing the precision, reliability, and interpretability of VLM-based robot control. Our dataset and code are available at https://github.com/jink-ucla/TRACE</summary>\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-03T19:13:26Z</published>\n    <arxiv:comment>ICCV 2025. *Equal contribution. †Corresponding author</arxiv:comment>\n    <arxiv:primary_category term=\"cs.RO\"/>\n    <author>\n      <name>Sangyun Park</name>\n    </author>\n    <author>\n      <name>Jin Kim</name>\n    </author>\n    <author>\n      <name>Yuchen Cui</name>\n    </author>\n    <author>\n      <name>Matthew S. Brown</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.01093v1</id>\n    <title>Continual Learning, Not Training: Online Adaptation For Agents</title>\n    <updated>2025-11-02T21:48:31Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.01093v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.01093v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Continual Learning (CL) methods have traditionally focused on mitigating catastrophic forgetting through gradient-based retraining, an approach ill-suited for deployed agents that must adapt in real time. We introduce our Adaptive Teaching and Learning System (ATLAS), a dual-agent architecture that decouples reasoning (Teacher) from execution (Student) and incorporates a persistent learning memory that stores distilled guidance from experience. This informs the orchestration layer, enabling the system to dynamically adjust its operational strategies, such as supervision level or initial plan selection, at inference time. In doing so, ATLAS achieves gradient-free continual learning, shifting the locus of adaptation from model parameters to system-level orchestration. We formulate this as a system-centric paradigm for continual learning, where the objective is adaptive efficiency: maximizing task success while minimizing computational cost through inference-time orchestration rather than parameter updates. Evaluated on Microsoft's ExCyTIn-Bench, an open-source benchmark simulating complex cyberthreat investigation, ATLAS achieves 54.1% success with GPT-5-mini as its Student, outperforming the larger GPT-5 (High) by 13% while reducing cost by 86%. Cross-incident validation demonstrates generalization: frozen pamphlets from Incident #5 improve accuracy from 28% to 41% with zero retraining, while shifting output composition from verbose exploration to structured reasoning. Together, these findings establish gradient-free continual learning as a viable path toward adaptive, deployable AI systems and provide causally annotated traces valuable for training explicit world models.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-02T21:48:31Z</published>\n    <arxiv:comment>12 pages, 4 figures</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Aman Jaglan</name>\n    </author>\n    <author>\n      <name>Jarrod Barnes</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.01078v1</id>\n    <title>Predictive Auxiliary Learning for Belief-based Multi-Agent Systems</title>\n    <updated>2025-11-02T21:05:03Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.01078v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.01078v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The performance of multi-agent reinforcement learning (MARL) in partially observable environments depends on effectively aggregating information from observations, communications, and reward signals. While most existing multi-agent systems primarily rely on rewards as the only feedback for policy training, our research shows that introducing auxiliary predictive tasks can significantly enhance learning efficiency and stability. We propose Belief-based Predictive Auxiliary Learning (BEPAL), a framework that incorporates auxiliary training objectives to support policy optimization. BEPAL follows the centralized training with decentralized execution paradigm. Each agent learns a belief model that predicts unobservable state information, such as other agents' rewards or motion directions, alongside its policy model. By enriching hidden state representations with information that does not directly contribute to immediate reward maximization, this auxiliary learning process stabilizes MARL training and improves overall performance. We evaluate BEPAL in the predator-prey environment and Google Research Football, where it achieves an average improvement of about 16 percent in performance metrics and demonstrates more stable convergence compared to baseline methods.</summary>\n    <category term=\"cs.MA\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-02T21:05:03Z</published>\n    <arxiv:primary_category term=\"cs.MA\"/>\n    <author>\n      <name>Qinwei Huang</name>\n    </author>\n    <author>\n      <name>Stefan Wang</name>\n    </author>\n    <author>\n      <name>Simon Khan</name>\n    </author>\n    <author>\n      <name>Garrett Katz</name>\n    </author>\n    <author>\n      <name>Qinru Qiu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.01006v1</id>\n    <title>None To Optima in Few Shots: Bayesian Optimization with MDP Priors</title>\n    <updated>2025-11-02T16:53:17Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.01006v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.01006v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Bayesian Optimization (BO) is an efficient tool for optimizing black-box functions, but its theoretical guarantees typically hold in the asymptotic regime. In many critical real-world applications such as drug discovery or materials design, where each evaluation can be very costly and time-consuming, BO becomes impractical for many evaluations. In this paper, we introduce the Procedure-inFormed BO (ProfBO) algorithm, which solves black-box optimization with remarkably few function evaluations. At the heart of our algorithmic design are Markov Decision Process (MDP) priors that model optimization trajectories from related source tasks, thereby capturing procedural knowledge on efficient optimization. We embed these MDP priors into a prior-fitted neural network and employ model-agnostic meta-learning for fast adaptation to new target tasks. Experiments on real-world Covid and Cancer benchmarks and hyperparameter tuning tasks demonstrate that ProfBO consistently outperforms state-of-the-art methods by achieving high-quality solutions with significantly fewer evaluations, making it ready for practical deployment.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-02T16:53:17Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Diantong Li</name>\n    </author>\n    <author>\n      <name>Kyunghyun Cho</name>\n    </author>\n    <author>\n      <name>Chong Liu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.00423v1</id>\n    <title>Bootstrap Off-policy with World Model</title>\n    <updated>2025-11-01T06:33:04Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.00423v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.00423v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Online planning has proven effective in reinforcement learning (RL) for improving sample efficiency and final performance. However, using planning for environment interaction inevitably introduces a divergence between the collected data and the policy's actual behaviors, degrading both model learning and policy improvement. To address this, we propose BOOM (Bootstrap Off-policy with WOrld Model), a framework that tightly integrates planning and off-policy learning through a bootstrap loop: the policy initializes the planner, and the planner refines actions to bootstrap the policy through behavior alignment. This loop is supported by a jointly learned world model, which enables the planner to simulate future trajectories and provides value targets to facilitate policy improvement. The core of BOOM is a likelihood-free alignment loss that bootstraps the policy using the planner's non-parametric action distribution, combined with a soft value-weighted mechanism that prioritizes high-return behaviors and mitigates variability in the planner's action quality within the replay buffer. Experiments on the high-dimensional DeepMind Control Suite and Humanoid-Bench show that BOOM achieves state-of-the-art results in both training stability and final performance. The code is accessible at https://github.com/molumitu/BOOM_MBRL.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.RO\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-01T06:33:04Z</published>\n    <arxiv:comment>NeurIPS 2025</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Guojian Zhan</name>\n    </author>\n    <author>\n      <name>Likun Wang</name>\n    </author>\n    <author>\n      <name>Xiangteng Zhang</name>\n    </author>\n    <author>\n      <name>Jiaxin Gao</name>\n    </author>\n    <author>\n      <name>Masayoshi Tomizuka</name>\n    </author>\n    <author>\n      <name>Shengbo Eben Li</name>\n    </author>\n  </entry>\n</feed>\n"
}