[
  {
    "arxiv_id": "2501.00644v1",
    "entry_id": "http://arxiv.org/abs/2501.00644v1",
    "title": "Efficient Standardization of Clinical Notes using Large Language Models",
    "summary": "Clinician notes are a rich source of patient information but often contain inconsistencies due to varied writing styles, colloquialisms, abbreviations, medical jargon, grammatical errors, and non-standard formatting. These inconsistencies hinder the extraction of meaningful data from electronic health records (EHRs), posing challenges for quality improvement, population health, precision medicine, decision support, and research.\n  We present a large language model approach to standardizing a corpus of 1,618 clinical notes. Standardization corrected an average of $4.9 +/- 1.8$ grammatical errors, $3.3 +/- 5.2$ spelling errors, converted $3.1 +/- 3.0$ non-standard terms to standard terminology, and expanded $15.8 +/- 9.1$ abbreviations and acronyms per note. Additionally, notes were re-organized into canonical sections with standardized headings. This process prepared notes for key concept extraction, mapping to medical ontologies, and conversion to interoperable data formats such as FHIR.\n  Expert review of randomly sampled notes found no significant data loss after standardization. This proof-of-concept study demonstrates that standardization of clinical notes can improve their readability, consistency, and usability, while also facilitating their conversion into interoperable data formats.",
    "authors": [
      "Daniel B. Hier",
      "Michael D. Carrithers",
      "Thanh Son Do",
      "Tayo Obafemi-Ajayi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-31T20:52:40Z",
    "pdf_url": "https://arxiv.org/pdf/2501.00644v1"
  },
  {
    "arxiv_id": "2501.00365v2",
    "entry_id": "http://arxiv.org/abs/2501.00365v2",
    "title": "Low-Rank Adaptation for Foundation Models: A Comprehensive Review",
    "summary": "The rapid advancement of foundation modelslarge-scale neural networks trained on diverse, extensive datasetshas revolutionized artificial intelligence, enabling unprecedented advancements across domains such as natural language processing, computer vision, and scientific discovery. However, the substantial parameter count of these models, often reaching billions or trillions, poses significant challenges in adapting them to specific downstream tasks. Low-Rank Adaptation (LoRA) has emerged as a highly promising approach for mitigating these challenges, offering a parameter-efficient mechanism to fine-tune foundation models with minimal computational overhead. This survey provides the first comprehensive review of LoRA techniques beyond large Language Models to general foundation models, including recent techniques foundations, emerging frontiers and applications of low-rank adaptation across multiple domains. Finally, this survey discusses key challenges and future research directions in theoretical understanding, scalability, and robustness. This survey serves as a valuable resource for researchers and practitioners working with efficient foundation model adaptation.",
    "authors": [
      "Menglin Yang",
      "Jialin Chen",
      "Jinkai Tao",
      "Yifei Zhang",
      "Jiahong Liu",
      "Jiasheng Zhang",
      "Qiyao Ma",
      "Harshit Verma",
      "Regina Zhang",
      "Min Zhou",
      "Irwin King",
      "Rex Ying"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-12-31T09:38:55Z",
    "pdf_url": "https://arxiv.org/pdf/2501.00365v2"
  },
  {
    "arxiv_id": "2501.00223v1",
    "entry_id": "http://arxiv.org/abs/2501.00223v1",
    "title": "CancerKG.ORG A Web-scale, Interactive, Verifiable Knowledge Graph-LLM Hybrid for Assisting with Optimal Cancer Treatment and Care",
    "summary": "Here, we describe one of the first Web-scale hybrid Knowledge Graph (KG)-Large Language Model (LLM), populated with the latest peer-reviewed medical knowledge on colorectal Cancer. It is currently being evaluated to assist with both medical research and clinical information retrieval tasks at Moffitt Cancer Center, which is one of the top Cancer centers in the U.S. and in the world. Our hybrid is remarkable as it serves the user needs better than just an LLM, KG or a search-engine in isolation. LLMs as is are known to exhibit hallucinations and catastrophic forgetting as well as are trained on outdated corpora. The state of the art KGs, such as PrimeKG, cBioPortal, ChEMBL, NCBI, and other require manual curation, hence are quickly getting stale. CancerKG is unsupervised and is capable of automatically ingesting and organizing the latest medical findings. To alleviate the LLMs shortcomings, the verified KG serves as a Retrieval Augmented Generation (RAG) guardrail. CancerKG exhibits 5 different advanced user interfaces, each tailored to serve different data modalities better and more convenient for the user.",
    "authors": [
      "Michael Gubanov",
      "Anna Pyayt",
      "Aleksandra Karolak"
    ],
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-12-31T02:19:16Z",
    "pdf_url": "https://arxiv.org/pdf/2501.00223v1"
  },
  {
    "arxiv_id": "2501.00062v2",
    "entry_id": "http://arxiv.org/abs/2501.00062v2",
    "title": "ELECTRA and GPT-4o: Cost-Effective Partners for Sentiment Analysis",
    "summary": "Bidirectional transformers excel at sentiment analysis, and Large Language Models (LLM) are effective zero-shot learners. Might they perform better as a team? This paper explores collaborative approaches between ELECTRA and GPT-4o for three-way sentiment classification. We fine-tuned (FT) four models (ELECTRA Base/Large, GPT-4o/4o-mini) using a mix of reviews from Stanford Sentiment Treebank (SST) and DynaSent. We provided input from ELECTRA to GPT as: predicted label, probabilities, and retrieved examples. Sharing ELECTRA Base FT predictions with GPT-4o-mini significantly improved performance over either model alone (82.50 macro F1 vs. 79.14 ELECTRA Base FT, 79.41 GPT-4o-mini) and yielded the lowest cost/performance ratio (\\$0.12/F1 point). However, when GPT models were fine-tuned, including predictions decreased performance. GPT-4o FT-M was the top performer (86.99), with GPT-4o-mini FT close behind (86.70) at much less cost (\\$0.38 vs. \\$1.59/F1 point). Our results show that augmenting prompts with predictions from fine-tuned encoders is an efficient way to boost performance, and a fine-tuned GPT-4o-mini is nearly as good as GPT-4o FT at 76% less cost. Both are affordable options for projects with limited resources.",
    "authors": [
      "James P. Beno"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-29T05:29:52Z",
    "pdf_url": "https://arxiv.org/pdf/2501.00062v2"
  },
  {
    "arxiv_id": "2412.20340v2",
    "entry_id": "http://arxiv.org/abs/2412.20340v2",
    "title": "Distilling Desired Comments for Enhanced Code Review with Large Language Models",
    "summary": "There has been a growing interest in using Large Language Models (LLMs) for code review thanks to their proven proficiency in code comprehension. The primary objective of most review scenarios is to generate desired review comments (DRCs) that explicitly identify issues to trigger code fixes. However, existing LLM-based solutions are not so effective in generating DRCs for various reasons such as hallucination. To enhance their code review ability, they need to be fine-tuned with a customized dataset that is ideally full of DRCs. Nevertheless, such a dataset is not yet available, while manual annotation of DRCs is too laborious to be practical. In this paper, we propose a dataset distillation method, Desiview, which can automatically construct a distilled dataset by identifying DRCs from a code review dataset. Experiments on the CodeReviewer dataset comprising more than 150K review entries show that Desiview achieves an impressive performance of 88.93%, 80.37%, 86.67%, and 84.44% in terms of Precision, Recall, Accuracy, and F1, respectively, surpassing state-of-the-art methods. To validate the effect of such a distilled dataset on enhancing LLMs' code review ability, we first fine-tune the latest LLaMA series (i.e., LLaMA 3 and LLaMA 3.1) to build model Desiview4FT. We then enhance the model training effect through KTO alignment by feeding those review comments identified as non-DRCs to the LLMs, resulting in model Desiview4FA. Verification results indicate that Desiview4FA slightly outperforms Desiview4FT, while both models have significantly improved against the base models in terms of generating DRCs. Human evaluation confirms that both models identify issues more accurately and tend to generate review comments that better describe the issues contained in the code than the base LLMs do.",
    "authors": [
      "Yongda Yu",
      "Lei Zhang",
      "Guoping Rong",
      "Haifeng Shen",
      "Jiahao Zhang",
      "Haoxiang Yan",
      "Guohao Shi",
      "Dong Shao",
      "Ruiqi Pan",
      "Yuan Li",
      "Qiushi Wang",
      "Zhao Tian"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-12-29T03:49:13Z",
    "pdf_url": "https://arxiv.org/pdf/2412.20340v2"
  },
  {
    "arxiv_id": "2412.19994v1",
    "entry_id": "http://arxiv.org/abs/2412.19994v1",
    "title": "From Generalist to Specialist: A Survey of Large Language Models for Chemistry",
    "summary": "Large Language Models (LLMs) have significantly transformed our daily life and established a new paradigm in natural language processing (NLP). However, the predominant pretraining of LLMs on extensive web-based texts remains insufficient for advanced scientific discovery, particularly in chemistry. The scarcity of specialized chemistry data, coupled with the complexity of multi-modal data such as 2D graph, 3D structure and spectrum, present distinct challenges. Although several studies have reviewed Pretrained Language Models (PLMs) in chemistry, there is a conspicuous absence of a systematic survey specifically focused on chemistry-oriented LLMs. In this paper, we outline methodologies for incorporating domain-specific chemistry knowledge and multi-modal information into LLMs, we also conceptualize chemistry LLMs as agents using chemistry tools and investigate their potential to accelerate scientific research. Additionally, we conclude the existing benchmarks to evaluate chemistry ability of LLMs. Finally, we critically examine the current challenges and identify promising directions for future research. Through this comprehensive survey, we aim to assist researchers in staying at the forefront of developments in chemistry LLMs and to inspire innovative applications in the field.",
    "authors": [
      "Yang Han",
      "Ziping Wan",
      "Lu Chen",
      "Kai Yu",
      "Xin Chen"
    ],
    "categories": [
      "physics.chem-ph",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-12-28T03:40:25Z",
    "pdf_url": "https://arxiv.org/pdf/2412.19994v1"
  },
  {
    "arxiv_id": "2412.19906v1",
    "entry_id": "http://arxiv.org/abs/2412.19906v1",
    "title": "Evaluate Summarization in Fine-Granularity: Auto Evaluation with LLM",
    "summary": "Due to the exponential growth of information and the need for efficient information consumption the task of summarization has gained paramount importance. Evaluating summarization accurately and objectively presents significant challenges, particularly when dealing with long and unstructured texts rich in content. Existing methods, such as ROUGE (Lin, 2004) and embedding similarities, often yield scores that have low correlation with human judgements and are also not intuitively understandable, making it difficult to gauge the true quality of the summaries. LLMs can mimic human in giving subjective reviews but subjective scores are hard to interpret and justify. They can be easily manipulated by altering the models and the tones of the prompts. In this paper, we introduce a novel evaluation methodology and tooling designed to address these challenges, providing a more comprehensive, accurate and interpretable assessment of summarization outputs. Our method (SumAutoEval) proposes and evaluates metrics at varying granularity levels, giving objective scores on 4 key dimensions such as completeness, correctness, Alignment and readability. We empirically demonstrate, that SumAutoEval enhances the understanding of output quality with better human correlation.",
    "authors": [
      "Dong Yuan",
      "Eti Rastogi",
      "Fen Zhao",
      "Sagar Goyal",
      "Gautam Naik",
      "Sree Prasanna Rajagopal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-27T19:42:25Z",
    "pdf_url": "https://arxiv.org/pdf/2412.19906v1"
  },
  {
    "arxiv_id": "2412.19442v3",
    "entry_id": "http://arxiv.org/abs/2412.19442v3",
    "title": "A Survey on Large Language Model Acceleration based on KV Cache Management",
    "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
    "authors": [
      "Haoyang Li",
      "Yiming Li",
      "Anxin Tian",
      "Tianhao Tang",
      "Zhanchao Xu",
      "Xuejia Chen",
      "Nicole Hu",
      "Wei Dong",
      "Qing Li",
      "Lei Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.DC"
    ],
    "published": "2024-12-27T04:17:57Z",
    "pdf_url": "https://arxiv.org/pdf/2412.19442v3"
  },
  {
    "arxiv_id": "2412.19363v2",
    "entry_id": "http://arxiv.org/abs/2412.19363v2",
    "title": "Large Language Models for Market Research: A Data-augmentation Approach",
    "summary": "Large Language Models (LLMs) have transformed artificial intelligence by excelling in complex natural language processing tasks. Their ability to generate human-like text has opened new possibilities for market research, particularly in conjoint analysis, where understanding consumer preferences is essential but often resource-intensive. Traditional survey-based methods face limitations in scalability and cost, making LLM-generated data a promising alternative. However, while LLMs have the potential to simulate real consumer behavior, recent studies highlight a significant gap between LLM-generated and human data, with biases introduced when substituting between the two. In this paper, we address this gap by proposing a novel statistical data augmentation approach that efficiently integrates LLM-generated data with real data in conjoint analysis. Our method leverages transfer learning principles to debias the LLM-generated data using a small amount of human data. This results in statistically robust estimators with consistent and asymptotically normal properties, in contrast to naive approaches that simply substitute human data with LLM-generated data, which can exacerbate bias. We validate our framework through an empirical study on COVID-19 vaccine preferences, demonstrating its superior ability to reduce estimation error and save data and costs by 24.9% to 79.8%. In contrast, naive approaches fail to save data due to the inherent biases in LLM-generated data compared to human data. Another empirical study on sports car choices validates the robustness of our results. Our findings suggest that while LLM-generated data is not a direct substitute for human responses, it can serve as a valuable complement when used within a robust statistical framework.",
    "authors": [
      "Mengxin Wang",
      "Dennis J. Zhang",
      "Heng Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ME",
      "stat.ML"
    ],
    "published": "2024-12-26T22:06:29Z",
    "pdf_url": "https://arxiv.org/pdf/2412.19363v2"
  },
  {
    "arxiv_id": "2412.19211v1",
    "entry_id": "http://arxiv.org/abs/2412.19211v1",
    "title": "Large Language Models Meet Graph Neural Networks: A Perspective of Graph Mining",
    "summary": "Graph mining is an important area in data mining and machine learning that involves extracting valuable information from graph-structured data. In recent years, significant progress has been made in this field through the development of graph neural networks (GNNs). However, GNNs are still deficient in generalizing to diverse graph data. Aiming to this issue, Large Language Models (LLMs) could provide new solutions for graph mining tasks with their superior semantic understanding. In this review, we systematically review the combination and application techniques of LLMs and GNNs and present a novel taxonomy for research in this interdisciplinary field, which involves three main categories: GNN-driving-LLM, LLM-driving-GNN, and GNN-LLM-co-driving. Within this framework, we reveal the capabilities of LLMs in enhancing graph feature extraction as well as improving the effectiveness of downstream tasks such as node classification, link prediction, and community detection. Although LLMs have demonstrated their great potential in handling graph-structured data, their high computational requirements and complexity remain challenges. Future research needs to continue to explore how to efficiently fuse LLMs and GNNs to achieve more powerful graph learning and reasoning capabilities and provide new impetus for the development of graph mining techniques.",
    "authors": [
      "Yuxin You",
      "Zhen Liu",
      "Xiangchao Wen",
      "Yongtao Zhang",
      "Wei Ai"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-12-26T13:21:09Z",
    "pdf_url": "https://arxiv.org/pdf/2412.19211v1"
  },
  {
    "arxiv_id": "2412.19198v1",
    "entry_id": "http://arxiv.org/abs/2412.19198v1",
    "title": "Multi-Attribute Constraint Satisfaction via Language Model Rewriting",
    "summary": "Obeying precise constraints on top of multiple external attributes is a common computational problem underlying seemingly different domains, from controlled text generation to protein engineering. Existing language model (LM) controllability methods for multi-attribute constraint satisfaction often rely on specialized architectures or gradient-based classifiers, limiting their flexibility to work with arbitrary black-box evaluators and pretrained models. Current general-purpose large language models, while capable, cannot achieve fine-grained multi-attribute control over external attributes. Thus, we create Multi-Attribute Constraint Satisfaction (MACS), a generalized method capable of finetuning language models on any sequential domain to satisfy user-specified constraints on multiple external real-value attributes. Our method trains LMs as editors by sampling diverse multi-attribute edit pairs from an initial set of paraphrased outputs. During inference, LM iteratively improves upon its previous solution to satisfy constraints for all attributes by leveraging our designed constraint satisfaction reward. We additionally experiment with reward-weighted behavior cloning to further improve the constraint satisfaction rate of LMs. To evaluate our approach, we present a new Fine-grained Constraint Satisfaction (FineCS) benchmark, featuring two challenging tasks: (1) Text Style Transfer, where the goal is to simultaneously modify the sentiment and complexity of reviews, and (2) Protein Design, focusing on modulating fluorescence and stability of Green Fluorescent Proteins (GFP). Our empirical results show that MACS achieves the highest threshold satisfaction in both FineCS tasks, outperforming strong domain-specific baselines. Our work opens new avenues for generalized and real-value multi-attribute control, with implications for diverse applications spanning NLP and bioinformatics.",
    "authors": [
      "Ashutosh Baheti",
      "Debanjana Chakraborty",
      "Faeze Brahman",
      "Ronan Le Bras",
      "Ximing Lu",
      "Nouha Dziri",
      "Yejin Choi",
      "Mark Riedl",
      "Maarten Sap"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-26T12:36:39Z",
    "pdf_url": "https://arxiv.org/pdf/2412.19198v1"
  },
  {
    "arxiv_id": "2412.18985v1",
    "entry_id": "http://arxiv.org/abs/2412.18985v1",
    "title": "TravelAgent: Generative Agents in the Built Environment",
    "summary": "Understanding human behavior in built environments is critical for designing functional, user centered urban spaces. Traditional approaches, such as manual observations, surveys, and simplified simulations, often fail to capture the complexity and dynamics of real world behavior. To address these limitations, we introduce TravelAgent, a novel simulation platform that models pedestrian navigation and activity patterns across diverse indoor and outdoor environments under varying contextual and environmental conditions. TravelAgent leverages generative agents integrated into 3D virtual environments, enabling agents to process multimodal sensory inputs and exhibit human-like decision-making, behavior, and adaptation. Through experiments, including navigation, wayfinding, and free exploration, we analyze data from 100 simulations comprising 1898 agent steps across diverse spatial layouts and agent archetypes, achieving an overall task completion rate of 76%. Using spatial, linguistic, and sentiment analyses, we show how agents perceive, adapt to, or struggle with their surroundings and assigned tasks. Our findings highlight the potential of TravelAgent as a tool for urban design, spatial cognition research, and agent-based modeling. We discuss key challenges and opportunities in deploying generative agents for the evaluation and refinement of spatial designs, proposing TravelAgent as a new paradigm for simulating and understanding human experiences in built environments.",
    "authors": [
      "Ariel Noyman",
      "Kai Hu",
      "Kent Larson"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-12-25T21:27:51Z",
    "pdf_url": "https://arxiv.org/pdf/2412.18985v1"
  },
  {
    "arxiv_id": "2412.18688v2",
    "entry_id": "http://arxiv.org/abs/2412.18688v2",
    "title": "Video Is Worth a Thousand Images: Exploring the Latest Trends in Long Video Generation",
    "summary": "An image may convey a thousand words, but a video composed of hundreds or thousands of image frames tells a more intricate story. Despite significant progress in multimodal large language models (MLLMs), generating extended videos remains a formidable challenge. As of this writing, OpenAI's Sora, the current state-of-the-art system, is still limited to producing videos that are up to one minute in length. This limitation stems from the complexity of long video generation, which requires more than generative AI techniques for approximating density functions essential aspects such as planning, story development, and maintaining spatial and temporal consistency present additional hurdles. Integrating generative AI with a divide-and-conquer approach could improve scalability for longer videos while offering greater control. In this survey, we examine the current landscape of long video generation, covering foundational techniques like GANs and diffusion models, video generation strategies, large-scale training datasets, quality metrics for evaluating long videos, and future research areas to address the limitations of the existing video generation capabilities. We believe it would serve as a comprehensive foundation, offering extensive information to guide future advancements and research in the field of long video generation.",
    "authors": [
      "Faraz Waseem",
      "Muhammad Shahzad"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-12-24T21:24:41Z",
    "pdf_url": "https://arxiv.org/pdf/2412.18688v2"
  },
  {
    "arxiv_id": "2412.18298v1",
    "entry_id": "http://arxiv.org/abs/2412.18298v1",
    "title": "Quo Vadis, Anomaly Detection? LLMs and VLMs in the Spotlight",
    "summary": "Video anomaly detection (VAD) has witnessed significant advancements through the integration of large language models (LLMs) and vision-language models (VLMs), addressing critical challenges such as interpretability, temporal reasoning, and generalization in dynamic, open-world scenarios. This paper presents an in-depth review of cutting-edge LLM-/VLM-based methods in 2024, focusing on four key aspects: (i) enhancing interpretability through semantic insights and textual explanations, making visual anomalies more understandable; (ii) capturing intricate temporal relationships to detect and localize dynamic anomalies across video frames; (iii) enabling few-shot and zero-shot detection to minimize reliance on large, annotated datasets; and (iv) addressing open-world and class-agnostic anomalies by using semantic understanding and motion features for spatiotemporal coherence. We highlight their potential to redefine the landscape of VAD. Additionally, we explore the synergy between visual and textual modalities offered by LLMs and VLMs, highlighting their combined strengths and proposing future directions to fully exploit the potential in enhancing video anomaly detection.",
    "authors": [
      "Xi Ding",
      "Lei Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-12-24T09:05:37Z",
    "pdf_url": "https://arxiv.org/pdf/2412.18298v1"
  },
  {
    "arxiv_id": "2412.18291v2",
    "entry_id": "http://arxiv.org/abs/2412.18291v2",
    "title": "DeepCRCEval: Revisiting the Evaluation of Code Review Comment Generation",
    "summary": "Code review is a vital but demanding aspect of software development, generating significant interest in automating review comments. Traditional evaluation methods for these comments, primarily based on text similarity, face two major challenges: inconsistent reliability of human-authored comments in open-source projects and the weak correlation of text similarity with objectives like enhancing code quality and detecting defects.\n  This study empirically analyzes benchmark comments using a novel set of criteria informed by prior research and developer interviews. We then similarly revisit the evaluation of existing methodologies. Our evaluation framework, DeepCRCEval, integrates human evaluators and Large Language Models (LLMs) for a comprehensive reassessment of current techniques based on the criteria set. Besides, we also introduce an innovative and efficient baseline, LLM-Reviewer, leveraging the few-shot learning capabilities of LLMs for a target-oriented comparison.\n  Our research highlights the limitations of text similarity metrics, finding that less than 10% of benchmark comments are high quality for automation. In contrast, DeepCRCEval effectively distinguishes between high and low-quality comments, proving to be a more reliable evaluation mechanism. Incorporating LLM evaluators into DeepCRCEval significantly boosts efficiency, reducing time and cost by 88.78% and 90.32%, respectively. Furthermore, LLM-Reviewer demonstrates significant potential of focusing task real targets in comment generation.",
    "authors": [
      "Junyi Lu",
      "Xiaojia Li",
      "Zihan Hua",
      "Lei Yu",
      "Shiqi Cheng",
      "Li Yang",
      "Fengjun Zhang",
      "Chun Zuo"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-12-24T08:53:54Z",
    "pdf_url": "https://arxiv.org/pdf/2412.18291v2"
  },
  {
    "arxiv_id": "2412.17767v2",
    "entry_id": "http://arxiv.org/abs/2412.17767v2",
    "title": "ResearchTown: Simulator of Human Research Community",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable potential in scientific domains, yet a fundamental question remains unanswered: Can we simulate human research communities with LLMs? Addressing this question can deepen our understanding of the processes behind idea brainstorming and inspire the automatic discovery of novel scientific insights. In this work, we propose ResearchTown, a multi-agent framework for research community simulation. Within this framework, the human research community is simplified as an agent-data graph, where researchers and papers are represented as agent-type and data-type nodes, respectively, and connected based on their collaboration relationships. We also introduce TextGNN, a text-based inference framework that models various research activities (e.g., paper reading, paper writing, and review writing) as special forms of a unified message-passing process on the agent-data graph. To evaluate the quality of the research community simulation, we present ResearchBench, a benchmark that uses a node-masking prediction task for scalable and objective assessment based on similarity. Our experiments reveal three key findings: (1) ResearchTown can provide a realistic simulation of collaborative research activities, including paper writing and review writing; (2) ResearchTown can maintain robust simulation with multiple researchers and diverse papers; (3) ResearchTown can generate interdisciplinary research ideas that potentially inspire pioneering research directions.",
    "authors": [
      "Haofei Yu",
      "Zhaochen Hong",
      "Zirui Cheng",
      "Kunlun Zhu",
      "Keyang Xuan",
      "Jinwei Yao",
      "Tao Feng",
      "Jiaxuan You"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-12-23T18:26:53Z",
    "pdf_url": "https://arxiv.org/pdf/2412.17767v2"
  },
  {
    "arxiv_id": "2412.17759v1",
    "entry_id": "http://arxiv.org/abs/2412.17759v1",
    "title": "Survey of Large Multimodal Model Datasets, Application Categories and Taxonomy",
    "summary": "Multimodal learning, a rapidly evolving field in artificial intelligence, seeks to construct more versatile and robust systems by integrating and analyzing diverse types of data, including text, images, audio, and video. Inspired by the human ability to assimilate information through many senses, this method enables applications such as text-to-video conversion, visual question answering, and image captioning. Recent developments in datasets that support multimodal language models (MLLMs) are highlighted in this overview. Large-scale multimodal datasets are essential because they allow for thorough testing and training of these models. With an emphasis on their contributions to the discipline, the study examines a variety of datasets, including those for training, domain-specific tasks, and real-world applications. It also emphasizes how crucial benchmark datasets are for assessing models' performance in a range of scenarios, scalability, and applicability. Since multimodal learning is always changing, overcoming these obstacles will help AI research and applications reach new heights.",
    "authors": [
      "Priyaranjan Pattnayak",
      "Hitesh Laxmichand Patel",
      "Bhargava Kumar",
      "Amit Agarwal",
      "Ishan Banerjee",
      "Srikant Panda",
      "Tejaswini Kumar"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-12-23T18:15:19Z",
    "pdf_url": "https://arxiv.org/pdf/2412.17759v1"
  },
  {
    "arxiv_id": "2412.17686v1",
    "entry_id": "http://arxiv.org/abs/2412.17686v1",
    "title": "Large Language Model Safety: A Holistic Survey",
    "summary": "The rapid development and deployment of large language models (LLMs) have introduced a new frontier in artificial intelligence, marked by unprecedented capabilities in natural language understanding and generation. However, the increasing integration of these models into critical applications raises substantial safety concerns, necessitating a thorough examination of their potential risks and associated mitigation strategies.\n  This survey provides a comprehensive overview of the current landscape of LLM safety, covering four major categories: value misalignment, robustness to adversarial attacks, misuse, and autonomous AI risks. In addition to the comprehensive review of the mitigation methodologies and evaluation resources on these four aspects, we further explore four topics related to LLM safety: the safety implications of LLM agents, the role of interpretability in enhancing LLM safety, the technology roadmaps proposed and abided by a list of AI companies and institutes for LLM safety, and AI governance aimed at LLM safety with discussions on international cooperation, policy proposals, and prospective regulatory directions.\n  Our findings underscore the necessity for a proactive, multifaceted approach to LLM safety, emphasizing the integration of technical solutions, ethical considerations, and robust governance frameworks. This survey is intended to serve as a foundational resource for academy researchers, industry practitioners, and policymakers, offering insights into the challenges and opportunities associated with the safe integration of LLMs into society. Ultimately, it seeks to contribute to the safe and beneficial development of LLMs, aligning with the overarching goal of harnessing AI for societal advancement and well-being. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLM-Safety-Papers.",
    "authors": [
      "Dan Shi",
      "Tianhao Shen",
      "Yufei Huang",
      "Zhigen Li",
      "Yongqi Leng",
      "Renren Jin",
      "Chuang Liu",
      "Xinwei Wu",
      "Zishan Guo",
      "Linhao Yu",
      "Ling Shi",
      "Bojian Jiang",
      "Deyi Xiong"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-23T16:11:27Z",
    "pdf_url": "https://arxiv.org/pdf/2412.17686v1"
  },
  {
    "arxiv_id": "2501.12394v2",
    "entry_id": "http://arxiv.org/abs/2501.12394v2",
    "title": "ELEVATE-GenAI: Reporting Guidelines for the Use of Large Language Models in Health Economics and Outcomes Research: an ISPOR Working Group on Generative AI Report",
    "summary": "Introduction: Generative artificial intelligence (AI), particularly large language models (LLMs), holds significant promise for Health Economics and Outcomes Research (HEOR). However, standardized reporting guidance for LLM-assisted research is lacking. This article introduces the ELEVATE GenAI framework and checklist - reporting guidelines specifically designed for HEOR studies involving LLMs.\n  Methods: The framework was developed through a targeted literature review of existing reporting guidelines, AI evaluation frameworks, and expert input from the ISPOR Working Group on Generative AI. It comprises ten domains, including model characteristics, accuracy, reproducibility, and fairness and bias. The accompanying checklist translates the framework into actionable reporting items. To illustrate its use, the framework was applied to two published HEOR studies: one focused on systematic literature review tasks and the other on economic modeling.\n  Results: The ELEVATE GenAI framework offers a comprehensive structure for reporting LLM-assisted HEOR research, while the checklist facilitates practical implementation. Its application to the two case studies demonstrates its relevance and usability across different HEOR contexts.\n  Limitations: Although the framework provides robust reporting guidance, further empirical testing is needed to assess its validity, completeness, usability, as well as its generalizability across diverse HEOR use cases.\n  Conclusion: The ELEVATE GenAI framework and checklist address a critical gap by offering structured guidance for transparent, accurate, and reproducible reporting of LLM-assisted HEOR research. Future work will focus on extensive testing and validation to support broader adoption and refinement.",
    "authors": [
      "Rachael L. Fleurence",
      "Dalia Dawoud",
      "Jiang Bian",
      "Mitchell K. Higashi",
      "Xiaoyan Wang",
      "Hua Xu",
      "Jagpreet Chhatwal",
      "Turgay Ayer"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-12-23T14:09:10Z",
    "pdf_url": "https://arxiv.org/pdf/2501.12394v2"
  },
  {
    "arxiv_id": "2412.17486v1",
    "entry_id": "http://arxiv.org/abs/2412.17486v1",
    "title": "Is ChatGPT Massively Used by Students Nowadays? A Survey on the Use of Large Language Models such as ChatGPT in Educational Settings",
    "summary": "The rapid adoption of Generative AI (GenAI) based on Large Language Models (LLMs) such as ChatGPT has recently and profoundly impacted education, offering transformative opportunities while raising significant concerns. In this study we present the results of a survey that investigates how 395 students aged 13 to 25 years old in France and Italy integrate LLMs into their educational routines.\n  Key findings include the widespread use of these tools across all age groups and disciplines, with older students and male students demonstrating higher usage frequencies, particularly in scientific contexts. The results also show gender disparities, raising concerns about an emerging AI literacy and technological gender gap. Additionally, while most students utilise LLMs constructively, the lack of systematic proofreading and critical evaluation among younger users suggests potential risks to cognitive skills development, including critical thinking and foundational knowledge. The survey results underscore the need for educational institutions to adapt their curricula to integrate AI tools effectively, promoting ethical use, critical thinking, and awareness of AI limitations and environmental costs. This paper provides actionable recommendations for fostering equitable and effective cohabitation of LLMs and education while addressing emerging challenges.",
    "authors": [
      "Jérémie Sublime",
      "Ilaria Renna"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-12-23T11:29:44Z",
    "pdf_url": "https://arxiv.org/pdf/2412.17486v1"
  },
  {
    "arxiv_id": "2412.17481v2",
    "entry_id": "http://arxiv.org/abs/2412.17481v2",
    "title": "A Survey on LLM-based Multi-Agent System: Recent Advances and New Frontiers in Application",
    "summary": "LLM-based Multi-Agent Systems ( LLM-MAS ) have become a research hotspot since the rise of large language models (LLMs). However, with the continuous influx of new related works, the existing reviews struggle to capture them comprehensively. This paper presents a comprehensive survey of these studies. We first discuss the definition of LLM-MAS, a framework encompassing much of previous work. We provide an overview of the various applications of LLM-MAS in (i) solving complex tasks, (ii) simulating specific scenarios, and (iii) evaluating generative agents. Building on previous studies, we also highlight several challenges and propose future directions for research in this field.",
    "authors": [
      "Shuaihang Chen",
      "Yuanxing Liu",
      "Wei Han",
      "Weinan Zhang",
      "Ting Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.MA"
    ],
    "published": "2024-12-23T11:11:51Z",
    "pdf_url": "https://arxiv.org/pdf/2412.17481v2"
  },
  {
    "arxiv_id": "2412.16746v4",
    "entry_id": "http://arxiv.org/abs/2412.16746v4",
    "title": "Beyond Partisan Leaning: A Comparative Analysis of Political Bias in Large Language Models",
    "summary": "As large language models (LLMs) become increasingly embedded in civic, educational, and political information environments, concerns about their potential political bias have grown. Prior research often evaluates such bias through simulated personas or predefined ideological typologies, which may introduce artificial framing effects or overlook how models behave in general use scenarios. This study adopts a persona-free, topic-specific approach to evaluate political behavior in LLMs, reflecting how users typically interact with these systems-without ideological role-play or conditioning. We introduce a two-dimensional framework: one axis captures partisan orientation on highly polarized topics (e.g., abortion, immigration), and the other assesses sociopolitical engagement on less polarized issues (e.g., climate change, foreign policy). Using survey-style prompts drawn from the ANES and Pew Research Center, we analyze responses from 43 LLMs developed in the U.S., Europe, China, and the Middle East. We propose an entropy-weighted bias score to quantify both the direction and consistency of partisan alignment, and identify four behavioral clusters through engagement profiles. Findings show most models lean center-left or left ideologically and vary in their nonpartisan engagement patterns. Model scale and openness are not strong predictors of behavior, suggesting that alignment strategy and institutional context play a more decisive role in shaping political expression.",
    "authors": [
      "Tai-Quan Peng",
      "Kaiqi Yang",
      "Sanguk Lee",
      "Hang Li",
      "Yucheng Chu",
      "Yuping Lin",
      "Hui Liu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-12-21T19:42:40Z",
    "pdf_url": "https://arxiv.org/pdf/2412.16746v4"
  },
  {
    "arxiv_id": "2412.16543v3",
    "entry_id": "http://arxiv.org/abs/2412.16543v3",
    "title": "Mathematics and Machine Creativity: A Survey on Bridging Mathematics with AI",
    "summary": "This paper presents a comprehensive overview on the applications of artificial intelligence (AI) in mathematical research, highlighting the transformative role AI has begun to play in this domain. Traditionally, AI advancements have heavily relied on theoretical foundations provided by mathematics and statistics. However, recent developments in AI, particularly in reinforcement learning (RL) and large language models (LLMs), have demonstrated the potential for AI to contribute back to mathematics by offering flexible algorithmic frameworks and powerful inductive reasoning capabilities that support various aspects of mathematical research. This survey aims to establish a bridge between AI and mathematics, providing insights into the mutual benefits and fostering deeper interdisciplinary understanding.\n  In particular, we argue that while current AI and LLMs may struggle with complex deductive reasoning, their \"inherent creativity\", the ability to generate outputs at high throughput based on recognition of shallow patterns, holds significant potential to support and inspire mathematical research. This creative capability, often overlooked, could be the key to unlocking new perspectives and methodologies in mathematics. Furthermore, we address the lack of cross-disciplinary communication: mathematicians may not fully comprehend the latest advances in AI, while AI researchers frequently prioritize benchmark performance over real-world applications in frontier mathematical research. This paper seeks to close that gap, offering a detailed exploration of AI fundamentals, its strengths, and its emerging applications in the mathematical sciences.",
    "authors": [
      "Shizhe Liang",
      "Wei Zhang",
      "Tianyang Zhong",
      "Tianming Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-21T08:58:36Z",
    "pdf_url": "https://arxiv.org/pdf/2412.16543v3"
  },
  {
    "arxiv_id": "2412.16504v2",
    "entry_id": "http://arxiv.org/abs/2412.16504v2",
    "title": "Privacy in Fine-tuning Large Language Models: Attacks, Defenses, and Future Directions",
    "summary": "Fine-tuning has emerged as a critical process in leveraging Large Language Models (LLMs) for specific downstream tasks, enabling these models to achieve state-of-the-art performance across various domains. However, the fine-tuning process often involves sensitive datasets, introducing privacy risks that exploit the unique characteristics of this stage. In this paper, we provide a comprehensive survey of privacy challenges associated with fine-tuning LLMs, highlighting vulnerabilities to various privacy attacks, including membership inference, data extraction, and backdoor attacks. We further review defense mechanisms designed to mitigate privacy risks in the fine-tuning phase, such as differential privacy, federated learning, and knowledge unlearning, discussing their effectiveness and limitations in addressing privacy risks and maintaining model utility. By identifying key gaps in existing research, we highlight challenges and propose directions to advance the development of privacy-preserving methods for fine-tuning LLMs, promoting their responsible use in diverse applications.",
    "authors": [
      "Hao Du",
      "Shang Liu",
      "Lele Zheng",
      "Yang Cao",
      "Atsuyoshi Nakamura",
      "Lei Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-21T06:41:29Z",
    "pdf_url": "https://arxiv.org/pdf/2412.16504v2"
  },
  {
    "arxiv_id": "2412.16468v3",
    "entry_id": "http://arxiv.org/abs/2412.16468v3",
    "title": "The Road to Artificial SuperIntelligence: A Comprehensive Survey of Superalignment",
    "summary": "The emergence of large language models (LLMs) has sparked the possibility of about Artificial Superintelligence (ASI), a hypothetical AI system surpassing human intelligence. However, existing alignment paradigms struggle to guide such advanced AI systems. Superalignment, the alignment of AI systems with human values and safety requirements at superhuman levels of capability aims to addresses two primary goals -- scalability in supervision to provide high-quality guidance signals and robust governance to ensure alignment with human values. In this survey, we examine scalable oversight methods and potential solutions for superalignment. Specifically, we explore the concept of ASI, the challenges it poses, and the limitations of current alignment paradigms in addressing the superalignment problem. Then we review scalable oversight methods for superalignment. Finally, we discuss the key challenges and propose pathways for the safe and continual improvement of ASI systems. By comprehensively reviewing the current literature, our goal is provide a systematical introduction of existing methods, analyze their strengths and limitations, and discuss potential future directions.",
    "authors": [
      "HyunJin Kim",
      "Xiaoyuan Yi",
      "Jing Yao",
      "Jianxun Lian",
      "Muhua Huang",
      "Shitong Duan",
      "JinYeong Bak",
      "Xing Xie"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-12-21T03:51:04Z",
    "pdf_url": "https://arxiv.org/pdf/2412.16468v3"
  },
  {
    "arxiv_id": "2501.00029v1",
    "entry_id": "http://arxiv.org/abs/2501.00029v1",
    "title": "A Breadth-First Catalog of Text Processing, Speech Processing and Multimodal Research in South Asian Languages",
    "summary": "We review the recent literature (January 2022- October 2024) in South Asian languages on text-based language processing, multimodal models, and speech processing, and provide a spotlight analysis focused on 21 low-resource South Asian languages, namely Saraiki, Assamese, Balochi, Bhojpuri, Bodo, Burmese, Chhattisgarhi, Dhivehi, Gujarati, Kannada, Kashmiri, Konkani, Khasi, Malayalam, Meitei, Nepali, Odia, Pashto, Rajasthani, Sindhi, and Telugu. We identify trends, challenges, and future research directions, using a step-wise approach that incorporates relevance classification and clustering based on large language models (LLMs). Our goal is to provide a breadth-first overview of the recent developments in South Asian language technologies to NLP researchers interested in working with South Asian languages.",
    "authors": [
      "Pranav Gupta"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-12-20T20:08:48Z",
    "pdf_url": "https://arxiv.org/pdf/2501.00029v1"
  },
  {
    "arxiv_id": "2412.16135v3",
    "entry_id": "http://arxiv.org/abs/2412.16135v3",
    "title": "Can LLMs Obfuscate Code? A Systematic Analysis of Large Language Models into Assembly Code Obfuscation",
    "summary": "Malware authors often employ code obfuscations to make their malware harder to detect. Existing tools for generating obfuscated code often require access to the original source code (e.g., C++ or Java), and adding new obfuscations is a non-trivial, labor-intensive process. In this study, we ask the following question: Can Large Language Models (LLMs) potentially generate a new obfuscated assembly code? If so, this poses a risk to anti-virus engines and potentially increases the flexibility of attackers to create new obfuscation patterns. We answer this in the affirmative by developing the MetamorphASM benchmark comprising MetamorphASM Dataset (MAD) along with three code obfuscation techniques: dead code, register substitution, and control flow change. The MetamorphASM systematically evaluates the ability of LLMs to generate and analyze obfuscated code using MAD, which contains 328,200 obfuscated assembly code samples. We release this dataset and analyze the success rate of various LLMs (e.g., GPT-3.5/4, GPT-4o-mini, Starcoder, CodeGemma, CodeLlama, CodeT5, and LLaMA 3.1) in generating obfuscated assembly code. The evaluation was performed using established information-theoretic metrics and manual human review to ensure correctness and provide the foundation for researchers to study and develop remediations to this risk.",
    "authors": [
      "Seyedreza Mohseni",
      "Seyedali Mohammadi",
      "Deepa Tilwani",
      "Yash Saxena",
      "Gerald Ketu Ndawula",
      "Sriram Vema",
      "Edward Raff",
      "Manas Gaur"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-20T18:31:24Z",
    "pdf_url": "https://arxiv.org/pdf/2412.16135v3"
  },
  {
    "arxiv_id": "2412.16089v1",
    "entry_id": "http://arxiv.org/abs/2412.16089v1",
    "title": "The Evolution of LLM Adoption in Industry Data Curation Practices",
    "summary": "As large language models (LLMs) grow increasingly adept at processing unstructured text data, they offer new opportunities to enhance data curation workflows. This paper explores the evolution of LLM adoption among practitioners at a large technology company, evaluating the impact of LLMs in data curation tasks through participants' perceptions, integration strategies, and reported usage scenarios. Through a series of surveys, interviews, and user studies, we provide a timely snapshot of how organizations are navigating a pivotal moment in LLM evolution. In Q2 2023, we conducted a survey to assess LLM adoption in industry for development tasks (N=84), and facilitated expert interviews to assess evolving data needs (N=10) in Q3 2023. In Q2 2024, we explored practitioners' current and anticipated LLM usage through a user study involving two LLM-based prototypes (N=12). While each study addressed distinct research goals, they revealed a broader narrative about evolving LLM usage in aggregate. We discovered an emerging shift in data understanding from heuristic-first, bottom-up approaches to insights-first, top-down workflows supported by LLMs. Furthermore, to respond to a more complex data landscape, data practitioners now supplement traditional subject-expert-created 'golden datasets' with LLM-generated 'silver' datasets and rigorously validated 'super golden' datasets curated by diverse experts. This research sheds light on the transformative role of LLMs in large-scale analysis of unstructured data and highlights opportunities for further tool development.",
    "authors": [
      "Crystal Qian",
      "Michael Xieyang Liu",
      "Emily Reif",
      "Grady Simon",
      "Nada Hussein",
      "Nathan Clement",
      "James Wexler",
      "Carrie J. Cai",
      "Michael Terry",
      "Minsuk Kahng"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-12-20T17:34:16Z",
    "pdf_url": "https://arxiv.org/pdf/2412.16089v1"
  },
  {
    "arxiv_id": "2412.16022v1",
    "entry_id": "http://arxiv.org/abs/2412.16022v1",
    "title": "The Only Way is Ethics: A Guide to Ethical Research with Large Language Models",
    "summary": "There is a significant body of work looking at the ethical considerations of large language models (LLMs): critiquing tools to measure performance and harms; proposing toolkits to aid in ideation; discussing the risks to workers; considering legislation around privacy and security etc. As yet there is no work that integrates these resources into a single practical guide that focuses on LLMs; we attempt this ambitious goal. We introduce 'LLM Ethics Whitepaper', which we provide as an open and living resource for NLP practitioners, and those tasked with evaluating the ethical implications of others' work. Our goal is to translate ethics literature into concrete recommendations and provocations for thinking with clear first steps, aimed at computer scientists. 'LLM Ethics Whitepaper' distils a thorough literature review into clear Do's and Don'ts, which we present also in this paper. We likewise identify useful toolkits to support ethical work. We refer the interested reader to the full LLM Ethics Whitepaper, which provides a succinct discussion of ethical considerations at each stage in a project lifecycle, as well as citations for the hundreds of papers from which we drew our recommendations. The present paper can be thought of as a pocket guide to conducting ethical research with LLMs.",
    "authors": [
      "Eddie L. Ungless",
      "Nikolas Vitsakis",
      "Zeerak Talat",
      "James Garforth",
      "Björn Ross",
      "Arno Onken",
      "Atoosa Kasirzadeh",
      "Alexandra Birch"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-20T16:14:43Z",
    "pdf_url": "https://arxiv.org/pdf/2412.16022v1"
  },
  {
    "arxiv_id": "2412.15907v1",
    "entry_id": "http://arxiv.org/abs/2412.15907v1",
    "title": "Development of a Large-scale Dataset of Chest Computed Tomography Reports in Japanese and a High-performance Finding Classification Model",
    "summary": "Background: Recent advances in large language models highlight the need for high-quality multilingual medical datasets. While Japan leads globally in CT scanner deployment and utilization, the lack of large-scale Japanese radiology datasets has hindered the development of specialized language models for medical imaging analysis. Objective: To develop a comprehensive Japanese CT report dataset through machine translation and establish a specialized language model for structured finding classification. Additionally, to create a rigorously validated evaluation dataset through expert radiologist review. Methods: We translated the CT-RATE dataset (24,283 CT reports from 21,304 patients) into Japanese using GPT-4o mini. The training dataset consisted of 22,778 machine-translated reports, while the validation dataset included 150 radiologist-revised reports. We developed CT-BERT-JPN based on \"tohoku-nlp/bert-base-japanese-v3\" architecture for extracting 18 structured findings from Japanese radiology reports. Results: Translation metrics showed strong performance with BLEU scores of 0.731 and 0.690, and ROUGE scores ranging from 0.770 to 0.876 for Findings and from 0.748 to 0.857 for Impression sections. CT-BERT-JPN demonstrated superior performance compared to GPT-4o in 11 out of 18 conditions, including lymphadenopathy (+14.2%), interlobular septal thickening (+10.9%), and atelectasis (+7.4%). The model maintained F1 scores exceeding 0.95 in 14 out of 18 conditions and achieved perfect scores in four conditions. Conclusions: Our study establishes a robust Japanese CT report dataset and demonstrates the effectiveness of a specialized language model for structured finding classification. The hybrid approach of machine translation and expert validation enables the creation of large-scale medical datasets while maintaining high quality.",
    "authors": [
      "Yosuke Yamagishi",
      "Yuta Nakamura",
      "Tomohiro Kikuchi",
      "Yuki Sonoda",
      "Hiroshi Hirakawa",
      "Shintaro Kano",
      "Satoshi Nakamura",
      "Shouhei Hanaoka",
      "Takeharu Yoshikawa",
      "Osamu Abe"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-20T13:59:11Z",
    "pdf_url": "https://arxiv.org/pdf/2412.15907v1"
  },
  {
    "arxiv_id": "2412.15748v2",
    "entry_id": "http://arxiv.org/abs/2412.15748v2",
    "title": "Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models",
    "summary": "Background: Despite the current ubiquity of Large Language Models (LLMs) across the medical domain, there is a surprising lack of studies which address their reasoning behaviour. We emphasise the importance of understanding reasoning behaviour as opposed to high-level prediction accuracies, since it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical LLMs used in the clinical domain will have a significant impact across the healthcare sector. Results: Therefore, in this work, we adapt the existing concept of reasoning behaviour and articulate its interpretation within the specific context of medical LLMs. We survey and categorise current state-of-the-art approaches for modeling and evaluating reasoning reasoning in medical LLMs. Additionally, we propose theoretical frameworks which can empower medical professionals or machine learning engineers to gain insight into the low-level reasoning operations of these previously obscure models. We also outline key open challenges facing the development of Large Reasoning Models. Conclusion: The subsequent increased transparency and trust in medical machine learning models by clinicians as well as patients will accelerate the integration, application as well as further development of medical AI for the healthcare system as a whole.",
    "authors": [
      "Shamus Sim",
      "Tyrone Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-12-20T10:06:52Z",
    "pdf_url": "https://arxiv.org/pdf/2412.15748v2"
  },
  {
    "arxiv_id": "2412.15547v1",
    "entry_id": "http://arxiv.org/abs/2412.15547v1",
    "title": "NGQA: A Nutritional Graph Question Answering Benchmark for Personalized Health-aware Nutritional Reasoning",
    "summary": "Diet plays a critical role in human health, yet tailoring dietary reasoning to individual health conditions remains a major challenge. Nutrition Question Answering (QA) has emerged as a popular method for addressing this problem. However, current research faces two critical limitations. On one hand, the absence of datasets involving user-specific medical information severely limits \\textit{personalization}. This challenge is further compounded by the wide variability in individual health needs. On the other hand, while large language models (LLMs), a popular solution for this task, demonstrate strong reasoning abilities, they struggle with the domain-specific complexities of personalized healthy dietary reasoning, and existing benchmarks fail to capture these challenges. To address these gaps, we introduce the Nutritional Graph Question Answering (NGQA) benchmark, the first graph question answering dataset designed for personalized nutritional health reasoning. NGQA leverages data from the National Health and Nutrition Examination Survey (NHANES) and the Food and Nutrient Database for Dietary Studies (FNDDS) to evaluate whether a food is healthy for a specific user, supported by explanations of the key contributing nutrients. The benchmark incorporates three question complexity settings and evaluates reasoning across three downstream tasks. Extensive experiments with LLM backbones and baseline models demonstrate that the NGQA benchmark effectively challenges existing models. In sum, NGQA addresses a critical real-world problem while advancing GraphQA research with a novel domain-specific benchmark.",
    "authors": [
      "Zheyuan Zhang",
      "Yiyang Li",
      "Nhi Ha Lan Le",
      "Zehong Wang",
      "Tianyi Ma",
      "Vincent Galassi",
      "Keerthiram Murugesan",
      "Nuno Moniz",
      "Werner Geyer",
      "Nitesh V Chawla",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-12-20T04:13:46Z",
    "pdf_url": "https://arxiv.org/pdf/2412.15547v1"
  },
  {
    "arxiv_id": "2412.15501v1",
    "entry_id": "http://arxiv.org/abs/2412.15501v1",
    "title": "Humanlike Cognitive Patterns as Emergent Phenomena in Large Language Models",
    "summary": "Research on emergent patterns in Large Language Models (LLMs) has gained significant traction in both psychology and artificial intelligence, motivating the need for a comprehensive review that offers a synthesis of this complex landscape. In this article, we systematically review LLMs' capabilities across three important cognitive domains: decision-making biases, reasoning, and creativity. We use empirical studies drawing on established psychological tests and compare LLMs' performance to human benchmarks. On decision-making, our synthesis reveals that while LLMs demonstrate several human-like biases, some biases observed in humans are absent, indicating cognitive patterns that only partially align with human decision-making. On reasoning, advanced LLMs like GPT-4 exhibit deliberative reasoning akin to human System-2 thinking, while smaller models fall short of human-level performance. A distinct dichotomy emerges in creativity: while LLMs excel in language-based creative tasks, such as storytelling, they struggle with divergent thinking tasks that require real-world context. Nonetheless, studies suggest that LLMs hold considerable potential as collaborators, augmenting creativity in human-machine problem-solving settings. Discussing key limitations, we also offer guidance for future research in areas such as memory, attention, and open-source model development.",
    "authors": [
      "Zhisheng Tang",
      "Mayank Kejriwal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-20T02:26:56Z",
    "pdf_url": "https://arxiv.org/pdf/2412.15501v1"
  },
  {
    "arxiv_id": "2412.15204v2",
    "entry_id": "http://arxiv.org/abs/2412.15204v2",
    "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
    "summary": "This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at https://longbench2.github.io.",
    "authors": [
      "Yushi Bai",
      "Shangqing Tu",
      "Jiajie Zhang",
      "Hao Peng",
      "Xiaozhi Wang",
      "Xin Lv",
      "Shulin Cao",
      "Jiazheng Xu",
      "Lei Hou",
      "Yuxiao Dong",
      "Jie Tang",
      "Juanzi Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-19T18:59:17Z",
    "pdf_url": "https://arxiv.org/pdf/2412.15204v2"
  },
  {
    "arxiv_id": "2412.15151v3",
    "entry_id": "http://arxiv.org/abs/2412.15151v3",
    "title": "Language Models as Continuous Self-Evolving Data Engineers",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities on various tasks, while the further evolvement is limited to the lack of high-quality training data. In addition, traditional training approaches rely too much on expert-labeled data, setting a ceiling on the performance of LLMs. To address this issue, we propose a novel paradigm named LANCE (LANguage models as Continuous self-Evolving data engineers) that enables LLMs to train themselves by autonomously generating, cleaning, reviewing, and annotating data with preference information. Our approach demonstrates that LLMs can serve as continuous self-evolving data engineers, significantly reducing the time and cost of the post-training data construction. Through iterative fine-tuning on Qwen2 series models, we validate the effectiveness of LANCE across various tasks, showing that it can maintain high-quality data generation and continuously improve model performance. Across multiple benchmark dimensions, LANCE results in an average score enhancement of 3.64 for Qwen2-7B and 1.75 for Qwen2-7B-Instruct. This training paradigm with autonomous data construction not only reduces the reliance on human experts or external models but also ensures that the data aligns with human preferences, paving the way for the development of future superintelligent systems that can exceed human capabilities. Codes are available at: https://github.com/Control-derek/LANCE.",
    "authors": [
      "Peidong Wang",
      "Ming Wang",
      "Zhiming Ma",
      "Xiaocui Yang",
      "Shi Feng",
      "Daling Wang",
      "Yifei Zhang",
      "Kaisong Song"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-19T18:28:41Z",
    "pdf_url": "https://arxiv.org/pdf/2412.15151v3"
  },
  {
    "arxiv_id": "2412.15004v3",
    "entry_id": "http://arxiv.org/abs/2412.15004v3",
    "title": "From Vulnerabilities to Remediation: A Systematic Literature Review of LLMs in Code Security",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for automating various programming tasks, including security-related ones, such as detecting and fixing vulnerabilities. Despite their promising capabilities, when required to produce or modify pre-existing code, LLMs could introduce vulnerabilities unbeknown to the programmer. When analyzing code, they could miss clear vulnerabilities or signal nonexistent ones. In this Systematic Literature Review (SLR), we aim to investigate both the security benefits and potential drawbacks of using LLMs for a variety of code-related tasks. In particular, first we focus on the types of vulnerabilities that could be introduced by LLMs, when used for producing code. Second, we analyze the capabilities of LLMs to detect and fix vulnerabilities, in any given code, and how the prompting strategy of choice impacts their performance in these two tasks. Last, we provide an in-depth analysis on how data poisoning attacks on LLMs can impact performance in the aforementioned tasks.",
    "authors": [
      "Enna Basic",
      "Alberto Giaretta"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-19T16:20:22Z",
    "pdf_url": "https://arxiv.org/pdf/2412.15004v3"
  },
  {
    "arxiv_id": "2412.16238v2",
    "entry_id": "http://arxiv.org/abs/2412.16238v2",
    "title": "Algebraic Evaluation Theorems",
    "summary": "Majority voting (MV) is the prototypical ``wisdom of the crowd'' algorithm. Theorems considering when MV is optimal for group decisions date back to Condorcet's 1785 jury \\emph{decision} theorem. The same error independence assumption underlying the theorem can be used to prove a jury \\emph{evaluation} theorem that does purely algebraic evaluation (AE) of juror performance based on a batch of their decisions. Three or more binary jurors are enough to obtain the only two possible statistics of their correctness on a test they took. AE is superior to MV in three ways. First, its empirical assumptions are looser and can handle jurors less than 50\\% accurate in making decisions. Second, it has point-like precision in evaluating them given its assumption of error independence. This precision enables a multi-accuracy approach that has higher labeling accuracy than MV and comes with empirical uncertainty bounds. And, third, it is self-alarming about the failure of its error independence assumption. Experiments using demographic data from the American Community Survey confirm the practical utility of AE over MV. Two implications of the theorem for AI safety are discussed - a principled way to terminate infinite monitoring chains (who grades the graders?) and the super-alignment problem (how do we evaluate agents doing tasks we do not understand?).",
    "authors": [
      "Andrés Corrada-Emmanuel"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-12-19T13:01:21Z",
    "pdf_url": "https://arxiv.org/pdf/2412.16238v2"
  },
  {
    "arxiv_id": "2412.14741v1",
    "entry_id": "http://arxiv.org/abs/2412.14741v1",
    "title": "Active Inference and Human--Computer Interaction",
    "summary": "Active Inference is a closed-loop computational theoretical basis for understanding behaviour, based on agents with internal probabilistic generative models that encode their beliefs about how hidden states in their environment cause their sensations. We review Active Inference and how it could be applied to model the human-computer interaction loop. Active Inference provides a coherent framework for managing generative models of humans, their environments, sensors and interface components. It informs off-line design and supports real-time, online adaptation. It provides model-based explanations for behaviours observed in HCI, and new tools to measure important concepts such as agency and engagement. We discuss how Active Inference offers a new basis for a theory of interaction in HCI, tools for design of modern, complex sensor-based systems, and integration of artificial intelligence technologies, enabling it to cope with diversity in human users and contexts. We discuss the practical challenges in implementing such Active Inference-based systems.",
    "authors": [
      "Roderick Murray-Smith",
      "John H. Williamson",
      "Sebastian Stein"
    ],
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-12-19T11:17:31Z",
    "pdf_url": "https://arxiv.org/pdf/2412.14741v1"
  },
  {
    "arxiv_id": "2412.14708v1",
    "entry_id": "http://arxiv.org/abs/2412.14708v1",
    "title": "Creation of AI-driven Smart Spaces for Enhanced Indoor Environments -- A Survey",
    "summary": "Smart spaces are ubiquitous computing environments that integrate diverse sensing and communication technologies to enhance space functionality, optimize energy utilization, and improve user comfort and well-being. The integration of emerging AI methodologies into these environments facilitates the formation of AI-driven smart spaces, which further enhance functionalities of the spaces by enabling advanced applications such as personalized comfort settings, interactive living spaces, and automatization of the space systems, all resulting in enhanced indoor experiences of the users. In this paper, we present a systematic survey of existing research on the foundational components of AI-driven smart spaces, including sensor technologies, data communication protocols, sensor network management and maintenance strategies, as well as the data collection, processing and analytics. Given the pivotal role of AI in establishing AI-powered smart spaces, we explore the opportunities and challenges associated with traditional machine learning (ML) approaches, such as deep learning (DL), and emerging methodologies including large language models (LLMs). Finally, we provide key insights necessary for the development of AI-driven smart spaces, propose future research directions, and sheds light on the path forward.",
    "authors": [
      "Aygün Varol",
      "Naser Hossein Motlagh",
      "Mirka Leino",
      "Sasu Tarkoma",
      "Johanna Virkki"
    ],
    "categories": [
      "cs.AI",
      "cs.DC",
      "cs.ET",
      "cs.HC"
    ],
    "published": "2024-12-19T10:20:34Z",
    "pdf_url": "https://arxiv.org/pdf/2412.14708v1"
  },
  {
    "arxiv_id": "2412.14056v1",
    "entry_id": "http://arxiv.org/abs/2412.14056v1",
    "title": "A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future",
    "summary": "Artificial intelligence (AI) has rapidly developed through advancements in computational power and the growth of massive datasets. However, this progress has also heightened challenges in interpreting the \"black-box\" nature of AI models. To address these concerns, eXplainable AI (XAI) has emerged with a focus on transparency and interpretability to enhance human understanding and trust in AI decision-making processes. In the context of multimodal data fusion and complex reasoning scenarios, the proposal of Multimodal eXplainable AI (MXAI) integrates multiple modalities for prediction and explanation tasks. Meanwhile, the advent of Large Language Models (LLMs) has led to remarkable breakthroughs in natural language processing, yet their complexity has further exacerbated the issue of MXAI. To gain key insights into the development of MXAI methods and provide crucial guidance for building more transparent, fair, and trustworthy AI systems, we review the MXAI methods from a historical perspective and categorize them across four eras: traditional machine learning, deep learning, discriminative foundation models, and generative LLMs. We also review evaluation metrics and datasets used in MXAI research, concluding with a discussion of future challenges and directions. A project related to this review has been created at https://github.com/ShilinSun/mxai_review.",
    "authors": [
      "Shilin Sun",
      "Wenbin An",
      "Feng Tian",
      "Fang Nan",
      "Qidong Liu",
      "Jun Liu",
      "Nazaraf Shah",
      "Ping Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2024-12-18T17:06:21Z",
    "pdf_url": "https://arxiv.org/pdf/2412.14056v1"
  },
  {
    "arxiv_id": "2412.14222v2",
    "entry_id": "http://arxiv.org/abs/2412.14222v2",
    "title": "A Survey on Large Language Model-based Agents for Statistics and Data Science",
    "summary": "In recent years, data science agents powered by Large Language Models (LLMs), known as \"data agents,\" have shown significant potential to transform the traditional data analysis paradigm. This survey provides an overview of the evolution, capabilities, and applications of LLM-based data agents, highlighting their role in simplifying complex data tasks and lowering the entry barrier for users without related expertise. We explore current trends in the design of LLM-based frameworks, detailing essential features such as planning, reasoning, reflection, multi-agent collaboration, user interface, knowledge integration, and system design, which enable agents to address data-centric problems with minimal human intervention. Furthermore, we analyze several case studies to demonstrate the practical applications of various data agents in real-world scenarios. Finally, we identify key challenges and propose future research directions to advance the development of data agents into intelligent statistical analysis software.",
    "authors": [
      "Maojun Sun",
      "Ruijian Han",
      "Binyan Jiang",
      "Houduo Qi",
      "Defeng Sun",
      "Yancheng Yuan",
      "Jian Huang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.OT"
    ],
    "published": "2024-12-18T15:03:26Z",
    "pdf_url": "https://arxiv.org/pdf/2412.14222v2"
  },
  {
    "arxiv_id": "2412.13765v2",
    "entry_id": "http://arxiv.org/abs/2412.13765v2",
    "title": "LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for E-Learning Platforms",
    "summary": "Current methods for analyzing student engagement in e-learning platforms, including automated systems, often struggle with challenges such as handling fuzzy sentiment in text comments and relying on limited metadata. Traditional approaches, such as surveys and questionnaires, also face issues like small sample sizes and scalability. In this paper, we introduce LLM-SEM (Language Model-Based Student Engagement Metric), a novel approach that leverages video metadata and sentiment analysis of student comments to measure engagement. By utilizing recent Large Language Models (LLMs), we generate high-quality sentiment predictions to mitigate text fuzziness and normalize key features such as views and likes. Our holistic method combines comprehensive metadata with sentiment polarity scores to gauge engagement at both the course and lesson levels. Extensive experiments were conducted to evaluate various LLM models, demonstrating the effectiveness of LLM-SEM in providing a scalable and accurate measure of student engagement. We fine-tuned TXLM-RoBERTa using human-annotated sentiment datasets to enhance prediction accuracy and utilized LLama 3B, and Gemma 9B from Ollama.",
    "authors": [
      "Ali Hamdi",
      "Ahmed Abdelmoneim Mazrou",
      "Mohamed Shaltout"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-18T12:01:53Z",
    "pdf_url": "https://arxiv.org/pdf/2412.13765v2"
  },
  {
    "arxiv_id": "2412.13612v5",
    "entry_id": "http://arxiv.org/abs/2412.13612v5",
    "title": "Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition",
    "summary": "Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.",
    "authors": [
      "Xuemei Tang",
      "Xufeng Duan",
      "Zhenguang G. Cai"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-18T08:42:25Z",
    "pdf_url": "https://arxiv.org/pdf/2412.13612v5"
  },
  {
    "arxiv_id": "2412.13501v3",
    "entry_id": "http://arxiv.org/abs/2412.13501v3",
    "title": "GUI Agents: A Survey",
    "summary": "Graphical User Interface (GUI) agents, powered by Large Foundation Models, have emerged as a transformative approach to automating human-computer interaction. These agents autonomously interact with digital systems or software applications via GUIs, emulating human actions such as clicking, typing, and navigating visual elements across diverse platforms. Motivated by the growing interest and fundamental importance of GUI agents, we provide a comprehensive survey that categorizes their benchmarks, evaluation metrics, architectures, and training methods. We propose a unified framework that delineates their perception, reasoning, planning, and acting capabilities. Furthermore, we identify important open challenges and discuss key future directions. Finally, this work serves as a basis for practitioners and researchers to gain an intuitive understanding of current progress, techniques, benchmarks, and critical open problems that remain to be addressed.",
    "authors": [
      "Dang Nguyen",
      "Jian Chen",
      "Yu Wang",
      "Gang Wu",
      "Namyong Park",
      "Zhengmian Hu",
      "Hanjia Lyu",
      "Junda Wu",
      "Ryan Aponte",
      "Yu Xia",
      "Xintong Li",
      "Jing Shi",
      "Hongjie Chen",
      "Viet Dac Lai",
      "Zhouhang Xie",
      "Sungchul Kim",
      "Ruiyi Zhang",
      "Tong Yu",
      "Mehrab Tanjim",
      "Nesreen K. Ahmed",
      "Puneet Mathur",
      "Seunghyun Yoon",
      "Lina Yao",
      "Branislav Kveton",
      "Jihyung Kil",
      "Thien Huu Nguyen",
      "Trung Bui",
      "Tianyi Zhou",
      "Ryan A. Rossi",
      "Franck Dernoncourt"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-12-18T04:48:28Z",
    "pdf_url": "https://arxiv.org/pdf/2412.13501v3"
  },
  {
    "arxiv_id": "2412.13437v1",
    "entry_id": "http://arxiv.org/abs/2412.13437v1",
    "title": "Deploying Foundation Model Powered Agent Services: A Survey",
    "summary": "Foundation model (FM) powered agent services are regarded as a promising solution to develop intelligent and personalized applications for advancing toward Artificial General Intelligence (AGI). To achieve high reliability and scalability in deploying these agent services, it is essential to collaboratively optimize computational and communication resources, thereby ensuring effective resource allocation and seamless service delivery. In pursuit of this vision, this paper proposes a unified framework aimed at providing a comprehensive survey on deploying FM-based agent services across heterogeneous devices, with the emphasis on the integration of model and resource optimization to establish a robust infrastructure for these services. Particularly, this paper begins with exploring various low-level optimization strategies during inference and studies approaches that enhance system scalability, such as parallelism techniques and resource scaling methods. The paper then discusses several prominent FMs and investigates research efforts focused on inference acceleration, including techniques such as model compression and token reduction. Moreover, the paper also investigates critical components for constructing agent services and highlights notable intelligent applications. Finally, the paper presents potential research directions for developing real-time agent services with high Quality of Service (QoS).",
    "authors": [
      "Wenchao Xu",
      "Jinyu Chen",
      "Peirong Zheng",
      "Xiaoquan Yi",
      "Tianyi Tian",
      "Wenhui Zhu",
      "Quan Wan",
      "Haozhao Wang",
      "Yunfeng Fan",
      "Qinliang Su",
      "Xuemin Shen"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2024-12-18T02:15:31Z",
    "pdf_url": "https://arxiv.org/pdf/2412.13437v1"
  },
  {
    "arxiv_id": "2412.13432v3",
    "entry_id": "http://arxiv.org/abs/2412.13432v3",
    "title": "Large Language Model Enhanced Recommender Systems: A Survey",
    "summary": "Large Language Model (LLM) has transformative potential in various domains, including recommender systems (RS). There have been a handful of research that focuses on empowering the RS by LLM. However, previous efforts mainly focus on LLM as RS, which may face the challenge of intolerant inference costs by LLM. Recently, the integration of LLM into RS, known as LLM-Enhanced Recommender Systems (LLMERS), has garnered significant interest due to its potential to address latency and memory constraints in real-world applications. This paper presents a comprehensive survey of the latest research efforts aimed at leveraging LLM to enhance RS capabilities. We identify a critical shift in the field with the move towards incorporating LLM into the online system, notably by avoiding their use during inference. Our survey categorizes the existing LLMERS approaches into three primary types based on the component of the RS model being augmented: Knowledge Enhancement, Interaction Enhancement, and Model Enhancement. We provide an in-depth analysis of each category, discussing the methodologies, challenges, and contributions of recent studies. Furthermore, we highlight several promising research directions that could further advance the field of LLMERS.",
    "authors": [
      "Qidong Liu",
      "Xiangyu Zhao",
      "Yuhao Wang",
      "Yejing Wang",
      "Zijian Zhang",
      "Yuqi Sun",
      "Xiang Li",
      "Maolin Wang",
      "Pengyue Jia",
      "Chong Chen",
      "Wei Huang",
      "Feng Tian"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-12-18T02:07:21Z",
    "pdf_url": "https://arxiv.org/pdf/2412.13432v3"
  },
  {
    "arxiv_id": "2412.12767v1",
    "entry_id": "http://arxiv.org/abs/2412.12767v1",
    "title": "A Survey of Calibration Process for Black-Box LLMs",
    "summary": "Large Language Models (LLMs) demonstrate remarkable performance in semantic understanding and generation, yet accurately assessing their output reliability remains a significant challenge. While numerous studies have explored calibration techniques, they primarily focus on White-Box LLMs with accessible parameters. Black-Box LLMs, despite their superior performance, pose heightened requirements for calibration techniques due to their API-only interaction constraints. Although recent researches have achieved breakthroughs in black-box LLMs calibration, a systematic survey of these methodologies is still lacking. To bridge this gap, we presents the first comprehensive survey on calibration techniques for black-box LLMs. We first define the Calibration Process of LLMs as comprising two interrelated key steps: Confidence Estimation and Calibration. Second, we conduct a systematic review of applicable methods within black-box settings, and provide insights on the unique challenges and connections in implementing these key steps. Furthermore, we explore typical applications of Calibration Process in black-box LLMs and outline promising future research directions, providing new perspectives for enhancing reliability and human-machine alignment. This is our GitHub link: https://github.com/LiangruXie/Calibration-Process-in-Black-Box-LLMs",
    "authors": [
      "Liangru Xie",
      "Hui Liu",
      "Jingying Zeng",
      "Xianfeng Tang",
      "Yan Han",
      "Chen Luo",
      "Jing Huang",
      "Zhen Li",
      "Suhang Wang",
      "Qi He"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-17T10:31:21Z",
    "pdf_url": "https://arxiv.org/pdf/2412.12767v1"
  },
  {
    "arxiv_id": "2412.12493v1",
    "entry_id": "http://arxiv.org/abs/2412.12493v1",
    "title": "A Simple and Fast Way to Handle Semantic Errors in Transactions",
    "summary": "Many computer systems are now being redesigned to incorporate LLM-powered agents, enabling natural language input and more flexible operations. This paper focuses on handling database transactions created by large language models (LLMs). Transactions generated by LLMs may include semantic errors, requiring systems to treat them as long-lived. This allows for human review and, if the transaction is incorrect, removal from the database history. Any removal action must ensure the database's consistency (the \"C\" in ACID principles) is maintained throughout the process.\n  We propose a novel middleware framework based on Invariant Satisfaction (I-Confluence), which ensures consistency by identifying and coordinating dependencies between long-lived transactions and new transactions. This middleware buffers suspicious or compensating transactions to manage coordination states. Using the TPC-C benchmark, we evaluate how transaction generation frequency, user reviews, and invariant completeness impact system performance. For system researchers, this study establishes an interactive paradigm between LLMs and database systems, providing an \"undoing\" mechanism for handling incorrect operations while guaranteeing database consistency. For system engineers, this paper offers a middleware design that integrates removable LLM-generated transactions into existing systems with minimal modifications.",
    "authors": [
      "Jinghan Zeng",
      "Eugene Wu",
      "Sanjay Krishnan"
    ],
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "published": "2024-12-17T02:47:18Z",
    "pdf_url": "https://arxiv.org/pdf/2412.12493v1"
  },
  {
    "arxiv_id": "2412.12456v1",
    "entry_id": "http://arxiv.org/abs/2412.12456v1",
    "title": "Graph Learning in the Era of LLMs: A Survey from the Perspective of Data, Models, and Tasks",
    "summary": "With the increasing prevalence of cross-domain Text-Attributed Graph (TAG) Data (e.g., citation networks, recommendation systems, social networks, and ai4science), the integration of Graph Neural Networks (GNNs) and Large Language Models (LLMs) into a unified Model architecture (e.g., LLM as enhancer, LLM as collaborators, LLM as predictor) has emerged as a promising technological paradigm. The core of this new graph learning paradigm lies in the synergistic combination of GNNs' ability to capture complex structural relationships and LLMs' proficiency in understanding informative contexts from the rich textual descriptions of graphs. Therefore, we can leverage graph description texts with rich semantic context to fundamentally enhance Data quality, thereby improving the representational capacity of model-centric approaches in line with data-centric machine learning principles. By leveraging the strengths of these distinct neural network architectures, this integrated approach addresses a wide range of TAG-based Task (e.g., graph learning, graph reasoning, and graph question answering), particularly in complex industrial scenarios (e.g., supervised, few-shot, and zero-shot settings). In other words, we can treat text as a medium to enable cross-domain generalization of graph learning Model, allowing a single graph model to effectively handle the diversity of downstream graph-based Task across different data domains. This work serves as a foundational reference for researchers and practitioners looking to advance graph learning methodologies in the rapidly evolving landscape of LLM. We consistently maintain the related open-source materials at \\url{https://github.com/xkLi-Allen/Awesome-GNN-in-LLMs-Papers}.",
    "authors": [
      "Xunkai Li",
      "Zhengyu Wu",
      "Jiayi Wu",
      "Hanwen Cui",
      "Jishuo Jia",
      "Rong-Hua Li",
      "Guoren Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DB"
    ],
    "published": "2024-12-17T01:41:17Z",
    "pdf_url": "https://arxiv.org/pdf/2412.12456v1"
  },
  {
    "arxiv_id": "2412.19823v1",
    "entry_id": "http://arxiv.org/abs/2412.19823v1",
    "title": "A Survey on Large Language Models for Communication, Network, and Service Management: Application Insights, Challenges, and Future Directions",
    "summary": "The rapid evolution of communication networks in recent decades has intensified the need for advanced Network and Service Management (NSM) strategies to address the growing demands for efficiency, scalability, enhanced performance, and reliability of these networks. Large Language Models (LLMs) have received tremendous attention due to their unparalleled capabilities in various Natural Language Processing (NLP) tasks and generating context-aware insights, offering transformative potential for automating diverse communication NSM tasks. Contrasting existing surveys that consider a single network domain, this survey investigates the integration of LLMs across different communication network domains, including mobile networks and related technologies, vehicular networks, cloud-based networks, and fog/edge-based networks. First, the survey provides foundational knowledge of LLMs, explicitly detailing the generic transformer architecture, general-purpose and domain-specific LLMs, LLM model pre-training and fine-tuning, and their relation to communication NSM. Under a novel taxonomy of network monitoring and reporting, AI-powered network planning, network deployment and distribution, and continuous network support, we extensively categorize LLM applications for NSM tasks in each of the different network domains, exploring existing literature and their contributions thus far. Then, we identify existing challenges and open issues, as well as future research directions for LLM-driven communication NSM, emphasizing the need for scalable, adaptable, and resource-efficient solutions that align with the dynamic landscape of communication networks. We envision that this survey serves as a holistic roadmap, providing critical insights for leveraging LLMs to enhance NSM.",
    "authors": [
      "Gordon Owusu Boateng",
      "Hani Sami",
      "Ahmed Alagha",
      "Hanae Elmekki",
      "Ahmad Hammoud",
      "Rabeb Mizouni",
      "Azzam Mourad",
      "Hadi Otrok",
      "Jamal Bentahar",
      "Sami Muhaidat",
      "Chamseddine Talhi",
      "Zbigniew Dziong",
      "Mohsen Guizani"
    ],
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-12-16T20:01:36Z",
    "pdf_url": "https://arxiv.org/pdf/2412.19823v1"
  },
  {
    "arxiv_id": "2412.11948v3",
    "entry_id": "http://arxiv.org/abs/2412.11948v3",
    "title": "OpenReviewer: A Specialized Large Language Model for Generating Critical Scientific Paper Reviews",
    "summary": "We present OpenReviewer, an open-source system for generating high-quality peer reviews of machine learning and AI conference papers. At its core is Llama-OpenReviewer-8B, an 8B parameter language model specifically fine-tuned on 79,000 expert reviews from top conferences. Given a PDF paper submission and review template as input, OpenReviewer extracts the full text, including technical content like equations and tables, and generates a structured review following conference-specific guidelines. Our evaluation on 400 test papers shows that OpenReviewer produces considerably more critical and realistic reviews compared to general-purpose LLMs like GPT-4 and Claude-3.5. While other LLMs tend toward overly positive assessments, OpenReviewer's recommendations closely match the distribution of human reviewer ratings. The system provides authors with rapid, constructive feedback to improve their manuscripts before submission, though it is not intended to replace human peer review. OpenReviewer is available as an online demo and open-source tool.",
    "authors": [
      "Maximilian Idahl",
      "Zahra Ahmadi"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-16T16:31:00Z",
    "pdf_url": "https://arxiv.org/pdf/2412.11948v3"
  },
  {
    "arxiv_id": "2412.11698v2",
    "entry_id": "http://arxiv.org/abs/2412.11698v2",
    "title": "On Large Language Models in Mission-Critical IT Governance: Are We Ready Yet?",
    "summary": "Context. The security of critical infrastructure has been a pressing concern since the advent of computers and has become even more critical in today's era of cyber warfare. Protecting mission-critical systems (MCSs), essential for national security, requires swift and robust governance, yet recent events reveal the increasing difficulty of meeting these challenges. Aim. Building on prior research showcasing the potential of Generative AI (GAI), such as Large Language Models, in enhancing risk analysis, we aim to explore practitioners' views on integrating GAI into the governance of IT MCSs. Our goal is to provide actionable insights and recommendations for stakeholders, including researchers, practitioners, and policymakers. Method. We designed a survey to collect practical experiences, concerns, and expectations of practitioners who develop and implement security solutions in the context of MCSs. Conclusions and Future Works. Our findings highlight that the safe use of LLMs in MCS governance requires interdisciplinary collaboration. Researchers should focus on designing regulation-oriented models and focus on accountability; practitioners emphasize data protection and transparency, while policymakers must establish a unified AI framework with global benchmarks to ensure ethical and secure LLMs-based MCS governance.",
    "authors": [
      "Matteo Esposito",
      "Francesco Palagiano",
      "Valentina Lenarduzzi",
      "Davide Taibi"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET",
      "cs.SE"
    ],
    "published": "2024-12-16T12:21:05Z",
    "pdf_url": "https://arxiv.org/pdf/2412.11698v2"
  },
  {
    "arxiv_id": "2412.11694v3",
    "entry_id": "http://arxiv.org/abs/2412.11694v3",
    "title": "From Specific-MLLMs to Omni-MLLMs: A Survey on MLLMs Aligned with Multi-modalities",
    "summary": "To tackle complex tasks in real-world scenarios, more researchers are focusing on Omni-MLLMs, which aim to achieve omni-modal understanding and generation. Beyond the constraints of any specific non-linguistic modality, Omni-MLLMs map various non-linguistic modalities into the embedding space of LLMs and enable the interaction and understanding of arbitrary combinations of modalities within a single model. In this paper, we systematically investigate relevant research and provide a comprehensive survey of Omni-MLLMs. Specifically, we first explain the four core components of Omni-MLLMs for unified multi-modal modeling with a meticulous taxonomy that offers novel perspectives. Then, we introduce the effective integration achieved through two-stage training and discuss the corresponding datasets as well as evaluation. Furthermore, we summarize the main challenges of current Omni-MLLMs and outline future directions. We hope this paper serves as an introduction for beginners and promotes the advancement of related research. Resources have been made publicly available at https://github.com/threegold116/Awesome-Omni-MLLMs.",
    "authors": [
      "Shixin Jiang",
      "Jiafeng Liang",
      "Jiyuan Wang",
      "Xuan Dong",
      "Heng Chang",
      "Weijiang Yu",
      "Jinhua Du",
      "Ming Liu",
      "Bing Qin"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-12-16T12:12:45Z",
    "pdf_url": "https://arxiv.org/pdf/2412.11694v3"
  },
  {
    "arxiv_id": "2412.18619v2",
    "entry_id": "http://arxiv.org/abs/2412.18619v2",
    "title": "Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey",
    "summary": "Building on the foundations of language modeling in natural language processing, Next Token Prediction (NTP) has evolved into a versatile training objective for machine learning tasks across various modalities, achieving considerable success. As Large Language Models (LLMs) have advanced to unify understanding and generation tasks within the textual modality, recent research has shown that tasks from different modalities can also be effectively encapsulated within the NTP framework, transforming the multimodal information into tokens and predict the next one given the context. This survey introduces a comprehensive taxonomy that unifies both understanding and generation within multimodal learning through the lens of NTP. The proposed taxonomy covers five key aspects: Multimodal tokenization, MMNTP model architectures, unified task representation, datasets \\& evaluation, and open challenges. This new taxonomy aims to aid researchers in their exploration of multimodal intelligence. An associated GitHub repository collecting the latest papers and repos is available at https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction",
    "authors": [
      "Liang Chen",
      "Zekun Wang",
      "Shuhuai Ren",
      "Lei Li",
      "Haozhe Zhao",
      "Yunshui Li",
      "Zefan Cai",
      "Hongcheng Guo",
      "Lei Zhang",
      "Yizhe Xiong",
      "Yichi Zhang",
      "Ruoyu Wu",
      "Qingxiu Dong",
      "Ge Zhang",
      "Jian Yang",
      "Lingwei Meng",
      "Shujie Hu",
      "Yulong Chen",
      "Junyang Lin",
      "Shuai Bai",
      "Andreas Vlachos",
      "Xu Tan",
      "Minjia Zhang",
      "Wen Xiao",
      "Aaron Yee",
      "Tianyu Liu",
      "Baobao Chang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM",
      "eess.AS"
    ],
    "published": "2024-12-16T05:02:25Z",
    "pdf_url": "https://arxiv.org/pdf/2412.18619v2"
  },
  {
    "arxiv_id": "2412.15249v2",
    "entry_id": "http://arxiv.org/abs/2412.15249v2",
    "title": "LitLLMs, LLMs for Literature Review: Are we there yet?",
    "summary": "Literature reviews are an essential component of scientific research, but they remain time-intensive and challenging to write, especially due to the recent influx of research papers. This paper explores the zero-shot abilities of recent Large Language Models (LLMs) in assisting with the writing of literature reviews based on an abstract. We decompose the task into two components: 1. Retrieving related works given a query abstract, and 2. Writing a literature review based on the retrieved results. We analyze how effective LLMs are for both components. For retrieval, we introduce a novel two-step search strategy that first uses an LLM to extract meaningful keywords from the abstract of a paper and then retrieves potentially relevant papers by querying an external knowledge base. Additionally, we study a prompting-based re-ranking mechanism with attribution and show that re-ranking doubles the normalized recall compared to naive search methods, while providing insights into the LLM's decision-making process. In the generation phase, we propose a two-step approach that first outlines a plan for the review and then executes steps in the plan to generate the actual review. To evaluate different LLM-based literature review methods, we create test sets from arXiv papers using a protocol designed for rolling use with newly released LLMs to avoid test set contamination in zero-shot evaluations. We release this evaluation protocol to promote additional research and development in this regard. Our empirical results suggest that LLMs show promising potential for writing literature reviews when the task is decomposed into smaller components of retrieval and planning. Our project page including a demonstration system and toolkit can be accessed here: https://litllm.github.io.",
    "authors": [
      "Shubham Agarwal",
      "Gaurav Sahu",
      "Abhay Puri",
      "Issam H. Laradji",
      "Krishnamurthy DJ Dvijotham",
      "Jason Stanley",
      "Laurent Charlin",
      "Christopher Pal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.LG"
    ],
    "published": "2024-12-15T01:12:26Z",
    "pdf_url": "https://arxiv.org/pdf/2412.15249v2"
  },
  {
    "arxiv_id": "2412.10999v3",
    "entry_id": "http://arxiv.org/abs/2412.10999v3",
    "title": "Cocoa: Co-Planning and Co-Execution with AI Agents",
    "summary": "Human collaboration benefits from continuous coordination -- planning, delegating tasks, sharing progress, and adjusting objectives -- to align on shared goals. However, agentic AI systems often limit users to previewing or reviewing an agent's plans for fully autonomous execution. While this may be useful for confirmation and correction, it does not support deeper collaboration between humans and AI agents. We present Cocoa, a system that introduces a novel design pattern -- interactive plans -- for collaborating with an AI agent on complex, multi-step tasks. Informed by a formative study ($n=9$), Cocoa builds on interaction designs from computational notebooks and document editors to support flexible delegation of agency through Co-planning and Co-execution, where users collaboratively compose and execute plans with an Agent. Using scientific research as a sample domain, our lab (n=16) and field deployment (n=7) studies found that Cocoa improved agent steerability without sacrificing ease-of-use compared to a strong chat baseline. Additionally, researchers valued Cocoa for real-world projects and saw the interleaving of co-planning and co-execution as an effective novel paradigm for human-AI collaboration.",
    "authors": [
      "K. J. Kevin Feng",
      "Kevin Pu",
      "Matt Latzke",
      "Tal August",
      "Pao Siangliulue",
      "Jonathan Bragg",
      "Daniel S. Weld",
      "Amy X. Zhang",
      "Joseph Chee Chang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-12-14T23:59:42Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10999v3"
  },
  {
    "arxiv_id": "2412.10982v2",
    "entry_id": "http://arxiv.org/abs/2412.10982v2",
    "title": "MedG-KRP: Medical Graph Knowledge Representation Probing",
    "summary": "Large language models (LLMs) have recently emerged as powerful tools, finding many medical applications. LLMs' ability to coalesce vast amounts of information from many sources to generate a response-a process similar to that of a human expert-has led many to see potential in deploying LLMs for clinical use. However, medicine is a setting where accurate reasoning is paramount. Many researchers are questioning the effectiveness of multiple choice question answering (MCQA) benchmarks, frequently used to test LLMs. Researchers and clinicians alike must have complete confidence in LLMs' abilities for them to be deployed in a medical setting. To address this need for understanding, we introduce a knowledge graph (KG)-based method to evaluate the biomedical reasoning abilities of LLMs. Essentially, we map how LLMs link medical concepts in order to better understand how they reason. We test GPT-4, Llama3-70b, and PalmyraMed-70b, a specialized medical model. We enlist a panel of medical students to review a total of 60 LLM-generated graphs and compare these graphs to BIOS, a large biomedical KG. We observe GPT-4 to perform best in our human review but worst in our ground truth comparison; vice-versa with PalmyraMed, the medical model. Our work provides a means of visualizing the medical reasoning pathways of LLMs so they can be implemented in clinical settings safely and effectively.",
    "authors": [
      "Gabriel R. Rosenbaum",
      "Lavender Yao Jiang",
      "Ivaxi Sheth",
      "Jaden Stryker",
      "Anton Alyakin",
      "Daniel Alexander Alber",
      "Nicolas K. Goff",
      "Young Joon Fred Kwon",
      "John Markert",
      "Mustafa Nasir-Moin",
      "Jan Moritz Niehues",
      "Karl L. Sangwon",
      "Eunice Yang",
      "Eric Karl Oermann"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-14T22:23:20Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10982v2"
  },
  {
    "arxiv_id": "2412.10904v1",
    "entry_id": "http://arxiv.org/abs/2412.10904v1",
    "title": "CEKER: A Generalizable LLM Framework for Literature Analysis with a Case Study in Unikernel Security",
    "summary": "Literature reviews are a critical component of formulating and justifying new research, but are a manual and often time-consuming process. This research introduces a novel, generalizable approach to literature analysis called CEKER which uses a three-step process to streamline the collection of literature, the extraction of key insights, and the summarized analysis of key trends and gaps. Leveraging Large Language Models (LLMs), this methodology represents a significant shift from traditional manual literature reviews, offering a scalable, flexible, and repeatable approach that can be applied across diverse research domains.\n  A case study on unikernel security illustrates CEKER's ability to generate novel insights validated against previous manual methods. CEKER's analysis highlighted reduced attack surface as the most prominent theme. Key security gaps included the absence of Address Space Layout Randomization, missing debugging tools, and limited entropy generation, all of which represent important challenges to unikernel security. The study also revealed a reliance on hypervisors as a potential attack vector and emphasized the need for dynamic security adjustments to address real-time threats.",
    "authors": [
      "Alex Wollman",
      "John Hastings"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-12-14T17:28:43Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10904v1"
  },
  {
    "arxiv_id": "2412.10609v1",
    "entry_id": "http://arxiv.org/abs/2412.10609v1",
    "title": "A systematic review of norm emergence in multi-agent systems",
    "summary": "Multi-agent systems (MAS) have gained relevance in the field of artificial intelligence by offering tools for modelling complex environments where autonomous agents interact to achieve common or individual goals. In these systems, norms emerge as a fundamental component to regulate the behaviour of agents, promoting cooperation, coordination and conflict resolution. This article presents a systematic review, following the PRISMA method, on the emergence of norms in MAS, exploring the main mechanisms and factors that influence this process. Sociological, structural, emotional and cognitive aspects that facilitate the creation, propagation and reinforcement of norms are addressed. The findings highlight the crucial role of social network topology, as well as the importance of emotions and shared values in the adoption and maintenance of norms. Furthermore, opportunities are identified for future research that more explicitly integrates emotional and ethical dynamics in the design of adaptive normative systems. This work provides a comprehensive overview of the current state of research on norm emergence in MAS, serving as a basis for advancing the development of more efficient and flexible systems in artificial and real-world contexts.",
    "authors": [
      "Carmengelys Cordova",
      "Joaquin Taverner",
      "Elena Del Val",
      "Estefania Argente"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2024-12-13T23:31:24Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10609v1"
  },
  {
    "arxiv_id": "2412.10220v1",
    "entry_id": "http://arxiv.org/abs/2412.10220v1",
    "title": "How good is my story? Towards quantitative metrics for evaluating LLM-generated XAI narratives",
    "summary": "A rapidly developing application of LLMs in XAI is to convert quantitative explanations such as SHAP into user-friendly narratives to explain the decisions made by smaller prediction models. Evaluating the narratives without relying on human preference studies or surveys is becoming increasingly important in this field. In this work we propose a framework and explore several automated metrics to evaluate LLM-generated narratives for explanations of tabular classification tasks. We apply our approach to compare several state-of-the-art LLMs across different datasets and prompt types. As a demonstration of their utility, these metrics allow us to identify new challenges related to LLM hallucinations for XAI narratives.",
    "authors": [
      "Timour Ichmoukhamedov",
      "James Hinns",
      "David Martens"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-13T15:45:45Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10220v1"
  },
  {
    "arxiv_id": "2412.10477v1",
    "entry_id": "http://arxiv.org/abs/2412.10477v1",
    "title": "Benchmarking large language models for materials synthesis: the case of atomic layer deposition",
    "summary": "In this work we introduce an open-ended question benchmark, ALDbench, to evaluate the performance of large language models (LLMs) in materials synthesis, and in particular in the field of atomic layer deposition, a thin film growth technique used in energy applications and microelectronics. Our benchmark comprises questions with a level of difficulty ranging from graduate level to domain expert current with the state of the art in the field. Human experts reviewed the questions along the criteria of difficulty and specificity, and the model responses along four different criteria: overall quality, specificity, relevance, and accuracy. We ran this benchmark on an instance of OpenAI's GPT-4o. The responses from the model received a composite quality score of 3.7 on a 1 to 5 scale, consistent with a passing grade. However, 36% of the questions received at least one below average score. An in-depth analysis of the responses identified at least five instances of suspected hallucination. Finally, we observed statistically significant correlations between the difficulty of the question and the quality of the response, the difficulty of the question and the relevance of the response, and the specificity of the question and the accuracy of the response as graded by the human experts. This emphasizes the need to evaluate LLMs across multiple criteria beyond difficulty or accuracy.",
    "authors": [
      "Angel Yanguas-Gil",
      "Matthew T. Dearing",
      "Jeffrey W. Elam",
      "Jessica C. Jones",
      "Sungjoon Kim",
      "Adnan Mohammad",
      "Chi Thang Nguyen",
      "Bratin Sengupta"
    ],
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "cs.AI"
    ],
    "published": "2024-12-13T05:10:29Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10477v1"
  },
  {
    "arxiv_id": "2412.09429v4",
    "entry_id": "http://arxiv.org/abs/2412.09429v4",
    "title": "From Intention To Implementation: Automating Biomedical Research via LLMs",
    "summary": "Conventional biomedical research is increasingly labor-intensive due to the exponential growth of scientific literature and datasets. Artificial intelligence (AI), particularly Large Language Models (LLMs), has the potential to revolutionize this process by automating various steps. Still, significant challenges remain, including the need for multidisciplinary expertise, logicality of experimental design, and performance measurements. This paper introduces BioResearcher, the first end-to-end automated system designed to streamline the entire biomedical research process involving dry lab experiments. BioResearcher employs a modular multi-agent architecture, integrating specialized agents for search, literature processing, experimental design, and programming. By decomposing complex tasks into logically related sub-tasks and utilizing a hierarchical learning approach, BioResearcher effectively addresses the challenges of multidisciplinary requirements and logical complexity. Furthermore, BioResearcher incorporates an LLM-based reviewer for in-process quality control and introduces novel evaluation metrics to assess the quality and automation of experimental protocols. BioResearcher successfully achieves an average execution success rate of 63.07% across eight previously unmet research objectives. The generated protocols, on average, outperform typical agent systems by 22.0% on five quality metrics. The system demonstrates significant potential to reduce researchers' workloads and accelerate biomedical discoveries, paving the way for future innovations in automated research systems.",
    "authors": [
      "Yi Luo",
      "Linghang Shi",
      "Yihao Li",
      "Aobo Zhuang",
      "Yeyun Gong",
      "Ling Liu",
      "Chen Lin"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-12T16:35:05Z",
    "pdf_url": "https://arxiv.org/pdf/2412.09429v4"
  },
  {
    "arxiv_id": "2412.09385v2",
    "entry_id": "http://arxiv.org/abs/2412.09385v2",
    "title": "AI Predicts AGI: Leveraging AGI Forecasting and Peer Review to Explore LLMs' Complex Reasoning Capabilities",
    "summary": "We tasked 16 state-of-the-art large language models (LLMs) with estimating the likelihood of Artificial General Intelligence (AGI) emerging by 2030. To assess the quality of these forecasts, we implemented an automated peer review process (LLM-PR). The LLMs' estimates varied widely, ranging from 3% (Reka- Core) to 47.6% (GPT-4o), with a median of 12.5%. These estimates closely align with a recent expert survey that projected a 10% likelihood of AGI by 2027, underscoring the relevance of LLMs in forecasting complex, speculative scenarios. The LLM-PR process demonstrated strong reliability, evidenced by a high Intraclass Correlation Coefficient (ICC = 0.79), reflecting notable consistency in scoring across the models. Among the models, Pplx-70b-online emerged as the top performer, while Gemini-1.5-pro-api ranked the lowest. A cross-comparison with external benchmarks, such as LMSYS Chatbot Arena, revealed that LLM rankings remained consistent across different evaluation methods, suggesting that existing benchmarks may not encapsulate some of the skills relevant for AGI prediction. We further explored the use of weighting schemes based on external benchmarks, optimizing the alignment of LLMs' predictions with human expert forecasts. This analysis led to the development of a new, 'AGI benchmark' designed to highlight performance differences in AGI-related tasks. Our findings offer insights into LLMs' capabilities in speculative, interdisciplinary forecasting tasks and emphasize the growing need for innovative evaluation frameworks for assessing AI performance in complex, uncertain real-world scenarios.",
    "authors": [
      "Fabrizio Davide",
      "Pietro Torre",
      "Leonardo Ercolani",
      "Andrea Gaggioli"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-12T15:52:41Z",
    "pdf_url": "https://arxiv.org/pdf/2412.09385v2"
  },
  {
    "arxiv_id": "2412.09237v2",
    "entry_id": "http://arxiv.org/abs/2412.09237v2",
    "title": "LMAgent: A Large-scale Multimodal Agents Society for Multi-user Simulation",
    "summary": "The believable simulation of multi-user behavior is crucial for understanding complex social systems. Recently, large language models (LLMs)-based AI agents have made significant progress, enabling them to achieve human-like intelligence across various tasks. However, real human societies are often dynamic and complex, involving numerous individuals engaging in multimodal interactions. In this paper, taking e-commerce scenarios as an example, we present LMAgent, a very large-scale and multimodal agents society based on multimodal LLMs. In LMAgent, besides freely chatting with friends, the agents can autonomously browse, purchase, and review products, even perform live streaming e-commerce. To simulate this complex system, we introduce a self-consistency prompting mechanism to augment agents' multimodal capabilities, resulting in significantly improved decision-making performance over the existing multi-agent system. Moreover, we propose a fast memory mechanism combined with the small-world model to enhance system efficiency, which supports more than 10,000 agent simulations in a society. Experiments on agents' behavior show that these agents achieve comparable performance to humans in behavioral indicators. Furthermore, compared with the existing LLMs-based multi-agent system, more different and valuable phenomena are exhibited, such as herd behavior, which demonstrates the potential of LMAgent in credible large-scale social behavior simulations.",
    "authors": [
      "Yijun Liu",
      "Wu Liu",
      "Xiaoyan Gu",
      "Yong Rui",
      "Xiaodong He",
      "Yongdong Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-12T12:47:09Z",
    "pdf_url": "https://arxiv.org/pdf/2412.09237v2"
  },
  {
    "arxiv_id": "2412.09165v4",
    "entry_id": "http://arxiv.org/abs/2412.09165v4",
    "title": "When Text Embedding Meets Large Language Model: A Comprehensive Survey",
    "summary": "Text embedding has become a foundational technology in natural language processing (NLP) during the deep learning era, driving advancements across a wide array of downstream tasks. While many natural language understanding challenges can now be modeled using generative paradigms and leverage the robust generative and comprehension capabilities of large language models (LLMs), numerous practical applications - such as semantic matching, clustering, and information retrieval - continue to rely on text embeddings for their efficiency and effectiveness. Therefore, integrating LLMs with text embeddings has become a major research focus in recent years. In this survey, we categorize the interplay between LLMs and text embeddings into three overarching themes: (1) LLM-augmented text embedding, enhancing traditional embedding methods with LLMs; (2) LLMs as text embedders, adapting their innate capabilities for high-quality embedding; and (3) Text embedding understanding with LLMs, leveraging LLMs to analyze and interpret embeddings. By organizing recent works based on interaction patterns rather than specific downstream applications, we offer a novel and systematic overview of contributions from various research and application domains in the era of LLMs. Furthermore, we highlight the unresolved challenges that persisted in the pre-LLM era with pre-trained language models (PLMs) and explore the emerging obstacles brought forth by LLMs. Building on this analysis, we outline prospective directions for the evolution of text embedding, addressing both theoretical and practical opportunities in the rapidly advancing landscape of NLP.",
    "authors": [
      "Zhijie Nie",
      "Zhangchi Feng",
      "Mingxin Li",
      "Cunwang Zhang",
      "Yanzhao Zhang",
      "Dingkun Long",
      "Richong Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-12-12T10:50:26Z",
    "pdf_url": "https://arxiv.org/pdf/2412.09165v4"
  },
  {
    "arxiv_id": "2412.08642v1",
    "entry_id": "http://arxiv.org/abs/2412.08642v1",
    "title": "Generative Semantic Communication: Architectures, Technologies, and Applications",
    "summary": "This paper delves into the applications of generative artificial intelligence (GAI) in semantic communication (SemCom) and presents a thorough study. Three popular SemCom systems enabled by classical GAI models are first introduced, including variational autoencoders, generative adversarial networks, and diffusion models. For each system, the fundamental concept of the GAI model, the corresponding SemCom architecture, and the associated literature review of recent efforts are elucidated. Then, a novel generative SemCom system is proposed by incorporating the cutting-edge GAI technology-large language models (LLMs). This system features two LLM-based AI agents at both the transmitter and receiver, serving as \"brains\" to enable powerful information understanding and content regeneration capabilities, respectively. This innovative design allows the receiver to directly generate the desired content, instead of recovering the bit stream, based on the coded semantic information conveyed by the transmitter. Therefore, it shifts the communication mindset from \"information recovery\" to \"information regeneration\" and thus ushers in a new era of generative SemCom. A case study on point-to-point video retrieval is presented to demonstrate the superiority of the proposed generative SemCom system, showcasing a 99.98% reduction in communication overhead and a 53% improvement in retrieval accuracy compared to the traditional communication system. Furthermore, four typical application scenarios for generative SemCom are delineated, followed by a discussion of three open issues warranting future investigation. In a nutshell, this paper provides a holistic set of guidelines for applying GAI in SemCom, paving the way for the efficient implementation of generative SemCom in future wireless networks.",
    "authors": [
      "Jinke Ren",
      "Yaping Sun",
      "Hongyang Du",
      "Weiwen Yuan",
      "Chongjie Wang",
      "Xianda Wang",
      "Yingbin Zhou",
      "Ziwei Zhu",
      "Fangxin Wang",
      "Shuguang Cui"
    ],
    "categories": [
      "cs.IT",
      "cs.LG",
      "cs.NI"
    ],
    "published": "2024-12-11T18:59:50Z",
    "pdf_url": "https://arxiv.org/pdf/2412.08642v1"
  },
  {
    "arxiv_id": "2412.08430v1",
    "entry_id": "http://arxiv.org/abs/2412.08430v1",
    "title": "Assessing Personalized AI Mentoring with Large Language Models in the Computing Field",
    "summary": "This paper provides an in-depth evaluation of three state-of-the-art Large Language Models (LLMs) for personalized career mentoring in the computing field, using three distinct student profiles that consider gender, race, and professional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2 using a zero-shot learning approach without human intervention. A quantitative evaluation was conducted through a custom natural language processing analytics pipeline to highlight the uniqueness of the responses and to identify words reflecting each student's profile, including race, gender, or professional level. The analysis of frequently used words in the responses indicates that GPT-4 offers more personalized mentoring compared to the other two LLMs. Additionally, a qualitative evaluation was performed to see if human experts reached similar conclusions. The analysis of survey responses shows that GPT-4 outperformed the other two LLMs in delivering more accurate and useful mentoring while addressing specific challenges with encouragement languages. Our work establishes a foundation for developing personalized mentoring tools based on LLMs, incorporating human mentors in the process to deliver a more impactful and tailored mentoring experience.",
    "authors": [
      "Xiao Luo",
      "Sean O'Connell",
      "Shamima Mithun"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-11T14:51:13Z",
    "pdf_url": "https://arxiv.org/pdf/2412.08430v1"
  },
  {
    "arxiv_id": "2412.08158v1",
    "entry_id": "http://arxiv.org/abs/2412.08158v1",
    "title": "How Vision-Language Tasks Benefit from Large Pre-trained Models: A Survey",
    "summary": "The exploration of various vision-language tasks, such as visual captioning, visual question answering, and visual commonsense reasoning, is an important area in artificial intelligence and continuously attracts the research community's attention. Despite the improvements in overall performance, classic challenges still exist in vision-language tasks and hinder the development of this area. In recent years, the rise of pre-trained models is driving the research on vision-language tasks. Thanks to the massive scale of training data and model parameters, pre-trained models have exhibited excellent performance in numerous downstream tasks. Inspired by the powerful capabilities of pre-trained models, new paradigms have emerged to solve the classic challenges. Such methods have become mainstream in current research with increasing attention and rapid advances. In this paper, we present a comprehensive overview of how vision-language tasks benefit from pre-trained models. First, we review several main challenges in vision-language tasks and discuss the limitations of previous solutions before the era of pre-training. Next, we summarize the recent advances in incorporating pre-trained models to address the challenges in vision-language tasks. Finally, we analyze the potential risks associated with the inherent limitations of pre-trained models and discuss possible solutions, attempting to provide future research directions.",
    "authors": [
      "Yayun Qi",
      "Hongxi Li",
      "Yiqi Song",
      "Xinxiao Wu",
      "Jiebo Luo"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-12-11T07:29:04Z",
    "pdf_url": "https://arxiv.org/pdf/2412.08158v1"
  },
  {
    "arxiv_id": "2412.08098v2",
    "entry_id": "http://arxiv.org/abs/2412.08098v2",
    "title": "What You See Is Not Always What You Get: An Empirical Study of Code Comprehension by Large Language Models",
    "summary": "Recent studies have demonstrated outstanding capabilities of large language models (LLMs) in software engineering tasks, including code generation and comprehension. While LLMs have shown significant potential in assisting with coding, it is perceived that LLMs are vulnerable to adversarial attacks. In this paper, we investigate the vulnerability of LLMs to imperceptible attacks, where hidden character manipulation in source code misleads LLMs' behaviour while remaining undetectable to human reviewers. We devise these attacks into four distinct categories and analyse their impacts on code analysis and comprehension tasks. These four types of imperceptible coding character attacks include coding reordering, invisible coding characters, code deletions, and code homoglyphs. To comprehensively benchmark the robustness of current LLMs solutions against the attacks, we present a systematic experimental evaluation on multiple state-of-the-art LLMs. Our experimental design introduces two key performance metrics, namely model confidence using log probabilities of response, and the response correctness. A set of controlled experiments are conducted using a large-scale perturbed and unperturbed code snippets as the primary prompt input. Our findings confirm the susceptibility of LLMs to imperceptible coding character attacks, while different LLMs present different negative correlations between perturbation magnitude and performance. These results highlight the urgent need for robust LLMs capable of manoeuvring behaviours under imperceptible adversarial conditions. We anticipate this work provides valuable insights for enhancing the security and trustworthiness of LLMs in software engineering applications.",
    "authors": [
      "Bangshuo Zhu",
      "Jiawen Wen",
      "Huaming Chen"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-12-11T04:52:41Z",
    "pdf_url": "https://arxiv.org/pdf/2412.08098v2"
  },
  {
    "arxiv_id": "2412.07951v3",
    "entry_id": "http://arxiv.org/abs/2412.07951v3",
    "title": "From Lived Experience to Insight: Unpacking the Psychological Risks of Using AI Conversational Agents",
    "summary": "Recent gains in popularity of AI conversational agents have led to their increased use for improving productivity and supporting well-being. While previous research has aimed to understand the risks associated with interactions with AI conversational agents, these studies often fall short in capturing the lived experiences of individuals. Additionally, psychological risks have often been presented as a sub-category within broader AI-related risks in past taxonomy works, leading to under-representation of the impact of psychological risks of AI use. To address these challenges, our work presents a novel risk taxonomy focusing on psychological risks of using AI gathered through the lived experiences of individuals. We employed a mixed-method approach, involving a comprehensive survey with 283 people with lived mental health experience and workshops involving experts with lived experience to develop a psychological risk taxonomy. Our taxonomy features 19 AI behaviors, 21 negative psychological impacts, and 15 contexts related to individuals. Additionally, we propose a novel multi-path vignette-based framework for understanding the complex interplay between AI behaviors, psychological impacts, and individual user contexts. Finally, based on the feedback obtained from the workshop sessions, we present design recommendations for developing safer and more robust AI agents. Our work offers an in-depth understanding of the psychological risks associated with AI conversational agents and provides actionable recommendations for policymakers, researchers, and developers.",
    "authors": [
      "Mohit Chandra",
      "Suchismita Naik",
      "Denae Ford",
      "Ebele Okoli",
      "Munmun De Choudhury",
      "Mahsa Ershadi",
      "Gonzalo Ramos",
      "Javier Hernandez",
      "Ananya Bhattacharjee",
      "Shahed Warreth",
      "Jina Suh"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-12-10T22:31:29Z",
    "pdf_url": "https://arxiv.org/pdf/2412.07951v3"
  },
  {
    "arxiv_id": "2501.14734v1",
    "entry_id": "http://arxiv.org/abs/2501.14734v1",
    "title": "Research on the Application of Spark Streaming Real-Time Data Analysis System and large language model Intelligent Agents",
    "summary": "This study explores the integration of Agent AI with LangGraph to enhance real-time data analysis systems in big data environments. The proposed framework overcomes limitations of static workflows, inefficient stateful computations, and lack of human intervention by leveraging LangGraph's graph-based workflow construction and dynamic decision-making capabilities. LangGraph allows large language models (LLMs) to dynamically determine control flows, invoke tools, and assess the necessity of further actions, improving flexibility and efficiency.\n  The system architecture incorporates Apache Spark Streaming, Kafka, and LangGraph to create a high-performance sentiment analysis system. LangGraph's capabilities include precise state management, dynamic workflow construction, and robust memory checkpointing, enabling seamless multi-turn interactions and context retention. Human-in-the-loop mechanisms are integrated to refine sentiment analysis, particularly in ambiguous or high-stakes scenarios, ensuring greater reliability and contextual relevance.\n  Key features such as real-time state streaming, debugging via LangGraph Studio, and efficient handling of large-scale data streams make this framework ideal for adaptive decision-making. Experimental results confirm the system's ability to classify inquiries, detect sentiment trends, and escalate complex issues for manual review, demonstrating a synergistic blend of LLM capabilities and human oversight.\n  This work presents a scalable, adaptable, and reliable solution for real-time sentiment analysis and decision-making, advancing the use of Agent AI and LangGraph in big data applications.",
    "authors": [
      "Jialin Wang",
      "Zhihua Duan"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2024-12-10T05:51:11Z",
    "pdf_url": "https://arxiv.org/pdf/2501.14734v1"
  },
  {
    "arxiv_id": "2412.07201v1",
    "entry_id": "http://arxiv.org/abs/2412.07201v1",
    "title": "A Review on the Applications of Transformer-based language models for Nucleotide Sequence Analysis",
    "summary": "In recent times, Transformer-based language models are making quite an impact in the field of natural language processing. As relevant parallels can be drawn between biological sequences and natural languages, the models used in NLP can be easily extended and adapted for various applications in bioinformatics. In this regard, this paper introduces the major developments of Transformer-based models in the recent past in the context of nucleotide sequences. We have reviewed and analysed a large number of application-based papers on this subject, giving evidence of the main characterizing features and to different approaches that may be adopted to customize such powerful computational machines. We have also provided a structured description of the functioning of Transformers, that may enable even first time users to grab the essence of such complex architectures. We believe this review will help the scientific community in understanding the various applications of Transformer-based language models to nucleotide sequences. This work will motivate the readers to build on these methodologies to tackle also various other problems in the field of bioinformatics.",
    "authors": [
      "Nimisha Ghosh",
      "Daniele Santoni",
      "Indrajit Saha",
      "Giovanni Felici"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-10T05:33:09Z",
    "pdf_url": "https://arxiv.org/pdf/2412.07201v1"
  },
  {
    "arxiv_id": "2412.07177v1",
    "entry_id": "http://arxiv.org/abs/2412.07177v1",
    "title": "Effective Reward Specification in Deep Reinforcement Learning",
    "summary": "In the last decade, Deep Reinforcement Learning has evolved into a powerful tool for complex sequential decision-making problems. It combines deep learning's proficiency in processing rich input signals with reinforcement learning's adaptability across diverse control tasks. At its core, an RL agent seeks to maximize its cumulative reward, enabling AI algorithms to uncover novel solutions previously unknown to experts. However, this focus on reward maximization also introduces a significant difficulty: improper reward specification can result in unexpected, misaligned agent behavior and inefficient learning. The complexity of accurately specifying the reward function is further amplified by the sequential nature of the task, the sparsity of learning signals, and the multifaceted aspects of the desired behavior.\n  In this thesis, we survey the literature on effective reward specification strategies, identify core challenges relating to each of these approaches, and propose original contributions addressing the issue of sample efficiency and alignment in deep reinforcement learning. Reward specification represents one of the most challenging aspects of applying reinforcement learning in real-world domains. Our work underscores the absence of a universal solution to this complex and nuanced challenge; solving it requires selecting the most appropriate tools for the specific requirements of each unique application.",
    "authors": [
      "Julien Roy"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-12-10T04:22:11Z",
    "pdf_url": "https://arxiv.org/pdf/2412.07177v1"
  },
  {
    "arxiv_id": "2412.07116v1",
    "entry_id": "http://arxiv.org/abs/2412.07116v1",
    "title": "A Review of Human Emotion Synthesis Based on Generative Technology",
    "summary": "Human emotion synthesis is a crucial aspect of affective computing. It involves using computational methods to mimic and convey human emotions through various modalities, with the goal of enabling more natural and effective human-computer interactions. Recent advancements in generative models, such as Autoencoders, Generative Adversarial Networks, Diffusion Models, Large Language Models, and Sequence-to-Sequence Models, have significantly contributed to the development of this field. However, there is a notable lack of comprehensive reviews in this field. To address this problem, this paper aims to address this gap by providing a thorough and systematic overview of recent advancements in human emotion synthesis based on generative models. Specifically, this review will first present the review methodology, the emotion models involved, the mathematical principles of generative models, and the datasets used. Then, the review covers the application of different generative models to emotion synthesis based on a variety of modalities, including facial images, speech, and text. It also examines mainstream evaluation metrics. Additionally, the review presents some major findings and suggests future research directions, providing a comprehensive understanding of the role of generative technology in the nuanced domain of emotion synthesis.",
    "authors": [
      "Fei Ma",
      "Yukan Li",
      "Yifan Xie",
      "Ying He",
      "Yi Zhang",
      "Hongwei Ren",
      "Zhou Liu",
      "Wei Yao",
      "Fuji Ren",
      "Fei Richard Yu",
      "Shiguang Ni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-10T02:06:10Z",
    "pdf_url": "https://arxiv.org/pdf/2412.07116v1"
  },
  {
    "arxiv_id": "2412.06878v1",
    "entry_id": "http://arxiv.org/abs/2412.06878v1",
    "title": "SafeWatch: An Efficient Safety-Policy Following Video Guardrail Model with Transparent Explanations",
    "summary": "With the rise of generative AI and rapid growth of high-quality video generation, video guardrails have become more crucial than ever to ensure safety and security across platforms. Current video guardrails, however, are either overly simplistic, relying on pure classification models trained on simple policies with limited unsafe categories, which lack detailed explanations, or prompting multimodal large language models (MLLMs) with long safety guidelines, which are inefficient and impractical for guardrailing real-world content. To bridge this gap, we propose SafeWatch, an efficient MLLM-based video guardrail model designed to follow customized safety policies and provide multi-label video guardrail outputs with content-specific explanations in a zero-shot manner. In particular, unlike traditional MLLM-based guardrails that encode all safety policies autoregressively, causing inefficiency and bias, SafeWatch uniquely encodes each policy chunk in parallel and eliminates their position bias such that all policies are attended simultaneously with equal importance. In addition, to improve efficiency and accuracy, SafeWatch incorporates a policy-aware visual token pruning algorithm that adaptively selects the most relevant video tokens for each policy, discarding noisy or irrelevant information. This allows for more focused, policy-compliant guardrail with significantly reduced computational overhead. Considering the limitations of existing video guardrail benchmarks, we propose SafeWatch-Bench, a large-scale video guardrail benchmark comprising over 2M videos spanning six safety categories which covers over 30 tasks to ensure a comprehensive coverage of all potential safety scenarios. SafeWatch outperforms SOTA by 28.2% on SafeWatch-Bench, 13.6% on benchmarks, cuts costs by 10%, and delivers top-tier explanations validated by LLM and human reviews.",
    "authors": [
      "Zhaorun Chen",
      "Francesco Pinto",
      "Minzhou Pan",
      "Bo Li"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-12-09T18:59:04Z",
    "pdf_url": "https://arxiv.org/pdf/2412.06878v1"
  },
  {
    "arxiv_id": "2412.06602v3",
    "entry_id": "http://arxiv.org/abs/2412.06602v3",
    "title": "Towards Controllable Speech Synthesis in the Era of Large Language Models: A Systematic Survey",
    "summary": "Text-to-speech (TTS) has advanced from generating natural-sounding speech to enabling fine-grained control over attributes like emotion, timbre, and style. Driven by rising industrial demand and breakthroughs in deep learning, e.g., diffusion and large language models (LLMs), controllable TTS has become a rapidly growing research area. This survey provides the first comprehensive review of controllable TTS methods, from traditional control techniques to emerging approaches using natural language prompts. We categorize model architectures, control strategies, and feature representations, while also summarizing challenges, datasets, and evaluations in controllable TTS. This survey aims to guide researchers and practitioners by offering a clear taxonomy and highlighting future directions in this fast-evolving field. One can visit https://github.com/imxtx/awesome-controllabe-speech-synthesis for a comprehensive paper list and updates.",
    "authors": [
      "Tianxin Xie",
      "Yan Rong",
      "Pengfei Zhang",
      "Wenwu Wang",
      "Li Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.MM",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2024-12-09T15:50:25Z",
    "pdf_url": "https://arxiv.org/pdf/2412.06602v3"
  },
  {
    "arxiv_id": "2412.06412v3",
    "entry_id": "http://arxiv.org/abs/2412.06412v3",
    "title": "StarWhisper Telescope: An AI framework for automating end-to-end astronomical observations",
    "summary": "The exponential growth of large-scale telescope arrays has boosted time-domain astronomy development but introduced operational bottlenecks, including labor-intensive observation planning, data processing, and real-time decision-making. Here we present the StarWhisper Telescope system, an AI agent framework automating end-to-end astronomical observations for surveys like the Nearby Galaxy Supernovae Survey. By integrating large language models with specialized function calls and modular workflows, StarWhisper Telescope autonomously generates site-specific observation lists, executes real-time image analysis via pipelines, and dynamically triggers follow-up proposals upon transient detection. The system reduces human intervention through automated observation planning, telescope controlling and data processing, while enabling seamless collaboration between amateur and professional astronomers. Deployed across Nearby Galaxy Supernovae Survey's network of 10 amateur telescopes, the StarWhisper Telescope has detected transients with promising response times relative to existing surveys. Furthermore, StarWhisper Telescope's scalable agent architecture provides a blueprint for future facilities like the Global Open Transient Telescope Array, where AI-driven autonomy will be critical for managing 60 telescopes.",
    "authors": [
      "Cunshi Wang",
      "Yu Zhang",
      "Yuyang Li",
      "Xinjie Hu",
      "Yiming Mao",
      "Xunhao Chen",
      "Pengliang Du",
      "Rui Wang",
      "Ying Wu",
      "Hang Yang",
      "Yansong Li",
      "Beichuan Wang",
      "Haiyang Mu",
      "Zheng Wang",
      "Jianfeng Tian",
      "Liang Ge",
      "Yongna Mao",
      "Shengming Li",
      "Xiaomeng Lu",
      "Jinhang Zou",
      "Yang Huang",
      "Ningchen Sun",
      "Jie Zheng",
      "Min He",
      "Yu Bai",
      "Junjie Jin",
      "Hong Wu",
      "Jifeng Liu"
    ],
    "categories": [
      "astro-ph.IM",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-09T11:40:06Z",
    "pdf_url": "https://arxiv.org/pdf/2412.06412v3"
  },
  {
    "arxiv_id": "2412.10415v1",
    "entry_id": "http://arxiv.org/abs/2412.10415v1",
    "title": "Generative Adversarial Reviews: When LLMs Become the Critic",
    "summary": "The peer review process is fundamental to scientific progress, determining which papers meet the quality standards for publication. Yet, the rapid growth of scholarly production and increasing specialization in knowledge areas strain traditional scientific feedback mechanisms. In light of this, we introduce Generative Agent Reviewers (GAR), leveraging LLM-empowered agents to simulate faithful peer reviewers. To enable generative reviewers, we design an architecture that extends a large language model with memory capabilities and equips agents with reviewer personas derived from historical data. Central to this approach is a graph-based representation of manuscripts, condensing content and logically organizing information - linking ideas with evidence and technical details. GAR's review process leverages external knowledge to evaluate paper novelty, followed by detailed assessment using the graph representation and multi-round assessment. Finally, a meta-reviewer aggregates individual reviews to predict the acceptance decision. Our experiments demonstrate that GAR performs comparably to human reviewers in providing detailed feedback and predicting paper outcomes. Beyond mere performance comparison, we conduct insightful experiments, such as evaluating the impact of reviewer expertise and examining fairness in reviews. By offering early expert-level feedback, typically restricted to a limited group of researchers, GAR democratizes access to transparent and in-depth evaluation.",
    "authors": [
      "Nicolas Bougie",
      "Narimasa Watanabe"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-09T06:58:17Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10415v1"
  },
  {
    "arxiv_id": "2412.06113v1",
    "entry_id": "http://arxiv.org/abs/2412.06113v1",
    "title": "Privacy-Preserving Large Language Models: Mechanisms, Applications, and Future Directions",
    "summary": "The rapid advancement of large language models (LLMs) has revolutionized natural language processing, enabling applications in diverse domains such as healthcare, finance and education. However, the growing reliance on extensive data for training and inference has raised significant privacy concerns, ranging from data leakage to adversarial attacks. This survey comprehensively explores the landscape of privacy-preserving mechanisms tailored for LLMs, including differential privacy, federated learning, cryptographic protocols, and trusted execution environments. We examine their efficacy in addressing key privacy challenges, such as membership inference and model inversion attacks, while balancing trade-offs between privacy and model utility. Furthermore, we analyze privacy-preserving applications of LLMs in privacy-sensitive domains, highlighting successful implementations and inherent limitations. Finally, this survey identifies emerging research directions, emphasizing the need for novel frameworks that integrate privacy by design into the lifecycle of LLMs. By synthesizing state-of-the-art approaches and future trends, this paper provides a foundation for developing robust, privacy-preserving large language models that safeguard sensitive information without compromising performance.",
    "authors": [
      "Guoshenghui Zhao",
      "Eric Song"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-12-09T00:24:09Z",
    "pdf_url": "https://arxiv.org/pdf/2412.06113v1"
  },
  {
    "arxiv_id": "2412.05731v1",
    "entry_id": "http://arxiv.org/abs/2412.05731v1",
    "title": "A Scoping Review of ChatGPT Research in Accounting and Finance",
    "summary": "This paper provides a review of recent publications and working papers on ChatGPT and related Large Language Models (LLMs) in accounting and finance. The aim is to understand the current state of research in these two areas and identify potential research opportunities for future inquiry. We identify three common themes from these earlier studies. The first theme focuses on applications of ChatGPT and LLMs in various fields of accounting and finance. The second theme utilizes ChatGPT and LLMs as a new research tool by leveraging their capabilities such as classification, summarization, and text generation. The third theme investigates implications of LLM adoption for accounting and finance professionals, as well as for various organizations and sectors. While these earlier studies provide valuable insights, they leave many important questions unanswered or partially addressed. We propose venues for further exploration and provide technical guidance for researchers seeking to employ ChatGPT and related LLMs as a tool for their research.",
    "authors": [
      "Mengming Michael Dong",
      "Theophanis C. Stratopoulos",
      "Victor Xiaoqi Wang"
    ],
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.CE",
      "cs.LG",
      "econ.GN"
    ],
    "published": "2024-12-07T19:45:46Z",
    "pdf_url": "https://arxiv.org/pdf/2412.05731v1"
  },
  {
    "arxiv_id": "2412.05563v2",
    "entry_id": "http://arxiv.org/abs/2412.05563v2",
    "title": "A Survey on Uncertainty Quantification of Large Language Models: Taxonomy, Open Research Challenges, and Future Directions",
    "summary": "The remarkable performance of large language models (LLMs) in content generation, coding, and common-sense reasoning has spurred widespread integration into many facets of society. However, integration of LLMs raises valid questions on their reliability and trustworthiness, given their propensity to generate hallucinations: plausible, factually-incorrect responses, which are expressed with striking confidence. Previous work has shown that hallucinations and other non-factual responses generated by LLMs can be detected by examining the uncertainty of the LLM in its response to the pertinent prompt, driving significant research efforts devoted to quantifying the uncertainty of LLMs. This survey seeks to provide an extensive review of existing uncertainty quantification methods for LLMs, identifying their salient features, along with their strengths and weaknesses. We present existing methods within a relevant taxonomy, unifying ostensibly disparate methods to aid understanding of the state of the art. Furthermore, we highlight applications of uncertainty quantification methods for LLMs, spanning chatbot and textual applications to embodied artificial intelligence applications in robotics. We conclude with open research challenges in uncertainty quantification of LLMs, seeking to motivate future research.",
    "authors": [
      "Ola Shorinwa",
      "Zhiting Mei",
      "Justin Lidard",
      "Allen Z. Ren",
      "Anirudha Majumdar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-07T06:56:01Z",
    "pdf_url": "https://arxiv.org/pdf/2412.05563v2"
  },
  {
    "arxiv_id": "2412.05445v2",
    "entry_id": "http://arxiv.org/abs/2412.05445v2",
    "title": "From Voice to Value: Leveraging AI to Enhance Spoken Online Reviews on the Go",
    "summary": "Online reviews help people make better decisions. Review platforms usually depend on typed input, where leaving a good review requires significant effort because users must carefully organize and articulate their thoughts. This may discourage users from leaving comprehensive and high-quality reviews, especially when they are on the go. To address this challenge, we developed Vocalizer, a mobile application that enables users to provide reviews through voice input, with enhancements from a large language model (LLM). In a longitudinal study, we analysed user interactions with the app, focusing on AI-driven features that help refine and improve reviews. Our findings show that users frequently utilized the AI agent to add more detailed information to their reviews. We also show how interactive AI features can improve users self-efficacy and willingness to share reviews online. Finally, we discuss the opportunities and challenges of integrating AI assistance into review-writing systems.",
    "authors": [
      "Kavindu Ravishan",
      "Dániel Szabó",
      "Niels van Berkel",
      "Aku Visuri",
      "Chi-Lan Yang",
      "Koji Yatani",
      "Simo Hosio"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-12-06T21:59:47Z",
    "pdf_url": "https://arxiv.org/pdf/2412.05445v2"
  },
  {
    "arxiv_id": "2412.06828v1",
    "entry_id": "http://arxiv.org/abs/2412.06828v1",
    "title": "Enhancing LLMs for Impression Generation in Radiology Reports through a Multi-Agent System",
    "summary": "This study introduces \"RadCouncil,\" a multi-agent Large Language Model (LLM) framework designed to enhance the generation of impressions in radiology reports from the finding section. RadCouncil comprises three specialized agents: 1) a \"Retrieval\" Agent that identifies and retrieves similar reports from a vector database, 2) a \"Radiologist\" Agent that generates impressions based on the finding section of the given report plus the exemplar reports retrieved by the Retrieval Agent, and 3) a \"Reviewer\" Agent that evaluates the generated impressions and provides feedback. The performance of RadCouncil was evaluated using both quantitative metrics (BLEU, ROUGE, BERTScore) and qualitative criteria assessed by GPT-4, using chest X-ray as a case study. Experiment results show improvements in RadCouncil over the single-agent approach across multiple dimensions, including diagnostic accuracy, stylistic concordance, and clarity. This study highlights the potential of utilizing multiple interacting LLM agents, each with a dedicated task, to enhance performance in specialized medical tasks and the development of more robust and adaptable healthcare AI solutions.",
    "authors": [
      "Fang Zeng",
      "Zhiliang Lyu",
      "Quanzheng Li",
      "Xiang Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-06T21:33:03Z",
    "pdf_url": "https://arxiv.org/pdf/2412.06828v1"
  },
  {
    "arxiv_id": "2412.05248v3",
    "entry_id": "http://arxiv.org/abs/2412.05248v3",
    "title": "Enhancing FKG.in: automating Indian food composition analysis",
    "summary": "This paper presents a novel approach to compute food composition data for Indian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The primary focus is to provide a broad overview of an automated food composition analysis workflow and describe its core functionalities: nutrition data aggregation, food composition analysis, and LLM-augmented information resolution. This workflow aims to complement FKG[.]in and iteratively supplement food composition data from verified knowledge bases. Additionally, this paper highlights the challenges of representing Indian food and accessing food composition data digitally. It also reviews three key sources of food composition data: the Indian Food Composition Tables, the Indian Nutrient Databank, and the Nutritionix API. Furthermore, it briefly outlines how users can interact with the workflow to obtain diet-based health recommendations and detailed food composition information for numerous recipes. We then explore the complex challenges of analyzing Indian recipe information across dimensions such as structure, multilingualism, and uncertainty as well as present our ongoing work on LLM-based solutions to address these issues. The methods proposed in this workshop paper for AI-driven knowledge curation and information resolution are application-agnostic, generalizable, and replicable for any domain.",
    "authors": [
      "Saransh Kumar Gupta",
      "Lipika Dey",
      "Partha Pratim Das",
      "Geeta Trilok-Kumar",
      "Ramesh Jain"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2024-12-06T18:27:15Z",
    "pdf_url": "https://arxiv.org/pdf/2412.05248v3"
  },
  {
    "arxiv_id": "2412.05208v2",
    "entry_id": "http://arxiv.org/abs/2412.05208v2",
    "title": "A Survey of Large Language Model-Based Generative AI for Text-to-SQL: Benchmarks, Applications, Use Cases, and Challenges",
    "summary": "Text-to-SQL systems facilitate smooth interaction with databases by translating natural language queries into Structured Query Language (SQL), bridging the gap between non-technical users and complex database management systems. This survey provides a comprehensive overview of the evolution of AI-driven text-to-SQL systems, highlighting their foundational components, advancements in large language model (LLM) architectures, and the critical role of datasets such as Spider, WikiSQL, and CoSQL in driving progress. We examine the applications of text-to-SQL in domains like healthcare, education, and finance, emphasizing their transformative potential for improving data accessibility. Additionally, we analyze persistent challenges, including domain generalization, query optimization, support for multi-turn conversational interactions, and the limited availability of datasets tailored for NoSQL databases and dynamic real-world scenarios. To address these challenges, we outline future research directions, such as extending text-to-SQL capabilities to support NoSQL databases, designing datasets for dynamic multi-turn interactions, and optimizing systems for real-world scalability and robustness. By surveying current advancements and identifying key gaps, this paper aims to guide the next generation of research and applications in LLM-based text-to-SQL systems.",
    "authors": [
      "Aditi Singh",
      "Akash Shetty",
      "Abul Ehtesham",
      "Saket Kumar",
      "Tala Talaei Khoei"
    ],
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "published": "2024-12-06T17:36:28Z",
    "pdf_url": "https://arxiv.org/pdf/2412.05208v2"
  },
  {
    "arxiv_id": "2412.05127v1",
    "entry_id": "http://arxiv.org/abs/2412.05127v1",
    "title": "The Prompt Canvas: A Literature-Based Practitioner Guide for Creating Effective Prompts in Large Language Models",
    "summary": "The rise of large language models (LLMs) has highlighted the importance of prompt engineering as a crucial technique for optimizing model outputs. While experimentation with various prompting methods, such as Few-shot, Chain-of-Thought, and role-based techniques, has yielded promising results, these advancements remain fragmented across academic papers, blog posts and anecdotal experimentation. The lack of a single, unified resource to consolidate the field's knowledge impedes the progress of both research and practical application. This paper argues for the creation of an overarching framework that synthesizes existing methodologies into a cohesive overview for practitioners. Using a design-based research approach, we present the Prompt Canvas, a structured framework resulting from an extensive literature review on prompt engineering that captures current knowledge and expertise. By combining the conceptual foundations and practical strategies identified in prompt engineering, the Prompt Canvas provides a practical approach for leveraging the potential of Large Language Models. It is primarily designed as a learning resource for pupils, students and employees, offering a structured introduction to prompt engineering. This work aims to contribute to the growing discourse on prompt engineering by establishing a unified methodology for researchers and providing guidance for practitioners.",
    "authors": [
      "Michael Hewing",
      "Vincent Leinhos"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-06T15:35:18Z",
    "pdf_url": "https://arxiv.org/pdf/2412.05127v1"
  },
  {
    "arxiv_id": "2412.04782v2",
    "entry_id": "http://arxiv.org/abs/2412.04782v2",
    "title": "A Survey of Sustainability in Large Language Models: Applications, Economics, and Challenges",
    "summary": "Large Language Models (LLMs) have transformed numerous domains by providing advanced capabilities in natural language understanding, generation, and reasoning. Despite their groundbreaking applications across industries such as research, healthcare, and creative media, their rapid adoption raises critical concerns regarding sustainability. This survey paper comprehensively examines the environmental, economic, and computational challenges associated with LLMs, focusing on energy consumption, carbon emissions, and resource utilization in data centers. By synthesizing insights from existing literature, this work explores strategies such as resource-efficient training, sustainable deployment practices, and lifecycle assessments to mitigate the environmental impacts of LLMs. Key areas of emphasis include energy optimization, renewable energy integration, and balancing performance with sustainability. The findings aim to guide researchers, practitioners, and policymakers in developing actionable strategies for sustainable AI systems, fostering a responsible and environmentally conscious future for artificial intelligence.",
    "authors": [
      "Aditi Singh",
      "Nirmal Prakashbhai Patel",
      "Abul Ehtesham",
      "Saket Kumar",
      "Tala Talaei Khoei"
    ],
    "categories": [
      "cs.AI",
      "cs.CE"
    ],
    "published": "2024-12-06T05:20:04Z",
    "pdf_url": "https://arxiv.org/pdf/2412.04782v2"
  },
  {
    "arxiv_id": "2412.04741v1",
    "entry_id": "http://arxiv.org/abs/2412.04741v1",
    "title": "Question Answering for Decisionmaking in Green Building Design: A Multimodal Data Reasoning Method Driven by Large Language Models",
    "summary": "In recent years, the critical role of green buildings in addressing energy consumption and environmental issues has become widely acknowledged. Research indicates that over 40% of potential energy savings can be achieved during the early design stage. Therefore, decision-making in green building design (DGBD), which is based on modeling and performance simulation, is crucial for reducing building energy costs. However, the field of green building encompasses a broad range of specialized knowledge, which involves significant learning costs and results in low decision-making efficiency. Many studies have already applied artificial intelligence (AI) methods to this field. Based on previous research, this study innovatively integrates large language models with DGBD, creating GreenQA, a question answering framework for multimodal data reasoning. Utilizing Retrieval Augmented Generation, Chain of Thought, and Function Call methods, GreenQA enables multimodal question answering, including weather data analysis and visualization, retrieval of green building cases, and knowledge query. Additionally, this study conducted a user survey using the GreenQA web platform. The results showed that 96% of users believed the platform helped improve design efficiency. This study not only effectively supports DGBD but also provides inspiration for AI-assisted design.",
    "authors": [
      "Yihui Li",
      "Xiaoyue Yan",
      "Hao Zhou",
      "Borong Lin"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2024-12-06T03:02:58Z",
    "pdf_url": "https://arxiv.org/pdf/2412.04741v1"
  },
  {
    "arxiv_id": "2412.04726v3",
    "entry_id": "http://arxiv.org/abs/2412.04726v3",
    "title": "BESSTIE: A Benchmark for Sentiment and Sarcasm Classification for Varieties of English",
    "summary": "Despite large language models (LLMs) being known to exhibit bias against non-standard language varieties, there are no known labelled datasets for sentiment analysis of English. To address this gap, we introduce BESSTIE, a benchmark for sentiment and sarcasm classification for three varieties of English: Australian (en-AU), Indian (en-IN), and British (en-UK). We collect datasets for these language varieties using two methods: location-based for Google Places reviews, and topic-based filtering for Reddit comments. To assess whether the dataset accurately represents these varieties, we conduct two validation steps: (a) manual annotation of language varieties and (b) automatic language variety prediction. Native speakers of the language varieties manually annotate the datasets with sentiment and sarcasm labels. We perform an additional annotation exercise to validate the reliance of the annotated labels. Subsequently, we fine-tune nine LLMs (representing a range of encoder/decoder and mono/multilingual models) on these datasets, and evaluate their performance on the two tasks. Our results show that the models consistently perform better on inner-circle varieties (i.e., en-AU and en-UK), in comparison with en-IN, particularly for sarcasm classification. We also report challenges in cross-variety generalisation, highlighting the need for language variety-specific datasets such as ours. BESSTIE promises to be a useful evaluative benchmark for future research in equitable LLMs, specifically in terms of language varieties. The BESSTIE dataset is publicly available at: https://huggingface.co/ datasets/unswnlporg/BESSTIE.",
    "authors": [
      "Dipankar Srirag",
      "Aditya Joshi",
      "Jordan Painter",
      "Diptesh Kanojia"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-06T02:34:40Z",
    "pdf_url": "https://arxiv.org/pdf/2412.04726v3"
  },
  {
    "arxiv_id": "2412.10400v3",
    "entry_id": "http://arxiv.org/abs/2412.10400v3",
    "title": "Reinforcement Learning Enhanced LLMs: A Survey",
    "summary": "Reinforcement learning (RL) enhanced large language models (LLMs), particularly exemplified by DeepSeek-R1, have exhibited outstanding performance. Despite the effectiveness in improving LLM capabilities, its implementation remains highly complex, requiring complex algorithms, reward modeling strategies, and optimization techniques. This complexity poses challenges for researchers and practitioners in developing a systematic understanding of RL-enhanced LLMs. Moreover, the absence of a comprehensive survey summarizing existing research on RL-enhanced LLMs has limited progress in this domain, hindering further advancements.\n  In this work, we are going to make a systematic review of the most up-to-date state of knowledge on RL-enhanced LLMs, attempting to consolidate and analyze the rapidly growing research in this field, helping researchers understand the current challenges and advancements. Specifically, we (1) detail the basics of RL; (2) introduce popular RL-enhanced LLMs; (3) review researches on two widely-used reward model-based RL techniques: Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning from AI Feedback (RLAIF); and (4) explore Direct Preference Optimization (DPO), a set of methods that bypass the reward model to directly use human preference data for aligning LLM outputs with human expectations. We will also point out current challenges and deficiencies of existing methods and suggest some avenues for further improvements. Project page of this work can be found at https://github.com/ShuheWang1998/Reinforcement-Learning-Enhanced-LLMs-A-Survey.",
    "authors": [
      "Shuhe Wang",
      "Shengyu Zhang",
      "Jie Zhang",
      "Runyi Hu",
      "Xiaoya Li",
      "Tianwei Zhang",
      "Jiwei Li",
      "Fei Wu",
      "Guoyin Wang",
      "Eduard Hovy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-12-05T16:10:42Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10400v3"
  },
  {
    "arxiv_id": "2412.03933v1",
    "entry_id": "http://arxiv.org/abs/2412.03933v1",
    "title": "Exploring AI Text Generation, Retrieval-Augmented Generation, and Detection Technologies: a Comprehensive Overview",
    "summary": "The rapid development of Artificial Intelligence (AI) has led to the creation of powerful text generation models, such as large language models (LLMs), which are widely used for diverse applications. However, concerns surrounding AI-generated content, including issues of originality, bias, misinformation, and accountability, have become increasingly prominent. This paper offers a comprehensive overview of AI text generators (AITGs), focusing on their evolution, capabilities, and ethical implications. This paper also introduces Retrieval-Augmented Generation (RAG), a recent approach that improves the contextual relevance and accuracy of text generation by integrating dynamic information retrieval. RAG addresses key limitations of traditional models, including their reliance on static knowledge and potential inaccuracies in handling real-world data. Additionally, the paper reviews detection tools that help differentiate AI-generated text from human-written content and discusses the ethical challenges these technologies pose. The paper explores future directions for improving detection accuracy, supporting ethical AI development, and increasing accessibility. The paper contributes to a more responsible and reliable use of AI in content creation through these discussions.",
    "authors": [
      "Fnu Neha",
      "Deepshikha Bhati",
      "Deepak Kumar Shukla",
      "Angela Guercio",
      "Ben Ward"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-12-05T07:23:14Z",
    "pdf_url": "https://arxiv.org/pdf/2412.03933v1"
  },
  {
    "arxiv_id": "2412.03920v2",
    "entry_id": "http://arxiv.org/abs/2412.03920v2",
    "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios",
    "summary": "Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities, as well as their interactions and synergistic effects on decision-making. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. Additionally, we analyze the performance of current social agents across various game scenarios. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios.",
    "authors": [
      "Xiachong Feng",
      "Longxu Dou",
      "Ella Li",
      "Qinghao Wang",
      "Haochuan Wang",
      "Yu Guo",
      "Chang Ma",
      "Lingpeng Kong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-05T06:46:46Z",
    "pdf_url": "https://arxiv.org/pdf/2412.03920v2"
  },
  {
    "arxiv_id": "2412.03856v1",
    "entry_id": "http://arxiv.org/abs/2412.03856v1",
    "title": "How Good is ChatGPT in Giving Adaptive Guidance Using Knowledge Graphs in E-Learning Environments?",
    "summary": "E-learning environments are increasingly harnessing large language models (LLMs) like GPT-3.5 and GPT-4 for tailored educational support. This study introduces an approach that integrates dynamic knowledge graphs with LLMs to offer nuanced student assistance. By evaluating past and ongoing student interactions, the system identifies and appends the most salient learning context to prompts directed at the LLM. Central to this method is the knowledge graph's role in assessing a student's comprehension of topic prerequisites. Depending on the categorized understanding (good, average, or poor), the LLM adjusts its guidance, offering advanced assistance, foundational reviews, or in-depth prerequisite explanations, respectively. Preliminary findings suggest students could benefit from this tiered support, achieving enhanced comprehension and improved task outcomes. However, several issues related to potential errors arising from LLMs were identified, which can potentially mislead students. This highlights the need for human intervention to mitigate these risks. This research aims to advance AI-driven personalized learning while acknowledging the limitations and potential pitfalls, thus guiding future research in technology and data-driven education.",
    "authors": [
      "Patrick Ocheja",
      "Brendan Flanagan",
      "Yiling Dai",
      "Hiroaki Ogata"
    ],
    "categories": [
      "cs.AI",
      "cs.ET"
    ],
    "published": "2024-12-05T04:05:43Z",
    "pdf_url": "https://arxiv.org/pdf/2412.03856v1"
  },
  {
    "arxiv_id": "2412.03531v1",
    "entry_id": "http://arxiv.org/abs/2412.03531v1",
    "title": "A Review on Scientific Knowledge Extraction using Large Language Models in Biomedical Sciences",
    "summary": "The rapid advancement of large language models (LLMs) has opened new boundaries in the extraction and synthesis of medical knowledge, particularly within evidence synthesis. This paper reviews the state-of-the-art applications of LLMs in the biomedical domain, exploring their effectiveness in automating complex tasks such as evidence synthesis and data extraction from a biomedical corpus of documents. While LLMs demonstrate remarkable potential, significant challenges remain, including issues related to hallucinations, contextual understanding, and the ability to generalize across diverse medical tasks. We highlight critical gaps in the current research literature, particularly the need for unified benchmarks to standardize evaluations and ensure reliability in real-world applications. In addition, we propose directions for future research, emphasizing the integration of state-of-the-art techniques such as retrieval-augmented generation (RAG) to enhance LLM performance in evidence synthesis. By addressing these challenges and utilizing the strengths of LLMs, we aim to improve access to medical literature and facilitate meaningful discoveries in healthcare.",
    "authors": [
      "Gabriel Lino Garcia",
      "João Renato Ribeiro Manesco",
      "Pedro Henrique Paiola",
      "Lucas Miranda",
      "Maria Paola de Salvo",
      "João Paulo Papa"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-12-04T18:26:13Z",
    "pdf_url": "https://arxiv.org/pdf/2412.03531v1"
  },
  {
    "arxiv_id": "2412.03220v1",
    "entry_id": "http://arxiv.org/abs/2412.03220v1",
    "title": "Survey of different Large Language Model Architectures: Trends, Benchmarks, and Challenges",
    "summary": "Large Language Models (LLMs) represent a class of deep learning models adept at understanding natural language and generating coherent responses to various prompts or queries. These models far exceed the complexity of conventional neural networks, often encompassing dozens of neural network layers and containing billions to trillions of parameters. They are typically trained on vast datasets, utilizing architectures based on transformer blocks. Present-day LLMs are multi-functional, capable of performing a range of tasks from text generation and language translation to question answering, as well as code generation and analysis. An advanced subset of these models, known as Multimodal Large Language Models (MLLMs), extends LLM capabilities to process and interpret multiple data modalities, including images, audio, and video. This enhancement empowers MLLMs with capabilities like video editing, image comprehension, and captioning for visual content. This survey provides a comprehensive overview of the recent advancements in LLMs. We begin by tracing the evolution of LLMs and subsequently delve into the advent and nuances of MLLMs. We analyze emerging state-of-the-art MLLMs, exploring their technical features, strengths, and limitations. Additionally, we present a comparative analysis of these models and discuss their challenges, potential limitations, and prospects for future development.",
    "authors": [
      "Minghao Shao",
      "Abdul Basit",
      "Ramesh Karri",
      "Muhammad Shafique"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-12-04T11:14:06Z",
    "pdf_url": "https://arxiv.org/pdf/2412.03220v1"
  },
  {
    "arxiv_id": "2412.02980v2",
    "entry_id": "http://arxiv.org/abs/2412.02980v2",
    "title": "Surveying the Effects of Quality, Diversity, and Complexity in Synthetic Data From Large Language Models",
    "summary": "Synthetic data generation with Large Language Models is a promising paradigm for augmenting natural data over a nearly infinite range of tasks. Given this variety, direct comparisons among synthetic data generation algorithms are scarce, making it difficult to understand where improvement comes from and what bottlenecks exist. We propose to evaluate algorithms via the makeup of synthetic data generated by each algorithm in terms of data quality, diversity, and complexity. We choose these three characteristics for their significance in open-ended processes and the impact each has on the capabilities of downstream models. We find quality to be essential for in-distribution model generalization, diversity to be essential for out-of-distribution generalization, and complexity to be beneficial for both. Further, we emphasize the existence of Quality-Diversity trade-offs in training data and the downstream effects on model performance. We then examine the effect of various components in the synthetic data pipeline on each data characteristic. This examination allows us to taxonomize and compare synthetic data generation algorithms through the components they utilize and the resulting effects on data QDC composition. This analysis extends into a discussion on the importance of balancing QDC in synthetic data for efficient reinforcement learning and self-improvement algorithms. Analogous to the QD trade-offs in training data, often there exist trade-offs between model output quality and output diversity which impact the composition of synthetic data. We observe that many models are currently evaluated and optimized only for output quality, thereby limiting output diversity and the potential for self-improvement. We argue that balancing these trade-offs is essential to the development of future self-improvement algorithms and highlight a number of works making progress in this direction.",
    "authors": [
      "Alex Havrilla",
      "Andrew Dai",
      "Laura O'Mahony",
      "Koen Oostermeijer",
      "Vera Zisler",
      "Alon Albalak",
      "Fabrizio Milo",
      "Sharath Chandra Raparthy",
      "Kanishk Gandhi",
      "Baber Abbasi",
      "Duy Phung",
      "Maia Iyer",
      "Dakota Mahan",
      "Chase Blagden",
      "Srishti Gureja",
      "Mohammed Hamdy",
      "Wen-Ding Li",
      "Giovanni Paolini",
      "Pawan Sasanka Ammanamanchi",
      "Elliot Meyerson"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-12-04T02:47:45Z",
    "pdf_url": "https://arxiv.org/pdf/2412.02980v2"
  },
  {
    "arxiv_id": "2412.02251v3",
    "entry_id": "http://arxiv.org/abs/2412.02251v3",
    "title": "Selective Reviews of Bandit Problems in AI via a Statistical View",
    "summary": "Reinforcement Learning (RL) is a widely researched area in artificial intelligence that focuses on teaching agents decision-making through interactions with their environment. A key subset includes stochastic multi-armed bandit (MAB) and continuum-armed bandit (SCAB) problems, which model sequential decision-making under uncertainty. This review outlines the foundational models and assumptions of bandit problems, explores non-asymptotic theoretical tools like concentration inequalities and minimax regret bounds, and compares frequentist and Bayesian algorithms for managing exploration-exploitation trade-offs. Additionally, we explore K-armed contextual bandits and SCAB, focusing on their methodologies and regret analyses. We also examine the connections between SCAB problems and functional data analysis. Finally, we highlight recent advances and ongoing challenges in the field.",
    "authors": [
      "Pengjie Zhou",
      "Haoyu Wei",
      "Huiming Zhang"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG",
      "econ.EM",
      "math.PR"
    ],
    "published": "2024-12-03T08:28:47Z",
    "pdf_url": "https://arxiv.org/pdf/2412.02251v3"
  },
  {
    "arxiv_id": "2412.02142v1",
    "entry_id": "http://arxiv.org/abs/2412.02142v1",
    "title": "Personalized Multimodal Large Language Models: A Survey",
    "summary": "Multimodal Large Language Models (MLLMs) have become increasingly important due to their state-of-the-art performance and ability to integrate multiple data modalities, such as text, images, and audio, to perform complex tasks with high accuracy. This paper presents a comprehensive survey on personalized multimodal large language models, focusing on their architecture, training methods, and applications. We propose an intuitive taxonomy for categorizing the techniques used to personalize MLLMs to individual users, and discuss the techniques accordingly. Furthermore, we discuss how such techniques can be combined or adapted when appropriate, highlighting their advantages and underlying rationale. We also provide a succinct summary of personalization tasks investigated in existing research, along with the evaluation metrics commonly used. Additionally, we summarize the datasets that are useful for benchmarking personalized MLLMs. Finally, we outline critical open challenges. This survey aims to serve as a valuable resource for researchers and practitioners seeking to understand and advance the development of personalized multimodal large language models.",
    "authors": [
      "Junda Wu",
      "Hanjia Lyu",
      "Yu Xia",
      "Zhehao Zhang",
      "Joe Barrow",
      "Ishita Kumar",
      "Mehrnoosh Mirtaheri",
      "Hongjie Chen",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Tong Yu",
      "Ruiyi Zhang",
      "Jiuxiang Gu",
      "Nesreen K. Ahmed",
      "Yu Wang",
      "Xiang Chen",
      "Hanieh Deilamsalehy",
      "Namyong Park",
      "Sungchul Kim",
      "Huanrui Yang",
      "Subrata Mitra",
      "Zhengmian Hu",
      "Nedim Lipka",
      "Dang Nguyen",
      "Yue Zhao",
      "Jiebo Luo",
      "Julian McAuley"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2024-12-03T03:59:03Z",
    "pdf_url": "https://arxiv.org/pdf/2412.02142v1"
  },
  {
    "arxiv_id": "2412.02113v2",
    "entry_id": "http://arxiv.org/abs/2412.02113v2",
    "title": "Trust & Safety of LLMs and LLMs in Trust & Safety",
    "summary": "In recent years, Large Language Models (LLMs) have garnered considerable attention for their remarkable abilities in natural language processing tasks. However, their widespread adoption has raised concerns pertaining to trust and safety. This systematic review investigates the current research landscape on trust and safety in LLMs, with a particular focus on the novel application of LLMs within the field of Trust and Safety itself. We delve into the complexities of utilizing LLMs in domains where maintaining trust and safety is paramount, offering a consolidated perspective on this emerging trend.\\\n  By synthesizing findings from various studies, we identify key challenges and potential solutions, aiming to benefit researchers and practitioners seeking to understand the nuanced interplay between LLMs and Trust and Safety.\n  This review provides insights on best practices for using LLMs in Trust and Safety, and explores emerging risks such as prompt injection and jailbreak attacks. Ultimately, this study contributes to a deeper understanding of how LLMs can be effectively and responsibly utilized to enhance trust and safety in the digital realm.",
    "authors": [
      "Doohee You",
      "Dan Chon"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-03T03:10:12Z",
    "pdf_url": "https://arxiv.org/pdf/2412.02113v2"
  },
  {
    "arxiv_id": "2412.02091v2",
    "entry_id": "http://arxiv.org/abs/2412.02091v2",
    "title": "The Problem of Social Cost in Multi-Agent General Reinforcement Learning: Survey and Synthesis",
    "summary": "The AI safety literature is full of examples of powerful AI agents that, in blindly pursuing a specific and usually narrow objective, ends up with unacceptable and even catastrophic collateral damage to others. In this paper, we consider the problem of social harms that can result from actions taken by learning and utility-maximising agents in a multi-agent environment. The problem of measuring social harms or impacts in such multi-agent settings, especially when the agents are artificial generally intelligent (AGI) agents, was listed as an open problem in Everitt et al, 2018. We attempt a partial answer to that open problem in the form of market-based mechanisms to quantify and control the cost of such social harms. The proposed setup captures many well-studied special cases and is more general than existing formulations of multi-agent reinforcement learning with mechanism design in two ways: (i) the underlying environment is a history-based general reinforcement learning environment like in AIXI; (ii) the reinforcement-learning agents participating in the environment can have different learning strategies and planning horizons. To demonstrate the practicality of the proposed setup, we survey some key classes of learning algorithms and present a few applications, including a discussion of the Paperclips problem and pollution control with a cap-and-trade system.",
    "authors": [
      "Kee Siong Ng",
      "Samuel Yang-Zhao",
      "Timothy Cadogan-Cowper"
    ],
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2024-12-03T02:22:55Z",
    "pdf_url": "https://arxiv.org/pdf/2412.02091v2"
  },
  {
    "arxiv_id": "2412.03597v1",
    "entry_id": "http://arxiv.org/abs/2412.03597v1",
    "title": "The Vulnerability of Language Model Benchmarks: Do They Accurately Reflect True LLM Performance?",
    "summary": "The pursuit of leaderboard rankings in Large Language Models (LLMs) has created a fundamental paradox: models excel at standardized tests while failing to demonstrate genuine language understanding and adaptability. Our systematic analysis of NLP evaluation frameworks reveals pervasive vulnerabilities across the evaluation spectrum, from basic metrics to complex benchmarks like GLUE and MMLU. These vulnerabilities manifest through benchmark exploitation, dataset contamination, and evaluation bias, creating a false perception of progress in language understanding capabilities. Through extensive review of contemporary evaluation approaches, we identify significant limitations in static benchmark designs, human evaluation protocols, and LLM-as-judge frameworks, all of which compromise the reliability of current performance assessments. As LLM capabilities evolve and existing benchmarks become redundant, we lay the groundwork for new evaluation methods that resist manipulation, minimize data contamination, and assess domain-specific tasks. This requires frameworks that are adapted dynamically, addressing current limitations and providing a more accurate reflection of LLM performance.",
    "authors": [
      "Sourav Banerjee",
      "Ayushi Agarwal",
      "Eishkaran Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2024-12-02T20:49:21Z",
    "pdf_url": "https://arxiv.org/pdf/2412.03597v1"
  },
  {
    "arxiv_id": "2412.01955v2",
    "entry_id": "http://arxiv.org/abs/2412.01955v2",
    "title": "The use of large language models to enhance cancer clinical trial educational materials",
    "summary": "Cancer clinical trials often face challenges in recruitment and engagement due to a lack of participant-facing informational and educational resources. This study investigated the potential of Large Language Models (LLMs), specifically GPT4, in generating patient-friendly educational content from clinical trial informed consent forms. Using data from ClinicalTrials.gov, we employed zero-shot learning for creating trial summaries and one-shot learning for developing multiple-choice questions, evaluating their effectiveness through patient surveys and crowdsourced annotation. Results showed that GPT4-generated summaries were both readable and comprehensive, and may improve patients' understanding and interest in clinical trials. The multiple-choice questions demonstrated high accuracy and agreement with crowdsourced annotators. For both resource types, hallucinations were identified that require ongoing human oversight. The findings demonstrate the potential of LLMs \"out-of-the-box\" to support the generation of clinical trial education materials with minimal trial-specific engineering, but implementation with a human-in-the-loop is still needed to avoid misinformation risks.",
    "authors": [
      "Mingye Gao",
      "Aman Varshney",
      "Shan Chen",
      "Vikram Goddla",
      "Jack Gallifant",
      "Patrick Doyle",
      "Claire Novack",
      "Maeve Dillon-Martin",
      "Teresia Perkins",
      "Xinrong Correia",
      "Erik Duhaime",
      "Howard Isenstein",
      "Elad Sharon",
      "Lisa Soleymani Lehmann",
      "David Kozono",
      "Brian Anthony",
      "Dmitriy Dligach",
      "Danielle S. Bitterman"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-12-02T20:31:27Z",
    "pdf_url": "https://arxiv.org/pdf/2412.01955v2"
  },
  {
    "arxiv_id": "2412.01708v1",
    "entry_id": "http://arxiv.org/abs/2412.01708v1",
    "title": "Are We There Yet? Revealing the Risks of Utilizing Large Language Models in Scholarly Peer Review",
    "summary": "Scholarly peer review is a cornerstone of scientific advancement, but the system is under strain due to increasing manuscript submissions and the labor-intensive nature of the process. Recent advancements in large language models (LLMs) have led to their integration into peer review, with promising results such as substantial overlaps between LLM- and human-generated reviews. However, the unchecked adoption of LLMs poses significant risks to the integrity of the peer review system. In this study, we comprehensively analyze the vulnerabilities of LLM-generated reviews by focusing on manipulation and inherent flaws. Our experiments show that injecting covert deliberate content into manuscripts allows authors to explicitly manipulate LLM reviews, leading to inflated ratings and reduced alignment with human reviews. In a simulation, we find that manipulating 5% of the reviews could potentially cause 12% of the papers to lose their position in the top 30% rankings. Implicit manipulation, where authors strategically highlight minor limitations in their papers, further demonstrates LLMs' susceptibility compared to human reviewers, with a 4.5 times higher consistency with disclosed limitations. Additionally, LLMs exhibit inherent flaws, such as potentially assigning higher ratings to incomplete papers compared to full papers and favoring well-known authors in single-blind review process. These findings highlight the risks of over-reliance on LLMs in peer review, underscoring that we are not yet ready for widespread adoption and emphasizing the need for robust safeguards.",
    "authors": [
      "Rui Ye",
      "Xianghe Pang",
      "Jingyi Chai",
      "Jiaao Chen",
      "Zhenfei Yin",
      "Zhen Xiang",
      "Xiaowen Dong",
      "Jing Shao",
      "Siheng Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-12-02T16:55:03Z",
    "pdf_url": "https://arxiv.org/pdf/2412.01708v1"
  },
  {
    "arxiv_id": "2412.01233v1",
    "entry_id": "http://arxiv.org/abs/2412.01233v1",
    "title": "Best Practices for Large Language Models in Radiology",
    "summary": "At the heart of radiological practice is the challenge of integrating complex imaging data with clinical information to produce actionable insights. Nuanced application of language is key for various activities, including managing requests, describing and interpreting imaging findings in the context of clinical data, and concisely documenting and communicating the outcomes. The emergence of large language models (LLMs) offers an opportunity to improve the management and interpretation of the vast data in radiology. Despite being primarily general-purpose, these advanced computational models demonstrate impressive capabilities in specialized language-related tasks, even without specific training. Unlocking the potential of LLMs for radiology requires basic understanding of their foundations and a strategic approach to navigate their idiosyncrasies. This review, drawing from practical radiology and machine learning expertise and recent literature, provides readers insight into the potential of LLMs in radiology. It examines best practices that have so far stood the test of time in the rapidly evolving landscape of LLMs. This includes practical advice for optimizing LLM characteristics for radiology practices along with limitations, effective prompting, and fine-tuning strategies.",
    "authors": [
      "Christian Bluethgen",
      "Dave Van Veen",
      "Cyril Zakka",
      "Katherine Link",
      "Aaron Fanous",
      "Roxana Daneshjou",
      "Thomas Frauenfelder",
      "Curtis Langlotz",
      "Sergios Gatidis",
      "Akshay Chaudhari"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-12-02T07:54:55Z",
    "pdf_url": "https://arxiv.org/pdf/2412.01233v1"
  },
  {
    "arxiv_id": "2412.00962v1",
    "entry_id": "http://arxiv.org/abs/2412.00962v1",
    "title": "LLMs as mirrors of societal moral standards: reflection of cultural divergence and agreement across ethical topics",
    "summary": "Large language models (LLMs) have become increasingly pivotal in various domains due the recent advancements in their performance capabilities. However, concerns persist regarding biases in LLMs, including gender, racial, and cultural biases derived from their training data. These biases raise critical questions about the ethical deployment and societal impact of LLMs. Acknowledging these concerns, this study investigates whether LLMs accurately reflect cross-cultural variations and similarities in moral perspectives. In assessing whether the chosen LLMs capture patterns of divergence and agreement on moral topics across cultures, three main methods are employed: (1) comparison of model-generated and survey-based moral score variances, (2) cluster alignment analysis to evaluate the correspondence between country clusters derived from model-generated moral scores and those derived from survey data, and (3) probing LLMs with direct comparative prompts. All three methods involve the use of systematic prompts and token pairs designed to assess how well LLMs understand and reflect cultural variations in moral attitudes. The findings of this study indicate overall variable and low performance in reflecting cross-cultural differences and similarities in moral values across the models tested, highlighting the necessity for improving models' accuracy in capturing these nuances effectively. The insights gained from this study aim to inform discussions on the ethical development and deployment of LLMs in global contexts, emphasizing the importance of mitigating biases and promoting fair representation across diverse cultural perspectives.",
    "authors": [
      "Mijntje Meijer",
      "Hadi Mohammadi",
      "Ayoub Bagheri"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "published": "2024-12-01T20:39:42Z",
    "pdf_url": "https://arxiv.org/pdf/2412.00962v1"
  },
  {
    "arxiv_id": "2412.00956v1",
    "entry_id": "http://arxiv.org/abs/2412.00956v1",
    "title": "Large Language Models as Mirrors of Societal Moral Standards",
    "summary": "Prior research has demonstrated that language models can, to a limited extent, represent moral norms in a variety of cultural contexts. This research aims to replicate these findings and further explore their validity, concentrating on issues like 'homosexuality' and 'divorce'. This study evaluates the effectiveness of these models using information from two surveys, the WVS and the PEW, that encompass moral perspectives from over 40 countries. The results show that biases exist in both monolingual and multilingual models, and they typically fall short of accurately capturing the moral intricacies of diverse cultures. However, the BLOOM model shows the best performance, exhibiting some positive correlations, but still does not achieve a comprehensive moral understanding. This research underscores the limitations of current PLMs in processing cross-cultural differences in values and highlights the importance of developing culturally aware AI systems that better align with universal human values.",
    "authors": [
      "Evi Papadopoulou",
      "Hadi Mohammadi",
      "Ayoub Bagheri"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SC"
    ],
    "published": "2024-12-01T20:20:35Z",
    "pdf_url": "https://arxiv.org/pdf/2412.00956v1"
  },
  {
    "arxiv_id": "2412.04498v2",
    "entry_id": "http://arxiv.org/abs/2412.04498v2",
    "title": "Large Language Models in Politics and Democracy: A Comprehensive Survey",
    "summary": "The advancement of generative AI, particularly large language models (LLMs), has a significant impact on politics and democracy, offering potential across various domains, including policymaking, political communication, analysis, and governance. This paper surveys the recent and potential applications of LLMs in politics, examining both their promises and the associated challenges. This paper examines the ways in which LLMs are being employed in legislative processes, political communication, and political analysis. Moreover, we investigate the potential of LLMs in diplomatic and national security contexts, economic and social modeling, and legal applications. While LLMs offer opportunities to enhance efficiency, inclusivity, and decision-making in political processes, they also present challenges related to bias, transparency, and accountability. The paper underscores the necessity for responsible development, ethical considerations, and governance frameworks to ensure that the integration of LLMs into politics aligns with democratic values and promotes a more just and equitable society.",
    "authors": [
      "Goshi Aoki"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-12-01T15:23:34Z",
    "pdf_url": "https://arxiv.org/pdf/2412.04498v2"
  },
  {
    "arxiv_id": "2412.00627v2",
    "entry_id": "http://arxiv.org/abs/2412.00627v2",
    "title": "ARChef: An iOS-Based Augmented Reality Cooking Assistant Powered by Multimodal Gemini LLM",
    "summary": "Cooking meals can be difficult, causing many to resort to cookbooks and online recipes. However, relying on these traditional methods of cooking often results in missing ingredients, nutritional hazards, and unsatisfactory meals. Using Augmented Reality (AR) can address these issues; however, current AR cooking applications have poor user interfaces and limited accessibility. This paper proposes a prototype of an iOS application that integrates AR and Computer Vision (CV) into the cooking process. We leverage Google's Gemini Large Language Model (LLM) to identify ingredients in the camera's field of vision and generate recipe choices with detailed nutritional information. Additionally, this application uses Apple's ARKit to create an AR user interface compatible with iOS devices. Users can personalize their meal suggestions by inputting their dietary preferences and rating each meal. The application's effectiveness is evaluated through three rounds of user experience surveys. This application advances the field of accessible cooking assistance technologies, aiming to reduce food wastage and improve the meal planning experience.",
    "authors": [
      "Rithik Vir",
      "Parsa Madinei"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-12-01T00:52:51Z",
    "pdf_url": "https://arxiv.org/pdf/2412.00627v2"
  },
  {
    "arxiv_id": "2412.00573v2",
    "entry_id": "http://arxiv.org/abs/2412.00573v2",
    "title": "Opus: A Large Work Model for Complex Workflow Generation",
    "summary": "This paper introduces Opus, a novel framework for generating and optimizing Workflows tailored to complex Business Process Outsourcing (BPO) use cases, focusing on cost reduction and quality enhancement while adhering to established industry processes and operational constraints. Our approach generates executable Workflows from Intention, defined as the alignment of Client Input, Client Output, and Process Context. These Workflows are represented as Directed Acyclic Graphs (DAGs), with nodes as Tasks consisting of sequences of executable Instructions, including tools and human expert reviews. We adopt a two-phase methodology: Workflow Generation and Workflow Optimization. In the Generation phase, Workflows are generated using a Large Work Model (LWM) informed by a Work Knowledge Graph (WKG) that encodes domain-specific procedural and operational knowledge. In the Optimization phase, Workflows are transformed into Workflow Graphs (WFGs), where optimal Workflows are determined through path optimization. Our experiments demonstrate that state-of-the-art Large Language Models (LLMs) face challenges in reliably retrieving detailed process data as well as generating industry-compliant workflows. The key contributions of this paper include integrating a Work Knowledge Graph (WKG) into a Large Work Model (LWM) to enable the generation of context-aware, semantically aligned, structured and auditable Workflows. It further introduces a two-phase approach that combines Workflow Generation from Intention with graph-based Workflow Optimization. Finally, we present Opus Alpha 1 Large and Opus Alpha 1 Small that outperform state-of-the-art LLMs by 38% and 29% respectively in Workflow Generation for a Medical Coding use case.",
    "authors": [
      "Théo Fagnoni",
      "Bellinda Mesbah",
      "Mahsun Altin",
      "Phillip Kingston"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-11-30T20:00:41Z",
    "pdf_url": "https://arxiv.org/pdf/2412.00573v2"
  },
  {
    "arxiv_id": "2412.10390v1",
    "entry_id": "http://arxiv.org/abs/2412.10390v1",
    "title": "Neural-Symbolic Reasoning over Knowledge Graphs: A Survey from a Query Perspective",
    "summary": "Knowledge graph reasoning is pivotal in various domains such as data mining, artificial intelligence, the Web, and social sciences. These knowledge graphs function as comprehensive repositories of human knowledge, facilitating the inference of new information. Traditional symbolic reasoning, despite its strengths, struggles with the challenges posed by incomplete and noisy data within these graphs. In contrast, the rise of Neural Symbolic AI marks a significant advancement, merging the robustness of deep learning with the precision of symbolic reasoning. This integration aims to develop AI systems that are not only highly interpretable and explainable but also versatile, effectively bridging the gap between symbolic and neural methodologies. Additionally, the advent of large language models (LLMs) has opened new frontiers in knowledge graph reasoning, enabling the extraction and synthesis of knowledge in unprecedented ways. This survey offers a thorough review of knowledge graph reasoning, focusing on various query types and the classification of neural symbolic reasoning. Furthermore, it explores the innovative integration of knowledge graph reasoning with large language models, highlighting the potential for groundbreaking advancements. This comprehensive overview is designed to support researchers and practitioners across multiple fields, including data mining, AI, the Web, and social sciences, by providing a detailed understanding of the current landscape and future directions in knowledge graph reasoning.",
    "authors": [
      "Lihui Liu",
      "Zihao Wang",
      "Hanghang Tong"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-11-30T18:54:08Z",
    "pdf_url": "https://arxiv.org/pdf/2412.10390v1"
  },
  {
    "arxiv_id": "2412.00465v2",
    "entry_id": "http://arxiv.org/abs/2412.00465v2",
    "title": "AgriBench: A Hierarchical Agriculture Benchmark for Multimodal Large Language Models",
    "summary": "We introduce AgriBench, the first agriculture benchmark designed to evaluate MultiModal Large Language Models (MM-LLMs) for agriculture applications. To further address the agriculture knowledge-based dataset limitation problem, we propose MM-LUCAS, a multimodal agriculture dataset, that includes 1,784 landscape images, segmentation masks, depth maps, and detailed annotations (geographical location, country, date, land cover and land use taxonomic details, quality scores, aesthetic scores, etc), based on the Land Use/Cover Area Frame Survey (LUCAS) dataset, which contains comparable statistics on land use and land cover for the European Union (EU) territory. This work presents a groundbreaking perspective in advancing agriculture MM-LLMs and is still in progress, offering valuable insights for future developments and innovations in specific expert knowledge-based MM-LLMs.",
    "authors": [
      "Yutong Zhou",
      "Masahiro Ryo"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-11-30T12:59:03Z",
    "pdf_url": "https://arxiv.org/pdf/2412.00465v2"
  },
  {
    "arxiv_id": "2412.00323v1",
    "entry_id": "http://arxiv.org/abs/2412.00323v1",
    "title": "Cognitive Biases in Large Language Models: A Survey and Mitigation Experiments",
    "summary": "Large Language Models (LLMs) are trained on large corpora written by humans and demonstrate high performance on various tasks. However, as humans are susceptible to cognitive biases, which can result in irrational judgments, LLMs can also be influenced by these biases, leading to irrational decision-making. For example, changing the order of options in multiple-choice questions affects the performance of LLMs due to order bias. In our research, we first conducted an extensive survey of existing studies examining LLMs' cognitive biases and their mitigation. The mitigation techniques in LLMs have the disadvantage that they are limited in the type of biases they can apply or require lengthy inputs or outputs. We then examined the effectiveness of two mitigation methods for humans, SoPro and AwaRe, when applied to LLMs, inspired by studies in crowdsourcing. To test the effectiveness of these methods, we conducted experiments on GPT-3.5 and GPT-4 to evaluate the influence of six biases on the outputs before and after applying these methods. The results demonstrate that while SoPro has little effect, AwaRe enables LLMs to mitigate the effect of these biases and make more rational responses.",
    "authors": [
      "Yasuaki Sumita",
      "Koh Takeuchi",
      "Hisashi Kashima"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-30T02:37:59Z",
    "pdf_url": "https://arxiv.org/pdf/2412.00323v1"
  },
  {
    "arxiv_id": "2412.00281v1",
    "entry_id": "http://arxiv.org/abs/2412.00281v1",
    "title": "Streamlining the review process: AI-generated annotations in research manuscripts",
    "summary": "The increasing volume of research paper submissions poses a significant challenge to the traditional academic peer-review system, leading to an overwhelming workload for reviewers. This study explores the potential of integrating Large Language Models (LLMs) into the peer-review process to enhance efficiency without compromising effectiveness. We focus on manuscript annotations, particularly excerpt highlights, as a potential area for AI-human collaboration. While LLMs excel in certain tasks like aspect coverage and informativeness, they often lack high-level analysis and critical thinking, making them unsuitable for replacing human reviewers entirely. Our approach involves using LLMs to assist with specific aspects of the review process. This paper introduces AnnotateGPT, a platform that utilizes GPT-4 for manuscript review, aiming to improve reviewers' comprehension and focus. We evaluate AnnotateGPT using a Technology Acceptance Model (TAM) questionnaire with nine participants and generalize the findings. Our work highlights annotation as a viable middle ground for AI-human collaboration in academic review, offering insights into integrating LLMs into the review process and tuning traditional annotation tools for LLM incorporation.",
    "authors": [
      "Oscar Díaz",
      "Xabier Garmendia",
      "Juanan Pereira"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-11-29T23:26:34Z",
    "pdf_url": "https://arxiv.org/pdf/2412.00281v1"
  },
  {
    "arxiv_id": "2411.18892v2",
    "entry_id": "http://arxiv.org/abs/2411.18892v2",
    "title": "A Comprehensive Survey of Reinforcement Learning: From Algorithms to Practical Challenges",
    "summary": "Reinforcement Learning (RL) has emerged as a powerful paradigm in Artificial Intelligence (AI), enabling agents to learn optimal behaviors through interactions with their environments. Drawing from the foundations of trial and error, RL equips agents to make informed decisions through feedback in the form of rewards or penalties. This paper presents a comprehensive survey of RL, meticulously analyzing a wide range of algorithms, from foundational tabular methods to advanced Deep Reinforcement Learning (DRL) techniques. We categorize and evaluate these algorithms based on key criteria such as scalability, sample efficiency, and suitability. We compare the methods in the form of their strengths and weaknesses in diverse settings. Additionally, we offer practical insights into the selection and implementation of RL algorithms, addressing common challenges like convergence, stability, and the exploration-exploitation dilemma. This paper serves as a comprehensive reference for researchers and practitioners aiming to harness the full potential of RL in solving complex, real-world problems.",
    "authors": [
      "Majid Ghasemi",
      "Amir Hossein Moosavi",
      "Dariush Ebrahimi"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-28T03:53:14Z",
    "pdf_url": "https://arxiv.org/pdf/2411.18892v2"
  },
  {
    "arxiv_id": "2411.18708v1",
    "entry_id": "http://arxiv.org/abs/2411.18708v1",
    "title": "Embracing AI in Education: Understanding the Surge in Large Language Model Use by Secondary Students",
    "summary": "The impressive essay writing and problem-solving capabilities of large language models (LLMs) like OpenAI's ChatGPT have opened up new avenues in education. Our goal is to gain insights into the widespread use of LLMs among secondary students to inform their future development. Despite school restrictions, our survey of over 300 middle and high school students revealed that a remarkable 70% of students have utilized LLMs, higher than the usage percentage among young adults, and this percentage remains consistent across 7th to 12th grade. Students also reported using LLMs for multiple subjects, including language arts, history, and math assignments, but expressed mixed thoughts on their effectiveness due to occasional hallucinations in historical contexts and incorrect answers for lack of rigorous reasoning. The survey feedback called for LLMs better adapted for students, and also raised questions to developers and educators on how to help students from underserved communities leverage LLMs' capabilities for equal access to advanced education resources. We propose a few ideas to address such issues, including subject-specific models, personalized learning, and AI classrooms.",
    "authors": [
      "Tiffany Zhu",
      "Kexun Zhang",
      "William Yang Wang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-11-27T19:19:34Z",
    "pdf_url": "https://arxiv.org/pdf/2411.18708v1"
  },
  {
    "arxiv_id": "2411.18583v1",
    "entry_id": "http://arxiv.org/abs/2411.18583v1",
    "title": "Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation",
    "summary": "This research presents and compares multiple approaches to automate the generation of literature reviews using several Natural Language Processing (NLP) techniques and retrieval-augmented generation (RAG) with a Large Language Model (LLM). The ever-increasing number of research articles provides a huge challenge for manual literature review. It has resulted in an increased demand for automation. Developing a system capable of automatically generating the literature reviews from only the PDF files as input is the primary objective of this research work. The effectiveness of several Natural Language Processing (NLP) strategies, such as the frequency-based method (spaCy), the transformer model (Simple T5), and retrieval-augmented generation (RAG) with Large Language Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR dataset is chosen for this research experiment and three distinct techniques are utilized to implement three different systems for auto-generating the literature reviews. The ROUGE scores are used for the evaluation of all three systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo achieved the highest ROUGE-1 score, 0.364. The transformer model comes in second place and spaCy is at the last position. Finally, a graphical user interface is created for the best system based on the large language model.",
    "authors": [
      "Nurshat Fateh Ali",
      "Md. Mahdi Mohtasim",
      "Shakil Mosharrof",
      "T. Gopi Krishna"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-11-27T18:27:07Z",
    "pdf_url": "https://arxiv.org/pdf/2411.18583v1"
  },
  {
    "arxiv_id": "2411.18279v12",
    "entry_id": "http://arxiv.org/abs/2411.18279v12",
    "title": "Large Language Model-Brained GUI Agents: A Survey",
    "summary": "GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.",
    "authors": [
      "Chaoyun Zhang",
      "Shilin He",
      "Jiaxu Qian",
      "Bowen Li",
      "Liqun Li",
      "Si Qin",
      "Yu Kang",
      "Minghua Ma",
      "Guyue Liu",
      "Qingwei Lin",
      "Saravan Rajmohan",
      "Dongmei Zhang",
      "Qi Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2024-11-27T12:13:39Z",
    "pdf_url": "https://arxiv.org/pdf/2411.18279v12"
  },
  {
    "arxiv_id": "2411.18157v1",
    "entry_id": "http://arxiv.org/abs/2411.18157v1",
    "title": "A survey on cutting-edge relation extraction techniques based on language models",
    "summary": "This comprehensive survey delves into the latest advancements in Relation Extraction (RE), a pivotal task in natural language processing essential for applications across biomedical, financial, and legal sectors. This study highlights the evolution and current state of RE techniques by analyzing 137 papers presented at the Association for Computational Linguistics (ACL) conferences over the past four years, focusing on models that leverage language models. Our findings underscore the dominance of BERT-based methods in achieving state-of-the-art results for RE while also noting the promising capabilities of emerging large language models (LLMs) like T5, especially in few-shot relation extraction scenarios where they excel in identifying previously unseen relations.",
    "authors": [
      "Jose A. Diaz-Garcia",
      "Julio Amador Diaz Lopez"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-27T09:04:47Z",
    "pdf_url": "https://arxiv.org/pdf/2411.18157v1"
  },
  {
    "arxiv_id": "2411.17338v1",
    "entry_id": "http://arxiv.org/abs/2411.17338v1",
    "title": "Different Bias Under Different Criteria: Assessing Bias in LLMs with a Fact-Based Approach",
    "summary": "Large language models (LLMs) often reflect real-world biases, leading to efforts to mitigate these effects and make the models unbiased. Achieving this goal requires defining clear criteria for an unbiased state, with any deviation from these criteria considered biased. Some studies define an unbiased state as equal treatment across diverse demographic groups, aiming for balanced outputs from LLMs. However, differing perspectives on equality and the importance of pluralism make it challenging to establish a universal standard. Alternatively, other approaches propose using fact-based criteria for more consistent and objective evaluations, though these methods have not yet been fully applied to LLM bias assessments. Thus, there is a need for a metric with objective criteria that offers a distinct perspective from equality-based approaches. Motivated by this need, we introduce a novel metric to assess bias using fact-based criteria and real-world statistics. In this paper, we conducted a human survey demonstrating that humans tend to perceive LLM outputs more positively when they align closely with real-world demographic distributions. Evaluating various LLMs with our proposed metric reveals that model bias varies depending on the criteria used, highlighting the need for multi-perspective assessment.",
    "authors": [
      "Changgeon Ko",
      "Jisu Shin",
      "Hoyun Song",
      "Jeongyeon Seo",
      "Jong C. Park"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-11-26T11:32:43Z",
    "pdf_url": "https://arxiv.org/pdf/2411.17338v1"
  },
  {
    "arxiv_id": "2411.17123v1",
    "entry_id": "http://arxiv.org/abs/2411.17123v1",
    "title": "Advancing Content Moderation: Evaluating Large Language Models for Detecting Sensitive Content Across Text, Images, and Videos",
    "summary": "The widespread dissemination of hate speech, harassment, harmful and sexual content, and violence across websites and media platforms presents substantial challenges and provokes widespread concern among different sectors of society. Governments, educators, and parents are often at odds with media platforms about how to regulate, control, and limit the spread of such content. Technologies for detecting and censoring the media contents are a key solution to addressing these challenges. Techniques from natural language processing and computer vision have been used widely to automatically identify and filter out sensitive content such as offensive languages, violence, nudity, and addiction in both text, images, and videos, enabling platforms to enforce content policies at scale. However, existing methods still have limitations in achieving high detection accuracy with fewer false positives and false negatives. Therefore, more sophisticated algorithms for understanding the context of both text and image may open rooms for improvement in content censorship to build a more efficient censorship system. In this paper, we evaluate existing LLM-based content moderation solutions such as OpenAI moderation model and Llama-Guard3 and study their capabilities to detect sensitive contents. Additionally, we explore recent LLMs such as GPT, Gemini, and Llama in identifying inappropriate contents across media outlets. Various textual and visual datasets like X tweets, Amazon reviews, news articles, human photos, cartoons, sketches, and violence videos have been utilized for evaluation and comparison. The results demonstrate that LLMs outperform traditional techniques by achieving higher accuracy and lower false positive and false negative rates. This highlights the potential to integrate LLMs into websites, social media platforms, and video-sharing services for regulatory and content moderation purposes.",
    "authors": [
      "Nouar AlDahoul",
      "Myles Joshua Toledo Tan",
      "Harishwar Reddy Kasireddy",
      "Yasir Zaki"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-11-26T05:29:18Z",
    "pdf_url": "https://arxiv.org/pdf/2411.17123v1"
  },
  {
    "arxiv_id": "2411.16594v7",
    "entry_id": "http://arxiv.org/abs/2411.16594v7",
    "title": "From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge",
    "summary": "Assessment and evaluation have long been critical challenges in artificial intelligence (AI) and natural language processing (NLP). Traditional methods, usually matching-based or small model-based, often fall short in open-ended and dynamic scenarios. Recent advancements in Large Language Models (LLMs) inspire the \"LLM-as-a-judge\" paradigm, where LLMs are leveraged to perform scoring, ranking, or selection for various machine learning evaluation scenarios. This paper presents a comprehensive survey of LLM-based judgment and assessment, offering an in-depth overview to review this evolving field. We first provide the definition from both input and output perspectives. Then we introduce a systematic taxonomy to explore LLM-as-a-judge along three dimensions: what to judge, how to judge, and how to benchmark. Finally, we also highlight key challenges and promising future directions for this emerging area. More resources on LLM-as-a-judge are on the website: https://llm-as-a-judge.github.io and https://github.com/llm-as-a-judge/Awesome-LLM-as-a-judge.",
    "authors": [
      "Dawei Li",
      "Bohan Jiang",
      "Liangjie Huang",
      "Alimohammad Beigi",
      "Chengshuai Zhao",
      "Zhen Tan",
      "Amrita Bhattacharjee",
      "Yuxuan Jiang",
      "Canyu Chen",
      "Tianhao Wu",
      "Kai Shu",
      "Lu Cheng",
      "Huan Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-11-25T17:28:44Z",
    "pdf_url": "https://arxiv.org/pdf/2411.16594v7"
  },
  {
    "arxiv_id": "2411.16809v1",
    "entry_id": "http://arxiv.org/abs/2411.16809v1",
    "title": "Blockchain Meets LLMs: A Living Survey on Bidirectional Integration",
    "summary": "In the domain of large language models, considerable advancements have been attained in multimodal large language models and explainability research, propelled by the continuous technological progress and innovation. Nonetheless, security and privacy concerns continue to pose as prominent challenges in this field. The emergence of blockchain technology, marked by its decentralized nature, tamper-proof attributes, distributed storage functionality, and traceability, has provided novel approaches for resolving these issues. Both of these technologies independently hold vast potential for development; yet, their combination uncovers substantial cross-disciplinary opportunities and growth prospects. The current research tendencies are increasingly concentrating on the integration of blockchain with large language models, with the aim of compensating for their respective limitations through this fusion and promoting further technological evolution. In this study, we evaluate the advantages and developmental constraints of the two technologies, and explore the possibility and development potential of their combination. This paper primarily investigates the technical convergence in two directions: Firstly, the application of large language models to blockchain, where we identify six major development directions and explore solutions to the shortcomings of blockchain technology and their application scenarios; Secondly, the application of blockchain technology to large language models, leveraging the characteristics of blockchain to remedy the deficiencies of large language models and exploring its application potential in multiple fields.",
    "authors": [
      "Jianghao Gong",
      "Peiqi Yan",
      "Yue Zhang",
      "Hongli An",
      "Logan Liu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-11-25T14:54:08Z",
    "pdf_url": "https://arxiv.org/pdf/2411.16809v1"
  },
  {
    "arxiv_id": "2411.16403v1",
    "entry_id": "http://arxiv.org/abs/2411.16403v1",
    "title": "Adapter-based Approaches to Knowledge-enhanced Language Models -- A Survey",
    "summary": "Knowledge-enhanced language models (KELMs) have emerged as promising tools to bridge the gap between large-scale language models and domain-specific knowledge. KELMs can achieve higher factual accuracy and mitigate hallucinations by leveraging knowledge graphs (KGs). They are frequently combined with adapter modules to reduce the computational load and risk of catastrophic forgetting. In this paper, we conduct a systematic literature review (SLR) on adapter-based approaches to KELMs. We provide a structured overview of existing methodologies in the field through quantitative and qualitative analysis and explore the strengths and potential shortcomings of individual approaches. We show that general knowledge and domain-specific approaches have been frequently explored along with various adapter architectures and downstream tasks. We particularly focused on the popular biomedical domain, where we provided an insightful performance comparison of existing KELMs. We outline the main trends and propose promising future directions.",
    "authors": [
      "Alexander Fichtl",
      "Juraj Vladika",
      "Georg Groh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-25T14:10:24Z",
    "pdf_url": "https://arxiv.org/pdf/2411.16403v1"
  },
  {
    "arxiv_id": "2411.16084v1",
    "entry_id": "http://arxiv.org/abs/2411.16084v1",
    "title": "Deciphering genomic codes using advanced NLP techniques: a scoping review",
    "summary": "Objectives: The vast and complex nature of human genomic sequencing data presents challenges for effective analysis. This review aims to investigate the application of Natural Language Processing (NLP) techniques, particularly Large Language Models (LLMs) and transformer architectures, in deciphering genomic codes, focusing on tokenization, transformer models, and regulatory annotation prediction. The goal of this review is to assess data and model accessibility in the most recent literature, gaining a better understanding of the existing capabilities and constraints of these tools in processing genomic sequencing data.\n  Methods: Following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, our scoping review was conducted across PubMed, Medline, Scopus, Web of Science, Embase, and ACM Digital Library. Studies were included if they focused on NLP methodologies applied to genomic sequencing data analysis, without restrictions on publication date or article type.\n  Results: A total of 26 studies published between 2021 and April 2024 were selected for review. The review highlights that tokenization and transformer models enhance the processing and understanding of genomic data, with applications in predicting regulatory annotations like transcription-factor binding sites and chromatin accessibility.\n  Discussion: The application of NLP and LLMs to genomic sequencing data interpretation is a promising field that can help streamline the processing of large-scale genomic data while also providing a better understanding of its complex structures. It has the potential to drive advancements in personalized medicine by offering more efficient and scalable solutions for genomic analysis. Further research is also needed to discuss and overcome current limitations, enhancing model transparency and applicability.",
    "authors": [
      "Shuyan Cheng",
      "Yishu Wei",
      "Yiliang Zhou",
      "Zihan Xu",
      "Drew N Wright",
      "Jinze Liu",
      "Yifan Peng"
    ],
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "published": "2024-11-25T04:35:56Z",
    "pdf_url": "https://arxiv.org/pdf/2411.16084v1"
  },
  {
    "arxiv_id": "2411.16077v1",
    "entry_id": "http://arxiv.org/abs/2411.16077v1",
    "title": "SAGEval: The frontiers of Satisfactory Agent based NLG Evaluation for reference-free open-ended text",
    "summary": "Large Language Model (LLM) integrations into applications like Microsoft365 suite and Google Workspace for creating/processing documents, emails, presentations, etc. has led to considerable enhancements in productivity and time savings. But as these integrations become more more complex, it is paramount to ensure that the quality of output from the LLM-integrated applications are relevant and appropriate for use. Identifying the need to develop robust evaluation approaches for natural language generation, wherein references/ground labels doesn't exist or isn't amply available, this paper introduces a novel framework called \"SAGEval\" which utilizes a critiquing Agent to provide feedback on scores generated by LLM evaluators. We show that the critiquing Agent is able to rectify scores from LLM evaluators, in absence of references/ground-truth labels, thereby reducing the need for labeled data even for complex NLG evaluation scenarios, like the generation of JSON-structured forms/surveys with responses in different styles like multiple choice, likert ratings, single choice questions, etc.",
    "authors": [
      "Reshmi Ghosh",
      "Tianyi Yao",
      "Lizzy Chen",
      "Sadid Hasan",
      "Tianwei Chen",
      "Dario Bernal",
      "Huitian Jiao",
      "H M Sajjad Hossain"
    ],
    "categories": [
      "cs.CL",
      "cs.MA"
    ],
    "published": "2024-11-25T04:07:16Z",
    "pdf_url": "https://arxiv.org/pdf/2411.16077v1"
  },
  {
    "arxiv_id": "2411.15664v1",
    "entry_id": "http://arxiv.org/abs/2411.15664v1",
    "title": "Enabling Efficient Serverless Inference Serving for LLM (Large Language Model) in the Cloud",
    "summary": "This review report discusses the cold start latency in serverless inference and existing solutions. It particularly reviews the ServerlessLLM method, a system designed to address the cold start problem in serverless inference for large language models. Traditional serverless approaches struggle with high latency due to the size of LLM checkpoints and the overhead of initializing GPU resources. ServerlessLLM introduces a multitier checkpoint loading system, leveraging underutilized GPU memory and storage to reduce startup times by 6--8x compared to existing methods. It also proposes live inference migration and a startup-time-optimized model scheduler, ensuring efficient resource allocation and minimizing delays. This system significantly improves performance and scalability in serverless environments for LLM workloads. Besides ServerlessLLM, several other methods from recent research literature, including Rainbowcake, are reviewed in this paper. Further discussions explore how FaaS providers tackle cold starts and the possible future scopes.",
    "authors": [
      "Himel Ghosh"
    ],
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2024-11-23T22:19:37Z",
    "pdf_url": "https://arxiv.org/pdf/2411.15664v1"
  },
  {
    "arxiv_id": "2411.15594v6",
    "entry_id": "http://arxiv.org/abs/2411.15594v6",
    "title": "A Survey on LLM-as-a-Judge",
    "summary": "Accurate and consistent evaluation is crucial for decision-making across numerous fields, yet it remains a challenging task due to inherent subjectivity, variability, and scale. Large Language Models (LLMs) have achieved remarkable success across diverse domains, leading to the emergence of \"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With their ability to process diverse data types and provide scalable, cost-effective, and consistent assessments, LLMs present a compelling alternative to traditional expert-driven evaluations. However, ensuring the reliability of LLM-as-a-Judge systems remains a significant challenge that requires careful design and standardization. This paper provides a comprehensive survey of LLM-as-a-Judge, addressing the core question: How can reliable LLM-as-a-Judge systems be built? We explore strategies to enhance reliability, including improving consistency, mitigating biases, and adapting to diverse assessment scenarios. Additionally, we propose methodologies for evaluating the reliability of LLM-as-a-Judge systems, supported by a novel benchmark designed for this purpose. To advance the development and real-world deployment of LLM-as-a-Judge systems, we also discussed practical applications, challenges, and future directions. This survey serves as a foundational reference for researchers and practitioners in this rapidly evolving field.",
    "authors": [
      "Jiawei Gu",
      "Xuhui Jiang",
      "Zhichao Shi",
      "Hexiang Tan",
      "Xuehao Zhai",
      "Chengjin Xu",
      "Wei Li",
      "Yinghan Shen",
      "Shengjie Ma",
      "Honghao Liu",
      "Saizhuo Wang",
      "Kun Zhang",
      "Yuanzhuo Wang",
      "Wen Gao",
      "Lionel Ni",
      "Jian Guo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-23T16:03:35Z",
    "pdf_url": "https://arxiv.org/pdf/2411.15594v6"
  },
  {
    "arxiv_id": "2411.15296v2",
    "entry_id": "http://arxiv.org/abs/2411.15296v2",
    "title": "MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs",
    "summary": "As a prominent direction of Artificial General Intelligence (AGI), Multimodal Large Language Models (MLLMs) have garnered increased attention from both industry and academia. Building upon pre-trained LLMs, this family of models further develops multimodal perception and reasoning capabilities that are impressive, such as writing code given a flow chart or creating stories based on an image. In the development process, evaluation is critical since it provides intuitive feedback and guidance on improving models. Distinct from the traditional train-eval-test paradigm that only favors a single task like image classification, the versatility of MLLMs has spurred the rise of various new benchmarks and evaluation methods. In this paper, we aim to present a comprehensive survey of MLLM evaluation, discussing four key aspects: 1) the summarised benchmarks types divided by the evaluation capabilities, including foundation capabilities, model self-analysis, and extented applications; 2) the typical process of benchmark counstruction, consisting of data collection, annotation, and precautions; 3) the systematic evaluation manner composed of judge, metric, and toolkit; 4) the outlook for the next benchmark. This work aims to offer researchers an easy grasp of how to effectively evaluate MLLMs according to different needs and to inspire better evaluation methods, thereby driving the progress of MLLM research.",
    "authors": [
      "Chaoyou Fu",
      "Yi-Fan Zhang",
      "Shukang Yin",
      "Bo Li",
      "Xinyu Fang",
      "Sirui Zhao",
      "Haodong Duan",
      "Xing Sun",
      "Ziwei Liu",
      "Liang Wang",
      "Caifeng Shan",
      "Ran He"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-11-22T18:59:54Z",
    "pdf_url": "https://arxiv.org/pdf/2411.15296v2"
  },
  {
    "arxiv_id": "2411.15287v1",
    "entry_id": "http://arxiv.org/abs/2411.15287v1",
    "title": "Sycophancy in Large Language Models: Causes and Mitigations",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of natural language processing tasks. However, their tendency to exhibit sycophantic behavior - excessively agreeing with or flattering users - poses significant risks to their reliability and ethical deployment. This paper provides a technical survey of sycophancy in LLMs, analyzing its causes, impacts, and potential mitigation strategies. We review recent work on measuring and quantifying sycophantic tendencies, examine the relationship between sycophancy and other challenges like hallucination and bias, and evaluate promising techniques for reducing sycophancy while maintaining model performance. Key approaches explored include improved training data, novel fine-tuning methods, post-deployment control mechanisms, and decoding strategies. We also discuss the broader implications of sycophancy for AI alignment and propose directions for future research. Our analysis suggests that mitigating sycophancy is crucial for developing more robust, reliable, and ethically-aligned language models.",
    "authors": [
      "Lars Malmqvist"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-22T16:56:49Z",
    "pdf_url": "https://arxiv.org/pdf/2411.15287v1"
  },
  {
    "arxiv_id": "2411.15234v3",
    "entry_id": "http://arxiv.org/abs/2411.15234v3",
    "title": "Adaptive Intelligence: leveraging insights from adaptive behavior in animals to build flexible AI systems",
    "summary": "Biological intelligence is inherently adaptive -- animals continually adjust their actions based on environmental feedback. However, creating adaptive artificial intelligence (AI) remains a major challenge. The next frontier is to go beyond traditional AI to develop \"adaptive intelligence,\" defined here as harnessing insights from biological intelligence to build agents that can learn online, generalize, and rapidly adapt to changes in their environment. Recent advances in neuroscience offer inspiration through studies that increasingly focus on how animals naturally learn and adapt their world models. In this Perspective, I will review the behavioral and neural foundations of adaptive biological intelligence, the parallel progress in AI, and explore brain-inspired approaches for building more adaptive algorithms.",
    "authors": [
      "Mackenzie Weygandt Mathis"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "published": "2024-11-21T20:26:29Z",
    "pdf_url": "https://arxiv.org/pdf/2411.15234v3"
  },
  {
    "arxiv_id": "2412.05288v1",
    "entry_id": "http://arxiv.org/abs/2412.05288v1",
    "title": "StackEval: Benchmarking LLMs in Coding Assistance",
    "summary": "We present two comprehensive benchmarks to evaluate the performance of language models in coding assistance tasks, covering code writing, debugging, code review, and conceptual understanding. Our main contribution includes two curated datasets: StackEval, a large-scale benchmark derived from Stack Overflow questions, and StackUnseen, a dynamic benchmark featuring the most recent Stack Overflow content. These benchmarks offer novel insights into the capabilities and limitations of LLMs, particularly in handling new and emerging content. Additionally, we assess LLMs' proficiency as judges for coding tasks using a curated, human-annotated dataset, exploring their evaluation capabilities and potential biases, including whether they favor their own generated solutions. Our findings underscore the potential of these benchmarks to advance LLM development and application in coding assistance. To ensure reproducibility, we publicly share our datasets and evaluation code at https://github.com/ProsusAI/stack-eval .",
    "authors": [
      "Nidhish Shah",
      "Zulkuf Genc",
      "Dogu Araci"
    ],
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-11-21T11:20:48Z",
    "pdf_url": "https://arxiv.org/pdf/2412.05288v1"
  },
  {
    "arxiv_id": "2411.14499v2",
    "entry_id": "http://arxiv.org/abs/2411.14499v2",
    "title": "Understanding World or Predicting Future? A Comprehensive Survey of World Models",
    "summary": "The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.",
    "authors": [
      "Jingtao Ding",
      "Yunke Zhang",
      "Yu Shang",
      "Yuheng Zhang",
      "Zefang Zong",
      "Jie Feng",
      "Yuan Yuan",
      "Hongyuan Su",
      "Nian Li",
      "Nicholas Sukiennik",
      "Fengli Xu",
      "Yong Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-21T03:58:50Z",
    "pdf_url": "https://arxiv.org/pdf/2411.14499v2"
  },
  {
    "arxiv_id": "2411.13768v2",
    "entry_id": "http://arxiv.org/abs/2411.13768v2",
    "title": "Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture",
    "summary": "Large Language Models (LLMs) have enabled the emergence of LLM agents: autonomous systems capable of achieving under-specified goals and adapting post-deployment, often without explicit code or model changes. Evaluating these agents is critical to ensuring their performance and safety, especially given their dynamic, probabilistic, and evolving nature. However, traditional approaches such as predefined test cases and standard redevelopment pipelines struggle to address the unique challenges of LLM agent evaluation. These challenges include capturing open-ended behaviors, handling emergent outcomes, and enabling continuous adaptation over the agent's lifecycle. To address these issues, we propose an evaluation-driven development approach, inspired by test-driven and behavior-driven development but reimagined for the unique characteristics of LLM agents. Through a multivocal literature review (MLR), we synthesize the limitations of existing LLM evaluation methods and introduce a novel process model and reference architecture tailored for evaluation-driven development of LLM agents. Our approach integrates online (runtime) and offline (redevelopment) evaluations, enabling adaptive runtime adjustments and systematic iterative refinement of pipelines, artifacts, system architecture, and LLMs themselves. By continuously incorporating evaluation results, including fine-grained feedback from human and AI evaluators, into each stage of development and operation, this framework ensures that LLM agents remain aligned with evolving goals, user needs, and governance standards.",
    "authors": [
      "Boming Xia",
      "Qinghua Lu",
      "Liming Zhu",
      "Zhenchang Xing",
      "Dehai Zhao",
      "Hao Zhang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-11-21T00:34:30Z",
    "pdf_url": "https://arxiv.org/pdf/2411.13768v2"
  },
  {
    "arxiv_id": "2411.13485v2",
    "entry_id": "http://arxiv.org/abs/2411.13485v2",
    "title": "Utilizing Large Language Models to Synthesize Product Desirability Datasets",
    "summary": "This research explores the application of large language models (LLMs) to generate synthetic datasets for Product Desirability Toolkit (PDT) testing, a key component in evaluating user sentiment and product experience. Utilizing gpt-4o-mini, a cost-effective alternative to larger commercial LLMs, three methods, Word+Review, Review+Word, and Supply-Word, were each used to synthesize 1000 product reviews. The generated datasets were assessed for sentiment alignment, textual diversity, and data generation cost. Results demonstrated high sentiment alignment across all methods, with Pearson correlations ranging from 0.93 to 0.97. Supply-Word exhibited the highest diversity and coverage of PDT terms, although with increased generation costs. Despite minor biases toward positive sentiments, in situations with limited test data, LLM-generated synthetic data offers significant advantages, including scalability, cost savings, and flexibility in dataset production.",
    "authors": [
      "John D. Hastings",
      "Sherri Weitl-Harms",
      "Joseph Doty",
      "Zachary J. Myers",
      "Warren Thompson"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-20T17:35:21Z",
    "pdf_url": "https://arxiv.org/pdf/2411.13485v2"
  },
  {
    "arxiv_id": "2411.13410v1",
    "entry_id": "http://arxiv.org/abs/2411.13410v1",
    "title": "A Survey On Enhancing Reinforcement Learning in Complex Environments: Insights from Human and LLM Feedback",
    "summary": "Reinforcement learning (RL) is one of the active fields in machine learning, demonstrating remarkable potential in tackling real-world challenges. Despite its promising prospects, this methodology has encountered with issues and challenges, hindering it from achieving the best performance. In particular, these approaches lack decent performance when navigating environments and solving tasks with large observation space, often resulting in sample-inefficiency and prolonged learning times. This issue, commonly referred to as the curse of dimensionality, complicates decision-making for RL agents, necessitating a careful balance between attention and decision-making. RL agents, when augmented with human or large language models' (LLMs) feedback, may exhibit resilience and adaptability, leading to enhanced performance and accelerated learning. Such feedback, conveyed through various modalities or granularities including natural language, serves as a guide for RL agents, aiding them in discerning relevant environmental cues and optimizing decision-making processes. In this survey paper, we mainly focus on problems of two-folds: firstly, we focus on humans or an LLMs assistance, investigating the ways in which these entities may collaborate with the RL agent in order to foster optimal behavior and expedite learning; secondly, we delve into the research papers dedicated to addressing the intricacies of environments characterized by large observation space.",
    "authors": [
      "Alireza Rashidi Laleh",
      "Majid Nili Ahmadabadi"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-11-20T15:52:03Z",
    "pdf_url": "https://arxiv.org/pdf/2411.13410v1"
  },
  {
    "arxiv_id": "2411.14491v3",
    "entry_id": "http://arxiv.org/abs/2411.14491v3",
    "title": "A Survey on Human-Centric LLMs",
    "summary": "The rapid evolution of large language models (LLMs) and their capacity to simulate human cognition and behavior has given rise to LLM-based frameworks and tools that are evaluated and applied based on their ability to perform tasks traditionally performed by humans, namely those involving cognition, decision-making, and social interaction. This survey provides a comprehensive examination of such human-centric LLM capabilities, focusing on their performance in both individual tasks (where an LLM acts as a stand-in for a single human) and collective tasks (where multiple LLMs coordinate to mimic group dynamics). We first evaluate LLM competencies across key areas including reasoning, perception, and social cognition, comparing their abilities to human-like skills. Then, we explore real-world applications of LLMs in human-centric domains such as behavioral science, political science, and sociology, assessing their effectiveness in replicating human behaviors and interactions. Finally, we identify challenges and future research directions, such as improving LLM adaptability, emotional intelligence, and cultural sensitivity, while addressing inherent biases and enhancing frameworks for human-AI collaboration. This survey aims to provide a foundational understanding of LLMs from a human-centric perspective, offering insights into their current capabilities and potential for future development.",
    "authors": [
      "Jing Yi Wang",
      "Nicholas Sukiennik",
      "Tong Li",
      "Weikang Su",
      "Qianyue Hao",
      "Jingbo Xu",
      "Zihan Huang",
      "Fengli Xu",
      "Yong Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-20T12:34:44Z",
    "pdf_url": "https://arxiv.org/pdf/2411.14491v3"
  },
  {
    "arxiv_id": "2411.13157v2",
    "entry_id": "http://arxiv.org/abs/2411.13157v2",
    "title": "Closer Look at Efficient Inference Methods: A Survey of Speculative Decoding",
    "summary": "Efficient inference in large language models (LLMs) has become a critical focus as their scale and complexity grow. Traditional autoregressive decoding, while effective, suffers from computational inefficiencies due to its sequential token generation process. Speculative decoding addresses this bottleneck by introducing a two-stage framework: drafting and verification. A smaller, efficient model generates a preliminary draft, which is then refined by a larger, more sophisticated model. This paper provides a comprehensive survey of speculative decoding methods, categorizing them into draft-centric and model-centric approaches. We discuss key ideas associated with each method, highlighting their potential for scaling LLM inference. This survey aims to guide future research in optimizing speculative decoding and its integration into real-world LLM applications.",
    "authors": [
      "Hyun Ryu",
      "Eric Kim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-20T09:46:30Z",
    "pdf_url": "https://arxiv.org/pdf/2411.13157v2"
  },
  {
    "arxiv_id": "2411.13032v1",
    "entry_id": "http://arxiv.org/abs/2411.13032v1",
    "title": "\"It was 80% me, 20% AI\": Seeking Authenticity in Co-Writing with Large Language Models",
    "summary": "Given the rising proliferation and diversity of AI writing assistance tools, especially those powered by large language models (LLMs), both writers and readers may have concerns about the impact of these tools on the authenticity of writing work. We examine whether and how writers want to preserve their authentic voice when co-writing with AI tools and whether personalization of AI writing support could help achieve this goal. We conducted semi-structured interviews with 19 professional writers, during which they co-wrote with both personalized and non-personalized AI writing-support tools. We supplemented writers' perspectives with opinions from 30 avid readers about the written work co-produced with AI collected through an online survey. Our findings illuminate conceptions of authenticity in human-AI co-creation, which focus more on the process and experience of constructing creators' authentic selves. While writers reacted positively to personalized AI writing tools, they believed the form of personalization needs to target writers' growth and go beyond the phase of text production. Overall, readers' responses showed less concern about human-AI co-writing. Readers could not distinguish AI-assisted work, personalized or not, from writers' solo-written work and showed positive attitudes toward writers experimenting with new technology for creative writing.",
    "authors": [
      "Angel Hsing-Chi Hwang",
      "Q. Vera Liao",
      "Su Lin Blodgett",
      "Alexandra Olteanu",
      "Adam Trischler"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-11-20T04:42:32Z",
    "pdf_url": "https://arxiv.org/pdf/2411.13032v1"
  },
  {
    "arxiv_id": "2411.12274v1",
    "entry_id": "http://arxiv.org/abs/2411.12274v1",
    "title": "A Review on Generative AI Models for Synthetic Medical Text, Time Series, and Longitudinal Data",
    "summary": "This paper presents the results of a novel scoping review on the practical models for generating three different types of synthetic health records (SHRs): medical text, time series, and longitudinal data. The innovative aspects of the review, which incorporate study objectives, data modality, and research methodology of the reviewed studies, uncover the importance and the scope of the topic for the digital medicine context. In total, 52 publications met the eligibility criteria for generating medical time series (22), longitudinal data (17), and medical text (13). Privacy preservation was found to be the main research objective of the studied papers, along with class imbalance, data scarcity, and data imputation as the other objectives. The adversarial network-based, probabilistic, and large language models exhibited superiority for generating synthetic longitudinal data, time series, and medical texts, respectively. Finding a reliable performance measure to quantify SHR re-identification risk is the major research gap of the topic.",
    "authors": [
      "Mohammad Loni",
      "Fatemeh Poursalim",
      "Mehdi Asadi",
      "Arash Gharehbaghi"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-11-19T06:53:54Z",
    "pdf_url": "https://arxiv.org/pdf/2411.12274v1"
  },
  {
    "arxiv_id": "2411.11795v1",
    "entry_id": "http://arxiv.org/abs/2411.11795v1",
    "title": "Exploring adversarial robustness of JPEG AI: methodology, comparison and new methods",
    "summary": "Adversarial robustness of neural networks is an increasingly important area of research, combining studies on computer vision models, large language models (LLMs), and others. With the release of JPEG AI - the first standard for end-to-end neural image compression (NIC) methods - the question of its robustness has become critically significant. JPEG AI is among the first international, real-world applications of neural-network-based models to be embedded in consumer devices. However, research on NIC robustness has been limited to open-source codecs and a narrow range of attacks. This paper proposes a new methodology for measuring NIC robustness to adversarial attacks. We present the first large-scale evaluation of JPEG AI's robustness, comparing it with other NIC models. Our evaluation results and code are publicly available online (link is hidden for a blind review).",
    "authors": [
      "Egor Kovalev",
      "Georgii Bychkov",
      "Khaled Abud",
      "Aleksandr Gushchin",
      "Anna Chistyakova",
      "Sergey Lavrushkin",
      "Dmitriy Vatolin",
      "Anastasia Antsiferova"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-11-18T18:08:52Z",
    "pdf_url": "https://arxiv.org/pdf/2411.11795v1"
  },
  {
    "arxiv_id": "2411.11937v1",
    "entry_id": "http://arxiv.org/abs/2411.11937v1",
    "title": "Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets",
    "summary": "LLMs are increasingly fine-tuned using RLHF datasets to align them with human preferences and values. However, very limited research has investigated which specific human values are operationalized through these datasets. In this paper, we introduce Value Imprint, a framework for auditing and classifying the human values embedded within RLHF datasets. To investigate the viability of this framework, we conducted three case study experiments by auditing the Anthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM datasets to examine the human values embedded within them. Our analysis involved a two-phase process. During the first phase, we developed a taxonomy of human values through an integrated review of prior works from philosophy, axiology, and ethics. Then, we applied this taxonomy to annotate 6,501 RLHF preferences. During the second phase, we employed the labels generated from the annotation as ground truth data for training a transformer-based machine learning model to audit and classify the three RLHF datasets. Through this approach, we discovered that information-utility values, including Wisdom/Knowledge and Information Seeking, were the most dominant human values within all three RLHF datasets. In contrast, prosocial and democratic values, including Well-being, Justice, and Human/Animal Rights, were the least represented human values. These findings have significant implications for developing language models that align with societal values and norms. We contribute our datasets to support further research in this area.",
    "authors": [
      "Ike Obi",
      "Rohan Pant",
      "Srishti Shekhar Agrawal",
      "Maham Ghazanfar",
      "Aaron Basiletti"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-11-18T16:12:24Z",
    "pdf_url": "https://arxiv.org/pdf/2411.11937v1"
  },
  {
    "arxiv_id": "2411.11616v2",
    "entry_id": "http://arxiv.org/abs/2411.11616v2",
    "title": "Signaling and Social Learning in Swarms of Robots",
    "summary": "This paper investigates the role of communication in improving coordination within robot swarms, focusing on a paradigm where learning and execution occur simultaneously in a decentralized manner. We highlight the role communication can play in addressing the credit assignment problem (individual contribution to the overall performance), and how it can be influenced by it. We propose a taxonomy of existing and future works on communication, focusing on information selection and physical abstraction as principal axes for classification: from low-level lossless compression with raw signal extraction and processing to high-level lossy compression with structured communication models. The paper reviews current research from evolutionary robotics, multi-agent (deep) reinforcement learning, language models, and biophysics models to outline the challenges and opportunities of communication in a collective of robots that continuously learn from one another through local message exchanges, illustrating a form of social learning.",
    "authors": [
      "Leo Cazenille",
      "Maxime Toquebiau",
      "Nicolas Lobato-Dauzier",
      "Alessia Loi",
      "Loona Macabre",
      "Nathanael Aubert-Kato",
      "Anthony Genot",
      "Nicolas Bredeche"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2024-11-18T14:42:15Z",
    "pdf_url": "https://arxiv.org/pdf/2411.11616v2"
  },
  {
    "arxiv_id": "2412.04326v1",
    "entry_id": "http://arxiv.org/abs/2412.04326v1",
    "title": "Understanding Student Sentiment on Mental Health Support in Colleges Using Large Language Models",
    "summary": "Mental health support in colleges is vital in educating students by offering counseling services and organizing supportive events. However, evaluating its effectiveness faces challenges like data collection difficulties and lack of standardized metrics, limiting research scope. Student feedback is crucial for evaluation but often relies on qualitative analysis without systematic investigation using advanced machine learning methods. This paper uses public Student Voice Survey data to analyze student sentiments on mental health support with large language models (LLMs). We created a sentiment analysis dataset, SMILE-College, with human-machine collaboration. The investigation of both traditional machine learning methods and state-of-the-art LLMs showed the best performance of GPT-3.5 and BERT on this new dataset. The analysis highlights challenges in accurately predicting response sentiments and offers practical insights on how LLMs can enhance mental health-related research and improve college mental health services. This data-driven approach will facilitate efficient and informed mental health support evaluation, management, and decision-making.",
    "authors": [
      "Palak Sood",
      "Chengyang He",
      "Divyanshu Gupta",
      "Yue Ning",
      "Ping Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-11-18T02:53:15Z",
    "pdf_url": "https://arxiv.org/pdf/2412.04326v1"
  },
  {
    "arxiv_id": "2411.11910v2",
    "entry_id": "http://arxiv.org/abs/2411.11910v2",
    "title": "AIGS: Generating Science from AI-Powered Automated Falsification",
    "summary": "Rapid development of artificial intelligence has drastically accelerated the development of scientific discovery. Trained with large-scale observation data, deep neural networks extract the underlying patterns in an end-to-end manner and assist human researchers with highly-precised predictions in unseen scenarios. The recent rise of Large Language Models (LLMs) and the empowered autonomous agents enable scientists to gain help through interaction in different stages of their research, including but not limited to literature review, research ideation, idea implementation, and academic writing. However, AI researchers instantiated by foundation model empowered agents with full-process autonomy are still in their infancy. In this paper, we study $\\textbf{AI-Generated Science}$ (AIGS), where agents independently and autonomously complete the entire research process and discover scientific laws. By revisiting the definition of scientific research, we argue that $\\textit{falsification}$ is the essence of both human research process and the design of an AIGS system. Through the lens of falsification, prior systems attempting towards AI-Generated Science either lack the part in their design, or rely heavily on existing verification engines that narrow the use in specialized domains. In this work, we propose Baby-AIGS as a baby-step demonstration of a full-process AIGS system, which is a multi-agent system with agents in roles representing key research process. By introducing FalsificationAgent, which identify and then verify possible scientific discoveries, we empower the system with explicit falsification. Experiments on three tasks preliminarily show that Baby-AIGS could produce meaningful scientific discoveries, though not on par with experienced human researchers. Finally, we discuss on the limitations of current Baby-AIGS, actionable insights, and related ethical issues in detail.",
    "authors": [
      "Zijun Liu",
      "Kaiming Liu",
      "Yiqi Zhu",
      "Xuanyu Lei",
      "Zonghan Yang",
      "Zhenhe Zhang",
      "Peng Li",
      "Yang Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-11-17T13:40:35Z",
    "pdf_url": "https://arxiv.org/pdf/2411.11910v2"
  },
  {
    "arxiv_id": "2411.10943v1",
    "entry_id": "http://arxiv.org/abs/2411.10943v1",
    "title": "Generalist Virtual Agents: A Survey on Autonomous Agents Across Digital Platforms",
    "summary": "In this paper, we introduce the Generalist Virtual Agent (GVA), an autonomous entity engineered to function across diverse digital platforms and environments, assisting users by executing a variety of tasks. This survey delves into the evolution of GVAs, tracing their progress from early intelligent assistants to contemporary implementations that incorporate large-scale models. We explore both the philosophical underpinnings and practical foundations of GVAs, addressing their developmental challenges and the methodologies currently employed in their design and operation. By presenting a detailed taxonomy of GVA environments, tasks, and capabilities, this paper aims to bridge the theoretical and practical aspects of GVAs, concluding those that operate in environments closely mirroring the real world are more likely to demonstrate human-like intelligence. We discuss potential future directions for GVA research, highlighting the necessity for realistic evaluation metrics and the enhancement of long-sequence decision-making capabilities to advance the field toward more systematic or embodied applications. This work not only synthesizes the existing body of literature but also proposes frameworks for future investigations, contributing significantly to the ongoing development of intelligent systems.",
    "authors": [
      "Minghe Gao",
      "Wendong Bu",
      "Bingchen Miao",
      "Yang Wu",
      "Yunfei Li",
      "Juncheng Li",
      "Siliang Tang",
      "Qi Wu",
      "Yueting Zhuang",
      "Meng Wang"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2024-11-17T02:44:56Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10943v1"
  },
  {
    "arxiv_id": "2411.10915v1",
    "entry_id": "http://arxiv.org/abs/2411.10915v1",
    "title": "Bias in Large Language Models: Origin, Evaluation, and Mitigation",
    "summary": "Large Language Models (LLMs) have revolutionized natural language processing, but their susceptibility to biases poses significant challenges. This comprehensive review examines the landscape of bias in LLMs, from its origins to current mitigation strategies. We categorize biases as intrinsic and extrinsic, analyzing their manifestations in various NLP tasks. The review critically assesses a range of bias evaluation methods, including data-level, model-level, and output-level approaches, providing researchers with a robust toolkit for bias detection. We further explore mitigation strategies, categorizing them into pre-model, intra-model, and post-model techniques, highlighting their effectiveness and limitations. Ethical and legal implications of biased LLMs are discussed, emphasizing potential harms in real-world applications such as healthcare and criminal justice. By synthesizing current knowledge on bias in LLMs, this review contributes to the ongoing effort to develop fair and responsible AI systems. Our work serves as a comprehensive resource for researchers and practitioners working towards understanding, evaluating, and mitigating bias in LLMs, fostering the development of more equitable AI technologies.",
    "authors": [
      "Yufei Guo",
      "Muzhe Guo",
      "Juntao Su",
      "Zhou Yang",
      "Mengqiu Zhu",
      "Hongfei Li",
      "Mengyang Qiu",
      "Shuo Shuo Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-11-16T23:54:53Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10915v1"
  },
  {
    "arxiv_id": "2411.10696v1",
    "entry_id": "http://arxiv.org/abs/2411.10696v1",
    "title": "HELENE: Hessian Layer-wise Clipping and Gradient Annealing for Accelerating Fine-tuning LLM with Zeroth-order Optimization",
    "summary": "Fine-tuning large language models (LLMs) poses significant memory challenges, as the back-propagation process demands extensive resources, especially with growing model sizes. Recent work, MeZO, addresses this issue using a zeroth-order (ZO) optimization method, which reduces memory consumption by matching the usage to the inference phase. However, MeZO experiences slow convergence due to varying curvatures across model parameters. To overcome this limitation, we introduce HELENE, a novel scalable and memory-efficient optimizer that integrates annealed A-GNB gradients with a diagonal Hessian estimation and layer-wise clipping, serving as a second-order pre-conditioner. This combination allows for faster and more stable convergence. Our theoretical analysis demonstrates that HELENE improves convergence rates, particularly for models with heterogeneous layer dimensions, by reducing the dependency on the total parameter space dimension. Instead, the method scales with the largest layer dimension, making it highly suitable for modern LLM architectures. Experimental results on RoBERTa-large and OPT-1.3B across multiple tasks show that HELENE achieves up to a 20x speedup compared to MeZO, with average accuracy improvements of 1.5%. Furthermore, HELENE remains compatible with both full parameter tuning and parameter-efficient fine-tuning (PEFT), outperforming several state-of-the-art optimizers. The codes will be released after reviewing.",
    "authors": [
      "Huaqin Zhao",
      "Jiaxi Li",
      "Yi Pan",
      "Shizhe Liang",
      "Xiaofeng Yang",
      "Wei Liu",
      "Xiang Li",
      "Fei Dou",
      "Tianming Liu",
      "Jin Lu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-11-16T04:27:22Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10696v1"
  },
  {
    "arxiv_id": "2411.12759v1",
    "entry_id": "http://arxiv.org/abs/2411.12759v1",
    "title": "A Novel Approach to Eliminating Hallucinations in Large Language Model-Assisted Causal Discovery",
    "summary": "The increasing use of large language models (LLMs) in causal discovery as a substitute for human domain experts highlights the need for optimal model selection. This paper presents the first hallucination survey of popular LLMs for causal discovery. We show that hallucinations exist when using LLMs in causal discovery so the choice of LLM is important. We propose using Retrieval Augmented Generation (RAG) to reduce hallucinations when quality data is available. Additionally, we introduce a novel method employing multiple LLMs with an arbiter in a debate to audit edges in causal graphs, achieving a comparable reduction in hallucinations to RAG.",
    "authors": [
      "Grace Sng",
      "Yanming Zhang",
      "Klaus Mueller"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-16T03:06:39Z",
    "pdf_url": "https://arxiv.org/pdf/2411.12759v1"
  },
  {
    "arxiv_id": "2411.10371v5",
    "entry_id": "http://arxiv.org/abs/2411.10371v5",
    "title": "A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects",
    "summary": "Event Causality Identification (ECI) has become an essential task in Natural Language Processing (NLP), focused on automatically detecting causal relationships between events within texts. This comprehensive survey systematically investigates fundamental concepts and models, developing a systematic taxonomy and critically evaluating diverse models. We begin by defining core concepts, formalizing the ECI problem, and outlining standard evaluation protocols. Our classification framework divides ECI models into two primary tasks: Sentence-level Event Causality Identification (SECI) and Document-level Event Causality Identification (DECI). For SECI, we review models employing feature pattern-based matching, machine learning classifiers, deep semantic encoding, prompt-based fine-tuning, and causal knowledge pre-training, alongside data augmentation strategies. For DECI, we focus on approaches utilizing deep semantic encoding, event graph reasoning, and prompt-based fine-tuning. Special attention is given to recent advancements in multi-lingual and cross-lingual ECI, as well as zero-shot ECI leveraging Large Language Models (LLMs). We analyze the strengths, limitations, and unresolved challenges associated with each approach. Extensive quantitative evaluations are conducted on four benchmark datasets to rigorously assess the performance of various ECI models. We conclude by discussing future research directions and highlighting opportunities to advance the field further.",
    "authors": [
      "Qing Cheng",
      "Zefan Zeng",
      "Xingchen Hu",
      "Yuehang Si",
      "Zhong Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-15T17:19:42Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10371v5"
  },
  {
    "arxiv_id": "2411.10268v2",
    "entry_id": "http://arxiv.org/abs/2411.10268v2",
    "title": "Towards Sample-Efficiency and Generalization of Transfer and Inverse Reinforcement Learning: A Comprehensive Literature Review",
    "summary": "Reinforcement learning (RL) is a sub-domain of machine learning, mainly concerned with solving sequential decision-making problems by a learning agent that interacts with the decision environment to improve its behavior through the reward it receives from the environment. This learning paradigm is, however, well-known for being time-consuming due to the necessity of collecting a large amount of data, making RL suffer from sample inefficiency and difficult generalization. Furthermore, the construction of an explicit reward function that accounts for the trade-off between multiple desiderata of a decision problem is often a laborious task. These challenges have been recently addressed utilizing transfer and inverse reinforcement learning (T-IRL). In this regard, this paper is devoted to a comprehensive review of realizing the sample efficiency and generalization of RL algorithms through T-IRL. Following a brief introduction to RL, the fundamental T-IRL methods are presented and the most recent advancements in each research field have been extensively reviewed. Our findings denote that a majority of recent research works have dealt with the aforementioned challenges by utilizing human-in-the-loop and sim-to-real strategies for the efficient transfer of knowledge from source domains to the target domain under the transfer learning scheme. Under the IRL structure, training schemes that require a low number of experience transitions and extension of such frameworks to multi-agent and multi-intention problems have been the priority of researchers in recent years.",
    "authors": [
      "Hossein Hassani",
      "Ehsan Hallaji",
      "Roozbeh Razavi-Far",
      "Mehrdad Saif",
      "Liang Lin"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-11-15T15:18:57Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10268v2"
  },
  {
    "arxiv_id": "2411.10137v1",
    "entry_id": "http://arxiv.org/abs/2411.10137v1",
    "title": "Legal Evalutions and Challenges of Large Language Models",
    "summary": "In this paper, we review legal testing methods based on Large Language Models (LLMs), using the OPENAI o1 model as a case study to evaluate the performance of large models in applying legal provisions. We compare current state-of-the-art LLMs, including open-source, closed-source, and legal-specific models trained specifically for the legal domain. Systematic tests are conducted on English and Chinese legal cases, and the results are analyzed in depth. Through systematic testing of legal cases from common law systems and China, this paper explores the strengths and weaknesses of LLMs in understanding and applying legal texts, reasoning through legal issues, and predicting judgments. The experimental results highlight both the potential and limitations of LLMs in legal applications, particularly in terms of challenges related to the interpretation of legal language and the accuracy of legal reasoning. Finally, the paper provides a comprehensive analysis of the advantages and disadvantages of various types of models, offering valuable insights and references for the future application of AI in the legal field.",
    "authors": [
      "Jiaqi Wang",
      "Huan Zhao",
      "Zhenyuan Yang",
      "Peng Shu",
      "Junhao Chen",
      "Haobo Sun",
      "Ruixi Liang",
      "Shixin Li",
      "Pengcheng Shi",
      "Longjun Ma",
      "Zongjia Liu",
      "Zhengliang Liu",
      "Tianyang Zhong",
      "Yutong Zhang",
      "Chong Ma",
      "Xin Zhang",
      "Tuo Zhang",
      "Tianli Ding",
      "Yudan Ren",
      "Tianming Liu",
      "Xi Jiang",
      "Shu Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-15T12:23:12Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10137v1"
  },
  {
    "arxiv_id": "2411.10129v1",
    "entry_id": "http://arxiv.org/abs/2411.10129v1",
    "title": "Prompting and Fine-tuning Large Language Models for Automated Code Review Comment Generation",
    "summary": "Generating accurate code review comments remains a significant challenge due to the inherently diverse and non-unique nature of the task output. Large language models pretrained on both programming and natural language data tend to perform well in code-oriented tasks. However, large-scale pretraining is not always feasible due to its environmental impact and project-specific generalizability issues. In this work, first we fine-tune open-source Large language models (LLM) in parameter-efficient, quantized low-rank (QLoRA) fashion on consumer-grade hardware to improve review comment generation. Recent studies demonstrate the efficacy of augmenting semantic metadata information into prompts to boost performance in other code-related tasks. To explore this in code review activities, we also prompt proprietary, closed-source LLMs augmenting the input code patch with function call graphs and code summaries. Both of our strategies improve the review comment generation performance, with function call graph augmented few-shot prompting on the GPT-3.5 model surpassing the pretrained baseline by around 90% BLEU-4 score on the CodeReviewer dataset. Moreover, few-shot prompted Gemini-1.0 Pro, QLoRA fine-tuned Code Llama and Llama 3.1 models achieve competitive results (ranging from 25% to 83% performance improvement) on this task. An additional human evaluation study further validates our experimental findings, reflecting real-world developers' perceptions of LLM-generated code review comments based on relevant qualitative metrics.",
    "authors": [
      "Md. Asif Haider",
      "Ayesha Binte Mostofa",
      "Sk. Sabit Bin Mosaddek",
      "Anindya Iqbal",
      "Toufique Ahmed"
    ],
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-11-15T12:01:38Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10129v1"
  },
  {
    "arxiv_id": "2411.10109v1",
    "entry_id": "http://arxiv.org/abs/2411.10109v1",
    "title": "Generative Agent Simulations of 1,000 People",
    "summary": "The promise of human behavioral simulation--general-purpose computational agents that replicate human behavior across domains--could enable broad applications in policymaking and social science. We present a novel agent architecture that simulates the attitudes and behaviors of 1,052 real individuals--applying large language models to qualitative interviews about their lives, then measuring how well these agents replicate the attitudes and behaviors of the individuals that they represent. The generative agents replicate participants' responses on the General Social Survey 85% as accurately as participants replicate their own answers two weeks later, and perform comparably in predicting personality traits and outcomes in experimental replications. Our architecture reduces accuracy biases across racial and ideological groups compared to agents given demographic descriptions. This work provides a foundation for new tools that can help investigate individual and collective behavior.",
    "authors": [
      "Joon Sung Park",
      "Carolyn Q. Zou",
      "Aaron Shaw",
      "Benjamin Mako Hill",
      "Carrie Cai",
      "Meredith Ringel Morris",
      "Robb Willer",
      "Percy Liang",
      "Michael S. Bernstein"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-11-15T11:14:34Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10109v1"
  },
  {
    "arxiv_id": "2411.09955v2",
    "entry_id": "http://arxiv.org/abs/2411.09955v2",
    "title": "Instruction-Guided Editing Controls for Images and Multimedia: A Survey in LLM era",
    "summary": "The rapid advancement of large language models (LLMs) and multimodal learning has transformed digital content creation and manipulation. Traditional visual editing tools require significant expertise, limiting accessibility. Recent strides in instruction-based editing have enabled intuitive interaction with visual content, using natural language as a bridge between user intent and complex editing operations. This survey provides an overview of these techniques, focusing on how LLMs and multimodal models empower users to achieve precise visual modifications without deep technical knowledge. By synthesizing over 100 publications, we explore methods from generative adversarial networks to diffusion models, examining multimodal integration for fine-grained content control. We discuss practical applications across domains such as fashion, 3D scene manipulation, and video synthesis, highlighting increased accessibility and alignment with human intuition. Our survey compares existing literature, emphasizing LLM-empowered editing, and identifies key challenges to stimulate further research. We aim to democratize powerful visual editing across various industries, from entertainment to education. Interested readers are encouraged to access our repository at https://github.com/tamlhp/awesome-instruction-editing.",
    "authors": [
      "Thanh Tam Nguyen",
      "Zhao Ren",
      "Trinh Pham",
      "Thanh Trung Huynh",
      "Phi Le Nguyen",
      "Hongzhi Yin",
      "Quoc Viet Hung Nguyen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2024-11-15T05:18:15Z",
    "pdf_url": "https://arxiv.org/pdf/2411.09955v2"
  },
  {
    "arxiv_id": "2411.13577v2",
    "entry_id": "http://arxiv.org/abs/2411.13577v2",
    "title": "WavChat: A Survey of Spoken Dialogue Models",
    "summary": "Recent advancements in spoken dialogue models, exemplified by systems like GPT-4o, have captured significant attention in the speech domain. Compared to traditional three-tier cascaded spoken dialogue models that comprise speech recognition (ASR), large language models (LLMs), and text-to-speech (TTS), modern spoken dialogue models exhibit greater intelligence. These advanced spoken dialogue models not only comprehend audio, music, and other speech-related features, but also capture stylistic and timbral characteristics in speech. Moreover, they generate high-quality, multi-turn speech responses with low latency, enabling real-time interaction through simultaneous listening and speaking capability. Despite the progress in spoken dialogue systems, there is a lack of comprehensive surveys that systematically organize and analyze these systems and the underlying technologies. To address this, we have first compiled existing spoken dialogue systems in the chronological order and categorized them into the cascaded and end-to-end paradigms. We then provide an in-depth overview of the core technologies in spoken dialogue models, covering aspects such as speech representation, training paradigm, streaming, duplex, and interaction capabilities. Each section discusses the limitations of these technologies and outlines considerations for future research. Additionally, we present a thorough review of relevant datasets, evaluation metrics, and benchmarks from the perspectives of training and evaluating spoken dialogue systems. We hope this survey will contribute to advancing both academic research and industrial applications in the field of spoken dialogue systems. The related material is available at https://github.com/jishengpeng/WavChat.",
    "authors": [
      "Shengpeng Ji",
      "Yifu Chen",
      "Minghui Fang",
      "Jialong Zuo",
      "Jingyu Lu",
      "Hanting Wang",
      "Ziyue Jiang",
      "Long Zhou",
      "Shujie Liu",
      "Xize Cheng",
      "Xiaoda Yang",
      "Zehan Wang",
      "Qian Yang",
      "Jian Li",
      "Yidi Jiang",
      "Jingzhen He",
      "Yunfei Chu",
      "Jin Xu",
      "Zhou Zhao"
    ],
    "categories": [
      "eess.AS",
      "cs.CL",
      "cs.LG",
      "cs.MM",
      "cs.SD"
    ],
    "published": "2024-11-15T04:16:45Z",
    "pdf_url": "https://arxiv.org/pdf/2411.13577v2"
  },
  {
    "arxiv_id": "2411.09580v1",
    "entry_id": "http://arxiv.org/abs/2411.09580v1",
    "title": "Software Performance Engineering for Foundation Model-Powered Software (FMware)",
    "summary": "The rise of Foundation Models (FMs) like Large Language Models (LLMs) is revolutionizing software development. Despite the impressive prototypes, transforming FMware into production-ready products demands complex engineering across various domains. A critical but overlooked aspect is performance engineering, which aims at ensuring FMware meets performance goals such as throughput and latency to avoid user dissatisfaction and financial loss. Often, performance considerations are an afterthought, leading to costly optimization efforts post-deployment. FMware's high computational resource demands highlight the need for efficient hardware use. Continuous performance engineering is essential to prevent degradation. This paper highlights the significance of Software Performance Engineering (SPE) in FMware, identifying four key challenges: cognitive architecture design, communication protocols, tuning and optimization, and deployment. These challenges are based on literature surveys and experiences from developing an in-house FMware system. We discuss problems, current practices, and innovative paths for the software engineering community.",
    "authors": [
      "Haoxiang Zhang",
      "Shi Chang",
      "Arthur Leung",
      "Kishanthan Thangarajah",
      "Boyuan Chen",
      "Hanan Lutfiyya",
      "Ahmed E. Hassan"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-11-14T16:42:19Z",
    "pdf_url": "https://arxiv.org/pdf/2411.09580v1"
  },
  {
    "arxiv_id": "2411.09539v2",
    "entry_id": "http://arxiv.org/abs/2411.09539v2",
    "title": "Fine-tuning Large Language Models with Limited Data: A Survey and Practical Guide",
    "summary": "Fine-tuning large language models (LLMs) with limited data poses a practical challenge in low-resource languages, specialized domains, and constrained deployment settings. While pre-trained LLMs provide strong foundations, effective adaptation under data scarcity requires focused and efficient fine-tuning techniques. This paper presents a structured and practical survey of recent methods for fine-tuning LLMs in data-scarce scenarios. We systematically review parameter-efficient fine-tuning techniques that lower training and deployment costs, domain and cross-lingual adaptation methods for both encoder and decoder models, and model specialization strategies. We further examine preference alignment approaches that guide model behavior using limited human or synthetic feedback, emphasizing sample and compute efficiency. Throughout, we highlight empirical trade-offs, selection criteria, and best practices for choosing suitable techniques based on task constraints, including model scaling, data scaling, and the mitigation of catastrophic forgetting. The aim is to equip researchers and practitioners with actionable insights for effectively fine-tuning LLMs when data and resources are limited.",
    "authors": [
      "Marton Szep",
      "Daniel Rueckert",
      "Rüdiger von Eisenhart-Rothe",
      "Florian Hinterwimmer"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-14T15:55:37Z",
    "pdf_url": "https://arxiv.org/pdf/2411.09539v2"
  },
  {
    "arxiv_id": "2411.09523v1",
    "entry_id": "http://arxiv.org/abs/2411.09523v1",
    "title": "Navigating the Risks: A Survey of Security, Privacy, and Ethics Threats in LLM-Based Agents",
    "summary": "With the continuous development of large language models (LLMs), transformer-based models have made groundbreaking advances in numerous natural language processing (NLP) tasks, leading to the emergence of a series of agents that use LLMs as their control hub. While LLMs have achieved success in various tasks, they face numerous security and privacy threats, which become even more severe in the agent scenarios. To enhance the reliability of LLM-based applications, a range of research has emerged to assess and mitigate these risks from different perspectives.\n  To help researchers gain a comprehensive understanding of various risks, this survey collects and analyzes the different threats faced by these agents. To address the challenges posed by previous taxonomies in handling cross-module and cross-stage threats, we propose a novel taxonomy framework based on the sources and impacts. Additionally, we identify six key features of LLM-based agents, based on which we summarize the current research progress and analyze their limitations. Subsequently, we select four representative agents as case studies to analyze the risks they may face in practical use. Finally, based on the aforementioned analyses, we propose future research directions from the perspectives of data, methodology, and policy, respectively.",
    "authors": [
      "Yuyou Gan",
      "Yong Yang",
      "Zhe Ma",
      "Ping He",
      "Rui Zeng",
      "Yiming Wang",
      "Qingming Li",
      "Chunyi Zhou",
      "Songze Li",
      "Ting Wang",
      "Yunjun Gao",
      "Yingcai Wu",
      "Shouling Ji"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-11-14T15:40:04Z",
    "pdf_url": "https://arxiv.org/pdf/2411.09523v1"
  },
  {
    "arxiv_id": "2411.09168v2",
    "entry_id": "http://arxiv.org/abs/2411.09168v2",
    "title": "An AI Theory of Mind Will Enhance Our Collective Intelligence",
    "summary": "Collective intelligence plays a central role in many fields, from economics and evolutionary theory to neural networks and eusocial insects, and is also core to work on emergence and self-organisation in complex-systems theory. However, in human collective intelligence there is still much to understand about how specific psychological processes at the individual level give rise to self-organised structures at the social level. Psychological factors have so far played a minor role in collective-intelligence studies because the principles are often general and applicable to agents without sophisticated psychologies. We emphasise, with examples from other complex adaptive systems, the broad applicability of collective-intelligence principles, while noting that mechanisms and time scales differ markedly between cases. We review evidence that flexible collective intelligence in human social settings is improved by a particular cognitive tool: our Theory of Mind. We then hypothesise that AIs equipped with a theory of mind will enhance collective intelligence in ways similar to human contributions. To make this case, we step back from the algorithmic basis of AI psychology and consider the large-scale impact AI can have as agential actors in a 'social ecology' rather than as mere technological tools. We identify several key characteristics of psychologically mediated collective intelligence and show that the development of a Theory of Mind is crucial in distinguishing human social collective intelligence from more general forms. Finally, we illustrate how individuals, human or otherwise, integrate within a collective not by being genetically or algorithmically programmed, but by growing and adapting into the socio-cognitive niche they occupy. AI can likewise inhabit one or multiple such niches, facilitated by a Theory of Mind.",
    "authors": [
      "Michael S. Harré",
      "Catherine Drysdale",
      "Jaime Ruiz-Serra"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY",
      "cs.GT",
      "nlin.AO"
    ],
    "published": "2024-11-14T03:58:50Z",
    "pdf_url": "https://arxiv.org/pdf/2411.09168v2"
  },
  {
    "arxiv_id": "2411.09050v1",
    "entry_id": "http://arxiv.org/abs/2411.09050v1",
    "title": "The Systems Engineering Approach in Times of Large Language Models",
    "summary": "Using Large Language Models (LLMs) to address critical societal problems requires adopting this novel technology into socio-technical systems. However, the complexity of such systems and the nature of LLMs challenge such a vision. It is unlikely that the solution to such challenges will come from the Artificial Intelligence (AI) community itself. Instead, the Systems Engineering approach is better equipped to facilitate the adoption of LLMs by prioritising the problems and their context before any other aspects. This paper introduces the challenges LLMs generate and surveys systems research efforts for engineering AI-based systems. We reveal how the systems engineering principles have supported addressing similar issues to the ones LLMs pose and discuss our findings to provide future directions for adopting LLMs.",
    "authors": [
      "Christian Cabrera",
      "Viviana Bastidas",
      "Jennifer Schooling",
      "Neil D. Lawrence"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.SE"
    ],
    "published": "2024-11-13T22:10:07Z",
    "pdf_url": "https://arxiv.org/pdf/2411.09050v1"
  },
  {
    "arxiv_id": "2411.07845v1",
    "entry_id": "http://arxiv.org/abs/2411.07845v1",
    "title": "Ethical Concern Identification in NLP: A Corpus of ACL Anthology Ethics Statements",
    "summary": "What ethical concerns, if any, do LLM researchers have? We introduce EthiCon, a corpus of 1,580 ethical concern statements extracted from scientific papers published in the ACL Anthology. We extract ethical concern keywords from the statements and show promising results in automating the concern identification process. Through a survey, we compare the ethical concerns of the corpus to the concerns listed by the general public and professionals in the field. Finally, we compare our retrieved ethical concerns with existing taxonomies pointing to gaps and future research directions.",
    "authors": [
      "Antonia Karamolegkou",
      "Sandrine Schiller Hansen",
      "Ariadni Christopoulou",
      "Filippos Stamatiou",
      "Anne Lauscher",
      "Anders Søgaard"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-11-12T14:53:12Z",
    "pdf_url": "https://arxiv.org/pdf/2411.07845v1"
  },
  {
    "arxiv_id": "2411.07690v1",
    "entry_id": "http://arxiv.org/abs/2411.07690v1",
    "title": "World Models: The Safety Perspective",
    "summary": "With the proliferation of the Large Language Model (LLM), the concept of World Models (WM) has recently attracted a great deal of attention in the AI research community, especially in the context of AI agents. It is arguably evolving into an essential foundation for building AI agent systems. A WM is intended to help the agent predict the future evolution of environmental states or help the agent fill in missing information so that it can plan its actions and behave safely. The safety property of WM plays a key role in their effective use in critical applications. In this work, we review and analyze the impacts of the current state-of-the-art in WM technology from the point of view of trustworthiness and safety based on a comprehensive survey and the fields of application envisaged. We provide an in-depth analysis of state-of-the-art WMs and derive technical research challenges and their impact in order to call on the research community to collaborate on improving the safety and trustworthiness of WM.",
    "authors": [
      "Zifan Zeng",
      "Chongzhe Zhang",
      "Feng Liu",
      "Joseph Sifakis",
      "Qunli Zhang",
      "Shiming Liu",
      "Peng Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-11-12T10:15:11Z",
    "pdf_url": "https://arxiv.org/pdf/2411.07690v1"
  },
  {
    "arxiv_id": "2411.07586v1",
    "entry_id": "http://arxiv.org/abs/2411.07586v1",
    "title": "A Comprehensive Survey of AI-Driven Advancements and Techniques in Automated Program Repair and Code Generation",
    "summary": "Bug fixing and code generation have been core research topics in software development for many years. The recent explosive growth in Large Language Models has completely transformed these spaces, putting in reach incredibly powerful tools for both. In this survey, 27 recent papers have been reviewed and split into two groups: one dedicated to Automated Program Repair (APR) and LLM integration and the other to code generation using LLMs. The first group consists of new methods for bug detection and repair, which include locating semantic errors, security vulnerabilities, and runtime failure bugs. The place of LLMs in reducing manual debugging efforts is emphasized in this work by APR toward context-aware fixes, with innovations that boost accuracy and efficiency in automatic debugging. The second group dwells on code generation, providing an overview of both general-purpose LLMs fine-tuned for programming and task-specific models. It also presents methods to improve code generation, such as identifier-aware training, fine-tuning at the instruction level, and incorporating semantic code structures. This survey work contrasts the methodologies in APR and code generation to identify trends such as using LLMs, feedback loops to enable iterative code improvement and open-source models. It also discusses the challenges of achieving functional correctness and security and outlines future directions for research in LLM-based software development.",
    "authors": [
      "Avinash Anand",
      "Akshit Gupta",
      "Nishchay Yadav",
      "Shaurya Bajaj"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-11-12T06:47:54Z",
    "pdf_url": "https://arxiv.org/pdf/2411.07586v1"
  },
  {
    "arxiv_id": "2411.07398v1",
    "entry_id": "http://arxiv.org/abs/2411.07398v1",
    "title": "Beyond Keywords: A Context-based Hybrid Approach to Mining Ethical Concern-related App Reviews",
    "summary": "With the increasing proliferation of mobile applications in our everyday experiences, the concerns surrounding ethics have surged significantly. Users generally communicate their feedback, report issues, and suggest new functionalities in application (app) reviews, frequently emphasizing safety, privacy, and accountability concerns. Incorporating these reviews is essential to developing successful products. However, app reviews related to ethical concerns generally use domain-specific language and are expressed using a more varied vocabulary. Thus making automated ethical concern-related app review extraction a challenging and time-consuming effort.\n  This study proposes a novel Natural Language Processing (NLP) based approach that combines Natural Language Inference (NLI), which provides a deep comprehension of language nuances, and a decoder-only (LLaMA-like) Large Language Model (LLM) to extract ethical concern-related app reviews at scale. Utilizing 43,647 app reviews from the mental health domain, the proposed methodology 1) Evaluates four NLI models to extract potential privacy reviews and compares the results of domain-specific privacy hypotheses with generic privacy hypotheses; 2) Evaluates four LLMs for classifying app reviews to privacy concerns; and 3) Uses the best NLI and LLM models further to extract new privacy reviews from the dataset. Results show that the DeBERTa-v3-base-mnli-fever-anli NLI model with domain-specific hypotheses yields the best performance, and Llama3.1-8B-Instruct LLM performs best in the classification of app reviews. Then, using NLI+LLM, an additional 1,008 new privacy-related reviews were extracted that were not identified through the keyword-based approach in previous research, thus demonstrating the effectiveness of the proposed approach.",
    "authors": [
      "Aakash Sorathiya",
      "Gouri Ginde"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-11-11T22:08:48Z",
    "pdf_url": "https://arxiv.org/pdf/2411.07398v1"
  },
  {
    "arxiv_id": "2411.10478v2",
    "entry_id": "http://arxiv.org/abs/2411.10478v2",
    "title": "Large Language Models for Constructing and Optimizing Machine Learning Workflows: A Survey",
    "summary": "Building effective machine learning (ML) workflows to address complex tasks is a primary focus of the Automatic ML (AutoML) community and a critical step toward achieving artificial general intelligence (AGI). Recently, the integration of Large Language Models (LLMs) into ML workflows has shown great potential for automating and enhancing various stages of the ML pipeline. This survey provides a comprehensive and up-to-date review of recent advancements in using LLMs to construct and optimize ML workflows, focusing on key components encompassing data and feature engineering, model selection and hyperparameter optimization, and workflow evaluation. We discuss both the advantages and limitations of LLM-driven approaches, emphasizing their capacity to streamline and enhance ML workflow modeling process through language understanding, reasoning, interaction, and generation. Finally, we highlight open challenges and propose future research directions to advance the effective application of LLMs in ML workflows.",
    "authors": [
      "Yang Gu",
      "Hengyu You",
      "Jian Cao",
      "Muran Yu",
      "Haoran Fan",
      "Shiyou Qian"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-11-11T21:54:26Z",
    "pdf_url": "https://arxiv.org/pdf/2411.10478v2"
  },
  {
    "arxiv_id": "2411.07127v2",
    "entry_id": "http://arxiv.org/abs/2411.07127v2",
    "title": "Benchmarking LLMs' Judgments with No Gold Standard",
    "summary": "We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.\n  GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.\n  We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.",
    "authors": [
      "Shengwei Xu",
      "Yuxuan Lu",
      "Grant Schoenebeck",
      "Yuqing Kong"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-11-11T16:58:36Z",
    "pdf_url": "https://arxiv.org/pdf/2411.07127v2"
  },
  {
    "arxiv_id": "2411.06284v2",
    "entry_id": "http://arxiv.org/abs/2411.06284v2",
    "title": "A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks",
    "summary": "This survey and application guide to multimodal large language models(MLLMs) explores the rapidly developing field of MLLMs, examining their architectures, applications, and impact on AI and Generative Models. Starting with foundational concepts, we delve into how MLLMs integrate various data types, including text, images, video and audio, to enable complex AI systems for cross-modal understanding and generation. It covers essential topics such as training methods, architectural components, and practical applications in various fields, from visual storytelling to enhanced accessibility. Through detailed case studies and technical analysis, the text examines prominent MLLM implementations while addressing key challenges in scalability, robustness, and cross-modal learning. Concluding with a discussion of ethical considerations, responsible AI development, and future directions, this authoritative resource provides both theoretical frameworks and practical insights. It offers a balanced perspective on the opportunities and challenges in the development and deployment of MLLMs, and is highly valuable for researchers, practitioners, and students interested in the intersection of natural language processing and computer vision.",
    "authors": [
      "Chia Xin Liang",
      "Pu Tian",
      "Caitlyn Heqi Yin",
      "Yao Yua",
      "Wei An-Hou",
      "Li Ming",
      "Tianyang Wang",
      "Ziqian Bi",
      "Ming Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-11-09T20:56:23Z",
    "pdf_url": "https://arxiv.org/pdf/2411.06284v2"
  },
  {
    "arxiv_id": "2411.07267v1",
    "entry_id": "http://arxiv.org/abs/2411.07267v1",
    "title": "A Survey on Data Markets",
    "summary": "Data is the new oil of the 21st century. The growing trend of trading data for greater welfare has led to the emergence of data markets. A data market is any mechanism whereby the exchange of data products including datasets and data derivatives takes place as a result of data buyers and data sellers being in contact with one another, either directly or through mediating agents. It serves as a coordinating mechanism by which several functions, including the pricing and the distribution of data as the most important ones, interact to make the value of data fully exploited and enhanced. In this article, we present a comprehensive survey of this important and emerging direction from the aspects of data search, data productization, data transaction, data pricing, revenue allocation as well as privacy, security, and trust issues. We also investigate the government policies and industry status of data markets across different countries and different domains. Finally, we identify the unresolved challenges and discuss possible future directions for the development of data markets.",
    "authors": [
      "Jiayao Zhang",
      "Yuran Bi",
      "Mengye Cheng",
      "Jinfei Liu",
      "Kui Ren",
      "Qiheng Sun",
      "Yihang Wu",
      "Yang Cao",
      "Raul Castro Fernandez",
      "Haifeng Xu",
      "Ruoxi Jia",
      "Yongchan Kwon",
      "Jian Pei",
      "Jiachen T. Wang",
      "Haocheng Xia",
      "Li Xiong",
      "Xiaohui Yu",
      "James Zou"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DB"
    ],
    "published": "2024-11-09T15:09:24Z",
    "pdf_url": "https://arxiv.org/pdf/2411.07267v1"
  },
  {
    "arxiv_id": "2411.05232v1",
    "entry_id": "http://arxiv.org/abs/2411.05232v1",
    "title": "Abstract2Appendix: Academic Reviews Enhance LLM Long-Context Capabilities",
    "summary": "Large language models (LLMs) have shown remarkable performance across various tasks, yet their ability to handle long-context reading remains challenging. This study explores the effectiveness of leveraging high-quality academic peer review data for fine-tuning LLMs to enhance their long-context capabilities. We compare the Direct Preference Optimization (DPO) method with the Supervised Fine-Tuning (SFT) method, demonstrating DPO's superiority and data efficiency. Our experiments show that the fine-tuned model achieves a 4.04-point improvement over phi-3 and a 2.6\\% increase on the Qasper benchmark using only 2000 samples. Despite facing limitations in data scale and processing costs, this study underscores the potential of DPO and high-quality data in advancing LLM performance.\n  Additionally, the zero-shot benchmark results indicate that aggregated high-quality human reviews are overwhelmingly preferred over LLM-generated responses, even for the most capable models like GPT-4o. This suggests that high-quality human reviews are extremely rich in information, reasoning, and long-context retrieval, capabilities that even the most advanced models have not fully captured. These findings highlight the high utility of leveraging human reviews to further advance the field.",
    "authors": [
      "Shengzhi Li",
      "Kittipat Kampa",
      "Rongyu Lin",
      "Bohang Li",
      "Shichao Pei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-07T22:57:02Z",
    "pdf_url": "https://arxiv.org/pdf/2411.05232v1"
  },
  {
    "arxiv_id": "2411.04890v2",
    "entry_id": "http://arxiv.org/abs/2411.04890v2",
    "title": "GUI Agents with Foundation Models: A Comprehensive Survey",
    "summary": "Recent advances in foundation models, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs), have facilitated the development of intelligent agents capable of performing complex tasks. By leveraging the ability of (M)LLMs to process and interpret Graphical User Interfaces (GUIs), these agents can autonomously execute user instructions, simulating human-like interactions such as clicking and typing. This survey consolidates recent research on (M)LLM-based GUI agents, highlighting key innovations in data resources, frameworks, and applications. We begin by reviewing representative datasets and benchmarks, followed by an overview of a generalized, unified framework that encapsulates the essential components of prior studies, supported by a detailed taxonomy. Additionally, we explore relevant commercial applications. Drawing insights from existing work, we identify key challenges and propose future research directions. We hope this survey will inspire further advancements in the field of (M)LLM-based GUI agents.",
    "authors": [
      "Shuai Wang",
      "Weiwen Liu",
      "Jingxuan Chen",
      "Yuqi Zhou",
      "Weinan Gan",
      "Xingshan Zeng",
      "Yuhan Che",
      "Shuai Yu",
      "Xinlong Hao",
      "Kun Shao",
      "Bin Wang",
      "Chuhan Wu",
      "Yasheng Wang",
      "Ruiming Tang",
      "Jianye Hao"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-11-07T17:28:10Z",
    "pdf_url": "https://arxiv.org/pdf/2411.04890v2"
  },
  {
    "arxiv_id": "2411.04832v2",
    "entry_id": "http://arxiv.org/abs/2411.04832v2",
    "title": "Plasticity Loss in Deep Reinforcement Learning: A Survey",
    "summary": "Akin to neuroplasticity in human brains, the plasticity of deep neural networks enables their quick adaption to new data. This makes plasticity particularly crucial for deep Reinforcement Learning (RL) agents: Once plasticity is lost, an agent's performance will inevitably plateau because it cannot improve its policy to account for changes in the data distribution, which are a necessary consequence of its learning process. Thus, developing well-performing and sample-efficient agents hinges on their ability to remain plastic during training. Furthermore, the loss of plasticity can be connected to many other issues plaguing deep RL, such as training instabilities, scaling failures, overestimation bias, and insufficient exploration. With this survey, we aim to provide an overview of the emerging research on plasticity loss for academics and practitioners of deep reinforcement learning. First, we propose a unified definition of plasticity loss based on recent works, relate it to definitions from the literature, and discuss metrics for measuring plasticity loss. Then, we categorize and discuss numerous possible causes of plasticity loss before reviewing currently employed mitigation strategies. Our taxonomy is the first systematic overview of the current state of the field. Lastly, we discuss prevalent issues within the literature, such as a necessity for broader evaluation, and provide recommendations for future research, like gaining a better understanding of an agent's neural activity and behavior.",
    "authors": [
      "Timo Klein",
      "Lukas Miklautz",
      "Kevin Sidak",
      "Claudia Plant",
      "Sebastian Tschiatschek"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-07T16:13:54Z",
    "pdf_url": "https://arxiv.org/pdf/2411.04832v2"
  },
  {
    "arxiv_id": "2411.05051v1",
    "entry_id": "http://arxiv.org/abs/2411.05051v1",
    "title": "Intellectual Property Protection for Deep Learning Model and Dataset Intelligence",
    "summary": "With the growing applications of Deep Learning (DL), especially recent spectacular achievements of Large Language Models (LLMs) such as ChatGPT and LLaMA, the commercial significance of these remarkable models has soared. However, acquiring well-trained models is costly and resource-intensive. It requires a considerable high-quality dataset, substantial investment in dedicated architecture design, expensive computational resources, and efforts to develop technical expertise. Consequently, safeguarding the Intellectual Property (IP) of well-trained models is attracting increasing attention. In contrast to existing surveys overwhelmingly focusing on model IPP mainly, this survey not only encompasses the protection on model level intelligence but also valuable dataset intelligence. Firstly, according to the requirements for effective IPP design, this work systematically summarizes the general and scheme-specific performance evaluation metrics. Secondly, from proactive IP infringement prevention and reactive IP ownership verification perspectives, it comprehensively investigates and analyzes the existing IPP methods for both dataset and model intelligence. Additionally, from the standpoint of training settings, it delves into the unique challenges that distributed settings pose to IPP compared to centralized settings. Furthermore, this work examines various attacks faced by deep IPP techniques. Finally, we outline prospects for promising future directions that may act as a guide for innovative research.",
    "authors": [
      "Yongqi Jiang",
      "Yansong Gao",
      "Chunyi Zhou",
      "Hongsheng Hu",
      "Anmin Fu",
      "Willy Susilo"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-07T09:02:41Z",
    "pdf_url": "https://arxiv.org/pdf/2411.05051v1"
  },
  {
    "arxiv_id": "2411.03964v1",
    "entry_id": "http://arxiv.org/abs/2411.03964v1",
    "title": "What Really is Commonsense Knowledge?",
    "summary": "Commonsense datasets have been well developed in Natural Language Processing, mainly through crowdsource human annotation. However, there are debates on the genuineness of commonsense reasoning benchmarks. In specific, a significant portion of instances in some commonsense benchmarks do not concern commonsense knowledge. That problem would undermine the measurement of the true commonsense reasoning ability of evaluated models. It is also suggested that the problem originated from a blurry concept of commonsense knowledge, as distinguished from other types of knowledge. To demystify all of the above claims, in this study, we survey existing definitions of commonsense knowledge, ground into the three frameworks for defining concepts, and consolidate them into a multi-framework unified definition of commonsense knowledge (so-called consolidated definition). We then use the consolidated definition for annotations and experiments on the CommonsenseQA and CommonsenseQA 2.0 datasets to examine the above claims. Our study shows that there exists a large portion of non-commonsense-knowledge instances in the two datasets, and a large performance gap on these two subsets where Large Language Models (LLMs) perform worse on commonsense-knowledge instances.",
    "authors": [
      "Quyet V. Do",
      "Junze Li",
      "Tung-Duong Vuong",
      "Zhaowei Wang",
      "Yangqiu Song",
      "Xiaojuan Ma"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-06T14:54:19Z",
    "pdf_url": "https://arxiv.org/pdf/2411.03964v1"
  },
  {
    "arxiv_id": "2411.03743v1",
    "entry_id": "http://arxiv.org/abs/2411.03743v1",
    "title": "Automating Exploratory Proteomics Research via Language Models",
    "summary": "With the development of artificial intelligence, its contribution to science is evolving from simulating a complex problem to automating entire research processes and producing novel discoveries. Achieving this advancement requires both specialized general models grounded in real-world scientific data and iterative, exploratory frameworks that mirror human scientific methodologies. In this paper, we present PROTEUS, a fully automated system for scientific discovery from raw proteomics data. PROTEUS uses large language models (LLMs) to perform hierarchical planning, execute specialized bioinformatics tools, and iteratively refine analysis workflows to generate high-quality scientific hypotheses. The system takes proteomics datasets as input and produces a comprehensive set of research objectives, analysis results, and novel biological hypotheses without human intervention. We evaluated PROTEUS on 12 proteomics datasets collected from various biological samples (e.g. immune cells, tumors) and different sample types (single-cell and bulk), generating 191 scientific hypotheses. These were assessed using both automatic LLM-based scoring on 5 metrics and detailed reviews from human experts. Results demonstrate that PROTEUS consistently produces reliable, logically coherent results that align well with existing literature while also proposing novel, evaluable hypotheses. The system's flexible architecture facilitates seamless integration of diverse analysis tools and adaptation to different proteomics data types. By automating complex proteomics analysis workflows and hypothesis generation, PROTEUS has the potential to considerably accelerate the pace of scientific discovery in proteomics research, enabling researchers to efficiently explore large-scale datasets and uncover biological insights.",
    "authors": [
      "Ning Ding",
      "Shang Qu",
      "Linhai Xie",
      "Yifei Li",
      "Zaoqu Liu",
      "Kaiyan Zhang",
      "Yibai Xiong",
      "Yuxin Zuo",
      "Zhangren Chen",
      "Ermo Hua",
      "Xingtai Lv",
      "Youbang Sun",
      "Yang Li",
      "Dong Li",
      "Fuchu He",
      "Bowen Zhou"
    ],
    "categories": [
      "cs.AI",
      "q-bio.QM"
    ],
    "published": "2024-11-06T08:16:56Z",
    "pdf_url": "https://arxiv.org/pdf/2411.03743v1"
  },
  {
    "arxiv_id": "2411.02973v1",
    "entry_id": "http://arxiv.org/abs/2411.02973v1",
    "title": "[Vision Paper] PRObot: Enhancing Patient-Reported Outcome Measures for Diabetic Retinopathy using Chatbots and Generative AI",
    "summary": "We present an outline of the first large language model (LLM) based chatbot application in the context of patient-reported outcome measures (PROMs) for diabetic retinopathy. By utilizing the capabilities of current LLMs, we enable patients to provide feedback about their quality of life and treatment progress via an interactive application. The proposed framework offers significant advantages over the current approach, which encompasses only qualitative collection of survey data or a static survey with limited answer options. Using the PROBot LLM-PROM application, patients will be asked tailored questions about their individual challenges, and can give more detailed feedback on the progress of their treatment. Based on this input, we will use machine learning to infer conventional PROM scores, which can be used by clinicians to evaluate the treatment status. The goal of the application is to improve adherence to the healthcare system and treatments, and thus ultimately reduce cases of subsequent vision impairment. The approach needs to be further validated using a survey and a clinical study.",
    "authors": [
      "Maren Pielka",
      "Tobias Schneider",
      "Jan Terheyden",
      "Rafet Sifa"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-05T10:18:53Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02973v1"
  },
  {
    "arxiv_id": "2411.02654v1",
    "entry_id": "http://arxiv.org/abs/2411.02654v1",
    "title": "Fair and Welfare-Efficient Constrained Multi-matchings under Uncertainty",
    "summary": "We study fair allocation of constrained resources, where a market designer optimizes overall welfare while maintaining group fairness. In many large-scale settings, utilities are not known in advance, but are instead observed after realizing the allocation. We therefore estimate agent utilities using machine learning. Optimizing over estimates requires trading-off between mean utilities and their predictive variances. We discuss these trade-offs under two paradigms for preference modeling -- in the stochastic optimization regime, the market designer has access to a probability distribution over utilities, and in the robust optimization regime they have access to an uncertainty set containing the true utilities with high probability. We discuss utilitarian and egalitarian welfare objectives, and we explore how to optimize for them under stochastic and robust paradigms. We demonstrate the efficacy of our approaches on three publicly available conference reviewer assignment datasets. The approaches presented enable scalable constrained resource allocation under uncertainty for many combinations of objectives and preference models.",
    "authors": [
      "Elita Lobo",
      "Justin Payan",
      "Cyrus Cousins",
      "Yair Zick"
    ],
    "categories": [
      "cs.GT",
      "cs.LG"
    ],
    "published": "2024-11-04T22:42:34Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02654v1"
  },
  {
    "arxiv_id": "2411.02645v1",
    "entry_id": "http://arxiv.org/abs/2411.02645v1",
    "title": "Fine Grained Insider Risk Detection",
    "summary": "We present a method to detect departures from business-justified workflows among support agents. Our goal is to assist auditors in identifying agent actions that cannot be explained by the activity within their surrounding context, where normal activity patterns are established from historical data. We apply our method to help audit millions of actions of over three thousand support agents.\n  We collect logs from the tools used by support agents and construct a bipartite graph of Actions and Entities representing all the actions of the agents, as well as background information about entities. From this graph, we sample subgraphs rooted on security-significant actions taken by the agents. Each subgraph captures the relevant context of the root action in terms of other actions, entities and their relationships. We then prioritize the rooted-subgraphs for auditor review using feed-forward and graph neural networks, as well as nearest neighbors techniques. To alleviate the issue of scarce labeling data, we use contrastive learning and domain-specific data augmentations.\n  Expert auditors label the top ranked subgraphs as ``worth auditing\" or ``not worth auditing\" based on the company's business policies. This system finds subgraphs that are worth auditing with high enough precision to be used in production.",
    "authors": [
      "Birkett Huber",
      "Casper Neo",
      "Keiran Sampson",
      "Alex Kantchelian",
      "Brett Ksobiech",
      "Yanis Pavlidis"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-11-04T22:07:38Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02645v1"
  },
  {
    "arxiv_id": "2411.02540v3",
    "entry_id": "http://arxiv.org/abs/2411.02540v3",
    "title": "GraphXAIN: Narratives to Explain Graph Neural Networks",
    "summary": "Graph Neural Networks (GNNs) are a powerful technique for machine learning on graph-structured data, yet they pose challenges in interpretability. Existing GNN explanation methods usually yield technical outputs, such as subgraphs and feature importance scores, that are difficult for non-data scientists to understand and thereby violate the purpose of explanations. Motivated by recent Explainable AI (XAI) research, we propose GraphXAIN, a method that generates natural language narratives explaining GNN predictions. GraphXAIN is a model- and explainer-agnostic method that uses Large Language Models (LLMs) to translate explanatory subgraphs and feature importance scores into coherent, story-like explanations of GNN decision-making processes. Evaluations on real-world datasets demonstrate GraphXAIN's ability to improve graph explanations. A survey of machine learning researchers and practitioners reveals that GraphXAIN enhances four explainability dimensions: understandability, satisfaction, convincingness, and suitability for communicating model predictions. When combined with another graph explainer method, GraphXAIN further improves trustworthiness, insightfulness, confidence, and usability. Notably, 95% of participants found GraphXAIN to be a valuable addition to the GNN explanation method. By incorporating natural language narratives, our approach serves both graph practitioners and non-expert users by providing clearer and more effective explanations.",
    "authors": [
      "Mateusz Cedro",
      "David Martens"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-11-04T19:21:06Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02540v3"
  },
  {
    "arxiv_id": "2411.02382v1",
    "entry_id": "http://arxiv.org/abs/2411.02382v1",
    "title": "Improving Scientific Hypothesis Generation with Knowledge Grounded Large Language Models",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in various scientific domains, from natural language processing to complex problem-solving tasks. Their ability to understand and generate human-like text has opened up new possibilities for advancing scientific research, enabling tasks such as data analysis, literature review, and even experimental design. One of the most promising applications of LLMs in this context is hypothesis generation, where they can identify novel research directions by analyzing existing knowledge. However, despite their potential, LLMs are prone to generating ``hallucinations'', outputs that are plausible-sounding but factually incorrect. Such a problem presents significant challenges in scientific fields that demand rigorous accuracy and verifiability, potentially leading to erroneous or misleading conclusions. To overcome these challenges, we propose KG-CoI (Knowledge Grounded Chain of Ideas), a novel system that enhances LLM hypothesis generation by integrating external, structured knowledge from knowledge graphs (KGs). KG-CoI guides LLMs through a structured reasoning process, organizing their output as a chain of ideas (CoI), and includes a KG-supported module for the detection of hallucinations. With experiments on our newly constructed hypothesis generation dataset, we demonstrate that KG-CoI not only improves the accuracy of LLM-generated hypotheses but also reduces the hallucination in their reasoning chains, highlighting its effectiveness in advancing real-world scientific research.",
    "authors": [
      "Guangzhi Xiong",
      "Eric Xie",
      "Amir Hassan Shariatmadari",
      "Sikun Guo",
      "Stefan Bekiranov",
      "Aidong Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-04T18:50:00Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02382v1"
  },
  {
    "arxiv_id": "2411.02328v1",
    "entry_id": "http://arxiv.org/abs/2411.02328v1",
    "title": "Disrupting Test Development with AI Assistants",
    "summary": "Recent advancements in large language models, including GPT-4 and its variants, and Generative AI-assisted coding tools like GitHub Copilot, ChatGPT, and Tabnine, have significantly transformed software development. This paper analyzes how these innovations impact productivity and software test development metrics. These tools enable developers to generate complete software programs with minimal human intervention before deployment. However, thorough review and testing by developers are still crucial. Utilizing the Test Pyramid concept, which categorizes tests into unit, integration, and end-to-end tests, we evaluate three popular AI coding assistants by generating and comparing unit tests for opensource modules. Our findings show that AI-generated tests are of equivalent quality to original tests, highlighting differences in usage and results among the tools. This research enhances the understanding and capabilities of AI-assistant tools in automated testing.",
    "authors": [
      "Vijay Joshi",
      "Iver Band"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-11-04T17:52:40Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02328v1"
  },
  {
    "arxiv_id": "2411.02018v2",
    "entry_id": "http://arxiv.org/abs/2411.02018v2",
    "title": "Shortcut Learning in In-Context Learning: A Survey",
    "summary": "Shortcut learning refers to the phenomenon where models employ simple, non-robust decision rules in practical tasks, which hinders their generalization and robustness. With the rapid development of large language models (LLMs) in recent years, an increasing number of studies have shown the impact of shortcut learning on LLMs. This paper provides a novel perspective to review relevant research on shortcut learning in In-Context Learning (ICL). It conducts a detailed exploration of the types of shortcuts in ICL tasks, their causes, available benchmarks, and strategies for mitigating shortcuts. Based on corresponding observations, it summarizes the unresolved issues in existing research and attempts to outline the future research landscape of shortcut learning.",
    "authors": [
      "Rui Song",
      "Yingji Li",
      "Lida Shi",
      "Fausto Giunchiglia",
      "Hao Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-11-04T12:13:04Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02018v2"
  },
  {
    "arxiv_id": "2411.02006v3",
    "entry_id": "http://arxiv.org/abs/2411.02006v3",
    "title": "Foundations and Recent Trends in Multimodal Mobile Agents: A Survey",
    "summary": "Mobile agents are essential for automating tasks in complex and dynamic mobile environments. As foundation models evolve, the demands for agents that can adapt in real-time and process multimodal data have grown. This survey provides a comprehensive review of mobile agent technologies, focusing on recent advancements that enhance real-time adaptability and multimodal interaction. Recent evaluation benchmarks have been developed better to capture the static and interactive environments of mobile tasks, offering more accurate assessments of agents' performance. We then categorize these advancements into two main approaches: prompt-based methods, which utilize large language models (LLMs) for instruction-based task execution, and training-based methods, which fine-tune multimodal models for mobile-specific applications. Additionally, we explore complementary technologies that augment agent performance. By discussing key challenges and outlining future research directions, this survey offers valuable insights for advancing mobile agent technologies. A comprehensive resource list is available at https://github.com/aialt/awesome-mobile-agents",
    "authors": [
      "Biao Wu",
      "Yanda Li",
      "Zhiwei Zhang",
      "Yunchao Wei",
      "Meng Fang",
      "Ling Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-11-04T11:50:58Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02006v3"
  },
  {
    "arxiv_id": "2411.03350v2",
    "entry_id": "http://arxiv.org/abs/2411.03350v2",
    "title": "A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness",
    "summary": "Large language models (LLMs) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs' challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively.",
    "authors": [
      "Fali Wang",
      "Zhiwei Zhang",
      "Xianren Zhang",
      "Zongyu Wu",
      "Tzuhao Mo",
      "Qiuhao Lu",
      "Wanjing Wang",
      "Rui Li",
      "Junjie Xu",
      "Xianfeng Tang",
      "Qi He",
      "Yao Ma",
      "Ming Huang",
      "Suhang Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-04T04:43:01Z",
    "pdf_url": "https://arxiv.org/pdf/2411.03350v2"
  },
  {
    "arxiv_id": "2411.02455v1",
    "entry_id": "http://arxiv.org/abs/2411.02455v1",
    "title": "An Exploration of Higher Education Course Evaluation by Large Language Models",
    "summary": "Course evaluation is a critical component in higher education pedagogy. It not only serves to identify limitations in existing course designs and provide a basis for curricular innovation, but also to offer quantitative insights for university administrative decision-making. Traditional evaluation methods, primarily comprising student surveys, instructor self-assessments, and expert reviews, often encounter challenges, including inherent subjectivity, feedback delays, inefficiencies, and limitations in addressing innovative teaching approaches. Recent advancements in large language models (LLMs) within artificial intelligence (AI) present promising new avenues for enhancing course evaluation processes. This study explores the application of LLMs in automated course evaluation from multiple perspectives and conducts rigorous experiments across 100 courses at a major university in China. The findings indicate that: (1) LLMs can be an effective tool for course evaluation; (2) their effectiveness is contingent upon appropriate fine-tuning and prompt engineering; and (3) LLM-generated evaluation results demonstrate a notable level of rationality and interpretability.",
    "authors": [
      "Bo Yuan",
      "Jiazi Hu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-11-03T20:43:52Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02455v1"
  },
  {
    "arxiv_id": "2411.01431v1",
    "entry_id": "http://arxiv.org/abs/2411.01431v1",
    "title": "Efficient Deep Learning Infrastructures for Embedded Computing Systems: A Comprehensive Survey and Future Envision",
    "summary": "Deep neural networks (DNNs) have recently achieved impressive success across a wide range of real-world vision and language processing tasks, spanning from image classification to many other downstream vision tasks, such as object detection, tracking, and segmentation. However, previous well-established DNNs, despite being able to maintain superior accuracy, have also been evolving to be deeper and wider and thus inevitably necessitate prohibitive computational resources for both training and inference. This trend further enlarges the computational gap between computation-intensive DNNs and resource-constrained embedded computing systems, making it challenging to deploy powerful DNNs upon real-world embedded computing systems towards ubiquitous embedded intelligence. To alleviate the above computational gap and enable ubiquitous embedded intelligence, we, in this survey, focus on discussing recent efficient deep learning infrastructures for embedded computing systems, spanning from training to inference, from manual to automated, from convolutional neural networks to transformers, from transformers to vision transformers, from vision models to large language models, from software to hardware, and from algorithms to applications. Specifically, we discuss recent efficient deep learning infrastructures for embedded computing systems from the lens of (1) efficient manual network design for embedded computing systems, (2) efficient automated network design for embedded computing systems, (3) efficient network compression for embedded computing systems, (4) efficient on-device learning for embedded computing systems, (5) efficient large language models for embedded computing systems, (6) efficient deep learning software and hardware for embedded computing systems, and (7) efficient intelligent applications for embedded computing systems.",
    "authors": [
      "Xiangzhong Luo",
      "Di Liu",
      "Hao Kong",
      "Shuo Huai",
      "Hui Chen",
      "Guochu Xiong",
      "Weichen Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-11-03T03:55:04Z",
    "pdf_url": "https://arxiv.org/pdf/2411.01431v1"
  },
  {
    "arxiv_id": "2411.01344v3",
    "entry_id": "http://arxiv.org/abs/2411.01344v3",
    "title": "Privacy Leakage Overshadowed by Views of AI: A Study on Human Oversight of Privacy in Language Model Agent",
    "summary": "Language model (LM) agents that act on users' behalf for personal tasks (e.g., replying emails) can boost productivity, but are also susceptible to unintended privacy leakage risks. We present the first study on people's capacity to oversee the privacy implications of the LM agents. By conducting a task-based survey ($N=300$), we investigate how people react to and assess the response generated by LM agents for asynchronous interpersonal communication tasks, compared with a response they wrote. We found that people may favor the agent response with more privacy leakage over the response they drafted or consider both good, leading to an increased harmful disclosure from 15.7% to 55.0%. We further identified six privacy behavior patterns reflecting varying concerns, trust levels, and privacy preferences underlying people's oversight of LM agents' actions. Our findings shed light on designing agentic systems that enable privacy-preserving interactions and achieve bidirectional alignment on privacy preferences to help users calibrate trust.",
    "authors": [
      "Zhiping Zhang",
      "Bingcan Guo",
      "Tianshi Li"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2024-11-02T19:15:42Z",
    "pdf_url": "https://arxiv.org/pdf/2411.01344v3"
  },
  {
    "arxiv_id": "2411.00625v3",
    "entry_id": "http://arxiv.org/abs/2411.00625v3",
    "title": "Toward Automated Algorithm Design: A Survey and Practical Guide to Meta-Black-Box-Optimization",
    "summary": "In this survey, we introduce Meta-Black-Box-Optimization~(MetaBBO) as an emerging avenue within the Evolutionary Computation~(EC) community, which incorporates Meta-learning approaches to assist automated algorithm design. Despite the success of MetaBBO, the current literature provides insufficient summaries of its key aspects and lacks practical guidance for implementation. To bridge this gap, we offer a comprehensive review of recent advances in MetaBBO, providing an in-depth examination of its key developments. We begin with a unified definition of the MetaBBO paradigm, followed by a systematic taxonomy of various algorithm design tasks, including algorithm selection, algorithm configuration, solution manipulation, and algorithm generation. Further, we conceptually summarize different learning methodologies behind current MetaBBO works, including reinforcement learning, supervised learning, neuroevolution, and in-context learning with Large Language Models. A comprehensive evaluation of the latest representative MetaBBO methods is then carried out, alongside an experimental analysis of their optimization performance, computational efficiency, and generalization ability. Based on the evaluation results, we meticulously identify a set of core designs that enhance the generalization and learning effectiveness of MetaBBO. Finally, we outline the vision for the field by providing insight into the latest trends and potential future directions. Relevant literature will be continuously collected and updated at https://github.com/MetaEvo/Awesome-MetaBBO.",
    "authors": [
      "Zeyuan Ma",
      "Hongshu Guo",
      "Yue-Jiao Gong",
      "Jun Zhang",
      "Kay Chen Tan"
    ],
    "categories": [
      "cs.NE",
      "cs.LG"
    ],
    "published": "2024-11-01T14:32:19Z",
    "pdf_url": "https://arxiv.org/pdf/2411.00625v3"
  },
  {
    "arxiv_id": "2411.00914v1",
    "entry_id": "http://arxiv.org/abs/2411.00914v1",
    "title": "AAD-LLM: Adaptive Anomaly Detection Using Large Language Models",
    "summary": "For data-constrained, complex and dynamic industrial environments, there is a critical need for transferable and multimodal methodologies to enhance anomaly detection and therefore, prevent costs associated with system failures. Typically, traditional PdM approaches are not transferable or multimodal. This work examines the use of Large Language Models (LLMs) for anomaly detection in complex and dynamic manufacturing systems. The research aims to improve the transferability of anomaly detection models by leveraging Large Language Models (LLMs) and seeks to validate the enhanced effectiveness of the proposed approach in data-sparse industrial applications. The research also seeks to enable more collaborative decision-making between the model and plant operators by allowing for the enriching of input series data with semantics. Additionally, the research aims to address the issue of concept drift in dynamic industrial settings by integrating an adaptability mechanism. The literature review examines the latest developments in LLM time series tasks alongside associated adaptive anomaly detection methods to establish a robust theoretical framework for the proposed architecture. This paper presents a novel model framework (AAD-LLM) that doesn't require any training or finetuning on the dataset it is applied to and is multimodal. Results suggest that anomaly detection can be converted into a \"language\" task to deliver effective, context-aware detection in data-constrained industrial applications. This work, therefore, contributes significantly to advancements in anomaly detection methodologies.",
    "authors": [
      "Alicia Russell-Gilbert",
      "Alexander Sommers",
      "Andrew Thompson",
      "Logan Cummins",
      "Sudip Mittal",
      "Shahram Rahimi",
      "Maria Seale",
      "Joseph Jaboure",
      "Thomas Arnold",
      "Joshua Church"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "published": "2024-11-01T13:43:28Z",
    "pdf_url": "https://arxiv.org/pdf/2411.00914v1"
  },
  {
    "arxiv_id": "2411.00515v2",
    "entry_id": "http://arxiv.org/abs/2411.00515v2",
    "title": "Zero-shot Generalization in Inventory Management: Train, then Estimate and Decide",
    "summary": "Deploying deep reinforcement learning (DRL) in real-world inventory management presents challenges, including dynamic environments and uncertain problem parameters, e.g. demand and lead time distributions. These challenges highlight a research gap, suggesting a need for a unifying framework to model and solve sequential decision-making under parameter uncertainty. We address this by exploring an underexplored area of DRL for inventory management: training generally capable agents (GCAs) under zero-shot generalization (ZSG). Here, GCAs are advanced DRL policies designed to handle a broad range of sampled problem instances with diverse inventory challenges. ZSG refers to the ability to successfully apply learned policies to unseen instances with unknown parameters without retraining.\n  We propose a unifying Super-Markov Decision Process formulation and the Train, then Estimate and Decide (TED) framework to train and deploy a GCA tailored to inventory management applications. The TED framework consists of three phases: training a GCA on varied problem instances, continuously estimating problem parameters during deployment, and making decisions based on these estimates. Applied to periodic review inventory problems with lost sales, cyclic demand patterns, and stochastic lead times, our trained agent, the Generally Capable Lost Sales Network (GC-LSN) consistently outperforms well-known traditional policies when problem parameters are known. Moreover, under conditions where demand and/or lead time distributions are initially unknown and must be estimated, we benchmark against online learning methods that provide worst-case performance guarantees. Our GC-LSN policy, paired with the Kaplan-Meier estimator, is demonstrated to complement these methods by providing superior empirical performance.",
    "authors": [
      "Tarkan Temizöz",
      "Christina Imdahl",
      "Remco Dijkman",
      "Douniel Lamghari-Idrissi",
      "Willem van Jaarsveld"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-11-01T11:20:05Z",
    "pdf_url": "https://arxiv.org/pdf/2411.00515v2"
  },
  {
    "arxiv_id": "2411.00299v2",
    "entry_id": "http://arxiv.org/abs/2411.00299v2",
    "title": "RadFlag: A Black-Box Hallucination Detection Method for Medical Vision Language Models",
    "summary": "Generating accurate radiology reports from medical images is a clinically important but challenging task. While current Vision Language Models (VLMs) show promise, they are prone to generating hallucinations, potentially compromising patient care. We introduce RadFlag, a black-box method to enhance the accuracy of radiology report generation. Our method uses a sampling-based flagging technique to find hallucinatory generations that should be removed. We first sample multiple reports at varying temperatures and then use a Large Language Model (LLM) to identify claims that are not consistently supported across samples, indicating that the model has low confidence in those claims. Using a calibrated threshold, we flag a fraction of these claims as likely hallucinations, which should undergo extra review or be automatically rejected. Our method achieves high precision when identifying both individual hallucinatory sentences and reports that contain hallucinations. As an easy-to-use, black-box system that only requires access to a model's temperature parameter, RadFlag is compatible with a wide range of radiology report generation models and has the potential to broadly improve the quality of automated radiology reporting.",
    "authors": [
      "Serena Zhang",
      "Sraavya Sambara",
      "Oishi Banerjee",
      "Julian Acosta",
      "L. John Fahrner",
      "Pranav Rajpurkar"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-11-01T01:38:42Z",
    "pdf_url": "https://arxiv.org/pdf/2411.00299v2"
  },
  {
    "arxiv_id": "2411.12746v1",
    "entry_id": "http://arxiv.org/abs/2411.12746v1",
    "title": "A Review of Reinforcement Learning in Financial Applications",
    "summary": "In recent years, there has been a growing trend of applying Reinforcement Learning (RL) in financial applications.\n  This approach has shown great potential to solve decision-making tasks in finance.\n  In this survey, we present a comprehensive study of the applications of RL in finance and conduct a series of meta-analyses to investigate the common themes in the literature, such as the factors that most significantly affect RL's performance compared to traditional methods.\n  Moreover, we identify challenges including explainability, Markov Decision Process (MDP) modeling, and robustness that hinder the broader utilization of RL in the financial industry and discuss recent advancements in overcoming these challenges.\n  Finally, we propose future research directions, such as benchmarking, contextual RL, multi-agent RL, and model-based RL to address these challenges and to further enhance the implementation of RL in finance.",
    "authors": [
      "Yahui Bai",
      "Yuhe Gao",
      "Runzhe Wan",
      "Sheng Zhang",
      "Rui Song"
    ],
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-11-01T01:03:10Z",
    "pdf_url": "https://arxiv.org/pdf/2411.12746v1"
  },
  {
    "arxiv_id": "2410.23822v1",
    "entry_id": "http://arxiv.org/abs/2410.23822v1",
    "title": "Parameter-Efficient Fine-Tuning Medical Multimodal Large Language Models for Medical Visual Grounding",
    "summary": "Multimodal Large Language Models (MLLMs) inherit the superior text understanding capabilities of LLMs and extend these capabilities to multimodal scenarios. These models achieve excellent results in the general domain of multimodal tasks. However, in the medical domain, the substantial training costs and the requirement for extensive medical data pose challenges to the development of medical MLLMs. Furthermore, due to the free-text form of answers, tasks such as visual grounding that need to produce output in a prescribed form become difficult for MLLMs. So far, there have been no medical MLLMs works in medical visual grounding area. For the medical vision grounding task, which involves identifying locations in medical images based on short text descriptions, we propose Parameter-efficient Fine-tuning medical multimodal large language models for Medcial Visual Grounding (PFMVG). To validate the performance of the model, we evaluate it on a public benchmark dataset for medical visual grounding, where it achieves competitive results, and significantly outperforming GPT-4v. Our code will be open sourced after peer review.",
    "authors": [
      "Jinlong He",
      "Pengfei Li",
      "Gang Liu",
      "Shenjun Zhong"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-10-31T11:07:26Z",
    "pdf_url": "https://arxiv.org/pdf/2410.23822v1"
  },
  {
    "arxiv_id": "2410.23069v1",
    "entry_id": "http://arxiv.org/abs/2410.23069v1",
    "title": "LLMs Integration in Software Engineering Team Projects: Roles, Impact, and a Pedagogical Design Space for AI Tools in Computing Education",
    "summary": "This work takes a pedagogical lens to explore the implications of generative AI (GenAI) models and tools, such as ChatGPT and GitHub Copilot, in a semester-long 2nd-year undergraduate Software Engineering Team Project. Qualitative findings from survey (39 students) and interviews (eight students) provide insights into the students' views on the impact of GenAI use on their coding experience, learning, and self-efficacy. Our results address a particular gap in understanding the role and implications of GenAI on teamwork, team-efficacy, and team dynamics. The analysis of the learning aspects is distinguished by the application of learning and pedagogy informed lenses to discuss the data. We propose a preliminary design space for GenAI-based programming learning tools highlighting the importance of considering the roles that GenAI can play during the learning process, the varying support-ability patterns that can be applied to each role, and the importance of supporting transparency in GenAI for team members and students in addition to educators.",
    "authors": [
      "Ahmed Kharrufa",
      "Sami Alghamdi",
      "Abeer Aziz",
      "Christopher Bull"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-10-30T14:43:33Z",
    "pdf_url": "https://arxiv.org/pdf/2410.23069v1"
  },
  {
    "arxiv_id": "2411.02530v1",
    "entry_id": "http://arxiv.org/abs/2411.02530v1",
    "title": "A Comprehensive Study on Quantization Techniques for Large Language Models",
    "summary": "Large Language Models (LLMs) have been extensively researched and used in both academia and industry since the rise in popularity of the Transformer model, which demonstrates excellent performance in AI. However, the computational demands of LLMs are immense, and the energy resources required to run them are often limited. For instance, popular models like GPT-3, with 175 billion parameters and a storage requirement of 350 GB, present significant challenges for deployment on resource-constrained IoT devices and embedded systems. These systems often lack the computational capacity to handle such large models. Quantization, a technique that reduces the precision of model values to a smaller set of discrete values, offers a promising solution by reducing the size of LLMs and accelerating inference. In this research, we provide a comprehensive analysis of quantization techniques within the machine learning field, with a particular focus on their application to LLMs. We begin by exploring the mathematical theory of quantization, followed by a review of common quantization methods and how they are implemented. Furthermore, we examine several prominent quantization methods applied to LLMs, detailing their algorithms and performance outcomes.",
    "authors": [
      "Jiedong Lang",
      "Zhehao Guo",
      "Shuyu Huang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-10-30T04:55:26Z",
    "pdf_url": "https://arxiv.org/pdf/2411.02530v1"
  },
  {
    "arxiv_id": "2411.05025v1",
    "entry_id": "http://arxiv.org/abs/2411.05025v1",
    "title": "LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and Perceptions",
    "summary": "The rise of large language models (LLMs) has led many researchers to consider their usage for scientific work. Some have found benefits using LLMs to augment or automate aspects of their research pipeline, while others have urged caution due to risks and ethical concerns. Yet little work has sought to quantify and characterize how researchers use LLMs and why. We present the first large-scale survey of 816 verified research article authors to understand how the research community leverages and perceives LLMs as research tools. We examine participants' self-reported LLM usage, finding that 81% of researchers have already incorporated LLMs into different aspects of their research workflow. We also find that traditionally disadvantaged groups in academia (non-White, junior, and non-native English speaking researchers) report higher LLM usage and perceived benefits, suggesting potential for improved research equity. However, women, non-binary, and senior researchers have greater ethical concerns, potentially hindering adoption.",
    "authors": [
      "Zhehui Liao",
      "Maria Antoniak",
      "Inyoung Cheong",
      "Evie Yu-Yen Cheng",
      "Ai-Heng Lee",
      "Kyle Lo",
      "Joseph Chee Chang",
      "Amy X. Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.DL",
      "cs.HC"
    ],
    "published": "2024-10-30T04:25:23Z",
    "pdf_url": "https://arxiv.org/pdf/2411.05025v1"
  },
  {
    "arxiv_id": "2410.21673v4",
    "entry_id": "http://arxiv.org/abs/2410.21673v4",
    "title": "Knowledge-Guided Prompt Learning for Request Quality Assurance in Public Code Review",
    "summary": "Public Code Review (PCR) is developed in the Software Question Answering (SQA) community, assisting developers in exploring high-quality and efficient review services. Current methods on PCR mainly focus on the reviewer's perspective, including finding a capable reviewer, predicting comment quality, and recommending/generating review comments. However, it is not well studied that how to satisfy the review necessity requests posted by developers which can increase their visibility, which in turn acts as a prerequisite for better review responses. To this end, we propose K nowledge-guided P rompt learning for P ublic Code Review (KP-PCR) to achieve developer-based code review request quality assurance (i.e., predicting request necessity and recommending tags subtask). Specifically, we reformulate the two subtasks via 1) text prompt tuning which converts both of them into a Masked Language Model (MLM) by constructing prompt templates using hard prompt; and 2) knowledge and code prefix tuning which introduces knowledge guidance from fine-tuned large language models by soft prompt, and uses program dependence graph to characterize code snippets. Finally, both of the request necessity prediction and tag recommendation subtasks output predicted results through an answer engineering module. In addition, we further analysis the time complexity of our KP-PCR that has lightweight prefix based the operation of introducing knowledge guidance. Experimental results on the PCR dataset for the period 2011-2023 demonstrate that our KP-PCR outperforms baselines by 2.3%-8.4% in the request necessity prediction and by 1.4%-6.9% in the tag recommendation. The code implementation is released at https://github.com/WUT-IDEA/KP-PCR",
    "authors": [
      "Lin Li",
      "Xinchun Yu",
      "Xinyu Chen",
      "Peng Liang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-10-29T02:48:41Z",
    "pdf_url": "https://arxiv.org/pdf/2410.21673v4"
  },
  {
    "arxiv_id": "2411.00024v3",
    "entry_id": "http://arxiv.org/abs/2411.00024v3",
    "title": "A Perspective for Adapting Generalist AI to Specialized Medical AI Applications and Their Challenges",
    "summary": "The integration of Large Language Models (LLMs) into medical applications has sparked widespread interest across the healthcare industry, from drug discovery and development to clinical decision support, assisting telemedicine, medical devices, and healthcare insurance applications. This perspective paper aims to discuss the inner workings of building LLM-powered medical AI applications and introduces a comprehensive framework for their development. We review existing literature and outline the unique challenges of applying LLMs in specialized medical contexts. Additionally, we introduce a three-step framework to organize medical LLM research activities: 1) Modeling: breaking down complex medical workflows into manageable steps for developing medical-specific models; 2) Optimization: optimizing the model performance with crafted prompts and integrating external knowledge and tools, and 3) System engineering: decomposing complex tasks into subtasks and leveraging human expertise for building medical AI applications. Furthermore, we offer a detailed use case playbook that describes various LLM-powered medical AI applications, such as optimizing clinical trial design, enhancing clinical decision support, and advancing medical imaging analysis. Finally, we discuss various challenges and considerations for building medical AI applications with LLMs, such as handling hallucination issues, data ownership and compliance, privacy, intellectual property considerations, compute cost, sustainability issues, and responsible AI requirements.",
    "authors": [
      "Zifeng Wang",
      "Hanyin Wang",
      "Benjamin Danek",
      "Ying Li",
      "Christina Mack",
      "Hoifung Poon",
      "Yajuan Wang",
      "Pranav Rajpurkar",
      "Jimeng Sun"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-28T22:30:06Z",
    "pdf_url": "https://arxiv.org/pdf/2411.00024v3"
  },
  {
    "arxiv_id": "2410.21490v1",
    "entry_id": "http://arxiv.org/abs/2410.21490v1",
    "title": "Can Large Language Models Act as Symbolic Reasoners?",
    "summary": "The performance of Large language models (LLMs) across a broad range of domains has been impressive but have been critiqued as not being able to reason about their process and conclusions derived. This is to explain the conclusions draw, and also for determining a plan or strategy for their approach. This paper explores the current research in investigating symbolic reasoning and LLMs, and whether an LLM can inherently provide some form of reasoning or whether supporting components are necessary, and, if there is evidence for a reasoning capability, is this evident in a specific domain or is this a general capability? In addition, this paper aims to identify the current research gaps and future trends of LLM explainability, presenting a review of the literature, identifying current research into this topic and suggests areas for future work.",
    "authors": [
      "Rob Sullivan",
      "Nelly Elsayed"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET"
    ],
    "published": "2024-10-28T20:01:50Z",
    "pdf_url": "https://arxiv.org/pdf/2410.21490v1"
  },
  {
    "arxiv_id": "2410.21169v4",
    "entry_id": "http://arxiv.org/abs/2410.21169v4",
    "title": "Document Parsing Unveiled: Techniques, Challenges, and Prospects for Structured Information Extraction",
    "summary": "Document parsing is essential for converting unstructured and semi-structured documents such as contracts, academic papers, and invoices into structured, machine-readable data. Document parsing reliable structured data from unstructured inputs, providing huge convenience for numerous applications. Especially with recent achievements in Large Language Models, document parsing plays an indispensable role in both knowledge base construction and training data generation. This survey presents a comprehensive review of the current state of document parsing, covering key methodologies, from modular pipeline systems to end-to-end models driven by large vision-language models. Core components such as layout detection, content extraction (including text, tables, and mathematical expressions), and multi-modal data integration are examined in detail. Additionally, this paper discusses the challenges faced by modular document parsing systems and vision-language models in handling complex layouts, integrating multiple modules, and recognizing high-density text. It outlines future research directions and emphasizes the importance of developing larger and more diverse datasets.",
    "authors": [
      "Qintong Zhang",
      "Bin Wang",
      "Victor Shea-Jay Huang",
      "Junyuan Zhang",
      "Zhengren Wang",
      "Hao Liang",
      "Conghui He",
      "Wentao Zhang"
    ],
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-10-28T16:11:35Z",
    "pdf_url": "https://arxiv.org/pdf/2410.21169v4"
  },
  {
    "arxiv_id": "2410.21348v3",
    "entry_id": "http://arxiv.org/abs/2410.21348v3",
    "title": "Large Language Model Benchmarks in Medical Tasks",
    "summary": "With the increasing application of large language models (LLMs) in the medical domain, evaluating these models' performance using benchmark datasets has become crucial. This paper presents a comprehensive survey of various benchmark datasets employed in medical LLM tasks. These datasets span multiple modalities including text, image, and multimodal benchmarks, focusing on different aspects of medical knowledge such as electronic health records (EHRs), doctor-patient dialogues, medical question-answering, and medical image captioning. The survey categorizes the datasets by modality, discussing their significance, data structure, and impact on the development of LLMs for clinical tasks such as diagnosis, report generation, and predictive decision support. Key benchmarks include MIMIC-III, MIMIC-IV, BioASQ, PubMedQA, and CheXpert, which have facilitated advancements in tasks like medical report generation, clinical summarization, and synthetic data generation. The paper summarizes the challenges and opportunities in leveraging these benchmarks for advancing multimodal medical intelligence, emphasizing the need for datasets with a greater degree of language diversity, structured omics data, and innovative approaches to synthesis. This work also provides a foundation for future research in the application of LLMs in medicine, contributing to the evolving field of medical artificial intelligence.",
    "authors": [
      "Lawrence K. Q. Yan",
      "Qian Niu",
      "Ming Li",
      "Yichao Zhang",
      "Caitlyn Heqi Yin",
      "Cheng Fei",
      "Benji Peng",
      "Ziqian Bi",
      "Pohsun Feng",
      "Keyu Chen",
      "Tianyang Wang",
      "Yunze Wang",
      "Silin Chen",
      "Ming Liu",
      "Junyu Liu",
      "Xinyuan Song",
      "Riyang Bao",
      "Zekun Jiang",
      "Ziyuan Qin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-28T11:07:33Z",
    "pdf_url": "https://arxiv.org/pdf/2410.21348v3"
  },
  {
    "arxiv_id": "2410.20848v1",
    "entry_id": "http://arxiv.org/abs/2410.20848v1",
    "title": "Deep Insights into Automated Optimization with Large Language Models and Evolutionary Algorithms",
    "summary": "Designing optimization approaches, whether heuristic or meta-heuristic, usually demands extensive manual intervention and has difficulty generalizing across diverse problem domains. The combination of Large Language Models (LLMs) and Evolutionary Algorithms (EAs) offers a promising new approach to overcome these limitations and make optimization more automated. In this setup, LLMs act as dynamic agents that can generate, refine, and interpret optimization strategies, while EAs efficiently explore complex solution spaces through evolutionary operators. Since this synergy enables a more efficient and creative search process, we first conduct an extensive review of recent research on the application of LLMs in optimization. We focus on LLMs' dual functionality as solution generators and algorithm designers. Then, we summarize the common and valuable designs in existing work and propose a novel LLM-EA paradigm for automated optimization. Furthermore, centered on this paradigm, we conduct an in-depth analysis of innovative methods for three key components: individual representation, variation operators, and fitness evaluation. We address challenges related to heuristic generation and solution exploration, especially from the LLM prompts' perspective. Our systematic review and thorough analysis of the paradigm can assist researchers in better understanding the current research and promoting the development of combining LLMs with EAs for automated optimization.",
    "authors": [
      "He Yu",
      "Jing Liu"
    ],
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "published": "2024-10-28T09:04:49Z",
    "pdf_url": "https://arxiv.org/pdf/2410.20848v1"
  },
  {
    "arxiv_id": "2411.00816v3",
    "entry_id": "http://arxiv.org/abs/2411.00816v3",
    "title": "CycleResearcher: Improving Automated Research via Automated Review",
    "summary": "The automation of scientific discovery has been a long-standing goal within the research community, driven by the potential to accelerate knowledge creation. While significant progress has been made using commercial large language models (LLMs) as research assistants or idea generators, the possibility of automating the entire research process with open-source LLMs remains largely unexplored. This paper explores the feasibility of using open-source post-trained LLMs as autonomous agents capable of performing the full cycle of automated research and review, from literature review and manuscript preparation to peer review and paper refinement. Our iterative preference training framework consists of CycleResearcher, which conducts research tasks, and CycleReviewer, which simulates the peer review process, providing iterative feedback via reinforcement learning. To train these models, we develop two new datasets, Review-5k and Research-14k, reflecting real-world machine learning research and peer review dynamics. Our results demonstrate that CycleReviewer achieves promising performance with a 26.89\\% reduction in mean absolute error (MAE) compared to individual human reviewers in predicting paper scores, indicating the potential of LLMs to effectively assist expert-level research evaluation. In research, the papers generated by the CycleResearcher model achieved a score of 5.36 in simulated peer reviews, showing some competitiveness in terms of simulated review scores compared to the preprint level of 5.24 from human experts, while still having room for improvement compared to the accepted paper level of 5.69. This work represents a significant step toward fully automated scientific inquiry, providing ethical safeguards and exploring AI-driven research capabilities. The code, dataset and model weight are released at https://wengsyx.github.io/Researcher/.",
    "authors": [
      "Yixuan Weng",
      "Minjun Zhu",
      "Guangsheng Bao",
      "Hongbo Zhang",
      "Jindong Wang",
      "Yue Zhang",
      "Linyi Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-10-28T08:10:21Z",
    "pdf_url": "https://arxiv.org/pdf/2411.00816v3"
  },
  {
    "arxiv_id": "2411.05801v1",
    "entry_id": "http://arxiv.org/abs/2411.05801v1",
    "title": "Do LLM Personas Dream of Bull Markets? Comparing Human and AI Investment Strategies Through the Lens of the Five-Factor Model",
    "summary": "Large Language Models (LLMs) have demonstrated the ability to adopt a personality and behave in a human-like manner. There is a large body of research that investigates the behavioural impacts of personality in less obvious areas such as investment attitudes or creative decision making. In this study, we investigated whether an LLM persona with a specific Big Five personality profile would perform an investment task similarly to a human with the same personality traits. We used a simulated investment task to determine if these results could be generalised into actual behaviours. In this simulated environment, our results show these personas produced meaningful behavioural differences in all assessed categories, with these behaviours generally being consistent with expectations derived from human research. We found that LLMs are able to generalise traits into expected behaviours in three areas: learning style, impulsivity and risk appetite while environmental attitudes could not be accurately represented. In addition, we showed that LLMs produce behaviour that is more reflective of human behaviour in a simulation environment compared to a survey environment.",
    "authors": [
      "Harris Borman",
      "Anna Leontjeva",
      "Luiz Pizzato",
      "Max Kun Jiang",
      "Dan Jermyn"
    ],
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.CY",
      "q-fin.GN"
    ],
    "published": "2024-10-28T02:50:41Z",
    "pdf_url": "https://arxiv.org/pdf/2411.05801v1"
  },
  {
    "arxiv_id": "2410.20238v2",
    "entry_id": "http://arxiv.org/abs/2410.20238v2",
    "title": "A Survey of Large Language Models for Arabic Language and its Dialects",
    "summary": "This survey offers a comprehensive overview of Large Language Models (LLMs) designed for Arabic language and its dialects. It covers key architectures, including encoder-only, decoder-only, and encoder-decoder models, along with the datasets used for pre-training, spanning Classical Arabic, Modern Standard Arabic, and Dialectal Arabic. The study also explores monolingual, bilingual, and multilingual LLMs, analyzing their architectures and performance across downstream tasks, such as sentiment analysis, named entity recognition, and question answering. Furthermore, it assesses the openness of Arabic LLMs based on factors, such as source code availability, training data, model weights, and documentation. The survey highlights the need for more diverse dialectal datasets and attributes the importance of openness for research reproducibility and transparency. It concludes by identifying key challenges and opportunities for future research and stressing the need for more inclusive and representative models.",
    "authors": [
      "Malak Mashaabi",
      "Shahad Al-Khalifa",
      "Hend Al-Khalifa"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-26T17:48:20Z",
    "pdf_url": "https://arxiv.org/pdf/2410.20238v2"
  },
  {
    "arxiv_id": "2410.20199v1",
    "entry_id": "http://arxiv.org/abs/2410.20199v1",
    "title": "Rethinking the Uncertainty: A Critical Review and Analysis in the Era of Large Language Models",
    "summary": "In recent years, Large Language Models (LLMs) have become fundamental to a broad spectrum of artificial intelligence applications. As the use of LLMs expands, precisely estimating the uncertainty in their predictions has become crucial. Current methods often struggle to accurately identify, measure, and address the true uncertainty, with many focusing primarily on estimating model confidence. This discrepancy is largely due to an incomplete understanding of where, when, and how uncertainties are injected into models. This paper introduces a comprehensive framework specifically designed to identify and understand the types and sources of uncertainty, aligned with the unique characteristics of LLMs. Our framework enhances the understanding of the diverse landscape of uncertainties by systematically categorizing and defining each type, establishing a solid foundation for developing targeted methods that can precisely quantify these uncertainties. We also provide a detailed introduction to key related concepts and examine the limitations of current methods in mission-critical and safety-sensitive applications. The paper concludes with a perspective on future directions aimed at enhancing the reliability and practical adoption of these methods in real-world scenarios.",
    "authors": [
      "Mohammad Beigi",
      "Sijia Wang",
      "Ying Shen",
      "Zihao Lin",
      "Adithya Kulkarni",
      "Jianfeng He",
      "Feng Chen",
      "Ming Jin",
      "Jin-Hee Cho",
      "Dawei Zhou",
      "Chang-Tien Lu",
      "Lifu Huang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-10-26T15:07:15Z",
    "pdf_url": "https://arxiv.org/pdf/2410.20199v1"
  },
  {
    "arxiv_id": "2410.19599v3",
    "entry_id": "http://arxiv.org/abs/2410.19599v3",
    "title": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina",
    "summary": "Recent studies suggest large language models (LLMs) can exhibit human-like reasoning, aligning with human behavior in economic experiments, surveys, and political discourse. This has led many to propose that LLMs can be used as surrogates or simulations for humans in social science research. However, LLMs differ fundamentally from humans, relying on probabilistic patterns, absent the embodied experiences or survival objectives that shape human cognition. We assess the reasoning depth of LLMs using the 11-20 money request game. Nearly all advanced approaches fail to replicate human behavior distributions across many models. Causes of failure are diverse and unpredictable, relating to input language, roles, and safeguarding. These results advise caution when using LLMs to study human behavior or as surrogates or simulations.",
    "authors": [
      "Yuan Gao",
      "Dokyun Lee",
      "Gordon Burtch",
      "Sina Fazelpour"
    ],
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2024-10-25T14:46:07Z",
    "pdf_url": "https://arxiv.org/pdf/2410.19599v3"
  },
  {
    "arxiv_id": "2410.18856v3",
    "entry_id": "http://arxiv.org/abs/2410.18856v3",
    "title": "Demystifying Large Language Models for Medicine: A Primer",
    "summary": "Large language models (LLMs) represent a transformative class of AI tools capable of revolutionizing various aspects of healthcare by generating human-like responses across diverse contexts and adapting to novel tasks following human instructions. Their potential application spans a broad range of medical tasks, such as clinical documentation, matching patients to clinical trials, and answering medical questions. In this primer paper, we propose an actionable guideline to help healthcare professionals more efficiently utilize LLMs in their work, along with a set of best practices. This approach consists of several main phases, including formulating the task, choosing LLMs, prompt engineering, fine-tuning, and deployment. We start with the discussion of critical considerations in identifying healthcare tasks that align with the core capabilities of LLMs and selecting models based on the selected task and data, performance requirements, and model interface. We then review the strategies, such as prompt engineering and fine-tuning, to adapt standard LLMs to specialized medical tasks. Deployment considerations, including regulatory compliance, ethical guidelines, and continuous monitoring for fairness and bias, are also discussed. By providing a structured step-by-step methodology, this tutorial aims to equip healthcare professionals with the tools necessary to effectively integrate LLMs into clinical practice, ensuring that these powerful technologies are applied in a safe, reliable, and impactful manner.",
    "authors": [
      "Qiao Jin",
      "Nicholas Wan",
      "Robert Leaman",
      "Shubo Tian",
      "Zhizheng Wang",
      "Yifan Yang",
      "Zifeng Wang",
      "Guangzhi Xiong",
      "Po-Ting Lai",
      "Qingqing Zhu",
      "Benjamin Hou",
      "Maame Sarfo-Gyamfi",
      "Gongbo Zhang",
      "Aidan Gilson",
      "Balu Bhasuran",
      "Zhe He",
      "Aidong Zhang",
      "Jimeng Sun",
      "Chunhua Weng",
      "Ronald M. Summers",
      "Qingyu Chen",
      "Yifan Peng",
      "Zhiyong Lu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-10-24T15:41:56Z",
    "pdf_url": "https://arxiv.org/pdf/2410.18856v3"
  },
  {
    "arxiv_id": "2410.19878v3",
    "entry_id": "http://arxiv.org/abs/2410.19878v3",
    "title": "Parameter-Efficient Fine-Tuning in Large Models: A Survey of Methodologies",
    "summary": "The large models, as predicted by scaling raw forecasts, have made groundbreaking progress in many fields, particularly in natural language generation tasks, where they have approached or even surpassed human levels. However, the unprecedented scale of their parameters brings significant computational and storage costs. These large models require substantial computational resources and GPU memory to operate. When adapting large models to specific downstream tasks, their massive parameter scale poses a significant challenge in fine-tuning on hardware platforms with limited computational power and GPU memory. To address this issue, Parameter-Efficient Fine-Tuning (PEFT) offers a practical solution by efficiently adjusting the parameters of large pre-trained models to suit various downstream tasks. Specifically, PEFT adjusts the parameters of pre-trained large models to adapt to specific tasks or domains, minimizing the introduction of additional parameters and the computational resources required. This review mainly introduces the preliminary knowledge of PEFT, the core ideas and principles of various PEFT algorithms, the applications of PEFT, and potential future research directions. By reading this review, we believe that interested parties can quickly grasp the PEFT methodology, thereby accelerating its development and innovation.",
    "authors": [
      "Luping Wang",
      "Sheng Chen",
      "Linnan Jiang",
      "Shu Pan",
      "Runze Cai",
      "Sen Yang",
      "Fei Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-24T13:58:59Z",
    "pdf_url": "https://arxiv.org/pdf/2410.19878v3"
  },
  {
    "arxiv_id": "2411.05013v1",
    "entry_id": "http://arxiv.org/abs/2411.05013v1",
    "title": "Enhancing literature review with LLM and NLP methods. Algorithmic trading case",
    "summary": "This study utilizes machine learning algorithms to analyze and organize knowledge in the field of algorithmic trading. By filtering a dataset of 136 million research papers, we identified 14,342 relevant articles published between 1956 and Q1 2020. We compare traditional practices-such as keyword-based algorithms and embedding techniques-with state-of-the-art topic modeling methods that employ dimensionality reduction and clustering. This comparison allows us to assess the popularity and evolution of different approaches and themes within algorithmic trading. We demonstrate the usefulness of Natural Language Processing (NLP) in the automatic extraction of knowledge, highlighting the new possibilities created by the latest iterations of Large Language Models (LLMs) like ChatGPT. The rationale for focusing on this topic stems from our analysis, which reveals that research articles on algorithmic trading are increasing at a faster rate than the overall number of publications. While stocks and main indices comprise more than half of all assets considered, certain asset classes, such as cryptocurrencies, exhibit a much stronger growth trend. Machine learning models have become the most popular methods in recent years. The study demonstrates the efficacy of LLMs in refining datasets and addressing intricate questions about the analyzed articles, such as comparing the efficiency of different models. Our research shows that by decomposing tasks into smaller components and incorporating reasoning steps, we can effectively tackle complex questions supported by case analyses. This approach contributes to a deeper understanding of algorithmic trading methodologies and underscores the potential of advanced NLP techniques in literature reviews.",
    "authors": [
      "Stanisław Łaniewski",
      "Robert Ślepaczuk"
    ],
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.LG",
      "q-fin.TR"
    ],
    "published": "2024-10-23T13:37:27Z",
    "pdf_url": "https://arxiv.org/pdf/2411.05013v1"
  },
  {
    "arxiv_id": "2410.17840v2",
    "entry_id": "http://arxiv.org/abs/2410.17840v2",
    "title": "Is the GPU Half-Empty or Half-Full? Practical Scheduling Techniques for LLMs",
    "summary": "Serving systems for Large Language Models (LLMs) improve throughput by processing several requests concurrently. However, multiplexing hardware resources between concurrent requests involves non-trivial scheduling decisions. Practical serving systems typically implement these decisions at two levels: First, a load balancer routes requests to different servers which each hold a replica of the LLM. Then, on each server, an engine-level scheduler decides when to run a request, or when to queue or preempt it. Improved scheduling policies may benefit a wide range of LLM deployments and can often be implemented as \"drop-in replacements\" to a system's current policy. In this work, we survey scheduling techniques from the literature and from practical serving systems. We find that schedulers from the literature often achieve good performance but introduce significant complexity. In contrast, schedulers in practical deployments often leave easy performance gains on the table but are easy to implement, deploy and configure. This finding motivates us to introduce two new scheduling techniques, which are both easy to implement, and outperform current techniques on production workload traces.",
    "authors": [
      "Ferdi Kossmann",
      "Bruce Fontaine",
      "Daya Khudia",
      "Michael Cafarella",
      "Samuel Madden"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-10-23T13:05:46Z",
    "pdf_url": "https://arxiv.org/pdf/2410.17840v2"
  },
  {
    "arxiv_id": "2410.17602v1",
    "entry_id": "http://arxiv.org/abs/2410.17602v1",
    "title": "Integrating Large Language Models for UAV Control in Simulated Environments: A Modular Interaction Approach",
    "summary": "The intersection of LLMs (Large Language Models) and UAV (Unoccupied Aerial Vehicles) technology represents a promising field of research with the potential to enhance UAV capabilities significantly. This study explores the application of LLMs in UAV control, focusing on the opportunities for integrating advanced natural language processing into autonomous aerial systems. By enabling UAVs to interpret and respond to natural language commands, LLMs simplify the UAV control and usage, making them accessible to a broader user base and facilitating more intuitive human-machine interactions. The paper discusses several key areas where LLMs can impact UAV technology, including autonomous decision-making, dynamic mission planning, enhanced situational awareness, and improved safety protocols. Through a comprehensive review of current developments and potential future directions, this study aims to highlight how LLMs can transform UAV operations, making them more adaptable, responsive, and efficient in complex environments. A template development framework for integrating LLMs in UAV control is also described. Proof of Concept results that integrate existing LLM models and popular robotic simulation platforms are demonstrated. The findings suggest that while there are substantial technical and ethical challenges to address, integrating LLMs into UAV control holds promising implications for advancing autonomous aerial systems.",
    "authors": [
      "Abhishek Phadke",
      "Alihan Hadimlioglu",
      "Tianxing Chu",
      "Chandra N Sekharan"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2024-10-23T06:56:53Z",
    "pdf_url": "https://arxiv.org/pdf/2410.17602v1"
  },
  {
    "arxiv_id": "2410.17532v1",
    "entry_id": "http://arxiv.org/abs/2410.17532v1",
    "title": "Responsible Multilingual Large Language Models: A Survey of Development, Applications, and Societal Impact",
    "summary": "Multilingual Large Language Models (MLLMs) represent a pivotal advancement in democratizing artificial intelligence across linguistic boundaries. While theoretical foundations are well-established, practical implementation guidelines remain scattered. This work bridges this gap by providing a comprehensive end-to-end framework for developing and deploying MLLMs in production environments. We make three distinctive contributions: First, we present an actionable pipeline from data pre-processing through deployment, integrating insights from academic research and industrial applications. Second, using Llama2 as a case study, we provide detailed optimization strategies for enhancing multilingual capabilities, including curriculum learning approaches for balancing high-resource and low-resource languages, tokenization strategies, and effective sampling methods. Third, we offer an interdisciplinary analysis that considers technical, linguistic, and cultural perspectives in MLLM development. Our findings reveal critical challenges in supporting linguistic diversity, with 88.38% of world languages categorized as low-resource, affecting over a billion speakers. We examine practical solutions through real-world applications in customer service, search engines, and machine translation. By synthesizing theoretical frameworks with production-ready implementation strategies, this survey provides essential guidance for practitioners and researchers working to develop more inclusive and effective multilingual AI systems.",
    "authors": [
      "Junhua Liu",
      "Bin Fu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-23T03:19:15Z",
    "pdf_url": "https://arxiv.org/pdf/2410.17532v1"
  },
  {
    "arxiv_id": "2410.17504v1",
    "entry_id": "http://arxiv.org/abs/2410.17504v1",
    "title": "An Ontology-Enabled Approach For User-Centered and Knowledge-Enabled Explanations of AI Systems",
    "summary": "Explainable Artificial Intelligence (AI) focuses on helping humans understand the working of AI systems or their decisions and has been a cornerstone of AI for decades. Recent research in explainability has focused on explaining the workings of AI models or model explainability. There have also been several position statements and review papers detailing the needs of end-users for user-centered explainability but fewer implementations. Hence, this thesis seeks to bridge some gaps between model and user-centered explainability. We create an explanation ontology (EO) to represent literature-derived explanation types via their supporting components. We implement a knowledge-augmented question-answering (QA) pipeline to support contextual explanations in a clinical setting. Finally, we are implementing a system to combine explanations from different AI methods and data modalities. Within the EO, we can represent fifteen different explanation types, and we have tested these representations in six exemplar use cases. We find that knowledge augmentations improve the performance of base large language models in the contextualized QA, and the performance is variable across disease groups. In the same setting, clinicians also indicated that they prefer to see actionability as one of the main foci in explanations. In our explanations combination method, we plan to use similarity metrics to determine the similarity of explanations in a chronic disease detection setting. Overall, through this thesis, we design methods that can support knowledge-enabled explanations across different use cases, accounting for the methods in today's AI era that can generate the supporting components of these explanations and domain knowledge sources that can enhance them.",
    "authors": [
      "Shruthi Chari"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-10-23T02:03:49Z",
    "pdf_url": "https://arxiv.org/pdf/2410.17504v1"
  },
  {
    "arxiv_id": "2410.16780v2",
    "entry_id": "http://arxiv.org/abs/2410.16780v2",
    "title": "Beyond Retrieval: Generating Narratives in Conversational Recommender Systems",
    "summary": "The recent advances in Large Language Model's generation and reasoning capabilities present an opportunity to develop truly conversational recommendation systems. However, effectively integrating recommender system knowledge into LLMs for natural language generation which is tailored towards recommendation tasks remains a challenge. This paper addresses this challenge by making two key contributions.\n  First, we introduce a new dataset (REGEN) for natural language generation tasks in conversational recommendations. REGEN (Reviews Enhanced with GEnerative Narratives) extends the Amazon Product Reviews dataset with rich user narratives, including personalized explanations of product preferences, product endorsements for recommended items, and summaries of user purchase history. REGEN is made publicly available to facilitate further research. Furthermore, we establish benchmarks using well-known generative metrics, and perform an automated evaluation of the new dataset using a rater LLM. Second, the paper introduces a fusion architecture (CF model with an LLM) which serves as a baseline for REGEN. And to the best of our knowledge, represents the first attempt to analyze the capabilities of LLMs in understanding recommender signals and generating rich narratives. We demonstrate that LLMs can effectively learn from simple fusion architectures utilizing interaction-based CF embeddings, and this can be further enhanced using the metadata and personalization data associated with items. Our experiments show that combining CF and content embeddings leads to improvements of 4-12% in key language metrics compared to using either type of embedding individually. We also provide an analysis to interpret how CF and content embeddings contribute to this new generative task.",
    "authors": [
      "Krishna Sayana",
      "Raghavendra Vasudeva",
      "Yuri Vasilevski",
      "Kun Su",
      "Liam Hebert",
      "James Pine",
      "Hubert Pham",
      "Ambarish Jash",
      "Sukhdeep Sodhi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-10-22T07:53:41Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16780v2"
  },
  {
    "arxiv_id": "2410.16662v1",
    "entry_id": "http://arxiv.org/abs/2410.16662v1",
    "title": "Visual Question Answering in Ophthalmology: A Progressive and Practical Perspective",
    "summary": "Accurate diagnosis of ophthalmic diseases relies heavily on the interpretation of multimodal ophthalmic images, a process often time-consuming and expertise-dependent. Visual Question Answering (VQA) presents a potential interdisciplinary solution by merging computer vision and natural language processing to comprehend and respond to queries about medical images. This review article explores the recent advancements and future prospects of VQA in ophthalmology from both theoretical and practical perspectives, aiming to provide eye care professionals with a deeper understanding and tools for leveraging the underlying models. Additionally, we discuss the promising trend of large language models (LLM) in enhancing various components of the VQA framework to adapt to multimodal ophthalmic tasks. Despite the promising outlook, ophthalmic VQA still faces several challenges, including the scarcity of annotated multimodal image datasets, the necessity of comprehensive and unified evaluation methods, and the obstacles to achieving effective real-world applications. This article highlights these challenges and clarifies future directions for advancing ophthalmic VQA with LLMs. The development of LLM-based ophthalmic VQA systems calls for collaborative efforts between medical professionals and AI experts to overcome existing obstacles and advance the diagnosis and care of eye diseases.",
    "authors": [
      "Xiaolan Chen",
      "Ruoyu Chen",
      "Pusheng Xu",
      "Weiyi Zhang",
      "Xianwen Shang",
      "Mingguang He",
      "Danli Shi"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-10-22T03:28:41Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16662v1"
  },
  {
    "arxiv_id": "2410.16645v1",
    "entry_id": "http://arxiv.org/abs/2410.16645v1",
    "title": "Chatting with Bots: AI, Speech Acts, and the Edge of Assertion",
    "summary": "This paper addresses the question of whether large language model-powered chatbots are capable of assertion. According to what we call the Thesis of Chatbot Assertion (TCA), chatbots are the kinds of things that can assert, and at least some of the output produced by current-generation chatbots qualifies as assertion. We provide some motivation for TCA, arguing that it ought to be taken seriously and not simply dismissed. We also review recent objections to TCA, arguing that these objections are weighty. We thus confront the following dilemma: how can we do justice to both the considerations for and against TCA? We consider two influential responses to this dilemma - the first appeals to the notion of proxy-assertion; the second appeals to fictionalism - and argue that neither is satisfactory. Instead, reflecting on the ontogenesis of assertion, we argue that we need to make space for a category of proto-assertion. We then apply the category of proto-assertion to chatbots, arguing that treating chatbots as proto-assertors provides a satisfactory resolution to the dilemma of chatbot assertion.",
    "authors": [
      "Iwan Williams",
      "Tim Bayne"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-22T02:45:09Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16645v1"
  },
  {
    "arxiv_id": "2410.16458v2",
    "entry_id": "http://arxiv.org/abs/2410.16458v2",
    "title": "STAR: A Simple Training-free Approach for Recommendations using Large Language Models",
    "summary": "Recent progress in large language models (LLMs) offers promising new approaches for recommendation system tasks. While the current state-of-the-art methods rely on fine-tuning LLMs to achieve optimal results, this process is costly and introduces significant engineering complexities. Conversely, methods that directly use LLMs without additional fine-tuning result in a large drop in recommendation quality, often due to the inability to capture collaborative information. In this paper, we propose a Simple Training-free Approach for Recommendation (STAR), a framework that utilizes LLMs and can be applied to various recommendation tasks without the need for fine-tuning, while maintaining high quality recommendation performance. Our approach involves a retrieval stage that uses semantic embeddings from LLMs combined with collaborative user information to retrieve candidate items. We then apply an LLM for pairwise ranking to enhance next-item prediction. Experimental results on the Amazon Review dataset show competitive performance for next item prediction, even with our retrieval stage alone. Our full method achieves Hits@10 performance of +23.8% on Beauty, +37.5% on Toys & Games, and -1.8% on Sports & Outdoors relative to the best supervised models. This framework offers an effective alternative to traditional supervised models, highlighting the potential of LLMs in recommendation systems without extensive training or custom architectures.",
    "authors": [
      "Dong-Ho Lee",
      "Adam Kraft",
      "Long Jin",
      "Nikhil Mehta",
      "Taibai Xu",
      "Lichan Hong",
      "Ed H. Chi",
      "Xinyang Yi"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-21T19:34:40Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16458v2"
  },
  {
    "arxiv_id": "2410.16411v2",
    "entry_id": "http://arxiv.org/abs/2410.16411v2",
    "title": "The Duality of Generative AI and Reinforcement Learning in Robotics: A Review",
    "summary": "Recently, generative AI and reinforcement learning (RL) have been redefining what is possible for AI agents that take information flows as input and produce intelligent behavior. As a result, we are seeing similar advancements in embodied AI and robotics for control policy generation. Our review paper examines the integration of generative AI models with RL to advance robotics. Our primary focus is on the duality between generative AI and RL for robotics downstream tasks. Specifically, we investigate: (1) The role of prominent generative AI tools as modular priors for multi-modal input fusion in RL tasks. (2) How RL can train, fine-tune and distill generative models for policy generation, such as VLA models, similarly to RL applications in large language models. We then propose a new taxonomy based on a considerable amount of selected papers.\n  Lastly, we identify open challenges accounting for model scalability, adaptation and grounding, giving recommendations and insights on future research directions. We reflect on which generative AI models best fit the RL tasks and why. On the other side, we reflect on important issues inherent to RL-enhanced generative policies, such as safety concerns and failure modes, and what are the limitations of current methods. A curated collection of relevant research papers is maintained on our GitHub repository, serving as a resource for ongoing research and development in this field: https://github.com/clmoro/Robotics-RL-FMs-Integration.",
    "authors": [
      "Angelo Moroncelli",
      "Vishal Soni",
      "Marco Forgione",
      "Dario Piga",
      "Blerina Spahiu",
      "Loris Roveda"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2024-10-21T18:27:48Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16411v2"
  },
  {
    "arxiv_id": "2410.16392v3",
    "entry_id": "http://arxiv.org/abs/2410.16392v3",
    "title": "Scaffolded Language Models with Language Supervision for Mixed-Autonomy: A Survey",
    "summary": "This survey organizes the intricate literature on the design and optimization of emerging structures around post-trained LMs. We refer to this overarching structure as scaffolded LMs and focus on LMs that are integrated into multi-step processes with tools. We view scaffolded LMs as semi-parametric models wherein we train non-parametric variables, including the prompt, tools, and scaffold's code. In particular, they interpret instructions, use tools, and receive feedback all in language. Recent works use an LM as an optimizer to interpret language supervision and update non-parametric variables according to intricate objectives. In this survey, we refer to this paradigm as training of scaffolded LMs with language supervision. A key feature of non-parametric training is the ability to learn from language. Parametric training excels in learning from demonstration (supervised learning), exploration (reinforcement learning), or observations (unsupervised learning), using well-defined loss functions. Language-based optimization enables rich, interpretable, and expressive objectives, while mitigating issues like catastrophic forgetting and supporting compatibility with closed-source models. Furthermore, agents are increasingly deployed as co-workers in real-world applications such as Copilot in Office tools or software development. In these mixed-autonomy settings, where control and decision-making are shared between human and AI, users point out errors or suggest corrections. Accordingly, we discuss agents that continuously improve by learning from this real-time, language-based feedback and refer to this setting as streaming learning from language supervision.",
    "authors": [
      "Matthieu Lin",
      "Jenny Sheng",
      "Andrew Zhao",
      "Shenzhi Wang",
      "Yang Yue",
      "Victor Shea Jay Huang",
      "Huan Liu",
      "Jun Liu",
      "Gao Huang",
      "Yong-Jin Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-10-21T18:06:25Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16392v3"
  },
  {
    "arxiv_id": "2410.16349v1",
    "entry_id": "http://arxiv.org/abs/2410.16349v1",
    "title": "Large Language Models in Computer Science Education: A Systematic Literature Review",
    "summary": "Large language models (LLMs) are becoming increasingly better at a wide range of Natural Language Processing tasks (NLP), such as text generation and understanding. Recently, these models have extended their capabilities to coding tasks, bridging the gap between natural languages (NL) and programming languages (PL). Foundational models such as the Generative Pre-trained Transformer (GPT) and LLaMA series have set strong baseline performances in various NL and PL tasks. Additionally, several models have been fine-tuned specifically for code generation, showing significant improvements in code-related applications. Both foundational and fine-tuned models are increasingly used in education, helping students write, debug, and understand code. We present a comprehensive systematic literature review to examine the impact of LLMs in computer science and computer engineering education. We analyze their effectiveness in enhancing the learning experience, supporting personalized education, and aiding educators in curriculum development. We address five research questions to uncover insights into how LLMs contribute to educational outcomes, identify challenges, and suggest directions for future research.",
    "authors": [
      "Nishat Raihan",
      "Mohammed Latif Siddiq",
      "Joanna C. S. Santos",
      "Marcos Zampieri"
    ],
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "published": "2024-10-21T17:49:50Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16349v1"
  },
  {
    "arxiv_id": "2410.19845v1",
    "entry_id": "http://arxiv.org/abs/2410.19845v1",
    "title": "Enhancing Trust and Safety in Digital Payments: An LLM-Powered Approach",
    "summary": "Digital payment systems have revolutionized financial transactions, offering unparalleled convenience and accessibility to users worldwide. However, the increasing popularity of these platforms has also attracted malicious actors seeking to exploit their vulnerabilities for financial gain. To address this challenge, robust and adaptable scam detection mechanisms are crucial for maintaining the trust and safety of digital payment ecosystems. This paper presents a comprehensive approach to scam detection, focusing on the Unified Payments Interface (UPI) in India, Google Pay (GPay) as a specific use case. The approach leverages Large Language Models (LLMs) to enhance scam classification accuracy and designs a digital assistant to aid human reviewers in identifying and mitigating fraudulent activities. The results demonstrate the potential of LLMs in augmenting existing machine learning models and improving the efficiency, accuracy, quality, and consistency of scam reviews, ultimately contributing to a safer and more secure digital payment landscape. Our evaluation of the Gemini Ultra model on curated transaction data showed a 93.33% accuracy in scam classification. Furthermore, the model demonstrated 89% accuracy in generating reasoning for these classifications. A promising fact, the model identified 32% new accurate reasons for suspected scams that human reviewers had not included in the review notes.",
    "authors": [
      "Devendra Dahiphale",
      "Naveen Madiraju",
      "Justin Lin",
      "Rutvik Karve",
      "Monu Agrawal",
      "Anant Modwal",
      "Ramanan Balakrishnan",
      "Shanay Shah",
      "Govind Kaushal",
      "Priya Mandawat",
      "Prakash Hariramani",
      "Arif Merchant"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "published": "2024-10-21T15:21:11Z",
    "pdf_url": "https://arxiv.org/pdf/2410.19845v1"
  },
  {
    "arxiv_id": "2410.16070v2",
    "entry_id": "http://arxiv.org/abs/2410.16070v2",
    "title": "On-Device LLMs for SMEs: Challenges and Opportunities",
    "summary": "This paper presents a systematic review of the infrastructure requirements for deploying Large Language Models (LLMs) on-device within the context of small and medium-sized enterprises (SMEs), focusing on both hardware and software perspectives. From the hardware viewpoint, we discuss the utilization of processing units like GPUs and TPUs, efficient memory and storage solutions, and strategies for effective deployment, addressing the challenges of limited computational resources typical in SME settings. From the software perspective, we explore framework compatibility, operating system optimization, and the use of specialized libraries tailored for resource-constrained environments. The review is structured to first identify the unique challenges faced by SMEs in deploying LLMs on-device, followed by an exploration of the opportunities that both hardware innovations and software adaptations offer to overcome these obstacles. Such a structured review provides practical insights, contributing significantly to the community by enhancing the technological resilience of SMEs in integrating LLMs.",
    "authors": [
      "Jeremy Stephen Gabriel Yee",
      "Pai Chet Ng",
      "Zhengkui Wang",
      "Ian McLoughlin",
      "Aik Beng Ng",
      "Simon See"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-10-21T14:48:35Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16070v2"
  },
  {
    "arxiv_id": "2410.15990v1",
    "entry_id": "http://arxiv.org/abs/2410.15990v1",
    "title": "Augmenting Legal Decision Support Systems with LLM-based NLI for Analyzing Social Media Evidence",
    "summary": "This paper presents our system description and error analysis of our entry for NLLP 2024 shared task on Legal Natural Language Inference (L-NLI) \\citep{hagag2024legallenssharedtask2024}. The task required classifying these relationships as entailed, contradicted, or neutral, indicating any association between the review and the complaint. Our system emerged as the winning submission, significantly outperforming other entries with a substantial margin and demonstrating the effectiveness of our approach in legal text analysis. We provide a detailed analysis of the strengths and limitations of each model and approach tested, along with a thorough error analysis and suggestions for future improvements. This paper aims to contribute to the growing field of legal NLP by offering insights into advanced techniques for natural language inference in legal contexts, making it accessible to both experts and newcomers in the field.",
    "authors": [
      "Ram Mohan Rao Kadiyala",
      "Siddartha Pullakhandam",
      "Kanwal Mehreen",
      "Subhasya Tippareddy",
      "Ashay Srivastava"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-21T13:20:15Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15990v1"
  },
  {
    "arxiv_id": "2410.15978v2",
    "entry_id": "http://arxiv.org/abs/2410.15978v2",
    "title": "PROMPTHEUS: A Human-Centered Pipeline to Streamline SLRs with LLMs",
    "summary": "The growing volume of academic publications poses significant challenges for researchers conducting timely and accurate Systematic Literature Reviews, particularly in fast-evolving fields like artificial intelligence. This growth of academic literature also makes it increasingly difficult for lay people to access scientific knowledge effectively, meaning academic literature is often misrepresented in the popular press and, more broadly, in society. Traditional SLR methods are labor-intensive and error-prone, and they struggle to keep up with the rapid pace of new research. To address these issues, we developed \\textit{PROMPTHEUS}: an AI-driven pipeline solution that automates the SLR process using Large Language Models. We aimed to enhance efficiency by reducing the manual workload while maintaining the precision and coherence required for comprehensive literature synthesis. PROMPTHEUS automates key stages of the SLR process, including systematic search, data extraction, topic modeling using BERTopic, and summarization with transformer models. Evaluations conducted across five research domains demonstrate that PROMPTHEUS reduces review time, achieves high precision, and provides coherent topic organization, offering a scalable and effective solution for conducting literature reviews in an increasingly crowded research landscape. In addition, such tools may reduce the increasing mistrust in science by making summarization more accessible to laypeople.\n  The code for this project can be found on the GitHub repository at https://github.com/joaopftorres/PROMPTHEUS.git",
    "authors": [
      "João Pedro Fernandes Torres",
      "Catherine Mulligan",
      "Joaquim Jorge",
      "Catarina Moreira"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-10-21T13:05:33Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15978v2"
  },
  {
    "arxiv_id": "2410.15644v1",
    "entry_id": "http://arxiv.org/abs/2410.15644v1",
    "title": "Procedural Content Generation in Games: A Survey with Insights on Emerging LLM Integration",
    "summary": "Procedural Content Generation (PCG) is defined as the automatic creation of game content using algorithms. PCG has a long history in both the game industry and the academic world. It can increase player engagement and ease the work of game designers. While recent advances in deep learning approaches in PCG have enabled researchers and practitioners to create more sophisticated content, it is the arrival of Large Language Models (LLMs) that truly disrupted the trajectory of PCG advancement.\n  This survey explores the differences between various algorithms used for PCG, including search-based methods, machine learning-based methods, other frequently used methods (e.g., noise functions), and the newcomer, LLMs. We also provide a detailed discussion on combined methods. Furthermore, we compare these methods based on the type of content they generate and the publication dates of their respective papers. Finally, we identify gaps in the existing academic work and suggest possible directions for future research.",
    "authors": [
      "Mahdi Farrokhi Maleki",
      "Richard Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-10-21T05:10:13Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15644v1"
  },
  {
    "arxiv_id": "2410.15595v3",
    "entry_id": "http://arxiv.org/abs/2410.15595v3",
    "title": "A Comprehensive Survey of Direct Preference Optimization: Datasets, Theories, Variants, and Applications",
    "summary": "With the rapid advancement of large language models (LLMs), aligning policy models with human preferences has become increasingly critical. Direct Preference Optimization (DPO) has emerged as a promising approach for alignment, acting as an RL-free alternative to Reinforcement Learning from Human Feedback (RLHF). Despite DPO's various advancements and inherent limitations, an in-depth review of these aspects is currently lacking in the literature. In this work, we present a comprehensive review of the challenges and opportunities in DPO, covering theoretical analyses, variants, relevant preference datasets, and applications. Specifically, we categorize recent studies on DPO based on key research questions to provide a thorough understanding of DPO's current landscape. Additionally, we propose several future research directions to offer insights on model alignment for the research community. An updated collection of relevant papers can be found on https://github.com/Mr-Loevan/DPO-Survey.",
    "authors": [
      "Wenyi Xiao",
      "Zechuan Wang",
      "Leilei Gan",
      "Shuai Zhao",
      "Zongrui Li",
      "Ruirui Lei",
      "Wanggui He",
      "Luu Anh Tuan",
      "Long Chen",
      "Hao Jiang",
      "Zhou Zhao",
      "Fei Wu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-10-21T02:27:24Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15595v3"
  },
  {
    "arxiv_id": "2410.15442v1",
    "entry_id": "http://arxiv.org/abs/2410.15442v1",
    "title": "Exploring Social Desirability Response Bias in Large Language Models: Evidence from GPT-4 Simulations",
    "summary": "Large language models (LLMs) are employed to simulate human-like responses in social surveys, yet it remains unclear if they develop biases like social desirability response (SDR) bias. To investigate this, GPT-4 was assigned personas from four societies, using data from the 2022 Gallup World Poll. These synthetic samples were then prompted with or without a commitment statement intended to induce SDR. The results were mixed. While the commitment statement increased SDR index scores, suggesting SDR bias, it reduced civic engagement scores, indicating an opposite trend. Additional findings revealed demographic associations with SDR scores and showed that the commitment statement had limited impact on GPT-4's predictive performance. The study underscores potential avenues for using LLMs to investigate biases in both humans and LLMs themselves.",
    "authors": [
      "Sanguk Lee",
      "Kai-Qi Yang",
      "Tai-Quan Peng",
      "Ruth Heo",
      "Hui Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-10-20T16:28:24Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15442v1"
  },
  {
    "arxiv_id": "2410.15359v1",
    "entry_id": "http://arxiv.org/abs/2410.15359v1",
    "title": "A Survey of Hallucination in Large Visual Language Models",
    "summary": "The Large Visual Language Models (LVLMs) enhances user interaction and enriches user experience by integrating visual modality on the basis of the Large Language Models (LLMs). It has demonstrated their powerful information processing and generation capabilities. However, the existence of hallucinations has limited the potential and practical effectiveness of LVLM in various fields. Although lots of work has been devoted to the issue of hallucination mitigation and correction, there are few reviews to summary this issue. In this survey, we first introduce the background of LVLMs and hallucinations. Then, the structure of LVLMs and main causes of hallucination generation are introduced. Further, we summary recent works on hallucination correction and mitigation. In addition, the available hallucination evaluation benchmarks for LVLMs are presented from judgmental and generative perspectives. Finally, we suggest some future research directions to enhance the dependability and utility of LVLMs.",
    "authors": [
      "Wei Lan",
      "Wenyi Chen",
      "Qingfeng Chen",
      "Shirui Pan",
      "Huiyu Zhou",
      "Yi Pan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-10-20T10:58:58Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15359v1"
  },
  {
    "arxiv_id": "2410.15319v1",
    "entry_id": "http://arxiv.org/abs/2410.15319v1",
    "title": "Causality for Large Language Models",
    "summary": "Recent breakthroughs in artificial intelligence have driven a paradigm shift, where large language models (LLMs) with billions or trillions of parameters are trained on vast datasets, achieving unprecedented success across a series of language tasks. However, despite these successes, LLMs still rely on probabilistic modeling, which often captures spurious correlations rooted in linguistic patterns and social stereotypes, rather than the true causal relationships between entities and events. This limitation renders LLMs vulnerable to issues such as demographic biases, social stereotypes, and LLM hallucinations. These challenges highlight the urgent need to integrate causality into LLMs, moving beyond correlation-driven paradigms to build more reliable and ethically aligned AI systems.\n  While many existing surveys and studies focus on utilizing prompt engineering to activate LLMs for causal knowledge or developing benchmarks to assess their causal reasoning abilities, most of these efforts rely on human intervention to activate pre-trained models. How to embed causality into the training process of LLMs and build more general and intelligent models remains unexplored. Recent research highlights that LLMs function as causal parrots, capable of reciting causal knowledge without truly understanding or applying it. These prompt-based methods are still limited to human interventional improvements. This survey aims to address this gap by exploring how causality can enhance LLMs at every stage of their lifecycle-from token embedding learning and foundation model training to fine-tuning, alignment, inference, and evaluation-paving the way for more interpretable, reliable, and causally-informed models. Additionally, we further outline six promising future directions to advance LLM development, enhance their causal reasoning capabilities, and address the current limitations these models face.",
    "authors": [
      "Anpeng Wu",
      "Kun Kuang",
      "Minqin Zhu",
      "Yingrong Wang",
      "Yujia Zheng",
      "Kairong Han",
      "Baohong Li",
      "Guangyi Chen",
      "Fei Wu",
      "Kun Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2024-10-20T07:22:23Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15319v1"
  },
  {
    "arxiv_id": "2410.15281v4",
    "entry_id": "http://arxiv.org/abs/2410.15281v4",
    "title": "LLM4AD: Large Language Models for Autonomous Driving -- Concept, Review, Benchmark, Experiments, and Future Trends",
    "summary": "With the broader adoption and highly successful development of Large Language Models (LLMs), there has been growing interest and demand for applying LLMs to autonomous driving technology. Driven by their natural language understanding and reasoning capabilities, LLMs have the potential to enhance various aspects of autonomous driving systems, from perception and scene understanding to interactive decision-making. In this paper, we first introduce the novel concept of designing Large Language Models for Autonomous Driving (LLM4AD), followed by a review of existing LLM4AD studies. Then, we propose a comprehensive benchmark for evaluating the instruction-following and reasoning abilities of LLM4AD systems, which includes LaMPilot-Bench, CARLA Leaderboard 1.0 Benchmark in simulation and NuPlanQA for multi-view visual question answering. Furthermore, we conduct extensive real-world experiments on autonomous vehicle platforms, examining both on-cloud and on-edge LLM deployment for personalized decision-making and motion control. Next, we explore the future trends of integrating language diffusion models into autonomous driving, exemplified by the proposed ViLaD (Vision-Language Diffusion) framework. Finally, we discuss the main challenges of LLM4AD, including latency, deployment, security and privacy, safety, trust and transparency, and personalization.",
    "authors": [
      "Can Cui",
      "Yunsheng Ma",
      "Sung-Yeon Park",
      "Zichong Yang",
      "Yupeng Zhou",
      "Juanwu Lu",
      "Juntong Peng",
      "Jiaru Zhang",
      "Ruqi Zhang",
      "Lingxi Li",
      "Yaobin Chen",
      "Jitesh H. Panchal",
      "Amr Abdelraouf",
      "Rohit Gupta",
      "Kyungtae Han",
      "Ziran Wang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2024-10-20T04:36:19Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15281v4"
  },
  {
    "arxiv_id": "2410.15236v2",
    "entry_id": "http://arxiv.org/abs/2410.15236v2",
    "title": "Jailbreaking and Mitigation of Vulnerabilities in Large Language Models",
    "summary": "Large Language Models (LLMs) have transformed artificial intelligence by advancing natural language understanding and generation, enabling applications across fields beyond healthcare, software engineering, and conversational systems. Despite these advancements in the past few years, LLMs have shown considerable vulnerabilities, particularly to prompt injection and jailbreaking attacks. This review analyzes the state of research on these vulnerabilities and presents available defense strategies. We roughly categorize attack approaches into prompt-based, model-based, multimodal, and multilingual, covering techniques such as adversarial prompting, backdoor injections, and cross-modality exploits. We also review various defense mechanisms, including prompt filtering, transformation, alignment techniques, multi-agent defenses, and self-regulation, evaluating their strengths and shortcomings. We also discuss key metrics and benchmarks used to assess LLM safety and robustness, noting challenges like the quantification of attack success in interactive contexts and biases in existing datasets. Identifying current research gaps, we suggest future directions for resilient alignment strategies, advanced defenses against evolving attacks, automation of jailbreak detection, and consideration of ethical and societal impacts. This review emphasizes the need for continued research and cooperation within the AI community to enhance LLM security and ensure their safe deployment.",
    "authors": [
      "Benji Peng",
      "Keyu Chen",
      "Qian Niu",
      "Ziqian Bi",
      "Ming Liu",
      "Pohsun Feng",
      "Tianyang Wang",
      "Lawrence K. Q. Yan",
      "Yizhu Wen",
      "Yichao Zhang",
      "Caitlyn Heqi Yin"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-20T00:00:56Z",
    "pdf_url": "https://arxiv.org/pdf/2410.15236v2"
  },
  {
    "arxiv_id": "2410.14475v1",
    "entry_id": "http://arxiv.org/abs/2410.14475v1",
    "title": "Enhancing Cryptocurrency Market Forecasting: Advanced Machine Learning Techniques and Industrial Engineering Contributions",
    "summary": "Cryptocurrencies, as decentralized digital assets, have experienced rapid growth and adoption, with over 23,000 cryptocurrencies and a market capitalization nearing \\$1.1 trillion (about \\$3,400 per person in the US) as of 2023. This dynamic market presents significant opportunities and risks, highlighting the need for accurate price prediction models to manage volatility. This chapter comprehensively reviews machine learning (ML) techniques applied to cryptocurrency price prediction from 2014 to 2024. We explore various ML algorithms, including linear models, tree-based approaches, and advanced deep learning architectures such as transformers and large language models. Additionally, we examine the role of sentiment analysis in capturing market sentiment from textual data like social media posts and news articles to anticipate price fluctuations. With expertise in optimizing complex systems and processes, industrial engineers are pivotal in enhancing these models. They contribute by applying principles of process optimization, efficiency, and risk mitigation to improve computational performance and data management. This chapter highlights the evolving landscape of cryptocurrency price prediction, the integration of emerging technologies, and the significant role of industrial engineers in refining predictive models. By addressing current limitations and exploring future research directions, this chapter aims to advance the development of more accurate and robust prediction systems, supporting better-informed investment decisions and more stable market behavior.",
    "authors": [
      "Jannatun Nayeem Pinky",
      "Ramya Akula"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-10-18T14:00:44Z",
    "pdf_url": "https://arxiv.org/pdf/2410.14475v1"
  },
  {
    "arxiv_id": "2410.16322v2",
    "entry_id": "http://arxiv.org/abs/2410.16322v2",
    "title": "SouLLMate: An Application Enhancing Diverse Mental Health Support with Adaptive LLMs, Prompt Engineering, and RAG Techniques",
    "summary": "Mental health issues significantly impact individuals' daily lives, yet many do not receive the help they need even with available online resources. This study aims to provide diverse, accessible, stigma-free, personalized, and real-time mental health support through cutting-edge AI technologies. It makes the following contributions: (1) Conducting an extensive survey of recent mental health support methods to identify prevalent functionalities and unmet needs. (2) Introducing SouLLMate, an adaptive LLM-driven system that integrates LLM technologies, Chain, Retrieval-Augmented Generation (RAG), prompt engineering, and domain knowledge. This system offers advanced features such as Risk Detection and Proactive Guidance Dialogue, and utilizes RAG for personalized profile uploads and Conversational Information Extraction. (3) Developing novel evaluation approaches for preliminary assessments and risk detection via professionally annotated interview data and real-life suicide tendency data. (4) Proposing the Key Indicator Summarization (KIS), Proactive Questioning Strategy (PQS), and Stacked Multi-Model Reasoning (SMMR) methods to enhance model performance and usability through context-sensitive response adjustments, semantic coherence evaluations, and enhanced accuracy of long-context reasoning in language models. This study contributes to advancing mental health support technologies, potentially improving the accessibility and effectiveness of mental health care globally.",
    "authors": [
      "Qiming Guo",
      "Jinwen Tang",
      "Wenbo Sun",
      "Haoteng Tang",
      "Yi Shang",
      "Wenlu Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-10-17T22:04:32Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16322v2"
  },
  {
    "arxiv_id": "2410.13854v1",
    "entry_id": "http://arxiv.org/abs/2410.13854v1",
    "title": "Can MLLMs Understand the Deep Implication Behind Chinese Images?",
    "summary": "As the capabilities of Multimodal Large Language Models (MLLMs) continue to improve, the need for higher-order capability evaluation of MLLMs is increasing. However, there is a lack of work evaluating MLLM for higher-order perception and understanding of Chinese visual content. To fill the gap, we introduce the **C**hinese **I**mage **I**mplication understanding **Bench**mark, **CII-Bench**, which aims to assess the higher-order perception and understanding capabilities of MLLMs for Chinese images. CII-Bench stands out in several ways compared to existing benchmarks. Firstly, to ensure the authenticity of the Chinese context, images in CII-Bench are sourced from the Chinese Internet and manually reviewed, with corresponding answers also manually crafted. Additionally, CII-Bench incorporates images that represent Chinese traditional culture, such as famous Chinese traditional paintings, which can deeply reflect the model's understanding of Chinese traditional culture. Through extensive experiments on CII-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on CII-Bench. The highest accuracy of MLLMs attains 64.4%, where as human accuracy averages 78.2%, peaking at an impressive 81.0%. Subsequently, MLLMs perform worse on Chinese traditional culture images, suggesting limitations in their ability to understand high-level semantics and lack a deep knowledge base of Chinese traditional culture. Finally, it is observed that most models exhibit enhanced accuracy when image emotion hints are incorporated into the prompts. We believe that CII-Bench will enable MLLMs to gain a better understanding of Chinese semantics and Chinese-specific images, advancing the journey towards expert artificial general intelligence (AGI). Our project is publicly available at https://cii-bench.github.io/.",
    "authors": [
      "Chenhao Zhang",
      "Xi Feng",
      "Yuelin Bai",
      "Xinrun Du",
      "Jinchang Hou",
      "Kaixin Deng",
      "Guangzeng Han",
      "Qinrui Li",
      "Bingli Wang",
      "Jiaheng Liu",
      "Xingwei Qu",
      "Yifei Zhang",
      "Qixuan Zhao",
      "Yiming Liang",
      "Ziqiang Liu",
      "Feiteng Fang",
      "Min Yang",
      "Wenhao Huang",
      "Chenghua Lin",
      "Ge Zhang",
      "Shiwen Ni"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "published": "2024-10-17T17:59:24Z",
    "pdf_url": "https://arxiv.org/pdf/2410.13854v1"
  },
  {
    "arxiv_id": "2410.13248v2",
    "entry_id": "http://arxiv.org/abs/2410.13248v2",
    "title": "Disentangling Likes and Dislikes in Personalized Generative Explainable Recommendation",
    "summary": "Recent research on explainable recommendation generally frames the task as a standard text generation problem, and evaluates models simply based on the textual similarity between the predicted and ground-truth explanations. However, this approach fails to consider one crucial aspect of the systems: whether their outputs accurately reflect the users' (post-purchase) sentiments, i.e., whether and why they would like and/or dislike the recommended items. To shed light on this issue, we introduce new datasets and evaluation methods that focus on the users' sentiments. Specifically, we construct the datasets by explicitly extracting users' positive and negative opinions from their post-purchase reviews using an LLM, and propose to evaluate systems based on whether the generated explanations 1) align well with the users' sentiments, and 2) accurately identify both positive and negative opinions of users on the target items. We benchmark several recent models on our datasets and demonstrate that achieving strong performance on existing metrics does not ensure that the generated explanations align well with the users' sentiments. Lastly, we find that existing models can provide more sentiment-aware explanations when the users' (predicted) ratings for the target items are directly fed into the models as input. The datasets and benchmark implementation are available at: https://github.com/jchanxtarov/sent_xrec.",
    "authors": [
      "Ryotaro Shimizu",
      "Takashi Wada",
      "Yu Wang",
      "Johannes Kruse",
      "Sean O'Brien",
      "Sai HtaungKham",
      "Linxin Song",
      "Yuya Yoshikawa",
      "Yuki Saito",
      "Fugee Tsung",
      "Masayuki Goto",
      "Julian McAuley"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2024-10-17T06:15:00Z",
    "pdf_url": "https://arxiv.org/pdf/2410.13248v2"
  },
  {
    "arxiv_id": "2410.13915v1",
    "entry_id": "http://arxiv.org/abs/2410.13915v1",
    "title": "A Simulation System Towards Solving Societal-Scale Manipulation",
    "summary": "The rise of AI-driven manipulation poses significant risks to societal trust and democratic processes. Yet, studying these effects in real-world settings at scale is ethically and logistically impractical, highlighting a need for simulation tools that can model these dynamics in controlled settings to enable experimentation with possible defenses. We present a simulation environment designed to address this. We elaborate upon the Concordia framework that simulates offline, `real life' activity by adding online interactions to the simulation through social media with the integration of a Mastodon server. We improve simulation efficiency and information flow, and add a set of measurement tools, particularly longitudinal surveys. We demonstrate the simulator with a tailored example in which we track agents' political positions and show how partisan manipulation of agents can affect election results.",
    "authors": [
      "Maximilian Puelma Touzel",
      "Sneheel Sarangi",
      "Austin Welch",
      "Gayatri Krishnakumar",
      "Dan Zhao",
      "Zachary Yang",
      "Hao Yu",
      "Ethan Kosak-Hine",
      "Tom Gibbs",
      "Andreea Musulan",
      "Camille Thibault",
      "Busra Tugce Gurbuz",
      "Reihaneh Rabbany",
      "Jean-François Godbout",
      "Kellin Pelrine"
    ],
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-10-17T03:16:24Z",
    "pdf_url": "https://arxiv.org/pdf/2410.13915v1"
  },
  {
    "arxiv_id": "2410.13037v1",
    "entry_id": "http://arxiv.org/abs/2410.13037v1",
    "title": "LFOSum: Summarizing Long-form Opinions with Large Language Models",
    "summary": "Online reviews play a pivotal role in influencing consumer decisions across various domains, from purchasing products to selecting hotels or restaurants. However, the sheer volume of reviews -- often containing repetitive or irrelevant content -- leads to information overload, making it challenging for users to extract meaningful insights. Traditional opinion summarization models face challenges in handling long inputs and large volumes of reviews, while newer Large Language Model (LLM) approaches often fail to generate accurate and faithful summaries. To address those challenges, this paper introduces (1) a new dataset of long-form user reviews, each entity comprising over a thousand reviews, (2) two training-free LLM-based summarization approaches that scale to long inputs, and (3) automatic evaluation metrics. Our dataset of user reviews is paired with in-depth and unbiased critical summaries by domain experts, serving as a reference for evaluation. Additionally, our novel reference-free evaluation metrics provide a more granular, context-sensitive assessment of summary faithfulness. We benchmark several open-source and closed-source LLMs using our methods. Our evaluation reveals that LLMs still face challenges in balancing sentiment and format adherence in long-form summaries, though open-source models can narrow the gap when relevant information is retrieved in a focused manner.",
    "authors": [
      "Mir Tafseer Nayeem",
      "Davood Rafiei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.IR"
    ],
    "published": "2024-10-16T20:52:39Z",
    "pdf_url": "https://arxiv.org/pdf/2410.13037v1"
  },
  {
    "arxiv_id": "2410.12893v3",
    "entry_id": "http://arxiv.org/abs/2410.12893v3",
    "title": "MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended Question Generation",
    "summary": "Automatic question generation is a critical task that involves evaluating question quality by considering factors such as engagement, pedagogical value, and the ability to stimulate critical thinking. These aspects require human-like understanding and judgment, which automated systems currently lack. However, human evaluations are costly and impractical for large-scale samples of generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM Iterative Review and Response for Optimized Rating), which leverages large language models (LLMs) to automate the evaluation process for questions generated by automated question generation systems. We experimented with several state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We observed that the scores of human evaluation metrics, namely relevance, appropriateness, novelty, complexity, and grammaticality, improved when using the feedback-based approach called MIRROR, tending to be closer to the human baseline scores. Furthermore, we observed that Pearson's correlation coefficient between GPT-4 and human experts improved when using our proposed feedback-based approach, MIRROR, compared to direct prompting for evaluation. Error analysis shows that our proposed approach, MIRROR, significantly helps to improve relevance and appropriateness.",
    "authors": [
      "Aniket Deroy",
      "Subhankar Maity",
      "Sudeshna Sarkar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-16T12:24:42Z",
    "pdf_url": "https://arxiv.org/pdf/2410.12893v3"
  },
  {
    "arxiv_id": "2411.00005v3",
    "entry_id": "http://arxiv.org/abs/2411.00005v3",
    "title": "Mastering the Craft of Data Synthesis for CodeLLMs",
    "summary": "Large language models (LLMs) have shown impressive performance in \\emph{code} understanding and generation, making coding tasks a key focus for researchers due to their practical applications and value as a testbed for LLM evaluation. Data synthesis and filtering techniques have been widely adopted and shown to be highly effective in this context. In this paper, we present a focused survey and taxonomy of these techniques, emphasizing recent advancements. We highlight key challenges, explore future research directions, and offer practical guidance for new researchers entering the field.",
    "authors": [
      "Meng Chen",
      "Philip Arthur",
      "Qianyu Feng",
      "Cong Duy Vu Hoang",
      "Yu-Heng Hong",
      "Mahdi Kazemi Moghaddam",
      "Omid Nezami",
      "Thien Nguyen",
      "Gioacchino Tangari",
      "Duy Vu",
      "Thanh Vu",
      "Mark Johnson",
      "Krishnaram Kenthapadi",
      "Don Dharmasiri",
      "Long Duong",
      "Yuan-Fang Li"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-10-16T11:57:14Z",
    "pdf_url": "https://arxiv.org/pdf/2411.00005v3"
  },
  {
    "arxiv_id": "2410.18125v3",
    "entry_id": "http://arxiv.org/abs/2410.18125v3",
    "title": "Towards Edge General Intelligence via Large Language Models: Opportunities and Challenges",
    "summary": "Edge Intelligence (EI) has been instrumental in delivering real-time, localized services by leveraging the computational capabilities of edge networks. The integration of Large Language Models (LLMs) empowers EI to evolve into the next stage: Edge General Intelligence (EGI), enabling more adaptive and versatile applications that require advanced understanding and reasoning capabilities. However, systematic exploration in this area remains insufficient. This survey delineates the distinctions between EGI and traditional EI, categorizing LLM-empowered EGI into three conceptual systems: centralized, hybrid, and decentralized. For each system, we detail the framework designs and review existing implementations. Furthermore, we evaluate the performance and throughput of various Small Language Models (SLMs) that are more suitable for development on edge devices. This survey provides researchers with a comprehensive vision of EGI, offering insights into its vast potential and establishing a foundation for future advancements in this rapidly evolving field.",
    "authors": [
      "Handi Chen",
      "Weipeng Deng",
      "Shuo Yang",
      "Jinfeng Xu",
      "Zhihan Jiang",
      "Edith C. H. Ngai",
      "Jiangchuan Liu",
      "Xue Liu"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.NI"
    ],
    "published": "2024-10-16T07:45:31Z",
    "pdf_url": "https://arxiv.org/pdf/2410.18125v3"
  },
  {
    "arxiv_id": "2410.12126v2",
    "entry_id": "http://arxiv.org/abs/2410.12126v2",
    "title": "What Do LLMs Need to Understand Graphs: A Survey of Parametric Representation of Graphs",
    "summary": "Graphs, as a relational data structure, have been widely used for various application scenarios, like molecule design and recommender systems. Recently, large language models (LLMs) are reorganizing in the AI community for their expected reasoning and inference abilities. Making LLMs understand graph-based relational data has great potential, including but not limited to (1) distillate external knowledge base for eliminating hallucination and breaking the context window limit for LLMs' inference during the retrieval augmentation generation process; (2) taking graph data as the input and directly solve the graph-based research tasks like protein design and drug discovery. However, inputting the entire graph data to LLMs is not practical due to its complex topological structure, data size, and the lack of effective and efficient semantic graph representations. A natural question arises: Is there a kind of graph representation that can be described by natural language for LLM's understanding and is also easy to require to serve as the raw input for LLMs? Based on statistical computation, graph laws pre-define a set of parameters (e.g., degree, time, diameter) and identifie their relationships and values by observing the topological distribution of plenty of real-world graph data. We believe this kind of parametric representation of graphs, graph laws, can be a solution for making LLMs understand graph data as the input. In this survey, we first review the previous study of graph laws from multiple perspectives, i.e., macroscope and microscope of graphs, low-order and high-order graphs, static and dynamic graphs, different observation spaces, and newly proposed graph parameters. After we review various real-world applications benefiting from the guidance of graph laws, we conclude the paper with current challenges and future research directions.",
    "authors": [
      "Dongqi Fu",
      "Liri Fang",
      "Zihao Li",
      "Hanghang Tong",
      "Vetle I. Torvik",
      "Jingrui He"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "published": "2024-10-16T00:01:31Z",
    "pdf_url": "https://arxiv.org/pdf/2410.12126v2"
  },
  {
    "arxiv_id": "2410.12071v2",
    "entry_id": "http://arxiv.org/abs/2410.12071v2",
    "title": "Beyond the Comfort Zone: Emerging Solutions to Overcome Challenges in Integrating LLMs into Software Products",
    "summary": "Large Language Models (LLMs) are increasingly embedded into software products across diverse industries, enhancing user experiences, but at the same time introducing numerous challenges for developers. Unique characteristics of LLMs force developers, who are accustomed to traditional software development and evaluation, out of their comfort zones as the LLM components shatter standard assumptions about software systems. This study explores the emerging solutions that software developers are adopting to navigate the encountered challenges. Leveraging a mixed-method research, including 26 interviews and a survey with 332 responses, the study identifies 19 emerging solutions regarding quality assurance that practitioners across several product teams at Microsoft are exploring. The findings provide valuable insights that can guide the development and evaluation of LLM-based products more broadly in the face of these challenges.",
    "authors": [
      "Nadia Nahar",
      "Christian Kästner",
      "Jenna Butler",
      "Chris Parnin",
      "Thomas Zimmermann",
      "Christian Bird"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2024-10-15T21:11:10Z",
    "pdf_url": "https://arxiv.org/pdf/2410.12071v2"
  },
  {
    "arxiv_id": "2410.12034v1",
    "entry_id": "http://arxiv.org/abs/2410.12034v1",
    "title": "A Survey on Deep Tabular Learning",
    "summary": "Tabular data, widely used in industries like healthcare, finance, and transportation, presents unique challenges for deep learning due to its heterogeneous nature and lack of spatial structure. This survey reviews the evolution of deep learning models for tabular data, from early fully connected networks (FCNs) to advanced architectures like TabNet, SAINT, TabTranSELU, and MambaNet. These models incorporate attention mechanisms, feature embeddings, and hybrid architectures to address tabular data complexities. TabNet uses sequential attention for instance-wise feature selection, improving interpretability, while SAINT combines self-attention and intersample attention to capture complex interactions across features and data points, both advancing scalability and reducing computational overhead. Hybrid architectures such as TabTransformer and FT-Transformer integrate attention mechanisms with multi-layer perceptrons (MLPs) to handle categorical and numerical data, with FT-Transformer adapting transformers for tabular datasets. Research continues to balance performance and efficiency for large datasets. Graph-based models like GNN4TDL and GANDALF combine neural networks with decision trees or graph structures, enhancing feature representation and mitigating overfitting in small datasets through advanced regularization techniques. Diffusion-based models like the Tabular Denoising Diffusion Probabilistic Model (TabDDPM) generate synthetic data to address data scarcity, improving model robustness. Similarly, models like TabPFN and Ptab leverage pre-trained language models, incorporating transfer learning and self-supervised techniques into tabular tasks. This survey highlights key advancements and outlines future research directions on scalability, generalization, and interpretability in diverse tabular data applications.",
    "authors": [
      "Shriyank Somvanshi",
      "Subasish Das",
      "Syed Aaqib Javed",
      "Gian Antariksa",
      "Ahmed Hossain"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-10-15T20:08:08Z",
    "pdf_url": "https://arxiv.org/pdf/2410.12034v1"
  },
  {
    "arxiv_id": "2410.11381v1",
    "entry_id": "http://arxiv.org/abs/2410.11381v1",
    "title": "Survey and Evaluation of Converging Architecture in LLMs based on Footsteps of Operations",
    "summary": "The advent of the Attention mechanism and Transformer architecture enables contextually natural text generation and compresses the burden of processing entire source information into singular vectors. Based on these two main ideas, model sizes gradually increases to accommodate more precise and comprehensive information, leading to the current state-of-the-art LLMs being very large, with parameters around 70 billion. As the model sizes are growing, the demand for substantial storage and computational capacity increases. This leads to the development of high-bandwidth memory and accelerators, as well as a variety of model architectures designed to meet these requirements. We note that LLM architectures have increasingly converged. This paper analyzes how these converged architectures perform in terms of layer configurations, operational mechanisms, and model sizes, considering various hyperparameter settings. In this paper, we conduct a concise survey of the history of LLMs by tracing the evolution of their operational improvements. Furthermore, we summarize the performance trends of LLMs under various hyperparameter settings using the RTX 6000, which features the state-of-the-art Ada Lovelace architecture. We conclude that even the same model can exhibit different behaviors depending on the hyperparameters or whether it is deployed in server or edge environments.",
    "authors": [
      "Seongho Kim",
      "Jihyun Moon",
      "Juntaek Oh",
      "Insu Choi",
      "Joon-Sung Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-10-15T08:19:24Z",
    "pdf_url": "https://arxiv.org/pdf/2410.11381v1"
  },
  {
    "arxiv_id": "2410.11291v2",
    "entry_id": "http://arxiv.org/abs/2410.11291v2",
    "title": "Enhancing Assamese NLP Capabilities: Introducing a Centralized Dataset Repository",
    "summary": "This paper introduces a centralized, open-source dataset repository designed to advance NLP and NMT for Assamese, a low-resource language. The repository, available at GitHub, supports various tasks like sentiment analysis, named entity recognition, and machine translation by providing both pre-training and fine-tuning corpora. We review existing datasets, highlighting the need for standardized resources in Assamese NLP, and discuss potential applications in AI-driven research, such as LLMs, OCR, and chatbots. While promising, challenges like data scarcity and linguistic diversity remain. The repository aims to foster collaboration and innovation, promoting Assamese language research in the digital age.",
    "authors": [
      "S. Tamang",
      "D. J. Bora"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-15T05:26:57Z",
    "pdf_url": "https://arxiv.org/pdf/2410.11291v2"
  },
  {
    "arxiv_id": "2410.10596v4",
    "entry_id": "http://arxiv.org/abs/2410.10596v4",
    "title": "Overcoming classic challenges for artificial neural networks by providing incentives and practice",
    "summary": "Since the earliest proposals for artificial neural network (ANN) models of the mind and brain, critics have pointed out key weaknesses in these models compared to human cognitive abilities. Here we review recent work that uses metalearning to overcome several classic challenges, which we characterize as addressing the Problem of Incentive and Practice -- that is, providing machines with both incentives to improve specific skills and opportunities to practice those skills. This explicit optimization contrasts with more conventional approaches that hope the desired behaviour will emerge through optimizing related but different objectives. We review applications of this principle to addressing four classic challenges for ANNs: systematic generalization, catastrophic forgetting, few-shot learning and multi-step reasoning. We also discuss how large language models incorporate key aspects of this metalearning framework (namely, sequence prediction with feedback trained on diverse data), which helps to explain some of their successes on these classic challenges. Finally, we discuss the prospects for understanding aspects of human development through this framework, and whether natural environments provide the right incentives and practice for learning how to make challenging generalizations.",
    "authors": [
      "Kazuki Irie",
      "Brenden M. Lake"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "published": "2024-10-14T15:07:37Z",
    "pdf_url": "https://arxiv.org/pdf/2410.10596v4"
  },
  {
    "arxiv_id": "2410.10489v1",
    "entry_id": "http://arxiv.org/abs/2410.10489v1",
    "title": "Cultural Fidelity in Large-Language Models: An Evaluation of Online Language Resources as a Driver of Model Performance in Value Representation",
    "summary": "The training data for LLMs embeds societal values, increasing their familiarity with the language's culture. Our analysis found that 44% of the variance in the ability of GPT-4o to reflect the societal values of a country, as measured by the World Values Survey, correlates with the availability of digital resources in that language. Notably, the error rate was more than five times higher for the languages of the lowest resource compared to the languages of the highest resource. For GPT-4-turbo, this correlation rose to 72%, suggesting efforts to improve the familiarity with the non-English language beyond the web-scraped data. Our study developed one of the largest and most robust datasets in this topic area with 21 country-language pairs, each of which contain 94 survey questions verified by native speakers. Our results highlight the link between LLM performance and digital data availability in target languages. Weaker performance in low-resource languages, especially prominent in the Global South, may worsen digital divides. We discuss strategies proposed to address this, including developing multilingual LLMs from the ground up and enhancing fine-tuning on diverse linguistic datasets, as seen in African language initiatives.",
    "authors": [
      "Sharif Kazemi",
      "Gloria Gerhardt",
      "Jonty Katz",
      "Caroline Ida Kuria",
      "Estelle Pan",
      "Umang Prabhakar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-14T13:33:00Z",
    "pdf_url": "https://arxiv.org/pdf/2410.10489v1"
  },
  {
    "arxiv_id": "2410.09948v1",
    "entry_id": "http://arxiv.org/abs/2410.09948v1",
    "title": "State of NLP in Kenya: A Survey",
    "summary": "Kenya, known for its linguistic diversity, faces unique challenges and promising opportunities in advancing Natural Language Processing (NLP) technologies, particularly for its underrepresented indigenous languages. This survey provides a detailed assessment of the current state of NLP in Kenya, emphasizing ongoing efforts in dataset creation, machine translation, sentiment analysis, and speech recognition for local dialects such as Kiswahili, Dholuo, Kikuyu, and Luhya. Despite these advancements, the development of NLP in Kenya remains constrained by limited resources and tools, resulting in the underrepresentation of most indigenous languages in digital spaces. This paper uncovers significant gaps by critically evaluating the available datasets and existing NLP models, most notably the need for large-scale language models and the insufficient digital representation of Indigenous languages. We also analyze key NLP applications: machine translation, information retrieval, and sentiment analysis-examining how they are tailored to address local linguistic needs. Furthermore, the paper explores the governance, policies, and regulations shaping the future of AI and NLP in Kenya and proposes a strategic roadmap to guide future research and development efforts. Our goal is to provide a foundation for accelerating the growth of NLP technologies that meet Kenya's diverse linguistic demands.",
    "authors": [
      "Cynthia Jayne Amol",
      "Everlyn Asiko Chimoto",
      "Rose Delilah Gesicho",
      "Antony M. Gitau",
      "Naome A. Etori",
      "Caringtone Kinyanjui",
      "Steven Ndung'u",
      "Lawrence Moruye",
      "Samson Otieno Ooko",
      "Kavengi Kitonga",
      "Brian Muhia",
      "Catherine Gitau",
      "Antony Ndolo",
      "Lilian D. A. Wanzare",
      "Albert Njoroge Kahira",
      "Ronald Tombe"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-13T18:08:24Z",
    "pdf_url": "https://arxiv.org/pdf/2410.09948v1"
  },
  {
    "arxiv_id": "2410.09770v1",
    "entry_id": "http://arxiv.org/abs/2410.09770v1",
    "title": "'Quis custodiet ipsos custodes?' Who will watch the watchmen? On Detecting AI-generated peer-reviews",
    "summary": "The integrity of the peer-review process is vital for maintaining scientific rigor and trust within the academic community. With the steady increase in the usage of large language models (LLMs) like ChatGPT in academic writing, there is a growing concern that AI-generated texts could compromise scientific publishing, including peer-reviews. Previous works have focused on generic AI-generated text detection or have presented an approach for estimating the fraction of peer-reviews that can be AI-generated. Our focus here is to solve a real-world problem by assisting the editor or chair in determining whether a review is written by ChatGPT or not. To address this, we introduce the Term Frequency (TF) model, which posits that AI often repeats tokens, and the Review Regeneration (RR) model, which is based on the idea that ChatGPT generates similar outputs upon re-prompting. We stress test these detectors against token attack and paraphrasing. Finally, we propose an effective defensive strategy to reduce the effect of paraphrasing on our models. Our findings suggest both our proposed methods perform better than the other AI text detectors. Our RR model is more robust, although our TF model performs better than the RR model without any attacks. We make our code, dataset, and model public.",
    "authors": [
      "Sandeep Kumar",
      "Mohit Sahu",
      "Vardhan Gacche",
      "Tirthankar Ghosal",
      "Asif Ekbal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.LG"
    ],
    "published": "2024-10-13T08:06:08Z",
    "pdf_url": "https://arxiv.org/pdf/2410.09770v1"
  },
  {
    "arxiv_id": "2410.09362v1",
    "entry_id": "http://arxiv.org/abs/2410.09362v1",
    "title": "SeRA: Self-Reviewing and Alignment of Large Language Models using Implicit Reward Margins",
    "summary": "Direct alignment algorithms (DAAs), such as direct preference optimization (DPO), have become popular alternatives for Reinforcement Learning from Human Feedback (RLHF) due to their simplicity, efficiency, and stability. However, the preferences used in DAAs are usually collected before the alignment training begins and remain unchanged (off-policy). This can lead to two problems where the policy model (1) picks up on spurious correlations in the dataset (as opposed to learning the intended alignment expressed in the human preference labels), and (2) overfits to feedback on off-policy trajectories that have less likelihood of being generated by an updated policy model. To address these issues, we introduce Self-Reviewing and Alignment (SeRA), a cost-efficient and effective method that can be readily combined with existing DAAs. SeRA comprises of two components: (1) sample selection using implicit reward margins, which helps alleviate over-fitting to some undesired features, and (2) preference bootstrapping using implicit rewards to augment preference data with updated policy models in a cost-efficient manner. Extensive experimentation, including some on instruction-following tasks, demonstrate the effectiveness and generality of SeRA in training LLMs on offline preference datasets with DAAs.",
    "authors": [
      "Jongwoo Ko",
      "Saket Dingliwal",
      "Bhavana Ganesh",
      "Sailik Sengupta",
      "Sravan Bodapati",
      "Aram Galstyan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-10-12T04:17:28Z",
    "pdf_url": "https://arxiv.org/pdf/2410.09362v1"
  },
  {
    "arxiv_id": "2410.12860v1",
    "entry_id": "http://arxiv.org/abs/2410.12860v1",
    "title": "LLMD: A Large Language Model for Interpreting Longitudinal Medical Records",
    "summary": "We introduce LLMD, a large language model designed to analyze a patient's medical history based on their medical records. Along with domain knowledge, LLMD is trained on a large corpus of records collected over time and across facilities, as well as tasks and labels that make nuanced connections among them. This approach is critical to an accurate picture of patient health, and has distinctive advantages over models trained on knowledge alone, unlabeled records, structured EHR data, or records from a single health system.\n  The recipe for LLMD continues pretraining a foundational model on both domain knowledge and the contents of millions of records. These span an average of 10 years of care and as many as 140 care sites per patient. LLMD is then instruction fine-tuned on structuring and abstraction tasks. The former jointly identify and normalize document metadata, provenance information, clinical named-entities, and ontology mappings, while the latter roll these into higher-level representations, such a continuous era of time a patient was on a medication. LLMD is deployed within a layered validation system that includes continual random audits and review by experts, e.g. based on uncertainty, disease-specific rules, or use-case.\n  LLMD exhibits large gains over both more-powerful generalized models and domain-specific models. On medical knowledge benchmarks, LLMD-8B achieves state of the art accuracy on PubMedQA text responses, besting orders-of-magnitude larger models. On production tasks, we show that LLMD significantly outperforms all other models evaluated, and among alternatives, large general purpose LLMs like GPT-4o are more accurate than models emphasizing medical knowledge. We find strong evidence that accuracy on today's medical benchmarks is not the most significant factor when analyzing real-world patient data, an insight with implications for future medical LLMs.'",
    "authors": [
      "Robert Porter",
      "Adam Diehl",
      "Benjamin Pastel",
      "J. Henry Hinnefeld",
      "Lawson Nerenberg",
      "Pye Maung",
      "Sebastien Kerbrat",
      "Gillian Hanson",
      "Troy Astorino",
      "Stephen J. Tarsa"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-11T20:55:51Z",
    "pdf_url": "https://arxiv.org/pdf/2410.12860v1"
  },
  {
    "arxiv_id": "2410.09186v3",
    "entry_id": "http://arxiv.org/abs/2410.09186v3",
    "title": "AI Learning Algorithms: Deep Learning, Hybrid Models, and Large-Scale Model Integration",
    "summary": "In this paper, we discuss learning algorithms and their importance in different types of applications which includes training to identify important patterns and features in a straightforward, easy-to-understand manner. We will review the main concepts of artificial intelligence (AI), machine learning (ML), deep learning (DL), and hybrid models. Some important subsets of Machine Learning algorithms such as supervised, unsupervised, and reinforcement learning are also discussed in this paper. These techniques can be used for some important tasks like prediction, classification, and segmentation. Convolutional Neural Networks (CNNs) are used for image and video processing and many more applications. We dive into the architecture of CNNs and how to integrate CNNs with ML algorithms to build hybrid models. This paper explores the vulnerability of learning algorithms to noise, leading to misclassification. We further discuss the integration of learning algorithms with Large Language Models (LLM) to generate coherent responses applicable to many domains such as healthcare, marketing, and finance by learning important patterns from large volumes of data. Furthermore, we discuss the next generation of learning algorithms and how we may have an unified Adaptive and Dynamic Network to perform important tasks. Overall, this article provides brief overview of learning algorithms, exploring their current state, applications and future direction.",
    "authors": [
      "Noorbakhsh Amiri Golilarz",
      "Elias Hossain",
      "Abdoljalil Addeh",
      "Keyan Alexander Rahimi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-10-11T18:39:25Z",
    "pdf_url": "https://arxiv.org/pdf/2410.09186v3"
  },
  {
    "arxiv_id": "2410.09012v2",
    "entry_id": "http://arxiv.org/abs/2410.09012v2",
    "title": "Software Engineering and Foundation Models: Insights from Industry Blogs Using a Jury of Foundation Models",
    "summary": "Foundation models (FMs) such as large language models (LLMs) have significantly impacted many fields, including software engineering (SE). The interaction between SE and FMs has led to the integration of FMs into SE practices (FM4SE) and the application of SE methodologies to FMs (SE4FM). While several literature surveys exist on academic contributions to these trends, we are the first to provide a practitioner's view. We analyze 155 FM4SE and 997 SE4FM blog posts from leading technology companies, leveraging an FM-powered surveying approach to systematically label and summarize the discussed activities and tasks. We observed that while code generation is the most prominent FM4SE task, FMs are leveraged for many other SE activities such as code understanding, summarization, and API recommendation. The majority of blog posts on SE4FM are about model deployment & operation, and system architecture & orchestration. Although the emphasis is on cloud deployments, there is a growing interest in compressing FMs and deploying them on smaller devices such as edge or mobile devices. We outline eight future research directions inspired by our gained insights, aiming to bridge the gap between academic findings and real-world applications. Our study not only enriches the body of knowledge on practical applications of FM4SE and SE4FM but also demonstrates the utility of FMs as a powerful and efficient approach in conducting literature surveys within technical and grey literature domains. Our dataset, results, code and used prompts can be found in our online replication package at https://github.com/SAILResearch/fmse-blogs.",
    "authors": [
      "Hao Li",
      "Cor-Paul Bezemer",
      "Ahmed E. Hassan"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-10-11T17:27:04Z",
    "pdf_url": "https://arxiv.org/pdf/2410.09012v2"
  },
  {
    "arxiv_id": "2410.14716v3",
    "entry_id": "http://arxiv.org/abs/2410.14716v3",
    "title": "A Systematic Survey on Large Language Models for Algorithm Design",
    "summary": "Algorithm Design (AD) is crucial for effective problem-solving across various domains. The advent of Large Language Models (LLMs) has notably enhanced the automation and innovation within this field, offering new perspectives and promising solutions. Over the past three years, the integration of LLMs into AD (LLM4AD) has seen substantial progress, with applications spanning optimization, machine learning, mathematical reasoning, and scientific discovery. Given the rapid advancements and expanding scope of this field, a systematic review is both timely and necessary. This paper provides a systematic review of LLM4AD. First, we offer an overview and summary of existing studies. Then, we introduce a taxonomy and review the literature across four dimensions: the roles of LLMs, search methods, prompt methods, and application domains with a discussion of potential and achievements of LLMs in AD. Finally, we identify current challenges and highlight several promising directions for future research.",
    "authors": [
      "Fei Liu",
      "Yiming Yao",
      "Ping Guo",
      "Zhiyuan Yang",
      "Zhe Zhao",
      "Xi Lin",
      "Xialiang Tong",
      "Mingxuan Yuan",
      "Zhichao Lu",
      "Zhenkun Wang",
      "Qingfu Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-10-11T13:17:19Z",
    "pdf_url": "https://arxiv.org/pdf/2410.14716v3"
  },
  {
    "arxiv_id": "2410.08406v1",
    "entry_id": "http://arxiv.org/abs/2410.08406v1",
    "title": "Promptly Yours? A Human Subject Study on Prompt Inference in AI-Generated Art",
    "summary": "The emerging field of AI-generated art has witnessed the rise of prompt marketplaces, where creators can purchase, sell, or share prompts for generating unique artworks. These marketplaces often assert ownership over prompts, claiming them as intellectual property. This paper investigates whether concealed prompts sold on prompt marketplaces can be considered as secure intellectual property, given that humans and AI tools may be able to approximately infer the prompts based on publicly advertised sample images accompanying each prompt on sale. Specifically, our survey aims to assess (i) how accurately can humans infer the original prompt solely by examining an AI-generated image, with the goal of generating images similar to the original image, and (ii) the possibility of improving upon individual human and AI prompt inferences by crafting human-AI combined prompts with the help of a large language model. Although previous research has explored the use of AI and machine learning to infer (and also protect against) prompt inference, we are the first to include humans in the loop. Our findings indicate that while humans and human-AI collaborations can infer prompts and generate similar images with high accuracy, they are not as successful as using the original prompt.",
    "authors": [
      "Khoi Trinh",
      "Joseph Spracklen",
      "Raveen Wijewickrama",
      "Bimal Viswanath",
      "Murtuza Jadliwala",
      "Anindya Maiti"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-10-10T22:41:13Z",
    "pdf_url": "https://arxiv.org/pdf/2410.08406v1"
  },
  {
    "arxiv_id": "2410.07994v3",
    "entry_id": "http://arxiv.org/abs/2410.07994v3",
    "title": "Neuroplastic Expansion in Deep Reinforcement Learning",
    "summary": "The loss of plasticity in learning agents, analogous to the solidification of neural pathways in biological brains, significantly impedes learning and adaptation in reinforcement learning due to its non-stationary nature. To address this fundamental challenge, we propose a novel approach, {\\it Neuroplastic Expansion} (NE), inspired by cortical expansion in cognitive science. NE maintains learnability and adaptability throughout the entire training process by dynamically growing the network from a smaller initial size to its full dimension. Our method is designed with three key components: (\\textit{1}) elastic topology generation based on potential gradients, (\\textit{2}) dormant neuron pruning to optimize network expressivity, and (\\textit{3}) neuron consolidation via experience review to strike a balance in the plasticity-stability dilemma. Extensive experiments demonstrate that NE effectively mitigates plasticity loss and outperforms state-of-the-art methods across various tasks in MuJoCo and DeepMind Control Suite environments. NE enables more adaptive learning in complex, dynamic environments, which represents a crucial step towards transitioning deep reinforcement learning from static, one-time training paradigms to more flexible, continually adapting models.",
    "authors": [
      "Jiashun Liu",
      "Johan Obando-Ceron",
      "Aaron Courville",
      "Ling Pan"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-10-10T14:51:14Z",
    "pdf_url": "https://arxiv.org/pdf/2410.07994v3"
  },
  {
    "arxiv_id": "2410.07959v2",
    "entry_id": "http://arxiv.org/abs/2410.07959v2",
    "title": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking Suite for the EU Artificial Intelligence Act",
    "summary": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards responsible AI development, but lacks clear technical interpretation, making it difficult to assess models' compliance. This work presents COMPL-AI, a comprehensive framework consisting of (i) the first technical interpretation of the EU AI Act, translating its broad regulatory requirements into measurable technical requirements, with the focus on large language models (LLMs), and (ii) an open-source Act-centered benchmarking suite, based on thorough surveying and implementation of state-of-the-art LLM benchmarks. By evaluating 12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in existing models and benchmarks, particularly in areas like robustness, safety, diversity, and fairness. This work highlights the need for a shift in focus towards these aspects, encouraging balanced development of LLMs and more comprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the first time demonstrates the possibilities and difficulties of bringing the Act's obligations to a more concrete, technical level. As such, our work can serve as a useful first step towards having actionable recommendations for model providers, and contributes to ongoing efforts of the EU to enable application of the Act, such as the drafting of the GPAI Code of Practice.",
    "authors": [
      "Philipp Guldimann",
      "Alexander Spiridonov",
      "Robin Staab",
      "Nikola Jovanović",
      "Mark Vero",
      "Velko Vechev",
      "Anna-Maria Gueorguieva",
      "Mislav Balunović",
      "Nikola Konstantinov",
      "Pavol Bielik",
      "Petar Tsankov",
      "Martin Vechev"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-10-10T14:23:51Z",
    "pdf_url": "https://arxiv.org/pdf/2410.07959v2"
  },
  {
    "arxiv_id": "2410.07880v1",
    "entry_id": "http://arxiv.org/abs/2410.07880v1",
    "title": "Unsupervised Data Validation Methods for Efficient Model Training",
    "summary": "This paper investigates the challenges and potential solutions for improving machine learning systems for low-resource languages. State-of-the-art models in natural language processing (NLP), text-to-speech (TTS), speech-to-text (STT), and vision-language models (VLM) rely heavily on large datasets, which are often unavailable for low-resource languages. This research explores key areas such as defining \"quality data,\" developing methods for generating appropriate data and enhancing accessibility to model training. A comprehensive review of current methodologies, including data augmentation, multilingual transfer learning, synthetic data generation, and data selection techniques, highlights both advancements and limitations. Several open research questions are identified, providing a framework for future studies aimed at optimizing data utilization, reducing the required data quantity, and maintaining high-quality model performance. By addressing these challenges, the paper aims to make advanced machine learning models more accessible for low-resource languages, enhancing their utility and impact across various sectors.",
    "authors": [
      "Yurii Paniv"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-10-10T13:00:53Z",
    "pdf_url": "https://arxiv.org/pdf/2410.07880v1"
  },
  {
    "arxiv_id": "2410.19744v1",
    "entry_id": "http://arxiv.org/abs/2410.19744v1",
    "title": "Towards Next-Generation LLM-based Recommender Systems: A Survey and Beyond",
    "summary": "Large language models (LLMs) have not only revolutionized the field of natural language processing (NLP) but also have the potential to bring a paradigm shift in many other fields due to their remarkable abilities of language understanding, as well as impressive generalization capabilities and reasoning skills. As a result, recent studies have actively attempted to harness the power of LLMs to improve recommender systems, and it is imperative to thoroughly review the recent advances and challenges of LLM-based recommender systems. Unlike existing work, this survey does not merely analyze the classifications of LLM-based recommendation systems according to the technical framework of LLMs. Instead, it investigates how LLMs can better serve recommendation tasks from the perspective of the recommender system community, thus enhancing the integration of large language models into the research of recommender system and its practical application. In addition, the long-standing gap between academic research and industrial applications related to recommender systems has not been well discussed, especially in the era of large language models. In this review, we introduce a novel taxonomy that originates from the intrinsic essence of recommendation, delving into the application of large language model-based recommendation systems and their industrial implementation. Specifically, we propose a three-tier structure that more accurately reflects the developmental progression of recommendation systems from research to practical implementation, including representing and understanding, scheming and utilizing, and industrial deployment. Furthermore, we discuss critical challenges and opportunities in this emerging field. A more up-to-date version of the papers is maintained at: https://github.com/jindongli-Ai/Next-Generation-LLM-based-Recommender-Systems-Survey.",
    "authors": [
      "Qi Wang",
      "Jindong Li",
      "Shiqi Wang",
      "Qianli Xing",
      "Runliang Niu",
      "He Kong",
      "Rui Li",
      "Guodong Long",
      "Yi Chang",
      "Chengqi Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-10-10T08:22:04Z",
    "pdf_url": "https://arxiv.org/pdf/2410.19744v1"
  },
  {
    "arxiv_id": "2410.12843v1",
    "entry_id": "http://arxiv.org/abs/2410.12843v1",
    "title": "Exploring Prompt Engineering: A Systematic Review with SWOT Analysis",
    "summary": "In this paper, we conduct a comprehensive SWOT analysis of prompt engineering techniques within the realm of Large Language Models (LLMs). Emphasizing linguistic principles, we examine various techniques to identify their strengths, weaknesses, opportunities, and threats. Our findings provide insights into enhancing AI interactions and improving language model comprehension of human prompts. The analysis covers techniques including template-based approaches and fine-tuning, addressing the problems and challenges associated with each. The conclusion offers future research directions aimed at advancing the effectiveness of prompt engineering in optimizing human-machine communication.",
    "authors": [
      "Aditi Singh",
      "Abul Ehtesham",
      "Gaurav Kumar Gupta",
      "Nikhil Kumar Chatta",
      "Saket Kumar",
      "Tala Talaei Khoei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-09T19:48:35Z",
    "pdf_url": "https://arxiv.org/pdf/2410.12843v1"
  },
  {
    "arxiv_id": "2410.07076v6",
    "entry_id": "http://arxiv.org/abs/2410.07076v6",
    "title": "MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses",
    "summary": "Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses - which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans.",
    "authors": [
      "Zonglin Yang",
      "Wanhao Liu",
      "Ben Gao",
      "Tong Xie",
      "Yuqiang Li",
      "Wanli Ouyang",
      "Soujanya Poria",
      "Erik Cambria",
      "Dongzhan Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-09T17:19:58Z",
    "pdf_url": "https://arxiv.org/pdf/2410.07076v6"
  },
  {
    "arxiv_id": "2410.09097v2",
    "entry_id": "http://arxiv.org/abs/2410.09097v2",
    "title": "Recent advancements in LLM Red-Teaming: Techniques, Defenses, and Ethical Considerations",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing tasks, but their vulnerability to jailbreak attacks poses significant security risks. This survey paper presents a comprehensive analysis of recent advancements in attack strategies and defense mechanisms within the field of Large Language Model (LLM) red-teaming. We analyze various attack methods, including gradient-based optimization, reinforcement learning, and prompt engineering approaches. We discuss the implications of these attacks on LLM safety and the need for improved defense mechanisms. This work aims to provide a thorough understanding of the current landscape of red-teaming attacks and defenses on LLMs, enabling the development of more secure and reliable language models.",
    "authors": [
      "Tarun Raheja",
      "Nilay Pochhi",
      "F. D. C. M. Curie"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-09T01:35:38Z",
    "pdf_url": "https://arxiv.org/pdf/2410.09097v2"
  },
  {
    "arxiv_id": "2410.07265v1",
    "entry_id": "http://arxiv.org/abs/2410.07265v1",
    "title": "A Survey: Collaborative Hardware and Software Design in the Era of Large Language Models",
    "summary": "The rapid development of large language models (LLMs) has significantly transformed the field of artificial intelligence, demonstrating remarkable capabilities in natural language processing and moving towards multi-modal functionality. These models are increasingly integrated into diverse applications, impacting both research and industry. However, their development and deployment present substantial challenges, including the need for extensive computational resources, high energy consumption, and complex software optimizations. Unlike traditional deep learning systems, LLMs require unique optimization strategies for training and inference, focusing on system-level efficiency. This paper surveys hardware and software co-design approaches specifically tailored to address the unique characteristics and constraints of large language models. This survey analyzes the challenges and impacts of LLMs on hardware and algorithm research, exploring algorithm optimization, hardware design, and system-level innovations. It aims to provide a comprehensive understanding of the trade-offs and considerations in LLM-centric computing systems, guiding future advancements in AI. Finally, we summarize the existing efforts in this space and outline future directions toward realizing production-grade co-design methodologies for the next generation of large language models and AI systems.",
    "authors": [
      "Cong Guo",
      "Feng Cheng",
      "Zhixu Du",
      "James Kiessling",
      "Jonathan Ku",
      "Shiyu Li",
      "Ziru Li",
      "Mingyuan Ma",
      "Tergel Molom-Ochir",
      "Benjamin Morris",
      "Haoxuan Shan",
      "Jingwei Sun",
      "Yitu Wang",
      "Chiyue Wei",
      "Xueying Wu",
      "Yuhao Wu",
      "Hao Frank Yang",
      "Jingyang Zhang",
      "Junyao Zhang",
      "Qilin Zheng",
      "Guanglei Zhou",
      "Hai",
      "Li",
      "Yiran Chen"
    ],
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "published": "2024-10-08T21:46:52Z",
    "pdf_url": "https://arxiv.org/pdf/2410.07265v1"
  },
  {
    "arxiv_id": "2410.06101v2",
    "entry_id": "http://arxiv.org/abs/2410.06101v2",
    "title": "Coevolving with the Other You: Fine-Tuning LLM with Sequential Cooperative Multi-Agent Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has emerged as a pivotal technique for fine-tuning large language models (LLMs) on specific tasks. However, prevailing RL fine-tuning methods predominantly rely on PPO and its variants. Though these algorithms are effective in general RL settings, they often exhibit suboptimal performance and vulnerability to distribution collapse when applied to the fine-tuning of LLMs. In this paper, we propose CORY, extending the RL fine-tuning of LLMs to a sequential cooperative multi-agent reinforcement learning framework, to leverage the inherent coevolution and emergent capabilities of multi-agent systems. In CORY, the LLM to be fine-tuned is initially duplicated into two autonomous agents: a pioneer and an observer. The pioneer generates responses based on queries, while the observer generates responses using both the queries and the pioneer's responses. The two agents are trained together. During training, the agents exchange roles periodically, fostering cooperation and coevolution between them. Experiments evaluate CORY's performance by fine-tuning GPT-2 and Llama-2 under subjective and objective reward functions on the IMDB Review and GSM8K datasets, respectively. Results show that CORY outperforms PPO in terms of policy optimality, resistance to distribution collapse, and training robustness, thereby underscoring its potential as a superior methodology for refining LLMs in real-world applications.",
    "authors": [
      "Hao Ma",
      "Tianyi Hu",
      "Zhiqiang Pu",
      "Boyin Liu",
      "Xiaolin Ai",
      "Yanyan Liang",
      "Min Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2024-10-08T14:55:26Z",
    "pdf_url": "https://arxiv.org/pdf/2410.06101v2"
  },
  {
    "arxiv_id": "2410.05080v3",
    "entry_id": "http://arxiv.org/abs/2410.05080v3",
    "title": "ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery",
    "summary": "The advancements of large language models (LLMs) have piqued growing interest in developing LLM-based language agents to automate scientific discovery end-to-end, which has sparked both excitement and skepticism about their true capabilities. In this work, we call for rigorous assessment of agents on individual tasks in a scientific workflow before making bold claims on end-to-end automation. To this end, we present ScienceAgentBench, a new benchmark for evaluating language agents for data-driven scientific discovery. To ensure the scientific authenticity and real-world relevance of our benchmark, we extract 102 tasks from 44 peer-reviewed publications in four disciplines and engage nine subject matter experts to validate them. We unify the target output for every task to a self-contained Python program file and employ an array of evaluation metrics to examine the generated programs, execution results, and costs. Each task goes through multiple rounds of manual validation by annotators and subject matter experts to ensure its annotation quality and scientific plausibility. We also propose two effective strategies to mitigate data contamination concerns. Using ScienceAgentBench, we evaluate five open-weight and proprietary LLMs, each with three frameworks: direct prompting, OpenHands CodeAct, and self-debug. Given three attempts for each task, the best-performing agent can only solve 32.4% of the tasks independently and 34.3% with expert-provided knowledge. In addition, we evaluate OpenAI o1-preview with direct prompting and self-debug, which can boost the performance to 42.2%, demonstrating the effectiveness of increasing inference-time compute but with more than 10 times the cost of other LLMs. Still, our results underscore the limitations of current language agents in generating code for data-driven discovery, let alone end-to-end automation for scientific research.",
    "authors": [
      "Ziru Chen",
      "Shijie Chen",
      "Yuting Ning",
      "Qianheng Zhang",
      "Boshi Wang",
      "Botao Yu",
      "Yifei Li",
      "Zeyi Liao",
      "Chen Wei",
      "Zitong Lu",
      "Vishal Dey",
      "Mingyi Xue",
      "Frazier N. Baker",
      "Benjamin Burns",
      "Daniel Adu-Ampratwum",
      "Xuhui Huang",
      "Xia Ning",
      "Song Gao",
      "Yu Su",
      "Huan Sun"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-07T14:33:50Z",
    "pdf_url": "https://arxiv.org/pdf/2410.05080v3"
  },
  {
    "arxiv_id": "2410.05349v1",
    "entry_id": "http://arxiv.org/abs/2410.05349v1",
    "title": "SoK: Towards Security and Safety of Edge AI",
    "summary": "Advanced AI applications have become increasingly available to a broad audience, e.g., as centrally managed large language models (LLMs). Such centralization is both a risk and a performance bottleneck - Edge AI promises to be a solution to these problems. However, its decentralized approach raises additional challenges regarding security and safety. In this paper, we argue that both of these aspects are critical for Edge AI, and even more so, their integration. Concretely, we survey security and safety threats, summarize existing countermeasures, and collect open challenges as a call for more research in this area.",
    "authors": [
      "Tatjana Wingarz",
      "Anne Lauscher",
      "Janick Edinger",
      "Dominik Kaaser",
      "Stefan Schulte",
      "Mathias Fischer"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-10-07T10:52:53Z",
    "pdf_url": "https://arxiv.org/pdf/2410.05349v1"
  },
  {
    "arxiv_id": "2410.04501v3",
    "entry_id": "http://arxiv.org/abs/2410.04501v3",
    "title": "Leveraging Large Language Models for Suicide Detection on Social Media with Limited Labels",
    "summary": "The increasing frequency of suicidal thoughts highlights the importance of early detection and intervention. Social media platforms, where users often share personal experiences and seek help, could be utilized to identify individuals at risk. However, the large volume of daily posts makes manual review impractical. This paper explores the use of Large Language Models (LLMs) to automatically detect suicidal content in text-based social media posts. We propose a novel method for generating pseudo-labels for unlabeled data by prompting LLMs, along with traditional classification fine-tuning techniques to enhance label accuracy. To create a strong suicide detection model, we develop an ensemble approach involving prompting with Qwen2-72B-Instruct, and using fine-tuned models such as Llama3-8B, Llama3.1-8B, and Gemma2-9B. We evaluate our approach on the dataset of the Suicide Ideation Detection on Social Media Challenge, a track of the IEEE Big Data 2024 Big Data Cup. Additionally, we conduct a comprehensive analysis to assess the impact of different models and fine-tuning strategies on detection performance. Experimental results show that the ensemble model significantly improves the detection accuracy, by 5% points compared with the individual models. It achieves a weight F1 score of 0.770 on the public test set, and 0.731 on the private test set, providing a promising solution for identifying suicidal content in social media. Our analysis shows that the choice of LLMs affects the prompting performance, with larger models providing better accuracy. Our code and checkpoints are publicly available at https://github.com/khanhvynguyen/Suicide_Detection_LLMs.",
    "authors": [
      "Vy Nguyen",
      "Chau Pham"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-06T14:45:01Z",
    "pdf_url": "https://arxiv.org/pdf/2410.04501v3"
  },
  {
    "arxiv_id": "2410.04466v4",
    "entry_id": "http://arxiv.org/abs/2410.04466v4",
    "title": "Large Language Model Inference Acceleration: A Comprehensive Hardware Perspective",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various fields, from natural language understanding to text generation. Compared to non-generative LLMs like BERT and DeBERTa, generative LLMs like GPT series and Llama series are currently the main focus due to their superior algorithmic performance. The advancements in generative LLMs are closely intertwined with the development of hardware capabilities. Various hardware platforms exhibit distinct hardware characteristics, which can help improve LLM inference performance. Therefore, this paper comprehensively surveys efficient generative LLM inference on different hardware platforms. First, we provide an overview of the algorithm architecture of mainstream generative LLMs and delve into the inference process. Then, we summarize different optimization methods for different platforms such as CPU, GPU, FPGA, ASIC, and PIM/NDP, and provide inference results for generative LLMs. Furthermore, we perform a qualitative and quantitative comparison of inference performance with batch sizes 1 and 8 on different hardware platforms by considering hardware power consumption, absolute inference speed (tokens/s), and energy efficiency (tokens/J). We compare the performance of the same optimization methods across different hardware platforms, the performance across different hardware platforms, and the performance of different methods on the same hardware platform. This provides a systematic and comprehensive summary of existing inference acceleration work by integrating software optimization methods and hardware platforms. We point out that three trends (multimodality, inference-time compute, and higher inference energy efficiency) are promising to redefine the capabilities of edge artificial intelligence systems. Our project is available at https://dai.sjtu.edu.cn/project.html.",
    "authors": [
      "Jinhao Li",
      "Jiaming Xu",
      "Shan Huang",
      "Yonghua Chen",
      "Wen Li",
      "Jun Liu",
      "Yaoxiu Lian",
      "Jiayi Pan",
      "Li Ding",
      "Hao Zhou",
      "Yu Wang",
      "Guohao Dai"
    ],
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "published": "2024-10-06T12:42:04Z",
    "pdf_url": "https://arxiv.org/pdf/2410.04466v4"
  },
  {
    "arxiv_id": "2410.16292v1",
    "entry_id": "http://arxiv.org/abs/2410.16292v1",
    "title": "An evaluation of LLM code generation capabilities through graded exercises",
    "summary": "Large Language Models have shown prominent capabilities in generating functional code from natural language descriptions. However, a standardized way to evaluate these capabilities in an objective and unbiased manner is still to be found. In this paper we review the current evaluation methods available to this end, and run a new evaluation of the performance of one state-of-the-art model (GPT4-o-mini) in solving curated coding challenges in 8 programming languages, obtained from Codewars, a software development community. Our analysis shows that the chance of success of the model has a positive correlation with the task difficulty, the popularity of the programming language being used and the time elapsed since the publication of the challenge. A further approximate explanatory analysis in terms of high-level features hints that while 46.6% of the model performance could be attributed to task difficulty, a 37.4% seems to be related to leakage of the challenge solutions into the model training set, while the remaining 16% depends on the programming language. These results suggest that current evaluation methodologies might be overestimating the actual skill of Large Language Models for generating functional code.",
    "authors": [
      "Álvaro Barbero Jiménez"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-10-06T09:54:54Z",
    "pdf_url": "https://arxiv.org/pdf/2410.16292v1"
  },
  {
    "arxiv_id": "2410.03981v3",
    "entry_id": "http://arxiv.org/abs/2410.03981v3",
    "title": "A Survey on LLM-based Code Generation for Low-Resource and Domain-Specific Programming Languages",
    "summary": "Large Language Models (LLMs) have shown impressive capabilities in code generation for popular programming languages. However, their performance on Low-Resource Programming Languages (LRPLs) and Domain-Specific Languages (DSLs) remains a significant challenge, affecting millions of developers-3.5 million users in Rust alone-who cannot fully utilize LLM capabilities. LRPLs and DSLs encounter unique obstacles, including data scarcity and, for DSLs, specialized syntax that is poorly represented in general-purpose datasets.\n  Addressing these challenges is crucial, as LRPLs and DSLs enhance development efficiency in specialized domains, such as finance and science. While several surveys discuss LLMs in software engineering, none focus specifically on the challenges and opportunities associated with LRPLs and DSLs. Our survey fills this gap by systematically reviewing the current state, methodologies, and challenges in leveraging LLMs for code generation in these languages. We filtered 111 papers from over 27,000 published studies between 2020 and 2024 to evaluate the capabilities and limitations of LLMs in LRPLs and DSLs. We report the LLMs used, benchmarks, and metrics for evaluation, strategies for enhancing performance, and methods for dataset collection and curation.\n  We identified four main evaluation techniques and several metrics for assessing code generation in LRPLs and DSLs. Our analysis categorizes improvement methods into six groups and summarizes novel architectures proposed by researchers. Despite various techniques and metrics, a standard approach and benchmark dataset for evaluating code generation in LRPLs and DSLs are lacking. This survey serves as a resource for researchers and practitioners at the intersection of LLMs, software engineering, and specialized programming languages, laying the groundwork for future advancements in code generation for LRPLs and DSLs.",
    "authors": [
      "Sathvik Joel",
      "Jie JW Wu",
      "Fatemeh H. Fard"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2024-10-04T23:45:17Z",
    "pdf_url": "https://arxiv.org/pdf/2410.03981v3"
  },
  {
    "arxiv_id": "2410.03663v4",
    "entry_id": "http://arxiv.org/abs/2410.03663v4",
    "title": "Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review",
    "summary": "While reasoning capabilities typically emerge in large language models (LLMs) with tens of billions of parameters, recent research focuses on improving smaller open-source models through knowledge distillation (KD) from commercial LLMs. However, many of these studies rely solely on responses from a single LLM as the gold rationale, unlike the natural human learning process, which involves understanding both the correct answers and the reasons behind mistakes. In this paper, we introduce a novel Fault-Aware DistIllation via Peer-Review (FAIR) approach: 1) instead of merely obtaining rationales from teachers, our method asks teachers to identify and explain the student's mistakes, providing customized instruction learning data; 2) we design a simulated peer-review process between teacher LLMs, and selects only the generated rationales above the acceptance threshold, which reduces the chance of teachers guessing correctly with flawed rationale, improving instructional data quality. Comprehensive experiments and analysis on mathematical, commonsense, and logical reasoning tasks demonstrate the effectiveness of our method. Our code is available at https://github.com/zhuochunli/Learn-from-Committee.",
    "authors": [
      "Zhuochun Li",
      "Yuelyu Ji",
      "Rui Meng",
      "Daqing He"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-04T17:59:41Z",
    "pdf_url": "https://arxiv.org/pdf/2410.03663v4"
  },
  {
    "arxiv_id": "2410.12837v1",
    "entry_id": "http://arxiv.org/abs/2410.12837v1",
    "title": "A Comprehensive Survey of Retrieval-Augmented Generation (RAG): Evolution, Current Landscape and Future Directions",
    "summary": "This paper presents a comprehensive study of Retrieval-Augmented Generation (RAG), tracing its evolution from foundational concepts to the current state of the art. RAG combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of LLMs. The study explores the basic architecture of RAG, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in RAG is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of RAG models, expanding the scope of application of RAG models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of RAG and its trajectory in natural language processing.",
    "authors": [
      "Shailja Gupta",
      "Rajesh Ranjan",
      "Surya Narayan Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-10-03T22:29:47Z",
    "pdf_url": "https://arxiv.org/pdf/2410.12837v1"
  },
  {
    "arxiv_id": "2410.03019v2",
    "entry_id": "http://arxiv.org/abs/2410.03019v2",
    "title": "Is Your Paper Being Reviewed by an LLM? Investigating AI Text Detectability in Peer Review",
    "summary": "Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in the linguistic capabilities of large language models (LLMs), a new potential risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. In this study, we investigate the ability of existing AI text detection algorithms to distinguish between peer reviews written by humans and different state-of-the-art LLMs. Our analysis shows that existing approaches fail to identify many GPT-4o written reviews without also producing a high number of false positive classifications. To address this deficiency, we propose a new detection approach which surpasses existing methods in the identification of GPT-4o written peer reviews at low levels of false positive classifications. Our work reveals the difficulty of accurately identifying AI-generated text at the individual review level, highlighting the urgent need for new tools and methods to detect this type of unethical application of generative AI.",
    "authors": [
      "Sungduk Yu",
      "Man Luo",
      "Avinash Madasu",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-03T22:05:06Z",
    "pdf_url": "https://arxiv.org/pdf/2410.03019v2"
  },
  {
    "arxiv_id": "2410.02683v3",
    "entry_id": "http://arxiv.org/abs/2410.02683v3",
    "title": "DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of Daily Life",
    "summary": "As users increasingly seek guidance from LLMs for decision-making in daily life, many of these decisions are not clear-cut and depend significantly on the personal values and ethical standards of people. We present DailyDilemmas, a dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma presents two possible actions, along with affected parties and relevant human values for each action. Based on these dilemmas, we gather a repository of human values covering diverse everyday topics, such as interpersonal relationships, workplace, and environmental issues. With DailyDilemmas, we evaluate LLMs on these dilemmas to determine what action they will choose and the values represented by these action choices. Then, we analyze values through the lens of five theoretical frameworks inspired by sociology, psychology, and philosophy, including the World Values Survey, Moral Foundations Theory, Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of Emotions. For instance, we find LLMs are most aligned with self-expression over survival in World Values Survey and care over loyalty in Moral Foundations Theory. Interestingly, we find substantial preference differences in models for some core values. For example, for truthfulness, Mixtral-8x7B neglects it by 9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand how their designated principles reflect their models' actual value prioritization when facing nuanced moral reasoning in daily-life settings. Finally, we find that end users cannot effectively steer such prioritization using system prompts.",
    "authors": [
      "Yu Ying Chiu",
      "Liwei Jiang",
      "Yejin Choi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-10-03T17:08:52Z",
    "pdf_url": "https://arxiv.org/pdf/2410.02683v3"
  },
  {
    "arxiv_id": "2410.02650v2",
    "entry_id": "http://arxiv.org/abs/2410.02650v2",
    "title": "Undesirable Memorization in Large Language Models: A Survey",
    "summary": "While recent research increasingly showcases the remarkable capabilities of Large Language Models (LLMs), it is equally crucial to examine their associated risks. Among these, privacy and security vulnerabilities are particularly concerning, posing significant ethical and legal challenges. At the heart of these vulnerabilities stands memorization, which refers to a model's tendency to store and reproduce phrases from its training data. This phenomenon has been shown to be a fundamental source to various privacy and security attacks against LLMs. In this paper, we provide a taxonomy of the literature on LLM memorization, exploring it across three dimensions: granularity, retrievability, and desirability. Next, we discuss the metrics and methods used to quantify memorization, followed by an analysis of the causes and factors that contribute to memorization phenomenon. We then explore strategies that are used so far to mitigate the undesirable aspects of this phenomenon. We conclude our survey by identifying potential research topics for the near future, including methods to balance privacy and performance, and the analysis of memorization in specific LLM contexts such as conversational agents, retrieval-augmented generation, and diffusion language models. Given the rapid research pace in this field, we also maintain a dedicated repository of the references discussed in this survey which will be regularly updated to reflect the latest developments.",
    "authors": [
      "Ali Satvaty",
      "Suzan Verberne",
      "Fatih Turkmen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-03T16:34:46Z",
    "pdf_url": "https://arxiv.org/pdf/2410.02650v2"
  },
  {
    "arxiv_id": "2410.02440v2",
    "entry_id": "http://arxiv.org/abs/2410.02440v2",
    "title": "Optimizing Adaptive Attacks against Watermarks for Language Models",
    "summary": "Large Language Models (LLMs) can be misused to spread unwanted content at scale. Content watermarking deters misuse by hiding messages in content, enabling its detection using a secret watermarking key. Robustness is a core security property, stating that evading detection requires (significant) degradation of the content's quality. Many LLM watermarking methods have been proposed, but robustness is tested only against non-adaptive attackers who lack knowledge of the watermarking method and can find only suboptimal attacks. We formulate watermark robustness as an objective function and use preference-based optimization to tune adaptive attacks against the specific watermarking method. Our evaluation shows that (i) adaptive attacks evade detection against all surveyed watermarks, (ii) training against any watermark succeeds in evading unseen watermarks, and (iii) optimization-based attacks are cost-effective. Our findings underscore the need to test robustness against adaptively tuned attacks. We release our adaptively optimized paraphrasers at https://github.com/nilslukas/ada-wm-evasion.",
    "authors": [
      "Abdulrahman Diaa",
      "Toluwani Aremu",
      "Nils Lukas"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-10-03T12:37:39Z",
    "pdf_url": "https://arxiv.org/pdf/2410.02440v2"
  },
  {
    "arxiv_id": "2410.02191v2",
    "entry_id": "http://arxiv.org/abs/2410.02191v2",
    "title": "A Survey on Point-of-Interest Recommendation: Models, Architectures, and Security",
    "summary": "The widespread adoption of smartphones and Location-Based Social Networks has led to a massive influx of spatio-temporal data, creating unparalleled opportunities for enhancing Point-of-Interest (POI) recommendation systems. These advanced POI systems are crucial for enriching user experiences, enabling personalized interactions, and optimizing decision-making processes in the digital landscape. However, existing surveys tend to focus on traditional approaches and few of them delve into cutting-edge developments, emerging architectures, as well as security considerations in POI recommendations. To address this gap, our survey stands out by offering a comprehensive, up-to-date review of POI recommendation systems, covering advancements in models, architectures, and security aspects. We systematically examine the transition from traditional models to advanced techniques such as large language models. Additionally, we explore the architectural evolution from centralized to decentralized and federated learning systems, highlighting the improvements in scalability and privacy. Furthermore, we address the increasing importance of security, examining potential vulnerabilities and privacy-preserving approaches. Our taxonomy provides a structured overview of the current state of POI recommendation, while we also identify promising directions for future research in this rapidly advancing field.",
    "authors": [
      "Qianru Zhang",
      "Peng Yang",
      "Junliang Yu",
      "Haixin Wang",
      "Xingwei He",
      "Siu-Ming Yiu",
      "Hongzhi Yin"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "published": "2024-10-03T04:11:42Z",
    "pdf_url": "https://arxiv.org/pdf/2410.02191v2"
  },
  {
    "arxiv_id": "2410.02026v1",
    "entry_id": "http://arxiv.org/abs/2410.02026v1",
    "title": "Zodiac: A Cardiologist-Level LLM Framework for Multi-Agent Diagnostics",
    "summary": "Large language models (LLMs) have demonstrated remarkable progress in healthcare. However, a significant gap remains regarding LLMs' professionalism in domain-specific clinical practices, limiting their application in real-world diagnostics. In this work, we introduce ZODIAC, an LLM-powered framework with cardiologist-level professionalism designed to engage LLMs in cardiological diagnostics. ZODIAC assists cardiologists by extracting clinically relevant characteristics from patient data, detecting significant arrhythmias, and generating preliminary reports for the review and refinement by cardiologists. To achieve cardiologist-level professionalism, ZODIAC is built on a multi-agent collaboration framework, enabling the processing of patient data across multiple modalities. Each LLM agent is fine-tuned using real-world patient data adjudicated by cardiologists, reinforcing the model's professionalism. ZODIAC undergoes rigorous clinical validation with independent cardiologists, evaluated across eight metrics that measure clinical effectiveness and address security concerns. Results show that ZODIAC outperforms industry-leading models, including OpenAI's GPT-4o, Meta's Llama-3.1-405B, and Google's Gemini-pro, as well as medical-specialist LLMs like Microsoft's BioGPT. ZODIAC demonstrates the transformative potential of specialized LLMs in healthcare by delivering domain-specific solutions that meet the stringent demands of medical practice. Notably, ZODIAC has been successfully integrated into electrocardiography (ECG) devices, exemplifying the growing trend of embedding LLMs into Software-as-Medical-Device (SaMD).",
    "authors": [
      "Yuan Zhou",
      "Peng Zhang",
      "Mengya Song",
      "Alice Zheng",
      "Yiwen Lu",
      "Zhiheng Liu",
      "Yong Chen",
      "Zhaohan Xi"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-10-02T20:46:39Z",
    "pdf_url": "https://arxiv.org/pdf/2410.02026v1"
  },
  {
    "arxiv_id": "2410.01899v1",
    "entry_id": "http://arxiv.org/abs/2410.01899v1",
    "title": "The potential of LLM-generated reports in DevSecOps",
    "summary": "Alert fatigue is a common issue faced by software teams using the DevSecOps paradigm. The overwhelming number of warnings and alerts generated by security and code scanning tools, particularly in smaller teams where resources are limited, leads to desensitization and diminished responsiveness to security warnings, potentially exposing systems to vulnerabilities. This paper explores the potential of LLMs in generating actionable security reports that emphasize the financial impact and consequences of detected security issues, such as credential leaks, if they remain unaddressed. A survey conducted among developers indicates that LLM-generated reports significantly enhance the likelihood of immediate action on security issues by providing clear, comprehensive, and motivating insights. Integrating these reports into DevSecOps workflows can mitigate attention saturation and alert fatigue, ensuring that critical security warnings are addressed effectively.",
    "authors": [
      "Nikolaos Lykousas",
      "Vasileios Argyropoulos",
      "Fran Casino"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-10-02T18:01:12Z",
    "pdf_url": "https://arxiv.org/pdf/2410.01899v1"
  },
  {
    "arxiv_id": "2410.03761v2",
    "entry_id": "http://arxiv.org/abs/2410.03761v2",
    "title": "Taxonomy Tree Generation from Citation Graph",
    "summary": "Constructing taxonomies from citation graphs is essential for organizing scientific knowledge, facilitating literature reviews, and identifying emerging research trends. However, manual taxonomy construction is labor-intensive, time-consuming, and prone to human biases, often overlooking pivotal but less-cited papers. In this paper, to enable automatic hierarchical taxonomy generation from citation graphs, we propose HiGTL (Hierarchical Graph Taxonomy Learning), a novel end-to-end framework guided by human-provided instructions or preferred topics. Specifically, we propose a hierarchical citation graph clustering method that recursively groups related papers based on both textual content and citation structure, ensuring semantically meaningful and structurally coherent clusters. Additionally, we develop a novel taxonomy node verbalization strategy that iteratively generates central concepts for each cluster, leveraging a pre-trained large language model (LLM) to maintain semantic consistency across hierarchical levels. To further enhance performance, we design a joint optimization framework that fine-tunes both the clustering and concept generation modules, aligning structural accuracy with the quality of generated taxonomies. Extensive experiments demonstrate that HiGTL effectively produces coherent, high-quality taxonomies.",
    "authors": [
      "Yuntong Hu",
      "Zhuofeng Li",
      "Zheng Zhang",
      "Chen Ling",
      "Raasikh Kanjiani",
      "Boxin Zhao",
      "Liang Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-10-02T13:02:03Z",
    "pdf_url": "https://arxiv.org/pdf/2410.03761v2"
  },
  {
    "arxiv_id": "2410.01098v2",
    "entry_id": "http://arxiv.org/abs/2410.01098v2",
    "title": "Exploring Gen-AI applications in building research and industry: A review",
    "summary": "This paper investigates the transformative potential of Generative AI (Gen-AI) technologies, particularly large language models, within the building industry. By leveraging these advanced AI tools, the study explores their application across key areas such as automated compliance checking and building design assistance. The research highlights how Gen-AI can automate labor-intensive processes, significantly improving efficiency and reducing costs in building practices. The paper first discusses the two widely applied fundamental models-Transformer and Diffusion model-and summarizes current pathways for accessing Gen-AI models and the most common techniques for customizing them. It then explores applications for text generation, such as compliance checking, control support, data mining, and building simulation input file editing. Additionally, it examines image generation, including direct generation through diffusion models and indirect generation through language model-supported template creation based on existing Computer-Aided Design or other design tools with rendering. The paper concludes with a comprehensive analysis of the current capabilities of Gen-AI in the building industry, outlining future directions for research and development, with the goal of paving the way for smarter, more effective, and responsive design, construction, and operational practices.",
    "authors": [
      "Hanlong Wan",
      "Jian Zhang",
      "Yan Chen",
      "Weili Xu",
      "Fan Feng"
    ],
    "categories": [
      "cs.AI",
      "eess.IV",
      "eess.SY"
    ],
    "published": "2024-10-01T21:59:08Z",
    "pdf_url": "https://arxiv.org/pdf/2410.01098v2"
  },
  {
    "arxiv_id": "2410.01066v2",
    "entry_id": "http://arxiv.org/abs/2410.01066v2",
    "title": "From Natural Language to SQL: Review of LLM-based Text-to-SQL Systems",
    "summary": "LLMs when used with Retrieval Augmented Generation (RAG), are greatly improving the SOTA of translating natural language queries to structured and correct SQL. Unlike previous reviews, this survey provides a comprehensive study of the evolution of LLM-based text-to-SQL systems, from early rule-based models to advanced LLM approaches that use (RAG) systems. We discuss benchmarks, evaluation methods, and evaluation metrics. Also, we uniquely study the use of Graph RAGs for better contextual accuracy and schema linking in these systems. Finally, we highlight key challenges such as computational efficiency, model robustness, and data privacy toward improvements of LLM-based text-to-SQL systems.",
    "authors": [
      "Ali Mohammadjafari",
      "Anthony S. Maida",
      "Raju Gottumukkala"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-10-01T20:46:25Z",
    "pdf_url": "https://arxiv.org/pdf/2410.01066v2"
  },
  {
    "arxiv_id": "2409.20252v2",
    "entry_id": "http://arxiv.org/abs/2409.20252v2",
    "title": "What is the Role of Large Language Models in the Evolution of Astronomy Research?",
    "summary": "ChatGPT and other state-of-the-art large language models (LLMs) are rapidly transforming multiple fields, offering powerful tools for a wide range of applications. These models, commonly trained on vast datasets, exhibit human-like text generation capabilities, making them useful for research tasks such as ideation, literature review, coding, drafting, and outreach. We conducted a study involving 13 astronomers at different career stages and research fields to explore LLM applications across diverse tasks over several months and to evaluate their performance in research-related activities. This work was accompanied by an anonymous survey assessing participants' experiences and attitudes towards LLMs. We provide a detailed analysis of the tasks attempted and the survey answers, along with specific output examples. Our findings highlight both the potential and limitations of LLMs in supporting research while also addressing general and research-specific ethical considerations. We conclude with a series of recommendations, emphasizing the need for researchers to complement LLMs with critical thinking and domain expertise, ensuring these tools serve as aids rather than substitutes for rigorous scientific inquiry.",
    "authors": [
      "Morgan Fouesneau",
      "Ivelina G. Momcheva",
      "Urmila Chadayammuri",
      "Mariia Demianenko",
      "Antoine Dumont",
      "Raphael E. Hviding",
      "K. Angelique Kahle",
      "Nadiia Pulatova",
      "Bhavesh Rajpoot",
      "Marten B. Scheuck",
      "Rhys Seeburger",
      "Dmitry Semenov",
      "Jaime I. Villaseñor"
    ],
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "published": "2024-09-30T12:42:25Z",
    "pdf_url": "https://arxiv.org/pdf/2409.20252v2"
  },
  {
    "arxiv_id": "2409.19993v1",
    "entry_id": "http://arxiv.org/abs/2409.19993v1",
    "title": "Mitigating Backdoor Threats to Large Language Models: Advancement and Challenges",
    "summary": "The advancement of Large Language Models (LLMs) has significantly impacted various domains, including Web search, healthcare, and software development. However, as these models scale, they become more vulnerable to cybersecurity risks, particularly backdoor attacks. By exploiting the potent memorization capacity of LLMs, adversaries can easily inject backdoors into LLMs by manipulating a small portion of training data, leading to malicious behaviors in downstream applications whenever the hidden backdoor is activated by the pre-defined triggers. Moreover, emerging learning paradigms like instruction tuning and reinforcement learning from human feedback (RLHF) exacerbate these risks as they rely heavily on crowdsourced data and human feedback, which are not fully controlled. In this paper, we present a comprehensive survey of emerging backdoor threats to LLMs that appear during LLM development or inference, and cover recent advancement in both defense and detection strategies for mitigating backdoor threats to LLMs. We also outline key challenges in addressing these threats, highlighting areas for future research.",
    "authors": [
      "Qin Liu",
      "Wenjie Mo",
      "Terry Tong",
      "Jiashu Xu",
      "Fei Wang",
      "Chaowei Xiao",
      "Muhao Chen"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.SY"
    ],
    "published": "2024-09-30T06:31:36Z",
    "pdf_url": "https://arxiv.org/pdf/2409.19993v1"
  },
  {
    "arxiv_id": "2409.19801v2",
    "entry_id": "http://arxiv.org/abs/2409.19801v2",
    "title": "CRScore: Grounding Automated Evaluation of Code Review Comments in Code Claims and Smells",
    "summary": "The task of automated code review has recently gained a lot of attention from the machine learning community. However, current review comment evaluation metrics rely on comparisons with a human-written reference for a given code change (also called a diff). Furthermore, code review is a one-to-many problem, like generation and summarization, with many \"valid reviews\" for a diff. Thus, we develop CRScore - a reference-free metric to measure dimensions of review quality like conciseness, comprehensiveness, and relevance. We design CRScore to evaluate reviews in a way that is grounded in claims and potential issues detected in the code by LLMs and static analyzers. We demonstrate that CRScore can produce valid, fine-grained scores of review quality that have the greatest alignment with human judgment among open source metrics (0.54 Spearman correlation) and are more sensitive than reference-based metrics. We also release a corpus of 2.9k human-annotated review quality scores for machine-generated and GitHub review comments to support the development of automated metrics.",
    "authors": [
      "Atharva Naik",
      "Marcus Alenius",
      "Daniel Fried",
      "Carolyn Rose"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-29T21:53:18Z",
    "pdf_url": "https://arxiv.org/pdf/2409.19801v2"
  },
  {
    "arxiv_id": "2409.19527v2",
    "entry_id": "http://arxiv.org/abs/2409.19527v2",
    "title": "BuildingView: Constructing Urban Building Exteriors Databases with Street View Imagery and Multimodal Large Language Mode",
    "summary": "Urban Building Exteriors are increasingly important in urban analytics, driven by advancements in Street View Imagery and its integration with urban research. Multimodal Large Language Models (LLMs) offer powerful tools for urban annotation, enabling deeper insights into urban environments. However, challenges remain in creating accurate and detailed urban building exterior databases, identifying critical indicators for energy efficiency, environmental sustainability, and human-centric design, and systematically organizing these indicators. To address these challenges, we propose BuildingView, a novel approach that integrates high-resolution visual data from Google Street View with spatial information from OpenStreetMap via the Overpass API. This research improves the accuracy of urban building exterior data, identifies key sustainability and design indicators, and develops a framework for their extraction and categorization. Our methodology includes a systematic literature review, building and Street View sampling, and annotation using the ChatGPT-4O API. The resulting database, validated with data from New York City, Amsterdam, and Singapore, provides a comprehensive tool for urban studies, supporting informed decision-making in urban planning, architectural design, and environmental policy. The code for BuildingView is available at https://github.com/Jasper0122/BuildingView.",
    "authors": [
      "Zongrong Li",
      "Yunlei Su",
      "Hongrong Wang",
      "Wufan Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.CY"
    ],
    "published": "2024-09-29T03:00:16Z",
    "pdf_url": "https://arxiv.org/pdf/2409.19527v2"
  },
  {
    "arxiv_id": "2409.19450v2",
    "entry_id": "http://arxiv.org/abs/2409.19450v2",
    "title": "Secret Use of Large Language Model (LLM)",
    "summary": "The advancements of Large Language Models (LLMs) have decentralized the responsibility for the transparency of AI usage. Specifically, LLM users are now encouraged or required to disclose the use of LLM-generated content for varied types of real-world tasks. However, an emerging phenomenon, users' secret use of LLM, raises challenges in ensuring end users adhere to the transparency requirement. Our study used mixed-methods with an exploratory survey (125 real-world secret use cases reported) and a controlled experiment among 300 users to investigate the contexts and causes behind the secret use of LLMs. We found that such secretive behavior is often triggered by certain tasks, transcending demographic and personality differences among users. Task types were found to affect users' intentions to use secretive behavior, primarily through influencing perceived external judgment regarding LLM usage. Our results yield important insights for future work on designing interventions to encourage more transparent disclosure of the use of LLMs or other AI technologies.",
    "authors": [
      "Zhiping Zhang",
      "Chenxinran Shen",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Tianshi Li"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-09-28T20:31:53Z",
    "pdf_url": "https://arxiv.org/pdf/2409.19450v2"
  },
  {
    "arxiv_id": "2409.19430v1",
    "entry_id": "http://arxiv.org/abs/2409.19430v1",
    "title": "'Simulacrum of Stories': Examining Large Language Models as Qualitative Research Participants",
    "summary": "The recent excitement around generative models has sparked a wave of proposals suggesting the replacement of human participation and labor in research and development--e.g., through surveys, experiments, and interviews--with synthetic research data generated by large language models (LLMs). We conducted interviews with 19 qualitative researchers to understand their perspectives on this paradigm shift. Initially skeptical, researchers were surprised to see similar narratives emerge in the LLM-generated data when using the interview probe. However, over several conversational turns, they went on to identify fundamental limitations, such as how LLMs foreclose participants' consent and agency, produce responses lacking in palpability and contextual depth, and risk delegitimizing qualitative research methods. We argue that the use of LLMs as proxies for participants enacts the surrogate effect, raising ethical and epistemological concerns that extend beyond the technical limitations of current models to the core of whether LLMs fit within qualitative ways of knowing.",
    "authors": [
      "Shivani Kapania",
      "William Agnew",
      "Motahhare Eslami",
      "Hoda Heidari",
      "Sarah Fox"
    ],
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-09-28T18:28:47Z",
    "pdf_url": "https://arxiv.org/pdf/2409.19430v1"
  },
  {
    "arxiv_id": "2410.12793v1",
    "entry_id": "http://arxiv.org/abs/2410.12793v1",
    "title": "Environment Scan of Generative AI Infrastructure for Clinical and Translational Science",
    "summary": "This study reports a comprehensive environmental scan of the generative AI (GenAI) infrastructure in the national network for clinical and translational science across 36 institutions supported by the Clinical and Translational Science Award (CTSA) Program led by the National Center for Advancing Translational Sciences (NCATS) of the National Institutes of Health (NIH) at the United States. With the rapid advancement of GenAI technologies, including large language models (LLMs), healthcare institutions face unprecedented opportunities and challenges. This research explores the current status of GenAI integration, focusing on stakeholder roles, governance structures, and ethical considerations by administering a survey among leaders of health institutions (i.e., representing academic medical centers and health systems) to assess the institutional readiness and approach towards GenAI adoption. Key findings indicate a diverse range of institutional strategies, with most organizations in the experimental phase of GenAI deployment. The study highlights significant variations in governance models, with a strong preference for centralized decision-making but notable gaps in workforce training and ethical oversight. Moreover, the results underscore the need for a more coordinated approach to GenAI governance, emphasizing collaboration among senior leaders, clinicians, information technology staff, and researchers. Our analysis also reveals concerns regarding GenAI bias, data security, and stakeholder trust, which must be addressed to ensure the ethical and effective implementation of GenAI technologies. This study offers valuable insights into the challenges and opportunities of GenAI integration in healthcare, providing a roadmap for institutions aiming to leverage GenAI for improved quality of care and operational efficiency.",
    "authors": [
      "Betina Idnay",
      "Zihan Xu",
      "William G. Adams",
      "Mohammad Adibuzzaman",
      "Nicholas R. Anderson",
      "Neil Bahroos",
      "Douglas S. Bell",
      "Cody Bumgardner",
      "Thomas Campion",
      "Mario Castro",
      "James J. Cimino",
      "I. Glenn Cohen",
      "David Dorr",
      "Peter L Elkin",
      "Jungwei W. Fan",
      "Todd Ferris",
      "David J. Foran",
      "David Hanauer",
      "Mike Hogarth",
      "Kun Huang",
      "Jayashree Kalpathy-Cramer",
      "Manoj Kandpal",
      "Niranjan S. Karnik",
      "Avnish Katoch",
      "Albert M. Lai",
      "Christophe G. Lambert",
      "Lang Li",
      "Christopher Lindsell",
      "Jinze Liu",
      "Zhiyong Lu",
      "Yuan Luo",
      "Peter McGarvey",
      "Eneida A. Mendonca",
      "Parsa Mirhaji",
      "Shawn Murphy",
      "John D. Osborne",
      "Ioannis C. Paschalidis",
      "Paul A. Harris",
      "Fred Prior",
      "Nicholas J. Shaheen",
      "Nawar Shara",
      "Ida Sim",
      "Umberto Tachinardi",
      "Lemuel R. Waitman",
      "Rosalind J. Wright",
      "Adrian H. Zai",
      "Kai Zheng",
      "Sandra Soo-Jin Lee",
      "Bradley A. Malin",
      "Karthik Natarajan",
      "W. Nicholson Price",
      "Rui Zhang",
      "Yiye Zhang",
      "Hua Xu",
      "Jiang Bian",
      "Chunhua Weng",
      "Yifan Peng"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-09-28T01:53:13Z",
    "pdf_url": "https://arxiv.org/pdf/2410.12793v1"
  },
  {
    "arxiv_id": "2409.18938v2",
    "entry_id": "http://arxiv.org/abs/2409.18938v2",
    "title": "From Seconds to Hours: Reviewing MultiModal Large Language Models on Comprehensive Long Video Understanding",
    "summary": "The integration of Large Language Models (LLMs) with visual encoders has recently shown promising performance in visual understanding tasks, leveraging their inherent capability to comprehend and generate human-like text for visual reasoning. Given the diverse nature of visual data, MultiModal Large Language Models (MM-LLMs) exhibit variations in model designing and training for understanding images, short videos, and long videos. Our paper focuses on the substantial differences and unique challenges posed by long video understanding compared to static image and short video understanding. Unlike static images, short videos encompass sequential frames with both spatial and within-event temporal information, while long videos consist of multiple events with between-event and long-term temporal information. In this survey, we aim to trace and summarize the advancements of MM-LLMs from image understanding to long video understanding. We review the differences among various visual understanding tasks and highlight the challenges in long video understanding, including more fine-grained spatiotemporal details, dynamic events, and long-term dependencies. We then provide a detailed summary of the advancements in MM-LLMs in terms of model design and training methodologies for understanding long videos. Finally, we compare the performance of existing MM-LLMs on video understanding benchmarks of various lengths and discuss potential future directions for MM-LLMs in long video understanding.",
    "authors": [
      "Heqing Zou",
      "Tianze Luo",
      "Guiyang Xie",
      "Victor",
      "Zhang",
      "Fengmao Lv",
      "Guangcong Wang",
      "Junyang Chen",
      "Zhuochen Wang",
      "Hansheng Zhang",
      "Huaijian Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-09-27T17:38:36Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18938v2"
  },
  {
    "arxiv_id": "2409.18786v1",
    "entry_id": "http://arxiv.org/abs/2409.18786v1",
    "title": "A Survey on the Honesty of Large Language Models",
    "summary": "Honesty is a fundamental principle for aligning large language models (LLMs) with human values, requiring these models to recognize what they know and don't know and be able to faithfully express their knowledge. Despite promising, current LLMs still exhibit significant dishonest behaviors, such as confidently presenting wrong answers or failing to express what they know. In addition, research on the honesty of LLMs also faces challenges, including varying definitions of honesty, difficulties in distinguishing between known and unknown knowledge, and a lack of comprehensive understanding of related research. To address these issues, we provide a survey on the honesty of LLMs, covering its clarification, evaluation approaches, and strategies for improvement. Moreover, we offer insights for future research, aiming to inspire further exploration in this important area.",
    "authors": [
      "Siheng Li",
      "Cheng Yang",
      "Taiqiang Wu",
      "Chufan Shi",
      "Yuji Zhang",
      "Xinyu Zhu",
      "Zesen Cheng",
      "Deng Cai",
      "Mo Yu",
      "Lemao Liu",
      "Jie Zhou",
      "Yujiu Yang",
      "Ngai Wong",
      "Xixin Wu",
      "Wai Lam"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-27T14:34:54Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18786v1"
  },
  {
    "arxiv_id": "2409.18286v1",
    "entry_id": "http://arxiv.org/abs/2409.18286v1",
    "title": "Advancing Object Detection in Transportation with Multimodal Large Language Models (MLLMs): A Comprehensive Review and Empirical Testing",
    "summary": "This study aims to comprehensively review and empirically evaluate the application of multimodal large language models (MLLMs) and Large Vision Models (VLMs) in object detection for transportation systems. In the first fold, we provide a background about the potential benefits of MLLMs in transportation applications and conduct a comprehensive review of current MLLM technologies in previous studies. We highlight their effectiveness and limitations in object detection within various transportation scenarios. The second fold involves providing an overview of the taxonomy of end-to-end object detection in transportation applications and future directions. Building on this, we proposed empirical analysis for testing MLLMs on three real-world transportation problems that include object detection tasks namely, road safety attributes extraction, safety-critical event detection, and visual reasoning of thermal images. Our findings provide a detailed assessment of MLLM performance, uncovering both strengths and areas for improvement. Finally, we discuss practical limitations and challenges of MLLMs in enhancing object detection in transportation, thereby offering a roadmap for future research and development in this critical area.",
    "authors": [
      "Huthaifa I. Ashqar",
      "Ahmed Jaber",
      "Taqwa I. Alhadidi",
      "Mohammed Elhenawy"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-26T20:58:11Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18286v1"
  },
  {
    "arxiv_id": "2409.18203v2",
    "entry_id": "http://arxiv.org/abs/2409.18203v2",
    "title": "Policy Maps: Tools for Guiding the Unbounded Space of LLM Behaviors",
    "summary": "AI policy sets boundaries on acceptable behavior for AI models, but this is challenging in the context of large language models (LLMs): how do you ensure coverage over a vast behavior space? We introduce policy maps, an approach to AI policy design inspired by the practice of physical mapmaking. Instead of aiming for full coverage, policy maps aid effective navigation through intentional design choices about which aspects to capture and which to abstract away. With Policy Projector, an interactive tool for designing LLM policy maps, an AI practitioner can survey the landscape of model input-output pairs, define custom regions (e.g., \"violence\"), and navigate these regions with if-then policy rules that can act on LLM outputs (e.g., if output contains \"violence\" and \"graphic details,\" then rewrite without \"graphic details\"). Policy Projector supports interactive policy authoring using LLM classification and steering and a map visualization reflecting the AI practitioner's work. In an evaluation with 12 AI safety experts, our system helps policy designers craft policies around problematic model behaviors such as incorrect gender assumptions and handling of immediate physical safety threats.",
    "authors": [
      "Michelle S. Lam",
      "Fred Hohman",
      "Dominik Moritz",
      "Jeffrey P. Bigham",
      "Kenneth Holstein",
      "Mary Beth Kery"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-09-26T18:34:16Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18203v2"
  },
  {
    "arxiv_id": "2409.18170v1",
    "entry_id": "http://arxiv.org/abs/2409.18170v1",
    "title": "Evaluation of Large Language Models for Summarization Tasks in the Medical Domain: A Narrative Review",
    "summary": "Large Language Models have advanced clinical Natural Language Generation, creating opportunities to manage the volume of medical text. However, the high-stakes nature of medicine requires reliable evaluation, which remains a challenge. In this narrative review, we assess the current evaluation state for clinical summarization tasks and propose future directions to address the resource constraints of expert human evaluation.",
    "authors": [
      "Emma Croxford",
      "Yanjun Gao",
      "Nicholas Pellegrino",
      "Karen K. Wong",
      "Graham Wills",
      "Elliot First",
      "Frank J. Liao",
      "Cherodeep Goswami",
      "Brian Patterson",
      "Majid Afshar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-26T17:58:26Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18170v1"
  },
  {
    "arxiv_id": "2409.18169v5",
    "entry_id": "http://arxiv.org/abs/2409.18169v5",
    "title": "Harmful Fine-tuning Attacks and Defenses for Large Language Models: A Survey",
    "summary": "Recent research demonstrates that the nascent fine-tuning-as-a-service business model exposes serious safety concerns -- fine-tuning over a few harmful data uploaded by the users can compromise the safety alignment of the model. The attack, known as harmful fine-tuning attack, has raised a broad research interest among the community. However, as the attack is still new, \\textbf{we observe that there are general misunderstandings within the research community.} To clear up concern, this paper provide a comprehensive overview to three aspects of harmful fine-tuning: attacks setting, defense design and evaluation methodology. Specifically, we first present the threat model of the problem, and introduce the harmful fine-tuning attack and its variants. Then we systematically survey the existing literature on attacks/defenses/mechanical analysis of the problem. Finally, we introduce the evaluation methodology and outline future research directions that might contribute to the development of the field. Additionally, we present a list of questions of interest, which might be useful to refer to when reviewers in the peer review process question the realism of the experiment/attack/defense setting. A curated list of relevant papers is maintained and made accessible at: https://github.com/git-disl/awesome_LLM-harmful-fine-tuning-papers.",
    "authors": [
      "Tiansheng Huang",
      "Sihao Hu",
      "Fatih Ilhan",
      "Selim Furkan Tekin",
      "Ling Liu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-09-26T17:55:22Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18169v5"
  },
  {
    "arxiv_id": "2409.18162v3",
    "entry_id": "http://arxiv.org/abs/2409.18162v3",
    "title": "The Nexus of AR/VR, AI, UI/UX, and Robotics Technologies in Enhancing Learning and Social Interaction for Children with Autism Spectrum Disorders: A Systematic Review",
    "summary": "The emergence of large language models (LLMs), augmented reality (AR), and user interface/user experience (UI/UX) design in therapies for children, especially with disorders like autism spectrum disorder (ASD), is studied in detail in this review study. 150 publications were collected by a thorough literature search throughout PubMed, ACM, IEEE Xplore, Elsevier, and Google Scholar; 60 of them were chosen based on their methodological rigor and relevance to the focus area. Three of the primary areas are studied and covered in this review: how AR can improve social and learning results, how LLMs can support communication, and how UI/UX design affects how effective these technologies can be. Results show that while LLMs can provide individualized learning and communication support, AR has shown promise in enhancing social skills, motivation, and attention. For children with ASD, accessible and engaging interventions rely heavily on effective UI/UX design, but there is still a significant lack of robotics-based education and therapeutic programs specifically tailored for autistic children. To optimize the benefits of these technologies in ASD therapies and immersive education, the study emphasizes the need for additional research to address difficulties related to customization, accessibility, and integration.",
    "authors": [
      "Biplov Paneru"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2024-09-26T17:19:25Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18162v3"
  },
  {
    "arxiv_id": "2409.17655v3",
    "entry_id": "http://arxiv.org/abs/2409.17655v3",
    "title": "AssistantX: An LLM-Powered Proactive Assistant in Collaborative Human-Populated Environment",
    "summary": "Current service robots suffer from limited natural language communication abilities, heavy reliance on predefined commands, ongoing human intervention, and, most notably, a lack of proactive collaboration awareness in human-populated environments. This results in narrow applicability and low utility. In this paper, we introduce AssistantX, an LLM-powered proactive assistant designed for autonomous operation in realworld scenarios with high accuracy. AssistantX employs a multi-agent framework consisting of 4 specialized LLM agents, each dedicated to perception, planning, decision-making, and reflective review, facilitating advanced inference capabilities and comprehensive collaboration awareness, much like a human assistant by your side. We built a dataset of 210 real-world tasks to validate AssistantX, which includes instruction content and status information on whether relevant personnel are available. Extensive experiments were conducted in both text-based simulations and a real office environment over the course of a month and a half. Our experiments demonstrate the effectiveness of the proposed framework, showing that AssistantX can reactively respond to user instructions, actively adjust strategies to adapt to contingencies, and proactively seek assistance from humans to ensure successful task completion. More details and videos can be found at https://assistantx-agent.github.io/AssistantX/.",
    "authors": [
      "Nan Sun",
      "Bo Mao",
      "Yongchang Li",
      "Di Guo",
      "Huaping Liu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2024-09-26T09:06:56Z",
    "pdf_url": "https://arxiv.org/pdf/2409.17655v3"
  },
  {
    "arxiv_id": "2410.08224v1",
    "entry_id": "http://arxiv.org/abs/2410.08224v1",
    "title": "A Survey of Spatio-Temporal EEG data Analysis: from Models to Applications",
    "summary": "In recent years, the field of electroencephalography (EEG) analysis has witnessed remarkable advancements, driven by the integration of machine learning and artificial intelligence. This survey aims to encapsulate the latest developments, focusing on emerging methods and technologies that are poised to transform our comprehension and interpretation of brain activity. We delve into self-supervised learning methods that enable the robust representation of brain signals, which are fundamental for a variety of downstream applications. We also explore emerging discriminative methods, including graph neural networks (GNN), foundation models, and large language models (LLMs)-based approaches. Furthermore, we examine generative technologies that harness EEG data to produce images or text, offering novel perspectives on brain activity visualization and interpretation. The survey provides an extensive overview of these cutting-edge techniques, their current applications, and the profound implications they hold for future research and clinical practice. The relevant literature and open-source materials have been compiled and are consistently being refreshed at \\url{https://github.com/wpf535236337/LLMs4TS}",
    "authors": [
      "Pengfei Wang",
      "Huanran Zheng",
      "Silong Dai",
      "Yiqiao Wang",
      "Xiaotian Gu",
      "Yuanbin Wu",
      "Xiaoling Wang"
    ],
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG",
      "q-bio.NC"
    ],
    "published": "2024-09-26T08:09:15Z",
    "pdf_url": "https://arxiv.org/pdf/2410.08224v1"
  },
  {
    "arxiv_id": "2409.16876v3",
    "entry_id": "http://arxiv.org/abs/2409.16876v3",
    "title": "Automating Traffic Model Enhancement with AI Research Agent",
    "summary": "Developing efficient traffic models is crucial for optimizing modern transportation systems. However, current modeling approaches remain labor-intensive and prone to human errors due to their dependence on manual workflows. These processes typically involve extensive literature reviews, formula tuning, and iterative testing, which often lead to inefficiencies. To address this, we propose TR-Agent, an AI-powered framework that autonomously develops and refines traffic models through a closed-loop, iterative process. We structure the research pipeline into four key stages: idea generation, theory formulation, theory evaluation, and iterative optimization, and implement TR-Agent with four corresponding modules. These modules collaborate to retrieve knowledge from external sources, generate novel hypotheses, implement and debug models, and evaluate their performance on evaluation datasets. Through iteratively feedback and refinement, TR-Agent improves both modeling efficiency and effectiveness. We validate the framework on three representative traffic models: the Intelligent Driver Model (IDM) for car-following behavior, the MOBIL model for lane-changing, and the Lighthill-Whitham-Richards (LWR) speed-density relationship for macroscopic traffic flow modeling. Experimental results show substantial performance gains over the original models. To assess the robustness and generalizability of the improvements, we conduct additional evaluations across multiple real-world datasets, demonstrating consistent performance gains beyond the original development data. Furthermore, TR-Agent produces interpretable explanations for each improvement, enabling researchers to easily verify and extend its results. This makes TR-Agent a valuable assistant for traffic modeling refinement and a promising tool for broader applications in transportation research.",
    "authors": [
      "Xusen Guo",
      "Xinxi Yang",
      "Mingxing Peng",
      "Hongliang Lu",
      "Meixin Zhu",
      "Hai Yang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-09-25T12:42:25Z",
    "pdf_url": "https://arxiv.org/pdf/2409.16876v3"
  },
  {
    "arxiv_id": "2409.16860v1",
    "entry_id": "http://arxiv.org/abs/2409.16860v1",
    "title": "The Role of Language Models in Modern Healthcare: A Comprehensive Review",
    "summary": "The application of large language models (LLMs) in healthcare has gained significant attention due to their ability to process complex medical data and provide insights for clinical decision-making. These models have demonstrated substantial capabilities in understanding and generating natural language, which is crucial for medical documentation, diagnostics, and patient interaction. This review examines the trajectory of language models from their early stages to the current state-of-the-art LLMs, highlighting their strengths in healthcare applications and discussing challenges such as data privacy, bias, and ethical considerations. The potential of LLMs to enhance healthcare delivery is explored, alongside the necessary steps to ensure their ethical and effective integration into medical practice.",
    "authors": [
      "Amna Khalid",
      "Ayma Khalid",
      "Umar Khalid"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-25T12:15:15Z",
    "pdf_url": "https://arxiv.org/pdf/2409.16860v1"
  },
  {
    "arxiv_id": "2409.16813v2",
    "entry_id": "http://arxiv.org/abs/2409.16813v2",
    "title": "PeerArg: Argumentative Peer Review with LLMs",
    "summary": "Peer review is an essential process to determine the quality of papers submitted to scientific conferences or journals. However, it is subjective and prone to biases. Several studies have been conducted to apply techniques from NLP to support peer review, but they are based on black-box techniques and their outputs are difficult to interpret and trust. In this paper, we propose a novel pipeline to support and understand the reviewing and decision-making processes of peer review: the PeerArg system combining LLMs with methods from knowledge representation. PeerArg takes in input a set of reviews for a paper and outputs the paper acceptance prediction. We evaluate the performance of the PeerArg pipeline on three different datasets, in comparison with a novel end-2-end LLM that uses few-shot learning to predict paper acceptance given reviews. The results indicate that the end-2-end LLM is capable of predicting paper acceptance from reviews, but a variant of the PeerArg pipeline outperforms this LLM.",
    "authors": [
      "Purin Sukpanichnant",
      "Anna Rapberger",
      "Francesca Toni"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-09-25T11:09:39Z",
    "pdf_url": "https://arxiv.org/pdf/2409.16813v2"
  },
  {
    "arxiv_id": "2409.16694v3",
    "entry_id": "http://arxiv.org/abs/2409.16694v3",
    "title": "A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms",
    "summary": "Large language models (LLMs) have achieved remarkable advancements in natural language processing, showcasing exceptional performance across various tasks. However, the expensive memory and computational requirements present significant challenges for their practical deployment. Low-bit quantization has emerged as a critical approach to mitigate these challenges by reducing the bit-width of model parameters, activations, and gradients, thus decreasing memory usage and computational demands. This paper presents a comprehensive survey of low-bit quantization methods tailored for LLMs, covering the fundamental principles, system implementations, and algorithmic strategies. An overview of basic concepts and new data formats specific to low-bit LLMs is first introduced, followed by a review of frameworks and systems that facilitate low-bit LLMs across various hardware platforms. Then, we categorize and analyze techniques and toolkits for efficient low-bit training and inference of LLMs. Finally, we conclude with a discussion of future trends and potential advancements of low-bit LLMs. Our systematic overview from basic, system, and algorithm perspectives can offer valuable insights and guidelines for future works to enhance the efficiency and applicability of LLMs through low-bit quantization.",
    "authors": [
      "Ruihao Gong",
      "Yifu Ding",
      "Zining Wang",
      "Chengtao Lv",
      "Xingyu Zheng",
      "Jinyang Du",
      "Haotong Qin",
      "Jinyang Guo",
      "Michele Magno",
      "Xianglong Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-09-25T07:38:02Z",
    "pdf_url": "https://arxiv.org/pdf/2409.16694v3"
  },
  {
    "arxiv_id": "2409.16605v1",
    "entry_id": "http://arxiv.org/abs/2409.16605v1",
    "title": "Evaluating and Enhancing Large Language Models for Novelty Assessment in Scholarly Publications",
    "summary": "Recent studies have evaluated the creativity/novelty of large language models (LLMs) primarily from a semantic perspective, using benchmarks from cognitive science. However, accessing the novelty in scholarly publications is a largely unexplored area in evaluating LLMs. In this paper, we introduce a scholarly novelty benchmark (SchNovel) to evaluate LLMs' ability to assess novelty in scholarly papers. SchNovel consists of 15000 pairs of papers across six fields sampled from the arXiv dataset with publication dates spanning 2 to 10 years apart. In each pair, the more recently published paper is assumed to be more novel. Additionally, we propose RAG-Novelty, which simulates the review process taken by human reviewers by leveraging the retrieval of similar papers to assess novelty. Extensive experiments provide insights into the capabilities of different LLMs to assess novelty and demonstrate that RAG-Novelty outperforms recent baseline models.",
    "authors": [
      "Ethan Lin",
      "Zhiyuan Peng",
      "Yi Fang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-09-25T04:12:38Z",
    "pdf_url": "https://arxiv.org/pdf/2409.16605v1"
  },
  {
    "arxiv_id": "2409.16430v1",
    "entry_id": "http://arxiv.org/abs/2409.16430v1",
    "title": "A Comprehensive Survey of Bias in LLMs: Current Landscape and Future Directions",
    "summary": "Large Language Models(LLMs) have revolutionized various applications in natural language processing (NLP) by providing unprecedented text generation, translation, and comprehension capabilities. However, their widespread deployment has brought to light significant concerns regarding biases embedded within these models. This paper presents a comprehensive survey of biases in LLMs, aiming to provide an extensive review of the types, sources, impacts, and mitigation strategies related to these biases. We systematically categorize biases into several dimensions. Our survey synthesizes current research findings and discusses the implications of biases in real-world applications. Additionally, we critically assess existing bias mitigation techniques and propose future research directions to enhance fairness and equity in LLMs. This survey serves as a foundational resource for researchers, practitioners, and policymakers concerned with addressing and understanding biases in LLMs.",
    "authors": [
      "Rajesh Ranjan",
      "Shailja Gupta",
      "Surya Narayan Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2024-09-24T19:50:38Z",
    "pdf_url": "https://arxiv.org/pdf/2409.16430v1"
  },
  {
    "arxiv_id": "2409.16241v1",
    "entry_id": "http://arxiv.org/abs/2409.16241v1",
    "title": "LLM Echo Chamber: personalized and automated disinformation",
    "summary": "Recent advancements have showcased the capabilities of Large Language Models like GPT4 and Llama2 in tasks such as summarization, translation, and content review. However, their widespread use raises concerns, particularly around the potential for LLMs to spread persuasive, humanlike misinformation at scale, which could significantly influence public opinion. This study examines these risks, focusing on LLMs ability to propagate misinformation as factual. To investigate this, we built the LLM Echo Chamber, a controlled digital environment simulating social media chatrooms, where misinformation often spreads. Echo chambers, where individuals only interact with like minded people, further entrench beliefs. By studying malicious bots spreading misinformation in this environment, we can better understand this phenomenon. We reviewed current LLMs, explored misinformation risks, and applied sota finetuning techniques. Using Microsoft phi2 model, finetuned with our custom dataset, we generated harmful content to create the Echo Chamber. This setup, evaluated by GPT4 for persuasiveness and harmfulness, sheds light on the ethical concerns surrounding LLMs and emphasizes the need for stronger safeguards against misinformation.",
    "authors": [
      "Tony Ma"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-09-24T17:04:12Z",
    "pdf_url": "https://arxiv.org/pdf/2409.16241v1"
  },
  {
    "arxiv_id": "2409.18807v1",
    "entry_id": "http://arxiv.org/abs/2409.18807v1",
    "title": "LLM With Tools: A Survey",
    "summary": "The integration of tools in augmenting large language models presents a novel approach toward enhancing the efficiency and accuracy of these models in handling specific, complex tasks. This paper delves into the methodology,challenges, and developments in the realm of teaching LLMs to use external tools, thereby pushing the boundaries of their capabilities beyond pre-existing knowledge bases. We introduce a standardized paradigm for tool integration guided by a series of functions that map user instructions to actionable plans and their execution, emphasizing the significance of understanding user intent, tool selection, and dynamic plan adjustment. Our exploration reveals the various challenges encountered, such as tool invocation timing, selection accuracy, and the need for robust reasoning processes. In addressing these challenges, we investigate techniques within the context of fine-tuning and incontext learning paradigms, highlighting innovative approaches to ensure diversity, augment datasets, and improve generalization.Furthermore, we investigate a perspective on enabling LLMs to not only utilize but also autonomously create tools, which may redefine their role from mere tool users to tool creators. Finally,we reproduced Chameleon's results on ScienceQA and analyzed the code structure.",
    "authors": [
      "Zhuocheng Shen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-09-24T14:08:11Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18807v1"
  },
  {
    "arxiv_id": "2409.15790v3",
    "entry_id": "http://arxiv.org/abs/2409.15790v3",
    "title": "Small Language Models: Survey, Measurements, and Insights",
    "summary": "Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data centers and cloud environments. While researchers continue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM research aims to make machine intelligence more accessible, affordable, and efficient for everyday tasks. Focusing on transformer-based, decoder-only language models with 100M-5B parameters, we survey 70 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms. In addition, we evaluate their capabilities in various domains, including commonsense reasoning, mathematics, in-context learning, and long context. To gain further insight into their on-device runtime costs, we benchmark their inference latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer valuable insights to advance research in this field.",
    "authors": [
      "Zhenyan Lu",
      "Xiang Li",
      "Dongqi Cai",
      "Rongjie Yi",
      "Fangming Liu",
      "Xiwen Zhang",
      "Nicholas D. Lane",
      "Mengwei Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-09-24T06:36:56Z",
    "pdf_url": "https://arxiv.org/pdf/2409.15790v3"
  },
  {
    "arxiv_id": "2409.15724v2",
    "entry_id": "http://arxiv.org/abs/2409.15724v2",
    "title": "LLM-Cure: LLM-based Competitor User Review Analysis for Feature Enhancement",
    "summary": "The exponential growth of the mobile app market underscores the importance of constant innovation and rapid response to user demands. As user satisfaction is paramount to the success of a mobile application (app), developers typically rely on user reviews, which represent user feedback that includes ratings and comments to identify areas for improvement. However, the sheer volume of user reviews poses challenges in manual analysis, necessitating automated approaches. Existing automated approaches either analyze only the target apps reviews, neglecting the comparison of similar features to competitors or fail to provide suggestions for feature enhancement. To address these gaps, we propose a Large Language Model (LLM)-based Competitive User Review Analysis for Feature Enhancement) (LLM-Cure), an approach powered by LLMs to automatically generate suggestion s for mobile app feature improvements. More specifically, LLM-Cure identifies and categorizes features within reviews by applying LLMs. When provided with a complaint in a user review, LLM-Cure curates highly rated (4 and 5 stars) reviews in competing apps related to the complaint and proposes potential improvements tailored to the target application. We evaluate LLM-Cure on 1,056,739 reviews of 70 popular Android apps. Our evaluation demonstrates that LLM-Cure significantly outperforms the state-of-the-art approaches in assigning features to reviews by up to 13% in F1-score, up to 16% in recall and up to 11% in precision. Additionally, LLM-Cure demonstrates its capability to provide suggestions for resolving user complaints. We verify the suggestions using the release notes that reflect the changes of features in the target mobile app. LLM-Cure achieves a promising average of 73% of the implementation of the provided suggestions.",
    "authors": [
      "Maram Assi",
      "Safwat Hassan",
      "Ying Zou"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-09-24T04:17:21Z",
    "pdf_url": "https://arxiv.org/pdf/2409.15724v2"
  },
  {
    "arxiv_id": "2409.15723v1",
    "entry_id": "http://arxiv.org/abs/2409.15723v1",
    "title": "Federated Large Language Models: Current Progress and Future Directions",
    "summary": "Large language models are rapidly gaining popularity and have been widely adopted in real-world applications. While the quality of training data is essential, privacy concerns arise during data collection. Federated learning offers a solution by allowing multiple clients to collaboratively train LLMs without sharing local data. However, FL introduces new challenges, such as model convergence issues due to heterogeneous data and high communication costs. A comprehensive study is required to address these challenges and guide future research. This paper surveys Federated learning for LLMs (FedLLM), highlighting recent advances and future directions. We focus on two key aspects: fine-tuning and prompt learning in a federated setting, discussing existing work and associated research challenges. We finally propose potential research directions for federated LLMs, including pre-training and how LLMs can further enhance federated learning.",
    "authors": [
      "Yuhang Yao",
      "Jianyi Zhang",
      "Junda Wu",
      "Chengkai Huang",
      "Yu Xia",
      "Tong Yu",
      "Ruiyi Zhang",
      "Sungchul Kim",
      "Ryan Rossi",
      "Ang Li",
      "Lina Yao",
      "Julian McAuley",
      "Yiran Chen",
      "Carlee Joe-Wong"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-09-24T04:14:33Z",
    "pdf_url": "https://arxiv.org/pdf/2409.15723v1"
  },
  {
    "arxiv_id": "2409.14993v2",
    "entry_id": "http://arxiv.org/abs/2409.14993v2",
    "title": "Multi-modal Generative AI: Multi-modal LLMs, Diffusions and the Unification",
    "summary": "Multi-modal generative AI (Artificial Intelligence) has attracted increasing attention from both academia and industry. Particularly, two dominant families of techniques have emerged: i) Multi-modal large language models (LLMs) demonstrate impressive ability for multi-modal understanding; and ii) Diffusion models exhibit remarkable multi-modal powers in terms of multi-modal generation. Therefore, this paper provides a comprehensive overview of multi-modal generative AI, including multi-modal LLMs, diffusions, and the unification for understanding and generation. To lay a solid foundation for unified models, we first provide a detailed review of both multi-modal LLMs and diffusion models respectively, including their probabilistic modeling procedure, multi-modal architecture design, and advanced applications to image/video LLMs as well as text-to-image/video generation. Furthermore, we explore the emerging efforts toward unified models for understanding and generation. To achieve the unification of understanding and generation, we investigate key designs including autoregressive-based and diffusion-based modeling, as well as dense and Mixture-of-Experts (MoE) architectures. We then introduce several strategies for unified models, analyzing their potential advantages and disadvantages. In addition, we summarize the common datasets widely used for multi-modal generative AI pretraining. Last but not least, we present several challenging future research directions which may contribute to the ongoing advancement of multi-modal generative AI.",
    "authors": [
      "Xin Wang",
      "Yuwei Zhou",
      "Bin Huang",
      "Hong Chen",
      "Wenwu Zhu"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-09-23T13:16:09Z",
    "pdf_url": "https://arxiv.org/pdf/2409.14993v2"
  },
  {
    "arxiv_id": "2409.14924v1",
    "entry_id": "http://arxiv.org/abs/2409.14924v1",
    "title": "Retrieval Augmented Generation (RAG) and Beyond: A Comprehensive Survey on How to Make your LLMs use External Data More Wisely",
    "summary": "Large language models (LLMs) augmented with external data have demonstrated remarkable capabilities in completing real-world tasks. Techniques for integrating external data into LLMs, such as Retrieval-Augmented Generation (RAG) and fine-tuning, are gaining increasing attention and widespread application. Nonetheless, the effective deployment of data-augmented LLMs across various specialized fields presents substantial challenges. These challenges encompass a wide range of issues, from retrieving relevant data and accurately interpreting user intent to fully harnessing the reasoning capabilities of LLMs for complex tasks. We believe that there is no one-size-fits-all solution for data-augmented LLM applications. In practice, underperformance often arises from a failure to correctly identify the core focus of a task or because the task inherently requires a blend of multiple capabilities that must be disentangled for better resolution. In this survey, we propose a RAG task categorization method, classifying user queries into four levels based on the type of external data required and primary focus of the task: explicit fact queries, implicit fact queries, interpretable rationale queries, and hidden rationale queries. We define these levels of queries, provide relevant datasets, and summarize the key challenges and most effective techniques for addressing these challenges. Finally, we discuss three main forms of integrating external data into LLMs: context, small model, and fine-tuning, highlighting their respective strengths, limitations, and the types of problems they are suited to solve. This work aims to help readers thoroughly understand and decompose the data requirements and key bottlenecks in building LLM applications, offering solutions to the different challenges and serving as a guide to systematically developing such applications.",
    "authors": [
      "Siyun Zhao",
      "Yuqing Yang",
      "Zilong Wang",
      "Zhiyuan He",
      "Luna K. Qiu",
      "Lili Qiu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-23T11:20:20Z",
    "pdf_url": "https://arxiv.org/pdf/2409.14924v1"
  },
  {
    "arxiv_id": "2409.14457v3",
    "entry_id": "http://arxiv.org/abs/2409.14457v3",
    "title": "Large Model Based Agents: State-of-the-Art, Cooperation Paradigms, Security and Privacy, and Future Trends",
    "summary": "With the rapid advancement of large models (LMs), the development of general-purpose intelligent agents powered by LMs has become a reality. It is foreseeable that in the near future, LM-driven general AI agents will serve as essential tools in production tasks, capable of autonomous communication and collaboration without human intervention. This paper investigates scenarios involving the autonomous collaboration of future LM agents. We review the current state of LM agents, the key technologies enabling LM agent collaboration, and the security and privacy challenges they face during cooperative operations. To this end, we first explore the foundational principles of LM agents, including their general architecture, key components, enabling technologies, and modern applications. We then discuss practical collaboration paradigms from data, computation, and knowledge perspectives to achieve connected intelligence among LM agents. After that, we analyze the security vulnerabilities and privacy risks associated with LM agents, particularly in multi-agent settings, examining underlying mechanisms and reviewing current and potential countermeasures. Lastly, we propose future research directions for building robust and secure LM agent ecosystems.",
    "authors": [
      "Yuntao Wang",
      "Yanghe Pan",
      "Zhou Su",
      "Yi Deng",
      "Quan Zhao",
      "Linkang Du",
      "Tom H. Luan",
      "Jiawen Kang",
      "Dusit Niyato"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-09-22T14:09:49Z",
    "pdf_url": "https://arxiv.org/pdf/2409.14457v3"
  },
  {
    "arxiv_id": "2409.18142v1",
    "entry_id": "http://arxiv.org/abs/2409.18142v1",
    "title": "A Survey on Multimodal Benchmarks: In the Era of Large AI Models",
    "summary": "The rapid evolution of Multimodal Large Language Models (MLLMs) has brought substantial advancements in artificial intelligence, significantly enhancing the capability to understand and generate multimodal content. While prior studies have largely concentrated on model architectures and training methodologies, a thorough analysis of the benchmarks used for evaluating these models remains underexplored. This survey addresses this gap by systematically reviewing 211 benchmarks that assess MLLMs across four core domains: understanding, reasoning, generation, and application. We provide a detailed analysis of task designs, evaluation metrics, and dataset constructions, across diverse modalities. We hope that this survey will contribute to the ongoing advancement of MLLM research by offering a comprehensive overview of benchmarking practices and identifying promising directions for future work. An associated GitHub repository collecting the latest papers is available.",
    "authors": [
      "Lin Li",
      "Guikun Chen",
      "Hanrong Shi",
      "Jun Xiao",
      "Long Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.MM"
    ],
    "published": "2024-09-21T15:22:26Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18142v1"
  },
  {
    "arxiv_id": "2409.14165v3",
    "entry_id": "http://arxiv.org/abs/2409.14165v3",
    "title": "A Survey on Large Language Model-empowered Autonomous Driving",
    "summary": "Artificial intelligence (AI) plays a crucial role in autonomous driving (AD) research, propelling its development towards intelligence and efficiency. Currently, the development of AD technology follows two main technical paths: modularization and end-to-end. Modularization decompose the driving task into modules such as perception, prediction, planning, and control, and train them separately. Due to the inconsistency of training objectives between modules, the integrated effect suffers from bias. End-to-end attempts to address this issue by utilizing a single model that directly maps from sensor data to control signals. This path has limited learning capabilities in a comprehensive set of features and struggles to handle unpredictable long-tail events and complex urban traffic scenarios. In the face of challenges encountered in both paths, many researchers believe that large language models (LLMs) with powerful reasoning capabilities and extensive knowledge understanding may be the solution, expecting LLMs to provide AD systems with deeper levels of understanding and decision-making capabilities. In light of the challenges faced by both paths, many researchers believe that LLMs, with their powerful reasoning abilities and extensive knowledge, could offer a solution. To understand if LLMs could enhance AD, this paper conducts a thorough analysis of the potential applications of LLMs in AD systems, including exploring their optimization strategies in both modular and end-to-end approaches, with a particular focus on how LLMs can tackle the problems and challenges present in current solutions. Furthermore, we discuss an important question: Can LLM-based artificial general intelligence (AGI) be a key to achieve high-level AD? We further analyze the potential limitations and challenges that LLMs may encounter in promoting the development of AD technology.",
    "authors": [
      "Yuxuan Zhu",
      "Shiyi Wang",
      "Wenqing Zhong",
      "Nianchen Shen",
      "Yunqi Li",
      "Siqi Wang",
      "Zhiheng Li",
      "Cathy Wu",
      "Zhengbing He",
      "Li Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO",
      "eess.SY"
    ],
    "published": "2024-09-21T15:07:37Z",
    "pdf_url": "https://arxiv.org/pdf/2409.14165v3"
  },
  {
    "arxiv_id": "2409.13524v1",
    "entry_id": "http://arxiv.org/abs/2409.13524v1",
    "title": "Contextualized AI for Cyber Defense: An Automated Survey using LLMs",
    "summary": "This paper surveys the potential of contextualized AI in enhancing cyber defense capabilities, revealing significant research growth from 2015 to 2024. We identify a focus on robustness, reliability, and integration methods, while noting gaps in organizational trust and governance frameworks. Our study employs two LLM-assisted literature survey methodologies: (A) ChatGPT 4 for exploration, and (B) Gemma 2:9b for filtering with Claude 3.5 Sonnet for full-text analysis. We discuss the effectiveness and challenges of using LLMs in academic research, providing insights for future researchers.",
    "authors": [
      "Christoforus Yoga Haryanto",
      "Anne Maria Elvira",
      "Trung Duc Nguyen",
      "Minh Hieu Vu",
      "Yoshiano Hartanto",
      "Emily Lomempow",
      "Arathi Arakala"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-09-20T14:05:40Z",
    "pdf_url": "https://arxiv.org/pdf/2409.13524v1"
  },
  {
    "arxiv_id": "2409.13354v1",
    "entry_id": "http://arxiv.org/abs/2409.13354v1",
    "title": "Recent Advancement of Emotion Cognition in Large Language Models",
    "summary": "Emotion cognition in large language models (LLMs) is crucial for enhancing performance across various applications, such as social media, human-computer interaction, and mental health assessment. We explore the current landscape of research, which primarily revolves around emotion classification, emotionally rich response generation, and Theory of Mind assessments, while acknowledge the challenges like dependency on annotated data and complexity in emotion processing. In this paper, we present a detailed survey of recent progress in LLMs for emotion cognition. We explore key research studies, methodologies, outcomes, and resources, aligning them with Ulric Neisser's cognitive stages. Additionally, we outline potential future directions for research in this evolving field, including unsupervised learning approaches and the development of more complex and interpretable emotion cognition LLMs. We also discuss advanced methods such as contrastive learning used to improve LLMs' emotion cognition capabilities.",
    "authors": [
      "Yuyan Chen",
      "Yanghua Xiao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-20T09:34:58Z",
    "pdf_url": "https://arxiv.org/pdf/2409.13354v1"
  },
  {
    "arxiv_id": "2409.12740v1",
    "entry_id": "http://arxiv.org/abs/2409.12740v1",
    "title": "HLLM: Enhancing Sequential Recommendations via Hierarchical Large Language Models for Item and User Modeling",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in various fields, prompting several studies to explore their potential in recommendation systems. However, these attempts have so far resulted in only modest improvements over traditional recommendation models. Moreover, three critical questions remain under-explored: firstly, the real value of LLMs' pre-trained weights, often considered to encapsulate world knowledge; secondly, the necessity of fine-tuning for recommendation tasks; lastly, whether LLMs can exhibit the same scalability benefits in recommendation systems as they do in other domains. In this paper, we propose a novel Hierarchical Large Language Model (HLLM) architecture designed to enhance sequential recommendation systems. Our approach employs a two-tier model: the first Item LLM extracts rich content features from the detailed text description of the item, while the second User LLM utilizes these features to predict users' future interests based on their interaction history. Extensive experiments demonstrate that our method effectively leverages the pre-trained capabilities of open-source LLMs, and further fine-tuning leads to significant performance boosts. Additionally, HLLM achieves excellent scalability, with the largest configuration utilizing 7B parameters for both item feature extraction and user interest modeling. Moreover, HLLM offers excellent training and serving efficiency, making it practical in real-world applications. Evaluations on two large-scale datasets, PixelRec and Amazon Reviews, show that HLLM achieves state-of-the-art results, outperforming traditional ID-based models by a wide margin. In online A/B testing, HLLM showcases notable gains, validating its practical impact in real-world recommendation scenarios. Codes are available at https://github.com/bytedance/HLLM.",
    "authors": [
      "Junyi Chen",
      "Lu Chi",
      "Bingyue Peng",
      "Zehuan Yuan"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-09-19T13:03:07Z",
    "pdf_url": "https://arxiv.org/pdf/2409.12740v1"
  },
  {
    "arxiv_id": "2409.18996v1",
    "entry_id": "http://arxiv.org/abs/2409.18996v1",
    "title": "From Linguistic Giants to Sensory Maestros: A Survey on Cross-Modal Reasoning with Large Language Models",
    "summary": "Cross-modal reasoning (CMR), the intricate process of synthesizing and drawing inferences across divergent sensory modalities, is increasingly recognized as a crucial capability in the progression toward more sophisticated and anthropomorphic artificial intelligence systems. Large Language Models (LLMs) represent a class of AI algorithms specifically engineered to parse, produce, and engage with human language on an extensive scale. The recent trend of deploying LLMs to tackle CMR tasks has marked a new mainstream of approaches for enhancing their effectiveness. This survey offers a nuanced exposition of current methodologies applied in CMR using LLMs, classifying these into a detailed three-tiered taxonomy. Moreover, the survey delves into the principal design strategies and operational techniques of prototypical models within this domain. Additionally, it articulates the prevailing challenges associated with the integration of LLMs in CMR and identifies prospective research directions. To sum up, this survey endeavors to expedite progress within this burgeoning field by endowing scholars with a holistic and detailed vista, showcasing the vanguard of current research whilst pinpointing potential avenues for advancement. An associated GitHub repository that collects the relevant papers can be found at https://github.com/ZuyiZhou/Awesome-Cross-modal-Reasoning-with-LLMs",
    "authors": [
      "Shengsheng Qian",
      "Zuyi Zhou",
      "Dizhan Xue",
      "Bing Wang",
      "Changsheng Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2024-09-19T02:51:54Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18996v1"
  },
  {
    "arxiv_id": "2409.12001v1",
    "entry_id": "http://arxiv.org/abs/2409.12001v1",
    "title": "Putting Data at the Centre of Offline Multi-Agent Reinforcement Learning",
    "summary": "Offline multi-agent reinforcement learning (MARL) is an exciting direction of research that uses static datasets to find optimal control policies for multi-agent systems. Though the field is by definition data-driven, efforts have thus far neglected data in their drive to achieve state-of-the-art results. We first substantiate this claim by surveying the literature, showing how the majority of works generate their own datasets without consistent methodology and provide sparse information about the characteristics of these datasets. We then show why neglecting the nature of the data is problematic, through salient examples of how tightly algorithmic performance is coupled to the dataset used, necessitating a common foundation for experiments in the field. In response, we take a big step towards improving data usage and data awareness in offline MARL, with three key contributions: (1) a clear guideline for generating novel datasets; (2) a standardisation of over 80 existing datasets, hosted in a publicly available repository, using a consistent storage format and easy-to-use API; and (3) a suite of analysis tools that allow us to understand these datasets better, aiding further development.",
    "authors": [
      "Claude Formanek",
      "Louise Beyers",
      "Callum Rhys Tilbury",
      "Jonathan P. Shock",
      "Arnu Pretorius"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2024-09-18T14:13:24Z",
    "pdf_url": "https://arxiv.org/pdf/2409.12001v1"
  },
  {
    "arxiv_id": "2409.11756v1",
    "entry_id": "http://arxiv.org/abs/2409.11756v1",
    "title": "Synthesizing Evolving Symbolic Representations for Autonomous Systems",
    "summary": "Recently, AI systems have made remarkable progress in various tasks. Deep Reinforcement Learning(DRL) is an effective tool for agents to learn policies in low-level state spaces to solve highly complex tasks. Researchers have introduced Intrinsic Motivation(IM) to the RL mechanism, which simulates the agent's curiosity, encouraging agents to explore interesting areas of the environment. This new feature has proved vital in enabling agents to learn policies without being given specific goals. However, even though DRL intelligence emerges through a sub-symbolic model, there is still a need for a sort of abstraction to understand the knowledge collected by the agent. To this end, the classical planning formalism has been used in recent research to explicitly represent the knowledge an autonomous agent acquires and effectively reach extrinsic goals. Despite classical planning usually presents limited expressive capabilities, PPDDL demonstrated usefulness in reviewing the knowledge gathered by an autonomous system, making explicit causal correlations, and can be exploited to find a plan to reach any state the agent faces during its experience. This work presents a new architecture implementing an open-ended learning system able to synthesize from scratch its experience into a PPDDL representation and update it over time. Without a predefined set of goals and tasks, the system integrates intrinsic motivations to explore the environment in a self-directed way, exploiting the high-level knowledge acquired during its experience. The system explores the environment and iteratively: (a) discover options, (b) explore the environment using options, (c) abstract the knowledge collected and (d) plan. This paper proposes an alternative approach to implementing open-ended learning architectures exploiting low-level and high-level representations to extend its knowledge in a virtuous loop.",
    "authors": [
      "Gabriele Sartor",
      "Angelo Oddi",
      "Riccardo Rasconi",
      "Vieri Giuliano Santucci",
      "Rosa Meo"
    ],
    "categories": [
      "cs.AI",
      "cs.SC"
    ],
    "published": "2024-09-18T07:23:26Z",
    "pdf_url": "https://arxiv.org/pdf/2409.11756v1"
  },
  {
    "arxiv_id": "2410.02779v1",
    "entry_id": "http://arxiv.org/abs/2410.02779v1",
    "title": "Learning variant product relationship and variation attributes from e-commerce website structures",
    "summary": "We introduce VARM, variant relationship matcher strategy, to identify pairs of variant products in e-commerce catalogs. Traditional definitions of entity resolution are concerned with whether product mentions refer to the same underlying product. However, this fails to capture product relationships that are critical for e-commerce applications, such as having similar, but not identical, products listed on the same webpage or share reviews. Here, we formulate a new type of entity resolution in variant product relationships to capture these similar e-commerce product links. In contrast with the traditional definition, the new definition requires both identifying if two products are variant matches of each other and what are the attributes that vary between them. To satisfy these two requirements, we developed a strategy that leverages the strengths of both encoding and generative AI models. First, we construct a dataset that captures webpage product links, and therefore variant product relationships, to train an encoding LLM to predict variant matches for any given pair of products. Second, we use RAG prompted generative LLMs to extract variation and common attributes amongst groups of variant products. To validate our strategy, we evaluated model performance using real data from one of the world's leading e-commerce retailers. The results showed that our strategy outperforms alternative solutions and paves the way to exploiting these new type of product relationships.",
    "authors": [
      "Pedro Herrero-Vidal",
      "You-Lin Chen",
      "Cris Liu",
      "Prithviraj Sen",
      "Lichao Wang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-09-17T18:24:27Z",
    "pdf_url": "https://arxiv.org/pdf/2410.02779v1"
  },
  {
    "arxiv_id": "2409.11192v1",
    "entry_id": "http://arxiv.org/abs/2409.11192v1",
    "title": "Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory",
    "summary": "One application area of long-term memory (LTM) capabilities with increasing traction is personal AI companions and assistants. With the ability to retain and contextualize past interactions and adapt to user preferences, personal AI companions and assistants promise a profound shift in how we interact with AI and are on track to become indispensable in personal and professional settings. However, this advancement introduces new challenges and vulnerabilities that require careful consideration regarding the deployment and widespread use of these systems. The goal of this paper is to explore the broader implications of building and deploying personal AI applications with LTM capabilities using a holistic evaluation approach. This will be done in three ways: 1) reviewing the technological underpinnings of LTM in Large Language Models, 2) surveying current personal AI companions and assistants, and 3) analyzing critical considerations and implications of deploying and using these applications.",
    "authors": [
      "Eunhae Lee"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-09-17T13:48:29Z",
    "pdf_url": "https://arxiv.org/pdf/2409.11192v1"
  },
  {
    "arxiv_id": "2409.10959v2",
    "entry_id": "http://arxiv.org/abs/2409.10959v2",
    "title": "Leveraging Reviewer Experience in Code Review Comment Generation",
    "summary": "Modern code review is a ubiquitous software quality assurance process aimed at identifying potential issues within newly written code. Despite its effectiveness, the process demands large amounts of effort from the human reviewers involved. To help alleviate this workload, researchers have trained deep learning models to imitate human reviewers in providing natural language code reviews. Formally, this task is known as code review comment generation. Prior work has demonstrated improvements in this task by leveraging machine learning techniques and neural models, such as transfer learning and the transformer architecture. However, the quality of the model generated reviews remain sub-optimal due to the quality of the open-source code review data used in model training. This is in part due to the data obtained from open-source projects where code reviews are conducted in a public forum, and reviewers possess varying levels of software development experience, potentially affecting the quality of their feedback. To accommodate for this variation, we propose a suite of experience-aware training methods that utilise the reviewers' past authoring and reviewing experiences as signals for review quality. Specifically, we propose experience-aware loss functions (ELF), which use the reviewers' authoring and reviewing ownership of a project as weights in the model's loss function. Through this method, experienced reviewers' code reviews yield larger influence over the model's behaviour. Compared to the SOTA model, ELF was able to generate higher quality reviews in terms of accuracy, informativeness, and comment types generated. The key contribution of this work is the demonstration of how traditional software engineering concepts such as reviewer experience can be integrated into the design of AI-based automated code review models.",
    "authors": [
      "Hong Yi Lin",
      "Patanamon Thongtanunam",
      "Christoph Treude",
      "Michael W. Godfrey",
      "Chunhua Liu",
      "Wachiraphan Charoenwet"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2024-09-17T07:52:50Z",
    "pdf_url": "https://arxiv.org/pdf/2409.10959v2"
  },
  {
    "arxiv_id": "2410.01824v2",
    "entry_id": "http://arxiv.org/abs/2410.01824v2",
    "title": "AI Conversational Interviewing: Transforming Surveys with LLMs as Adaptive Interviewers",
    "summary": "Traditional methods for eliciting people's opinions face a trade-off between depth and scale: structured surveys enable large-scale data collection but limit respondents' ability to voice their opinions in their own words, while conversational interviews provide deeper insights but are resource-intensive. This study explores the potential of replacing human interviewers with large language models (LLMs) to conduct scalable conversational interviews. Our goal is to assess the performance of AI Conversational Interviewing and to identify opportunities for improvement in a controlled environment. We conducted a small-scale, in-depth study with university students who were randomly assigned to a conversational interview by either AI or human interviewers, both employing identical questionnaires on political topics. Various quantitative and qualitative measures assessed interviewer adherence to guidelines, response quality, participant engagement, and overall interview efficacy. The findings indicate the viability of AI Conversational Interviewing in producing quality data comparable to traditional methods, with the added benefit of scalability. We publish our data and materials for re-use and present specific recommendations for effective implementation.",
    "authors": [
      "Alexander Wuttke",
      "Matthias Aßenmacher",
      "Christopher Klamm",
      "Max M. Lang",
      "Quirin Würschinger",
      "Frauke Kreuter"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-16T16:03:08Z",
    "pdf_url": "https://arxiv.org/pdf/2410.01824v2"
  },
  {
    "arxiv_id": "2409.10102v1",
    "entry_id": "http://arxiv.org/abs/2409.10102v1",
    "title": "Trustworthiness in Retrieval-Augmented Generation Systems: A Survey",
    "summary": "Retrieval-Augmented Generation (RAG) has quickly grown into a pivotal paradigm in the development of Large Language Models (LLMs). While much of the current research in this field focuses on performance optimization, particularly in terms of accuracy and efficiency, the trustworthiness of RAG systems remains an area still under exploration. From a positive perspective, RAG systems are promising to enhance LLMs by providing them with useful and up-to-date knowledge from vast external databases, thereby mitigating the long-standing problem of hallucination. While from a negative perspective, RAG systems are at the risk of generating undesirable contents if the retrieved information is either inappropriate or poorly utilized. To address these concerns, we propose a unified framework that assesses the trustworthiness of RAG systems across six key dimensions: factuality, robustness, fairness, transparency, accountability, and privacy. Within this framework, we thoroughly review the existing literature on each dimension. Additionally, we create the evaluation benchmark regarding the six dimensions and conduct comprehensive evaluations for a variety of proprietary and open-source models. Finally, we identify the potential challenges for future research based on our investigation results. Through this work, we aim to lay a structured foundation for future investigations and provide practical insights for enhancing the trustworthiness of RAG systems in real-world applications.",
    "authors": [
      "Yujia Zhou",
      "Yan Liu",
      "Xiaoxi Li",
      "Jiajie Jin",
      "Hongjin Qian",
      "Zheng Liu",
      "Chaozhuo Li",
      "Zhicheng Dou",
      "Tsung-Yi Ho",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-16T09:06:44Z",
    "pdf_url": "https://arxiv.org/pdf/2409.10102v1"
  },
  {
    "arxiv_id": "2409.09989v1",
    "entry_id": "http://arxiv.org/abs/2409.09989v1",
    "title": "Comprehensive Study on Sentiment Analysis: From Rule-based to modern LLM based system",
    "summary": "This paper provides a comprehensive survey of sentiment analysis within the context of artificial intelligence (AI) and large language models (LLMs). Sentiment analysis, a critical aspect of natural language processing (NLP), has evolved significantly from traditional rule-based methods to advanced deep learning techniques. This study examines the historical development of sentiment analysis, highlighting the transition from lexicon-based and pattern-based approaches to more sophisticated machine learning and deep learning models. Key challenges are discussed, including handling bilingual texts, detecting sarcasm, and addressing biases. The paper reviews state-of-the-art approaches, identifies emerging trends, and outlines future research directions to advance the field. By synthesizing current methodologies and exploring future opportunities, this survey aims to understand sentiment analysis in the AI and LLM context thoroughly.",
    "authors": [
      "Shailja Gupta",
      "Rajesh Ranjan",
      "Surya Narayan Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2024-09-16T04:44:52Z",
    "pdf_url": "https://arxiv.org/pdf/2409.09989v1"
  },
  {
    "arxiv_id": "2409.09822v3",
    "entry_id": "http://arxiv.org/abs/2409.09822v3",
    "title": "Causal Inference with Large Language Model: A Survey",
    "summary": "Causal inference has been a pivotal challenge across diverse domains such as medicine and economics, demanding a complicated integration of human knowledge, mathematical reasoning, and data mining capabilities. Recent advancements in natural language processing (NLP), particularly with the advent of large language models (LLMs), have introduced promising opportunities for traditional causal inference tasks. This paper reviews recent progress in applying LLMs to causal inference, encompassing various tasks spanning different levels of causation. We summarize the main causal problems and approaches, and present a comparison of their evaluation results in different causal scenarios. Furthermore, we discuss key findings and outline directions for future research, underscoring the potential implications of integrating LLMs in advancing causal inference methodologies.",
    "authors": [
      "Jing Ma"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-15T18:43:11Z",
    "pdf_url": "https://arxiv.org/pdf/2409.09822v3"
  },
  {
    "arxiv_id": "2409.10579v1",
    "entry_id": "http://arxiv.org/abs/2409.10579v1",
    "title": "Recent advances in deep learning and language models for studying the microbiome",
    "summary": "Recent advancements in deep learning, particularly large language models (LLMs), made a significant impact on how researchers study microbiome and metagenomics data. Microbial protein and genomic sequences, like natural languages, form a language of life, enabling the adoption of LLMs to extract useful insights from complex microbial ecologies. In this paper, we review applications of deep learning and language models in analyzing microbiome and metagenomics data. We focus on problem formulations, necessary datasets, and the integration of language modeling techniques. We provide an extensive overview of protein/genomic language modeling and their contributions to microbiome studies. We also discuss applications such as novel viromics language modeling, biosynthetic gene cluster prediction, and knowledge integration for metagenomics studies.",
    "authors": [
      "Binghao Yan",
      "Yunbi Nam",
      "Lingyao Li",
      "Rebecca A. Deek",
      "Hongzhe Li",
      "Siyuan Ma"
    ],
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-09-15T18:32:31Z",
    "pdf_url": "https://arxiv.org/pdf/2409.10579v1"
  },
  {
    "arxiv_id": "2409.09704v1",
    "entry_id": "http://arxiv.org/abs/2409.09704v1",
    "title": "AlpaPICO: Extraction of PICO Frames from Clinical Trial Documents Using LLMs",
    "summary": "In recent years, there has been a surge in the publication of clinical trial reports, making it challenging to conduct systematic reviews. Automatically extracting Population, Intervention, Comparator, and Outcome (PICO) from clinical trial studies can alleviate the traditionally time-consuming process of manually scrutinizing systematic reviews. Existing approaches of PICO frame extraction involves supervised approach that relies on the existence of manually annotated data points in the form of BIO label tagging. Recent approaches, such as In-Context Learning (ICL), which has been shown to be effective for a number of downstream NLP tasks, require the use of labeled examples. In this work, we adopt ICL strategy by employing the pretrained knowledge of Large Language Models (LLMs), gathered during the pretraining phase of an LLM, to automatically extract the PICO-related terminologies from clinical trial documents in unsupervised set up to bypass the availability of large number of annotated data instances. Additionally, to showcase the highest effectiveness of LLM in oracle scenario where large number of annotated samples are available, we adopt the instruction tuning strategy by employing Low Rank Adaptation (LORA) to conduct the training of gigantic model in low resource environment for the PICO frame extraction task. Our empirical results show that our proposed ICL-based framework produces comparable results on all the version of EBM-NLP datasets and the proposed instruction tuned version of our framework produces state-of-the-art results on all the different EBM-NLP datasets. Our project is available at \\url{https://github.com/shrimonmuke0202/AlpaPICO.git}.",
    "authors": [
      "Madhusudan Ghosh",
      "Shrimon Mukherjee",
      "Asmit Ganguly",
      "Partha Basuchowdhuri",
      "Sudip Kumar Naskar",
      "Debasis Ganguly"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-09-15T11:53:24Z",
    "pdf_url": "https://arxiv.org/pdf/2409.09704v1"
  },
  {
    "arxiv_id": "2409.09601v1",
    "entry_id": "http://arxiv.org/abs/2409.09601v1",
    "title": "A Survey of Foundation Models for Music Understanding",
    "summary": "Music is essential in daily life, fulfilling emotional and entertainment needs, and connecting us personally, socially, and culturally. A better understanding of music can enhance our emotions, cognitive skills, and cultural connections. The rapid advancement of artificial intelligence (AI) has introduced new ways to analyze music, aiming to replicate human understanding of music and provide related services. While the traditional models focused on audio features and simple tasks, the recent development of large language models (LLMs) and foundation models (FMs), which excel in various fields by integrating semantic information and demonstrating strong reasoning abilities, could capture complex musical features and patterns, integrate music with language and incorporate rich musical, emotional and psychological knowledge. Therefore, they have the potential in handling complex music understanding tasks from a semantic perspective, producing outputs closer to human perception. This work, to our best knowledge, is one of the early reviews of the intersection of AI techniques and music understanding. We investigated, analyzed, and tested recent large-scale music foundation models in respect of their music comprehension abilities. We also discussed their limitations and proposed possible future directions, offering insights for researchers in this field.",
    "authors": [
      "Wenjun Li",
      "Ying Cai",
      "Ziyang Wu",
      "Wenyi Zhang",
      "Yifan Chen",
      "Rundong Qi",
      "Mengqi Dong",
      "Peigen Chen",
      "Xiao Dong",
      "Fenghao Shi",
      "Lei Guo",
      "Junwei Han",
      "Bao Ge",
      "Tianming Liu",
      "Lin Gan",
      "Tuo Zhang"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.MM",
      "eess.AS"
    ],
    "published": "2024-09-15T03:34:14Z",
    "pdf_url": "https://arxiv.org/pdf/2409.09601v1"
  },
  {
    "arxiv_id": "2409.09586v3",
    "entry_id": "http://arxiv.org/abs/2409.09586v3",
    "title": "ValueCompass: A Framework for Measuring Contextual Value Alignment Between Human and LLMs",
    "summary": "As AI systems become more advanced, ensuring their alignment with a diverse range of individuals and societal values becomes increasingly critical. But how can we capture fundamental human values and assess the degree to which AI systems align with them? We introduce ValueCompass, a framework of fundamental values, grounded in psychological theory and a systematic review, to identify and evaluate human-AI alignment. We apply ValueCompass to measure the value alignment of humans and large language models (LLMs) across four real-world scenarios: collaborative writing, education, public sectors, and healthcare. Our findings reveal concerning misalignments between humans and LLMs, such as humans frequently endorse values like \"National Security\" which were largely rejected by LLMs. We also observe that values differ across scenarios, highlighting the need for context-aware AI alignment strategies. This work provides valuable insights into the design space of human-AI alignment, laying the foundations for developing AI systems that responsibly reflect societal values and ethics.",
    "authors": [
      "Hua Shen",
      "Tiffany Knearem",
      "Reshmi Ghosh",
      "Yu-Ju Yang",
      "Nicholas Clark",
      "Tanushree Mitra",
      "Yun Huang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-15T02:13:03Z",
    "pdf_url": "https://arxiv.org/pdf/2409.09586v3"
  },
  {
    "arxiv_id": "2409.09403v2",
    "entry_id": "http://arxiv.org/abs/2409.09403v2",
    "title": "AI-Driven Virtual Teacher for Enhanced Educational Efficiency: Leveraging Large Pretrain Models for Autonomous Error Analysis and Correction",
    "summary": "Students frequently make mistakes while solving mathematical problems, and traditional error correction methods are both time-consuming and labor-intensive. This paper introduces an innovative \\textbf{V}irtual \\textbf{A}I \\textbf{T}eacher system designed to autonomously analyze and correct student \\textbf{E}rrors (VATE). Leveraging advanced large language models (LLMs), the system uses student drafts as a primary source for error analysis, which enhances understanding of the student's learning process. It incorporates sophisticated prompt engineering and maintains an error pool to reduce computational overhead. The AI-driven system also features a real-time dialogue component for efficient student interaction. Our approach demonstrates significant advantages over traditional and machine learning-based error correction methods, including reduced educational costs, high scalability, and superior generalizability. The system has been deployed on the Squirrel AI learning platform for elementary mathematics education, where it achieves 78.3\\% accuracy in error analysis and shows a marked improvement in student learning efficiency. Satisfaction surveys indicate a strong positive reception, highlighting the system's potential to transform educational practices.",
    "authors": [
      "Tianlong Xu",
      "Yi-Fan Zhang",
      "Zhendong Chu",
      "Shen Wang",
      "Qingsong Wen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "published": "2024-09-14T10:27:36Z",
    "pdf_url": "https://arxiv.org/pdf/2409.09403v2"
  },
  {
    "arxiv_id": "2410.01812v5",
    "entry_id": "http://arxiv.org/abs/2410.01812v5",
    "title": "From Text to Multimodality: Exploring the Evolution and Impact of Large Language Models in Medical Practice",
    "summary": "Large Language Models (LLMs) have rapidly evolved from text-based systems to multimodal platforms, significantly impacting various sectors including healthcare. This comprehensive review explores the progression of LLMs to Multimodal Large Language Models (MLLMs) and their growing influence in medical practice. We examine the current landscape of MLLMs in healthcare, analyzing their applications across clinical decision support, medical imaging, patient engagement, and research. The review highlights the unique capabilities of MLLMs in integrating diverse data types, such as text, images, and audio, to provide more comprehensive insights into patient health. We also address the challenges facing MLLM implementation, including data limitations, technical hurdles, and ethical considerations. By identifying key research gaps, this paper aims to guide future investigations in areas such as dataset development, modality alignment methods, and the establishment of ethical guidelines. As MLLMs continue to shape the future of healthcare, understanding their potential and limitations is crucial for their responsible and effective integration into medical practice.",
    "authors": [
      "Qian Niu",
      "Keyu Chen",
      "Ming Li",
      "Pohsun Feng",
      "Ziqian Bi",
      "Lawrence KQ Yan",
      "Yichao Zhang",
      "Caitlyn Heqi Yin",
      "Cheng Fei",
      "Junyu Liu",
      "Benji Peng",
      "Tianyang Wang",
      "Yunze Wang",
      "Silin Chen",
      "Ming Liu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-14T02:35:29Z",
    "pdf_url": "https://arxiv.org/pdf/2410.01812v5"
  },
  {
    "arxiv_id": "2409.09030v2",
    "entry_id": "http://arxiv.org/abs/2409.09030v2",
    "title": "Agents in Software Engineering: Survey, Landscape, and Vision",
    "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable success and have been widely used in various downstream tasks, especially in the tasks of the software engineering (SE) field. We find that many studies combining LLMs with SE have employed the concept of agents either explicitly or implicitly. However, there is a lack of an in-depth survey to sort out the development context of existing works, analyze how existing works combine the LLM-based agent technologies to optimize various tasks, and clarify the framework of LLM-based agents in SE. In this paper, we conduct the first survey of the studies on combining LLM-based agents with SE and present a framework of LLM-based agents in SE which includes three key modules: perception, memory, and action. We also summarize the current challenges in combining the two fields and propose future opportunities in response to existing challenges. We maintain a GitHub repository of the related papers at: https://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",
    "authors": [
      "Yanlin Wang",
      "Wanjun Zhong",
      "Yanxian Huang",
      "Ensheng Shi",
      "Min Yang",
      "Jiachi Chen",
      "Hui Li",
      "Yuchi Ma",
      "Qianxiang Wang",
      "Zibin Zheng"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-13T17:55:58Z",
    "pdf_url": "https://arxiv.org/pdf/2409.09030v2"
  },
  {
    "arxiv_id": "2409.07569v3",
    "entry_id": "http://arxiv.org/abs/2409.07569v3",
    "title": "A Comprehensive Survey on Inverse Constrained Reinforcement Learning: Definitions, Progress and Challenges",
    "summary": "Inverse Constrained Reinforcement Learning (ICRL) is the task of inferring the implicit constraints that expert agents adhere to, based on their demonstration data. As an emerging research topic, ICRL has received considerable attention in recent years. This article presents a categorical survey of the latest advances in ICRL. It serves as a comprehensive reference for machine learning researchers and practitioners, as well as starters seeking to comprehend the definitions, advancements, and important challenges in ICRL. We begin by formally defining the problem and outlining the algorithmic framework that facilitates constraint inference across various scenarios. These include deterministic or stochastic environments, environments with limited demonstrations, and multiple agents. For each context, we illustrate the critical challenges and introduce a series of fundamental methods to tackle these issues. This survey encompasses discrete, virtual, and realistic environments for evaluating ICRL agents. We also delve into the most pertinent applications of ICRL, such as autonomous driving, robot control, and sports analytics. To stimulate continuing research, we conclude the survey with a discussion of key unresolved questions in ICRL that can effectively foster a bridge between theoretical understanding and practical industrial applications. The papers referenced in this survey can be found at https://github.com/Jasonxu1225/Awesome-Constraint-Inference-in-RL.",
    "authors": [
      "Guiliang Liu",
      "Sheng Xu",
      "Shicheng Liu",
      "Ashish Gaurav",
      "Sriram Ganapathi Subramanian",
      "Pascal Poupart"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-09-11T18:49:03Z",
    "pdf_url": "https://arxiv.org/pdf/2409.07569v3"
  },
  {
    "arxiv_id": "2409.07368v3",
    "entry_id": "http://arxiv.org/abs/2409.07368v3",
    "title": "Demo: SGCode: A Flexible Prompt-Optimizing System for Secure Generation of Code",
    "summary": "This paper introduces SGCode, a flexible prompt-optimizing system to generate secure code with large language models (LLMs). SGCode integrates recent prompt-optimization approaches with LLMs in a unified system accessible through front-end and back-end APIs, enabling users to 1) generate secure code, which is free of vulnerabilities, 2) review and share security analysis, and 3) easily switch from one prompt optimization approach to another, while providing insights on model and system performance. We populated SGCode on an AWS server with PromSec, an approach that optimizes prompts by combining an LLM and security tools with a lightweight generative adversarial graph neural network to detect and fix security vulnerabilities in the generated code. Extensive experiments show that SGCode is practical as a public tool to gain insights into the trade-offs between model utility, secure code generation, and system cost. SGCode has only a marginal cost compared with prompting LLMs. SGCode is available at: https://sgcode.codes/.",
    "authors": [
      "Khiem Ton",
      "Nhi Nguyen",
      "Mahmoud Nazzal",
      "Abdallah Khreishah",
      "Cristian Borcea",
      "NhatHai Phan",
      "Ruoming Jin",
      "Issa Khalil",
      "Yelong Shen"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-09-11T15:56:15Z",
    "pdf_url": "https://arxiv.org/pdf/2409.07368v3"
  },
  {
    "arxiv_id": "2409.18968v2",
    "entry_id": "http://arxiv.org/abs/2409.18968v2",
    "title": "Safety challenges of AI in medicine in the era of large language models",
    "summary": "Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs), have unlocked significant potential to enhance the quality and efficiency of medical care. By introducing a novel way to interact with AI and data through natural language, LLMs offer new opportunities for medical practitioners, patients, and researchers. However, as AI and LLMs become more powerful and especially achieve superhuman performance in some medical tasks, public concerns over their safety have intensified. These concerns about AI safety have emerged as the most significant obstacles to the adoption of AI in medicine. In response, this review examines emerging risks in AI utilization during the LLM era. First, we explore LLM-specific safety challenges from functional and communication perspectives, addressing issues across data collection, model training, and real-world application. We then consider inherent safety problems shared by all AI systems, along with additional complications introduced by LLMs. Last, we discussed how safety issues of using AI in clinical practice and healthcare system operation would undermine trust among patient, clinicians and the public, and how to build confidence in these systems. By emphasizing the development of safe AI, we believe these technologies can be more rapidly and reliably integrated into everyday medical practice to benefit both patients and clinicians.",
    "authors": [
      "Xiaoye Wang",
      "Nicole Xi Zhang",
      "Hongyu He",
      "Trang Nguyen",
      "Kun-Hsing Yu",
      "Hao Deng",
      "Cynthia Brandt",
      "Danielle S. Bitterman",
      "Ling Pan",
      "Ching-Yu Cheng",
      "James Zou",
      "Dianbo Liu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-09-11T13:47:47Z",
    "pdf_url": "https://arxiv.org/pdf/2409.18968v2"
  },
  {
    "arxiv_id": "2409.07253v3",
    "entry_id": "http://arxiv.org/abs/2409.07253v3",
    "title": "Alignment of Diffusion Models: Fundamentals, Challenges, and Future",
    "summary": "Diffusion models have emerged as the leading paradigm in generative modeling, excelling in various applications. Despite their success, these models often misalign with human intentions and generate results with undesired properties or even harmful content. Inspired by the success and popularity of alignment in tuning large language models, recent studies have investigated aligning diffusion models with human expectations and preferences. This work mainly reviews alignment of text-to-image diffusion models, covering advancements in fundamentals of alignment, alignment techniques of diffusion models, preference benchmarks, and evaluation for diffusion models. Moreover, we discuss key perspectives on current challenges and promising future directions on solving the remaining challenges in alignment of diffusion models. To the best of our knowledge, our work is the first comprehensive review paper for researchers and engineers to comprehend, practice, and research alignment of diffusion models.",
    "authors": [
      "Buhua Liu",
      "Shitong Shao",
      "Bao Li",
      "Lichen Bai",
      "Zhiqiang Xu",
      "Haoyi Xiong",
      "James Kwok",
      "Sumi Helal",
      "Zeke Xie"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2024-09-11T13:21:32Z",
    "pdf_url": "https://arxiv.org/pdf/2409.07253v3"
  },
  {
    "arxiv_id": "2409.07189v2",
    "entry_id": "http://arxiv.org/abs/2409.07189v2",
    "title": "AI-Guided Molecular Simulations in VR: Exploring Strategies for Imitation Learning in Hyperdimensional Molecular Systems",
    "summary": "Molecular dynamics (MD) simulations are a crucial computational tool for researchers to understand and engineer molecular structure and function in areas such as drug discovery, protein engineering, and material design. Despite their utility, MD simulations are expensive, owing to the high dimensionality of molecular systems. Interactive molecular dynamics in virtual reality (iMD-VR) has recently emerged as a \"human-in-the-loop\" strategy for efficiently navigating hyper-dimensional molecular systems. By providing an immersive 3D environment that enables visualization and manipulation of real-time molecular simulations running on high-performance computing architectures, iMD-VR enables researchers to reach out and guide molecular conformational dynamics, in order to efficiently explore complex, high-dimensional molecular systems. Moreover, iMD-VR simulations generate rich datasets that capture human experts' spatial insight regarding molecular structure and function. This paper explores the use of researcher-generated iMD-VR datasets to train AI agents via imitation learning (IL). IL enables agents to mimic complex behaviours from expert demonstrations, circumventing the need for explicit programming or intricate reward design. In this article, we review IL across robotics and Multi-agents systems domains which are comparable to iMD-VR, and discuss how iMD-VR recordings could be used to train IL models to interact with MD simulations. We then illustrate the applications of these ideas through a proof-of-principle study where iMD-VR data was used to train a CNN network on a simple molecular manipulation task; namely, threading a small molecule through a nanotube pore. Finally, we outline future research directions and potential challenges of using AI agents to augment human expertise in navigating vast molecular conformational spaces.",
    "authors": [
      "Mohamed Dhouioui",
      "Jonathan Barnoud",
      "Rhoslyn Roebuck Williams",
      "Harry J. Stroud",
      "Phil Bates",
      "David R. Glowacki"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC",
      "q-bio.BM"
    ],
    "published": "2024-09-11T11:21:02Z",
    "pdf_url": "https://arxiv.org/pdf/2409.07189v2"
  },
  {
    "arxiv_id": "2409.06255v3",
    "entry_id": "http://arxiv.org/abs/2409.06255v3",
    "title": "Market Reaction to News Flows in Supply Chain Networks",
    "summary": "This study examines how positive and negative news about firms affects their stock prices and, moreover, how it affects stock prices of the firms' suppliers and clients, using a large sample of publicly listed firms around the world and another of Japanese listed firms. The level of positiveness and negativeness of each news article is determined by FinBERT, a natural language processing model fine-tuned specifically for financial information. Supply chains of firms across the world are identified mostly by financial statements, while those of Japanese firms are taken from large-scale firm-level surveys. We find that positive news increases the change rate of stock prices of firms mentioned in the news before its disclosure, most likely because of diffusion of information through unofficial information channels. Positive news also raises stock prices of the firms' suppliers and clients before and after its disclosure, confirming propagation of market values through supply chains. In addition, we generally find a larger post-news effect on stock prices of the mentioned firms and their suppliers and clients than the pre-news effect. The positive difference between the post- and pre-news effects can be considered as the net effect of the disclosure of positive news, controlling for information diffusion through private channels. However, the post-news effect on suppliers and clients in Japan is smaller than the pre-news effect, which is the opposite result to non-domestic firms from around the world.",
    "authors": [
      "Hiroyasu Inoue",
      "Yasuyuki Todo"
    ],
    "categories": [
      "cs.SI",
      "cs.LG"
    ],
    "published": "2024-09-10T06:55:17Z",
    "pdf_url": "https://arxiv.org/pdf/2409.06255v3"
  },
  {
    "arxiv_id": "2409.06131v2",
    "entry_id": "http://arxiv.org/abs/2409.06131v2",
    "title": "Accelerating Large Language Model Pretraining via LFR Pedagogy: Learn, Focus, and Review",
    "summary": "Traditional Large Language Model (LLM) pretraining relies on autoregressive language modeling with randomly sampled data from web-scale datasets. Inspired by human learning techniques like spaced repetition, we hypothesize that random sampling leads to high training costs, lower-quality models, and significant data forgetting. To address these inefficiencies, we propose the Learn-Focus-Review (LFR) paradigm -- a dynamic training approach that adapts to the model's learning progress. LFR tracks the model's learning performance across data blocks (sequences of tokens) and prioritizes revisiting challenging regions of the dataset that are more prone to being forgotten, enabling better retention and more efficient learning. Using the LFR paradigm, we pretrained Llama and GPT models on the SlimPajama and OpenWebText datasets, respectively. These models were evaluated on downstream tasks across various domains, including question answering, problem-solving, commonsense reasoning, language modeling, and translation. Compared to baseline models trained on the full datasets, LFR consistently achieved lower perplexity and higher accuracy, while using only 5%--19% of the training tokens. Furthermore, LFR matched the performance of industry-standard Pythia models with up to 2$\\times$ the parameter count, using just 3.2% of the training tokens, demonstrating its effectiveness and efficiency.",
    "authors": [
      "Neha Prakriya",
      "Jui-Nan Yen",
      "Cho-Jui Hsieh",
      "Jason Cong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-10T00:59:18Z",
    "pdf_url": "https://arxiv.org/pdf/2409.06131v2"
  },
  {
    "arxiv_id": "2409.05405v2",
    "entry_id": "http://arxiv.org/abs/2409.05405v2",
    "title": "A Survey of Multimodal Composite Editing and Retrieval",
    "summary": "In the real world, where information is abundant and diverse across different modalities, understanding and utilizing various data types to improve retrieval systems is a key focus of research. Multimodal composite retrieval integrates diverse modalities such as text, image and audio, etc. to provide more accurate, personalized, and contextually relevant results. To facilitate a deeper understanding of this promising direction, this survey explores multimodal composite editing and retrieval in depth, covering image-text composite editing, image-text composite retrieval, and other multimodal composite retrieval. In this survey, we systematically organize the application scenarios, methods, benchmarks, experiments, and future directions. Multimodal learning is a hot topic in large model era, and have also witnessed some surveys in multimodal learning and vision-language models with transformers published in the PAMI journal. To the best of our knowledge, this survey is the first comprehensive review of the literature on multimodal composite retrieval, which is a timely complement of multimodal fusion to existing reviews. To help readers' quickly track this field, we build the project page for this survey, which can be found at https://github.com/fuxianghuang1/Multimodal-Composite-Editing-and-Retrieval.",
    "authors": [
      "Suyan Li",
      "Fuxiang Huang",
      "Lei Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.IR",
      "cs.MM"
    ],
    "published": "2024-09-09T08:06:50Z",
    "pdf_url": "https://arxiv.org/pdf/2409.05405v2"
  },
  {
    "arxiv_id": "2409.04833v1",
    "entry_id": "http://arxiv.org/abs/2409.04833v1",
    "title": "Achieving Peak Performance for Large Language Models: A Systematic Review",
    "summary": "In recent years, large language models (LLMs) have achieved remarkable success in natural language processing (NLP). LLMs require an extreme amount of parameters to attain high performance. As models grow into the trillion-parameter range, computational and memory costs increase significantly. This makes it difficult for many researchers to access the resources needed to train or apply these models. Optimizing LLM performance involves two main approaches: fine-tuning pre-trained models for specific tasks to achieve state-of-the-art performance, and reducing costs or improving training time while maintaining similar performance. This paper presents a systematic literature review (SLR) following the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) statement. We reviewed 65 publications out of 983 from 2017 to December 2023, retrieved from 5 databases. The study presents methods to optimize and accelerate LLMs while achieving cutting-edge results without sacrificing accuracy. We begin with an overview of the development of language modeling, followed by a detailed explanation of commonly used frameworks and libraries, and a taxonomy for improving and speeding up LLMs based on three classes: LLM training, LLM inference, and system serving. We then delve into recent optimization and acceleration strategies such as training optimization, hardware optimization, scalability and reliability, accompanied by the taxonomy and categorization of these strategies. Finally, we provide an in-depth comparison of each class and strategy, with two case studies on optimizing model training and enhancing inference efficiency. These case studies showcase practical approaches to address LLM resource limitations while maintaining performance.",
    "authors": [
      "Zhyar Rzgar K Rostam",
      "Sándor Szénási",
      "Gábor Kertész"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-07T13:57:41Z",
    "pdf_url": "https://arxiv.org/pdf/2409.04833v1"
  },
  {
    "arxiv_id": "2409.04711v1",
    "entry_id": "http://arxiv.org/abs/2409.04711v1",
    "title": "Algorithmic Scenario Generation as Quality Diversity Optimization",
    "summary": "The increasing complexity of robots and autonomous agents that interact with people highlights the critical need for approaches that systematically test them before deployment. This review paper presents a general framework for solving this problem, describes the insights that we have gained from working on each component of the framework, and shows how integrating these components leads to the discovery of a diverse range of realistic and challenging scenarios that reveal previously unknown failures in deployed robotic systems interacting with people.",
    "authors": [
      "Stefanos Nikolaidis"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-09-07T05:20:41Z",
    "pdf_url": "https://arxiv.org/pdf/2409.04711v1"
  },
  {
    "arxiv_id": "2409.13712v1",
    "entry_id": "http://arxiv.org/abs/2409.13712v1",
    "title": "Good Idea or Not, Representation of LLM Could Tell",
    "summary": "In the ever-expanding landscape of academic research, the proliferation of ideas presents a significant challenge for researchers: discerning valuable ideas from the less impactful ones. The ability to efficiently evaluate the potential of these ideas is crucial for the advancement of science and paper review. In this work, we focus on idea assessment, which aims to leverage the knowledge of large language models to assess the merit of scientific ideas. First, we investigate existing text evaluation research and define the problem of quantitative evaluation of ideas. Second, we curate and release a benchmark dataset from nearly four thousand manuscript papers with full texts, meticulously designed to train and evaluate the performance of different approaches to this task. Third, we establish a framework for quantifying the value of ideas by employing representations in a specific layer of large language models. Experimental results show that the scores predicted by our method are relatively consistent with those of humans. Our findings suggest that the representations of large language models hold more potential in quantifying the value of ideas than their generative outputs, demonstrating a promising avenue for automating the idea assessment process.",
    "authors": [
      "Yi Xu",
      "Bo Xue",
      "Shuqian Sheng",
      "Cheng Deng",
      "Jiaxin Ding",
      "Zanwei Shen",
      "Luoyi Fu",
      "Xinbing Wang",
      "Chenghu Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-07T02:07:22Z",
    "pdf_url": "https://arxiv.org/pdf/2409.13712v1"
  },
  {
    "arxiv_id": "2409.04600v1",
    "entry_id": "http://arxiv.org/abs/2409.04600v1",
    "title": "The emergence of Large Language Models (LLM) as a tool in literature reviews: an LLM automated systematic review",
    "summary": "Objective: This study aims to summarize the usage of Large Language Models (LLMs) in the process of creating a scientific review. We look at the range of stages in a review that can be automated and assess the current state-of-the-art research projects in the field. Materials and Methods: The search was conducted in June 2024 in PubMed, Scopus, Dimensions, and Google Scholar databases by human reviewers. Screening and extraction process took place in Covidence with the help of LLM add-on which uses OpenAI gpt-4o model. ChatGPT was used to clean extracted data and generate code for figures in this manuscript, ChatGPT and Scite.ai were used in drafting all components of the manuscript, except the methods and discussion sections. Results: 3,788 articles were retrieved, and 172 studies were deemed eligible for the final review. ChatGPT and GPT-based LLM emerged as the most dominant architecture for review automation (n=126, 73.2%). A significant number of review automation projects were found, but only a limited number of papers (n=26, 15.1%) were actual reviews that used LLM during their creation. Most citations focused on automation of a particular stage of review, such as Searching for publications (n=60, 34.9%), and Data extraction (n=54, 31.4%). When comparing pooled performance of GPT-based and BERT-based models, the former were better in data extraction with mean precision 83.0% (SD=10.4), and recall 86.0% (SD=9.8), while being slightly less accurate in title and abstract screening stage (Maccuracy=77.3%, SD=13.0). Discussion/Conclusion: Our LLM-assisted systematic review revealed a significant number of research projects related to review automation using LLMs. The results looked promising, and we anticipate that LLMs will change in the near future the way the scientific reviews are conducted.",
    "authors": [
      "Dmitry Scherbakov",
      "Nina Hubig",
      "Vinita Jansari",
      "Alexander Bakumenko",
      "Leslie A. Lenert"
    ],
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "published": "2024-09-06T20:12:57Z",
    "pdf_url": "https://arxiv.org/pdf/2409.04600v1"
  },
  {
    "arxiv_id": "2409.04432v3",
    "entry_id": "http://arxiv.org/abs/2409.04432v3",
    "title": "A Survey on Knowledge Organization Systems of Research Fields: Resources and Challenges",
    "summary": "Knowledge Organization Systems (KOSs), such as term lists, thesauri, taxonomies, and ontologies, play a fundamental role in categorising, managing, and retrieving information. In the academic domain, KOSs are often adopted for representing research areas and their relationships, primarily aiming to classify research articles, academic courses, patents, books, scientific venues, domain experts, grants, software, experiment materials, and several other relevant products and agents. These structured representations of research areas, widely embraced by many academic fields, have proven effective in empowering AI-based systems to i) enhance retrievability of relevant documents, ii) enable advanced analytic solutions to quantify the impact of academic research, and iii) analyse and forecast research dynamics. This paper aims to present a comprehensive survey of the current KOS for academic disciplines. We analysed and compared 45 KOSs according to five main dimensions: scope, structure, curation, usage, and links to other KOSs. Our results reveal a very heterogeneous scenario in terms of scope, scale, quality, and usage, highlighting the need for more integrated solutions for representing research knowledge across academic fields. We conclude by discussing the main challenges and the most promising future directions.",
    "authors": [
      "Angelo Salatino",
      "Tanay Aggarwal",
      "Andrea Mannocci",
      "Francesco Osborne",
      "Enrico Motta"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-09-06T17:54:43Z",
    "pdf_url": "https://arxiv.org/pdf/2409.04432v3"
  },
  {
    "arxiv_id": "2409.04109v1",
    "entry_id": "http://arxiv.org/abs/2409.04109v1",
    "title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
    "summary": "Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.",
    "authors": [
      "Chenglei Si",
      "Diyi Yang",
      "Tatsunori Hashimoto"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-09-06T08:25:03Z",
    "pdf_url": "https://arxiv.org/pdf/2409.04109v1"
  },
  {
    "arxiv_id": "2409.04481v1",
    "entry_id": "http://arxiv.org/abs/2409.04481v1",
    "title": "Large Language Models in Drug Discovery and Development: From Disease Mechanisms to Clinical Trials",
    "summary": "The integration of Large Language Models (LLMs) into the drug discovery and development field marks a significant paradigm shift, offering novel methodologies for understanding disease mechanisms, facilitating drug discovery, and optimizing clinical trial processes. This review highlights the expanding role of LLMs in revolutionizing various stages of the drug development pipeline. We investigate how these advanced computational models can uncover target-disease linkage, interpret complex biomedical data, enhance drug molecule design, predict drug efficacy and safety profiles, and facilitate clinical trial processes. Our paper aims to provide a comprehensive overview for researchers and practitioners in computational biology, pharmacology, and AI4Science by offering insights into the potential transformative impact of LLMs on drug discovery and development.",
    "authors": [
      "Yizhen Zheng",
      "Huan Yee Koh",
      "Maddie Yang",
      "Li Li",
      "Lauren T. May",
      "Geoffrey I. Webb",
      "Shirui Pan",
      "George Church"
    ],
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-09-06T02:03:38Z",
    "pdf_url": "https://arxiv.org/pdf/2409.04481v1"
  },
  {
    "arxiv_id": "2409.03384v1",
    "entry_id": "http://arxiv.org/abs/2409.03384v1",
    "title": "Hardware Acceleration of LLMs: A comprehensive survey and comparison",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. In this paper, we present a comprehensive survey of the several research efforts that have been presented for the acceleration of transformer networks for Large Language Models using hardware accelerators.\n  The survey presents the frameworks that have been proposed and then performs a qualitative and quantitative comparison regarding the technology, the processing platform (FPGA, ASIC, In-Memory, GPU), the speedup, the energy efficiency, the performance (GOPs), and the energy efficiency (GOPs/W) of each framework. The main challenge in comparison is that every proposed scheme is implemented on a different process technology making hard a fair comparison. The main contribution of this paper is that we extrapolate the results of the performance and the energy efficiency on the same technology to make a fair comparison; one theoretical and one more practical. We implement part of the LLMs on several FPGA chips to extrapolate the results to the same process technology and then we make a fair comparison of the performance.",
    "authors": [
      "Nikoletta Koilia",
      "Christoforos Kachris"
    ],
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "published": "2024-09-05T09:43:25Z",
    "pdf_url": "https://arxiv.org/pdf/2409.03384v1"
  },
  {
    "arxiv_id": "2409.15310v1",
    "entry_id": "http://arxiv.org/abs/2409.15310v1",
    "title": "Visual Prompting in Multimodal Large Language Models: A Survey",
    "summary": "Multimodal large language models (MLLMs) equip pre-trained large-language models (LLMs) with visual capabilities. While textual prompting in LLMs has been widely studied, visual prompting has emerged for more fine-grained and free-form visual instructions. This paper presents the first comprehensive survey on visual prompting methods in MLLMs, focusing on visual prompting, prompt generation, compositional reasoning, and prompt learning. We categorize existing visual prompts and discuss generative methods for automatic prompt annotations on the images. We also examine visual prompting methods that enable better alignment between visual encoders and backbone LLMs, concerning MLLM's visual grounding, object referring, and compositional reasoning abilities. In addition, we provide a summary of model training and in-context learning methods to improve MLLM's perception and understanding of visual prompts. This paper examines visual prompting methods developed in MLLMs and provides a vision of the future of these methods.",
    "authors": [
      "Junda Wu",
      "Zhehao Zhang",
      "Yu Xia",
      "Xintong Li",
      "Zhaoyang Xia",
      "Aaron Chang",
      "Tong Yu",
      "Sungchul Kim",
      "Ryan A. Rossi",
      "Ruiyi Zhang",
      "Subrata Mitra",
      "Dimitris N. Metaxas",
      "Lina Yao",
      "Jingbo Shang",
      "Julian McAuley"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2024-09-05T08:47:34Z",
    "pdf_url": "https://arxiv.org/pdf/2409.15310v1"
  },
  {
    "arxiv_id": "2409.03274v3",
    "entry_id": "http://arxiv.org/abs/2409.03274v3",
    "title": "Recent Advances in Attack and Defense Approaches of Large Language Models",
    "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence and machine learning through their advanced text processing and generating capabilities. However, their widespread deployment has raised significant safety and reliability concerns. Established vulnerabilities in deep neural networks, coupled with emerging threat models, may compromise security evaluations and create a false sense of security. Given the extensive research in the field of LLM security, we believe that summarizing the current state of affairs will help the research community better understand the present landscape and inform future developments. This paper reviews current research on LLM vulnerabilities and threats, and evaluates the effectiveness of contemporary defense mechanisms. We analyze recent studies on attack vectors and model weaknesses, providing insights into attack mechanisms and the evolving threat landscape. We also examine current defense strategies, highlighting their strengths and limitations. By contrasting advancements in attack and defense methodologies, we identify research gaps and propose future directions to enhance LLM security. Our goal is to advance the understanding of LLM safety challenges and guide the development of more robust security measures.",
    "authors": [
      "Jing Cui",
      "Yishi Xu",
      "Zhewei Huang",
      "Shuchang Zhou",
      "Jianbin Jiao",
      "Junge Zhang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-09-05T06:31:37Z",
    "pdf_url": "https://arxiv.org/pdf/2409.03274v3"
  },
  {
    "arxiv_id": "2409.03219v2",
    "entry_id": "http://arxiv.org/abs/2409.03219v2",
    "title": "Content Moderation by LLM: From Accuracy to Legitimacy",
    "summary": "One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy -- the extent to which LLMs make correct decisions about content. This article argues that accuracy is insufficient and misleading because it fails to grasp the distinction between easy cases and hard cases, as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLMs is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework for evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed, and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLMs' real potential in moderation is not accuracy improvement. Rather, LLMs can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. To realize these contributions, this article proposes a workflow for incorporating LLMs into the content moderation system. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLMs' role in content moderation and redirect relevant research in this field.",
    "authors": [
      "Tao Huang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-09-05T03:33:54Z",
    "pdf_url": "https://arxiv.org/pdf/2409.03219v2"
  },
  {
    "arxiv_id": "2409.02977v1",
    "entry_id": "http://arxiv.org/abs/2409.02977v1",
    "title": "Large Language Model-Based Agents for Software Engineering: A Survey",
    "summary": "The recent advance in Large Language Models (LLMs) has shaped a new paradigm of AI agents, i.e., LLM-based agents. Compared to standalone LLMs, LLM-based agents substantially extend the versatility and expertise of LLMs by enhancing LLMs with the capabilities of perceiving and utilizing external resources and tools. To date, LLM-based agents have been applied and shown remarkable effectiveness in Software Engineering (SE). The synergy between multiple agents and human interaction brings further promise in tackling complex real-world SE problems. In this work, we present a comprehensive and systematic survey on LLM-based agents for SE. We collect 106 papers and categorize them from two perspectives, i.e., the SE and agent perspectives. In addition, we discuss open challenges and future directions in this critical domain. The repository of this survey is at https://github.com/FudanSELab/Agent4SE-Paper-List.",
    "authors": [
      "Junwei Liu",
      "Kaixin Wang",
      "Yixuan Chen",
      "Xin Peng",
      "Zhenpeng Chen",
      "Lingming Zhang",
      "Yiling Lou"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-09-04T15:59:41Z",
    "pdf_url": "https://arxiv.org/pdf/2409.02977v1"
  },
  {
    "arxiv_id": "2409.02691v1",
    "entry_id": "http://arxiv.org/abs/2409.02691v1",
    "title": "LLM-Assisted Visual Analytics: Opportunities and Challenges",
    "summary": "We explore the integration of large language models (LLMs) into visual analytics (VA) systems to transform their capabilities through intuitive natural language interactions. We survey current research directions in this emerging field, examining how LLMs are integrated into data management, language interaction, visualisation generation, and language generation processes. We highlight the new possibilities that LLMs bring to VA, especially how they can change VA processes beyond the usual use cases. We especially highlight building new visualisation-language models, allowing access of a breadth of domain knowledge, multimodal interaction, and opportunities with guidance. Finally, we carefully consider the prominent challenges of using current LLMs in VA tasks. Our discussions in this paper aim to guide future researchers working on LLM-assisted VA systems and help them navigate common obstacles when developing these systems.",
    "authors": [
      "Maeve Hutchinson",
      "Radu Jianu",
      "Aidan Slingsby",
      "Pranava Madhyastha"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-09-04T13:24:03Z",
    "pdf_url": "https://arxiv.org/pdf/2409.02691v1"
  },
  {
    "arxiv_id": "2409.02645v2",
    "entry_id": "http://arxiv.org/abs/2409.02645v2",
    "title": "Emergent Language: A Survey and Taxonomy",
    "summary": "The field of emergent language represents a novel area of research within the domain of artificial intelligence, particularly within the context of multi-agent reinforcement learning. Although the concept of studying language emergence is not new, early approaches were primarily concerned with explaining human language formation, with little consideration given to its potential utility for artificial agents. In contrast, studies based on reinforcement learning aim to develop communicative capabilities in agents that are comparable to or even superior to human language. Thus, they extend beyond the learned statistical representations that are common in natural language processing research. This gives rise to a number of fundamental questions, from the prerequisites for language emergence to the criteria for measuring its success. This paper addresses these questions by providing a comprehensive review of 181 scientific publications on emergent language in artificial intelligence. Its objective is to serve as a reference for researchers interested in or proficient in the field. Consequently, the main contributions are the definition and overview of the prevailing terminology, the analysis of existing evaluation methods and metrics, and the description of the identified research gaps.",
    "authors": [
      "Jannik Peters",
      "Constantin Waubert de Puiseau",
      "Hasan Tercan",
      "Arya Gopikrishnan",
      "Gustavo Adolpho Lucas De Carvalho",
      "Christian Bitter",
      "Tobias Meisen"
    ],
    "categories": [
      "cs.MA",
      "cs.CL"
    ],
    "published": "2024-09-04T12:22:05Z",
    "pdf_url": "https://arxiv.org/pdf/2409.02645v2"
  },
  {
    "arxiv_id": "2409.02413v1",
    "entry_id": "http://arxiv.org/abs/2409.02413v1",
    "title": "Abstractive Text Summarization: State of the Art, Challenges, and Improvements",
    "summary": "Specifically focusing on the landscape of abstractive text summarization, as opposed to extractive techniques, this survey presents a comprehensive overview, delving into state-of-the-art techniques, prevailing challenges, and prospective research directions. We categorize the techniques into traditional sequence-to-sequence models, pre-trained large language models, reinforcement learning, hierarchical methods, and multi-modal summarization. Unlike prior works that did not examine complexities, scalability and comparisons of techniques in detail, this review takes a comprehensive approach encompassing state-of-the-art methods, challenges, solutions, comparisons, limitations and charts out future improvements - providing researchers an extensive overview to advance abstractive summarization research. We provide vital comparison tables across techniques categorized - offering insights into model complexity, scalability and appropriate applications. The paper highlights challenges such as inadequate meaning representation, factual consistency, controllable text summarization, cross-lingual summarization, and evaluation metrics, among others. Solutions leveraging knowledge incorporation and other innovative strategies are proposed to address these challenges. The paper concludes by highlighting emerging research areas like factual inconsistency, domain-specific, cross-lingual, multilingual, and long-document summarization, as well as handling noisy data. Our objective is to provide researchers and practitioners with a structured overview of the domain, enabling them to better understand the current landscape and identify potential areas for further research and improvement.",
    "authors": [
      "Hassan Shakil",
      "Ahmad Farooq",
      "Jugal Kalita"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-09-04T03:39:23Z",
    "pdf_url": "https://arxiv.org/pdf/2409.02413v1"
  },
  {
    "arxiv_id": "2409.02387v6",
    "entry_id": "http://arxiv.org/abs/2409.02387v6",
    "title": "Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges",
    "summary": "This comprehensive review explores the intersection of Large Language Models (LLMs) and cognitive science, examining similarities and differences between LLMs and human cognitive processes. We analyze methods for evaluating LLMs cognitive abilities and discuss their potential as cognitive models. The review covers applications of LLMs in various cognitive fields, highlighting insights gained for cognitive science research. We assess cognitive biases and limitations of LLMs, along with proposed methods for improving their performance. The integration of LLMs with cognitive architectures is examined, revealing promising avenues for enhancing artificial intelligence (AI) capabilities. Key challenges and future research directions are identified, emphasizing the need for continued refinement of LLMs to better align with human cognition. This review provides a balanced perspective on the current state and future potential of LLMs in advancing our understanding of both artificial and human intelligence.",
    "authors": [
      "Qian Niu",
      "Junyu Liu",
      "Ziqian Bi",
      "Pohsun Feng",
      "Benji Peng",
      "Keyu Chen",
      "Ming Li",
      "Lawrence KQ Yan",
      "Yichao Zhang",
      "Caitlyn Heqi Yin",
      "Cheng Fei",
      "Tianyang Wang",
      "Yunze Wang",
      "Silin Chen",
      "Ming Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-04T02:30:12Z",
    "pdf_url": "https://arxiv.org/pdf/2409.02387v6"
  },
  {
    "arxiv_id": "2409.01990v5",
    "entry_id": "http://arxiv.org/abs/2409.01990v5",
    "title": "Designing Large Foundation Models for Efficient Training and Inference: A Survey",
    "summary": "This paper focuses on modern efficient training and inference technologies on foundation models and illustrates them from two perspectives: model and system design. Model and System Design optimize LLM training and inference from different aspects to save computational resources, making LLMs more efficient, affordable, and more accessible. The paper list repository is available at https://github.com/NoakLiu/Efficient-Foundation-Models-Survey.",
    "authors": [
      "Dong Liu",
      "Yanxuan Yu",
      "Yite Wang",
      "Jing Wu",
      "Zhongwei Wan",
      "Sina Alinejad",
      "Benjamin Lengerich",
      "Ying Nian Wu"
    ],
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2024-09-03T15:35:01Z",
    "pdf_url": "https://arxiv.org/pdf/2409.01990v5"
  },
  {
    "arxiv_id": "2409.01980v3",
    "entry_id": "http://arxiv.org/abs/2409.01980v3",
    "title": "Large Language Models for Anomaly and Out-of-Distribution Detection: A Survey",
    "summary": "Detecting anomalies or out-of-distribution (OOD) samples is critical for maintaining the reliability and trustworthiness of machine learning systems. Recently, Large Language Models (LLMs) have demonstrated their effectiveness not only in natural language processing but also in broader applications due to their advanced comprehension and generative capabilities. The integration of LLMs into anomaly and OOD detection marks a significant shift from the traditional paradigm in the field. This survey focuses on the problem of anomaly and OOD detection under the context of LLMs. We propose a new taxonomy to categorize existing approaches into two classes based on the role played by LLMs. Following our proposed taxonomy, we further discuss the related work under each of the categories and finally discuss potential challenges and directions for future research in this field. We also provide an up-to-date reading list of relevant papers.",
    "authors": [
      "Ruiyao Xu",
      "Kaize Ding"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-09-03T15:22:41Z",
    "pdf_url": "https://arxiv.org/pdf/2409.01980v3"
  },
  {
    "arxiv_id": "2409.01806v1",
    "entry_id": "http://arxiv.org/abs/2409.01806v1",
    "title": "LASP: Surveying the State-of-the-Art in Large Language Model-Assisted AI Planning",
    "summary": "Effective planning is essential for the success of any task, from organizing a vacation to routing autonomous vehicles and developing corporate strategies. It involves setting goals, formulating plans, and allocating resources to achieve them. LLMs are particularly well-suited for automated planning due to their strong capabilities in commonsense reasoning. They can deduce a sequence of actions needed to achieve a goal from a given state and identify an effective course of action. However, it is frequently observed that plans generated through direct prompting often fail upon execution. Our survey aims to highlight the existing challenges in planning with language models, focusing on key areas such as embodied environments, optimal scheduling, competitive and cooperative games, task decomposition, reasoning, and planning. Through this study, we explore how LLMs transform AI planning and provide unique insights into the future of LM-assisted planning.",
    "authors": [
      "Haoming Li",
      "Zhaoliang Chen",
      "Jonathan Zhang",
      "Fei Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-09-03T11:39:52Z",
    "pdf_url": "https://arxiv.org/pdf/2409.01806v1"
  },
  {
    "arxiv_id": "2409.12973v1",
    "entry_id": "http://arxiv.org/abs/2409.12973v1",
    "title": "The Era of Foundation Models in Medical Imaging is Approaching : A Scoping Review of the Clinical Value of Large-Scale Generative AI Applications in Radiology",
    "summary": "Social problems stemming from the shortage of radiologists are intensifying, and artificial intelligence is being highlighted as a potential solution. Recently emerging large-scale generative AI has expanded from large language models (LLMs) to multi-modal models, showing potential to revolutionize the entire process of medical imaging. However, comprehensive reviews on their development status and future challenges are currently lacking. This scoping review systematically organizes existing literature on the clinical value of large-scale generative AI applications by following PCC guidelines. A systematic search was conducted across four databases: PubMed, EMbase, IEEE-Xplore, and Google Scholar, and 15 studies meeting the inclusion/exclusion criteria set by the researchers were reviewed. Most of these studies focused on improving the efficiency of report generation in specific parts of the interpretation process or on translating reports to aid patient understanding, with the latest studies extending to AI applications performing direct interpretations. All studies were quantitatively evaluated by clinicians, with most utilizing LLMs and only three employing multi-modal models. Both LLMs and multi-modal models showed excellent results in specific areas, but none yet outperformed radiologists in diagnostic performance. Most studies utilized GPT, with few using models specialized for the medical imaging domain. This study provides insights into the current state and limitations of large-scale generative AI-based applications in the medical imaging field, offering foundational data and suggesting that the era of medical imaging foundation models is on the horizon, which may fundamentally transform clinical practice in the near future.",
    "authors": [
      "Inwoo Seo",
      "Eunkyoung Bae",
      "Joo-Young Jeon",
      "Young-Sang Yoon",
      "Jiho Cha"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-09-03T00:48:50Z",
    "pdf_url": "https://arxiv.org/pdf/2409.12973v1"
  },
  {
    "arxiv_id": "2409.01207v2",
    "entry_id": "http://arxiv.org/abs/2409.01207v2",
    "title": "Towards General Industrial Intelligence: A Survey of Continual Large Models in Industrial IoT",
    "summary": "Industrial AI is transitioning from traditional deep learning models to large-scale transformer-based architectures, with the Industrial Internet of Things (IIoT) playing a pivotal role. IIoT evolves from a simple data pipeline to an intelligent infrastructure, enabling and enhancing these advanced AI systems. This survey explores the integration of IIoT with large models (LMs) and their potential applications in industrial environments. We focus on four primary types of industrial LMs: language-based, vision-based, time-series, and multimodal models. The lifecycle of LMs is segmented into four critical phases: data foundation, model training, model connectivity, and continuous evolution. First, we analyze how IIoT provides abundant and diverse data resources, supporting the training and fine-tuning of LMs. Second, we discuss how IIoT offers an efficient training infrastructure in low-latency and bandwidth-optimized environments. Third, we highlight the deployment advantages of LMs within IIoT, emphasizing IIoT's role as a connectivity nexus fostering emergent intelligence through modular design, dynamic routing, and model merging to enhance system scalability and adaptability. Finally, we demonstrate how IIoT supports continual learning mechanisms, enabling LMs to adapt to dynamic industrial conditions and ensure long-term effectiveness. This paper underscores IIoT's critical role in the evolution of industrial intelligence with large models, offering a theoretical framework and actionable insights for future research.",
    "authors": [
      "Jiao Chen",
      "Jiayi He",
      "Fangfang Chen",
      "Zuohong Lv",
      "Jianhua Tang",
      "Weihua Li",
      "Zuozhu Liu",
      "Howard H. Yang",
      "Guangjie Han"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-09-02T12:35:59Z",
    "pdf_url": "https://arxiv.org/pdf/2409.01207v2"
  },
  {
    "arxiv_id": "2409.01087v1",
    "entry_id": "http://arxiv.org/abs/2409.01087v1",
    "title": "Pre-Trained Language Models for Keyphrase Prediction: A Review",
    "summary": "Keyphrase Prediction (KP) is essential for identifying keyphrases in a document that can summarize its content. However, recent Natural Language Processing (NLP) advances have developed more efficient KP models using deep learning techniques. The limitation of a comprehensive exploration jointly both keyphrase extraction and generation using pre-trained language models spotlights a critical gap in the literature, compelling our survey paper to bridge this deficiency and offer a unified and in-depth analysis to address limitations in previous surveys. This paper extensively examines the topic of pre-trained language models for keyphrase prediction (PLM-KP), which are trained on large text corpora via different learning (supervisor, unsupervised, semi-supervised, and self-supervised) techniques, to provide respective insights into these two types of tasks in NLP, precisely, Keyphrase Extraction (KPE) and Keyphrase Generation (KPG). We introduce appropriate taxonomies for PLM-KPE and KPG to highlight these two main tasks of NLP. Moreover, we point out some promising future directions for predicting keyphrases.",
    "authors": [
      "Muhammad Umair",
      "Tangina Sultana",
      "Young-Koo Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-09-02T09:15:44Z",
    "pdf_url": "https://arxiv.org/pdf/2409.01087v1"
  },
  {
    "arxiv_id": "2409.01073v1",
    "entry_id": "http://arxiv.org/abs/2409.01073v1",
    "title": "SCOPE: Sign Language Contextual Processing with Embedding from LLMs",
    "summary": "Sign languages, used by around 70 million Deaf individuals globally, are visual languages that convey visual and contextual information. Current methods in vision-based sign language recognition (SLR) and translation (SLT) struggle with dialogue scenes due to limited dataset diversity and the neglect of contextually relevant information. To address these challenges, we introduce SCOPE (Sign language Contextual Processing with Embedding from LLMs), a novel context-aware vision-based SLR and SLT framework. For SLR, we utilize dialogue contexts through a multi-modal encoder to enhance gloss-level recognition. For subsequent SLT, we further fine-tune a Large Language Model (LLM) by incorporating prior conversational context. We also contribute a new sign language dataset that contains 72 hours of Chinese sign language videos in contextual dialogues across various scenarios. Experimental results demonstrate that our SCOPE framework achieves state-of-the-art performance on multiple datasets, including Phoenix-2014T, CSL-Daily, and our SCOPE dataset. Moreover, surveys conducted with participants from the Deaf community further validate the robustness and effectiveness of our approach in real-world applications. Both our dataset and code will be open-sourced to facilitate further research.",
    "authors": [
      "Yuqi Liu",
      "Wenqian Zhang",
      "Sihan Ren",
      "Chengyu Huang",
      "Jingyi Yu",
      "Lan Xu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-09-02T08:56:12Z",
    "pdf_url": "https://arxiv.org/pdf/2409.01073v1"
  },
  {
    "arxiv_id": "2409.00494v2",
    "entry_id": "http://arxiv.org/abs/2409.00494v2",
    "title": "GenAI-powered Multi-Agent Paradigm for Smart Urban Mobility: Opportunities and Challenges for Integrating Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG) with Intelligent Transportation Systems",
    "summary": "Leveraging recent advances in generative AI, multi-agent systems are increasingly being developed to enhance the functionality and efficiency of smart city applications. This paper explores the transformative potential of large language models (LLMs) and emerging Retrieval-Augmented Generation (RAG) technologies in Intelligent Transportation Systems (ITS), paving the way for innovative solutions to address critical challenges in urban mobility. We begin by providing a comprehensive overview of the current state-of-the-art in mobility data, ITS, and Connected Vehicles (CV) applications. Building on this review, we discuss the rationale behind RAG and examine the opportunities for integrating these Generative AI (GenAI) technologies into the smart mobility sector. We propose a conceptual framework aimed at developing multi-agent systems capable of intelligently and conversationally delivering smart mobility services to urban commuters, transportation operators, and decision-makers. Our approach seeks to foster an autonomous and intelligent approach that (a) promotes science-based advisory to reduce traffic congestion, accidents, and carbon emissions at multiple scales, (b) facilitates public education and engagement in participatory mobility management, and (c) automates specialized transportation management tasks and the development of critical ITS platforms, such as data analytics and interpretation, knowledge representation, and traffic simulations. By integrating LLM and RAG, our approach seeks to overcome the limitations of traditional rule-based multi-agent systems, which rely on fixed knowledge bases and limited reasoning capabilities. This integration paves the way for a more scalable, intuitive, and automated multi-agent paradigm, driving advancements in ITS and urban mobility.",
    "authors": [
      "Haowen Xu",
      "Jinghui Yuan",
      "Anye Zhou",
      "Guanhao Xu",
      "Wan Li",
      "Xuegang Ban",
      "Xinyue Ye"
    ],
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-08-31T16:14:42Z",
    "pdf_url": "https://arxiv.org/pdf/2409.00494v2"
  },
  {
    "arxiv_id": "2409.10527v1",
    "entry_id": "http://arxiv.org/abs/2409.10527v1",
    "title": "Towards Empathetic Conversational Recommender Systems",
    "summary": "Conversational recommender systems (CRSs) are able to elicit user preferences through multi-turn dialogues. They typically incorporate external knowledge and pre-trained language models to capture the dialogue context. Most CRS approaches, trained on benchmark datasets, assume that the standard items and responses in these benchmarks are optimal. However, they overlook that users may express negative emotions with the standard items and may not feel emotionally engaged by the standard responses. This issue leads to a tendency to replicate the logic of recommenders in the dataset instead of aligning with user needs. To remedy this misalignment, we introduce empathy within a CRS. With empathy we refer to a system's ability to capture and express emotions. We propose an empathetic conversational recommender (ECR) framework.\n  ECR contains two main modules: emotion-aware item recommendation and emotion-aligned response generation. Specifically, we employ user emotions to refine user preference modeling for accurate recommendations. To generate human-like emotional responses, ECR applies retrieval-augmented prompts to fine-tune a pre-trained language model aligning with emotions and mitigating hallucination. To address the challenge of insufficient supervision labels, we enlarge our empathetic data using emotion labels annotated by large language models and emotional reviews collected from external resources. We propose novel evaluation metrics to capture user satisfaction in real-world CRS scenarios. Our experiments on the ReDial dataset validate the efficacy of our framework in enhancing recommendation accuracy and improving user satisfaction.",
    "authors": [
      "Xiaoyu Zhang",
      "Ruobing Xie",
      "Yougang Lyu",
      "Xin Xin",
      "Pengjie Ren",
      "Mingfei Liang",
      "Bo Zhang",
      "Zhanhui Kang",
      "Maarten de Rijke",
      "Zhaochun Ren"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-08-30T15:43:07Z",
    "pdf_url": "https://arxiv.org/pdf/2409.10527v1"
  },
  {
    "arxiv_id": "2408.16984v2",
    "entry_id": "http://arxiv.org/abs/2408.16984v2",
    "title": "Beyond Preferences in AI Alignment",
    "summary": "The dominant practice of AI alignment assumes (1) that preferences are an adequate representation of human values, (2) that human rationality can be understood in terms of maximizing the satisfaction of preferences, and (3) that AI systems should be aligned with the preferences of one or more humans to ensure that they behave safely and in accordance with our values. Whether implicitly followed or explicitly endorsed, these commitments constitute what we term a preferentist approach to AI alignment. In this paper, we characterize and challenge the preferentist approach, describing conceptual and technical alternatives that are ripe for further research. We first survey the limits of rational choice theory as a descriptive model, explaining how preferences fail to capture the thick semantic content of human values, and how utility representations neglect the possible incommensurability of those values. We then critique the normativity of expected utility theory (EUT) for humans and AI, drawing upon arguments showing how rational agents need not comply with EUT, while highlighting how EUT is silent on which preferences are normatively acceptable. Finally, we argue that these limitations motivate a reframing of the targets of AI alignment: Instead of alignment with the preferences of a human user, developer, or humanity-writ-large, AI systems should be aligned with normative standards appropriate to their social roles, such as the role of a general-purpose assistant. Furthermore, these standards should be negotiated and agreed upon by all relevant stakeholders. On this alternative conception of alignment, a multiplicity of AI systems will be able to serve diverse ends, aligned with normative standards that promote mutual benefit and limit harm despite our plural and divergent values.",
    "authors": [
      "Tan Zhi-Xuan",
      "Micah Carroll",
      "Matija Franklin",
      "Hal Ashton"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-30T03:14:20Z",
    "pdf_url": "https://arxiv.org/pdf/2408.16984v2"
  },
  {
    "arxiv_id": "2408.16966v2",
    "entry_id": "http://arxiv.org/abs/2408.16966v2",
    "title": "UserSumBench: A Benchmark Framework for Evaluating User Summarization Approaches",
    "summary": "Large language models (LLMs) have shown remarkable capabilities in generating user summaries from a long list of raw user activity data. These summaries capture essential user information such as preferences and interests, and therefore are invaluable for LLM-based personalization applications, such as explainable recommender systems. However, the development of new summarization techniques is hindered by the lack of ground-truth labels, the inherent subjectivity of user summaries, and human evaluation which is often costly and time-consuming. To address these challenges, we introduce \\UserSumBench, a benchmark framework designed to facilitate iterative development of LLM-based summarization approaches. This framework offers two key components: (1) A reference-free summary quality metric. We show that this metric is effective and aligned with human preferences across three diverse datasets (MovieLens, Yelp and Amazon Review). (2) A novel robust summarization method that leverages time-hierarchical summarizer and self-critique verifier to produce high-quality summaries while eliminating hallucination. This method serves as a strong baseline for further innovation in summarization techniques.",
    "authors": [
      "Chao Wang",
      "Neo Wu",
      "Lin Ning",
      "Jiaxing Wu",
      "Luyang Liu",
      "Jun Xie",
      "Shawn O'Banion",
      "Bradley Green"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-08-30T01:56:57Z",
    "pdf_url": "https://arxiv.org/pdf/2408.16966v2"
  },
  {
    "arxiv_id": "2409.09045v2",
    "entry_id": "http://arxiv.org/abs/2409.09045v2",
    "title": "United in Diversity? Contextual Biases in LLM-Based Predictions of the 2024 European Parliament Elections",
    "summary": "\"Synthetic samples\" based on large language models (LLMs) have been argued to serve as efficient alternatives to surveys of humans, assuming that their training data includes information on human attitudes and behavior. However, LLM-synthetic samples might exhibit bias, for example due to training data and fine-tuning processes being unrepresentative of diverse contexts. Such biases risk reinforcing existing biases in research, policymaking, and society. Therefore, researchers need to investigate if and under which conditions LLM-generated synthetic samples can be used for public opinion prediction. In this study, we examine to what extent LLM-based predictions of individual public opinion exhibit context-dependent biases by predicting the results of the 2024 European Parliament elections. Prompting three LLMs with individual-level background information of 26,000 eligible European voters, we ask the LLMs to predict each person's voting behavior. By comparing them to the actual results, we show that LLM-based predictions of future voting behavior largely fail, their accuracy is unequally distributed across national and linguistic contexts, and they require detailed attitudinal information in the prompt. The findings emphasize the limited applicability of LLM-synthetic samples to public opinion prediction. In investigating their contextual biases, this study contributes to the understanding and mitigation of inequalities in the development of LLMs and their applications in computational social science.",
    "authors": [
      "Leah von der Heyde",
      "Anna-Carolina Haensch",
      "Alexander Wenz",
      "Bolei Ma"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "stat.AP"
    ],
    "published": "2024-08-29T16:01:06Z",
    "pdf_url": "https://arxiv.org/pdf/2409.09045v2"
  },
  {
    "arxiv_id": "2409.00133v1",
    "entry_id": "http://arxiv.org/abs/2409.00133v1",
    "title": "A Survey for Large Language Models in Biomedicine",
    "summary": "Recent breakthroughs in large language models (LLMs) offer unprecedented natural language understanding and generation capabilities. However, existing surveys on LLMs in biomedicine often focus on specific applications or model architectures, lacking a comprehensive analysis that integrates the latest advancements across various biomedical domains. This review, based on an analysis of 484 publications sourced from databases including PubMed, Web of Science, and arXiv, provides an in-depth examination of the current landscape, applications, challenges, and prospects of LLMs in biomedicine, distinguishing itself by focusing on the practical implications of these models in real-world biomedical contexts. Firstly, we explore the capabilities of LLMs in zero-shot learning across a broad spectrum of biomedical tasks, including diagnostic assistance, drug discovery, and personalized medicine, among others, with insights drawn from 137 key studies. Then, we discuss adaptation strategies of LLMs, including fine-tuning methods for both uni-modal and multi-modal LLMs to enhance their performance in specialized biomedical contexts where zero-shot fails to achieve, such as medical question answering and efficient processing of biomedical literature. Finally, we discuss the challenges that LLMs face in the biomedicine domain including data privacy concerns, limited model interpretability, issues with dataset quality, and ethics due to the sensitive nature of biomedical data, the need for highly reliable model outputs, and the ethical implications of deploying AI in healthcare. To address these challenges, we also identify future research directions of LLM in biomedicine including federated learning methods to preserve data privacy and integrating explainable AI methodologies to enhance the transparency of LLMs.",
    "authors": [
      "Chong Wang",
      "Mengyao Li",
      "Junjun He",
      "Zhongruo Wang",
      "Erfan Darzi",
      "Zan Chen",
      "Jin Ye",
      "Tianbin Li",
      "Yanzhou Su",
      "Jing Ke",
      "Kaili Qu",
      "Shuxin Li",
      "Yi Yu",
      "Pietro Liò",
      "Tianyun Wang",
      "Yu Guang Wang",
      "Yiqing Shen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-29T12:39:16Z",
    "pdf_url": "https://arxiv.org/pdf/2409.00133v1"
  },
  {
    "arxiv_id": "2408.15879v2",
    "entry_id": "http://arxiv.org/abs/2408.15879v2",
    "title": "Persuasion Games using Large Language Models",
    "summary": "Large Language Models (LLMs) have emerged as formidable instruments capable of comprehending and producing human-like text. This paper explores the potential of LLMs, to shape user perspectives and subsequently influence their decisions on particular tasks. This capability finds applications in diverse domains such as Investment, Credit cards and Insurance, wherein they assist users in selecting appropriate insurance policies, investment plans, Credit cards, Retail, as well as in Behavioral Change Support Systems (BCSS).\n  We present a sophisticated multi-agent framework wherein a consortium of agents operate in collaborative manner. The primary agent engages directly with user agents through persuasive dialogue, while the auxiliary agents perform tasks such as information retrieval, response analysis, development of persuasion strategies, and validation of facts. Empirical evidence from our experiments demonstrates that this collaborative methodology significantly enhances the persuasive efficacy of the LLM. We continuously analyze the resistance of the user agent to persuasive efforts and counteract it by employing a combination of rule-based and LLM-based resistance-persuasion mapping techniques.\n  We employ simulated personas and generate conversations in insurance, banking, and retail domains to evaluate the proficiency of large language models (LLMs) in recognizing, adjusting to, and influencing various personality types. Concurrently, we examine the resistance mechanisms employed by LLM simulated personas. Persuasion is quantified via measurable surveys before and after interaction, LLM-generated scores on conversation, and user decisions (purchase or non-purchase).",
    "authors": [
      "Ganesh Prasath Ramani",
      "Shirish Karande",
      "Santhosh V",
      "Yash Bhatia"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-08-28T15:50:41Z",
    "pdf_url": "https://arxiv.org/pdf/2408.15879v2"
  },
  {
    "arxiv_id": "2408.15769v1",
    "entry_id": "http://arxiv.org/abs/2408.15769v1",
    "title": "A Survey on Evaluation of Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) mimic human perception and reasoning system by integrating powerful Large Language Models (LLMs) with various modality encoders (e.g., vision, audio), positioning LLMs as the \"brain\" and various modality encoders as sensory organs. This framework endows MLLMs with human-like capabilities, and suggests a potential pathway towards achieving artificial general intelligence (AGI). With the emergence of all-round MLLMs like GPT-4V and Gemini, a multitude of evaluation methods have been developed to assess their capabilities across different dimensions. This paper presents a systematic and comprehensive review of MLLM evaluation methods, covering the following key aspects: (1) the background of MLLMs and their evaluation; (2) \"what to evaluate\" that reviews and categorizes existing MLLM evaluation tasks based on the capabilities assessed, including general multimodal recognition, perception, reasoning and trustworthiness, and domain-specific applications such as socioeconomic, natural sciences and engineering, medical usage, AI agent, remote sensing, video and audio processing, 3D point cloud analysis, and others; (3) \"where to evaluate\" that summarizes MLLM evaluation benchmarks into general and specific benchmarks; (4) \"how to evaluate\" that reviews and illustrates MLLM evaluation steps and metrics; Our overarching goal is to provide valuable insights for researchers in the field of MLLM evaluation, thereby facilitating the development of more capable and reliable MLLMs. We emphasize that evaluation should be regarded as a critical discipline, essential for advancing the field of MLLMs.",
    "authors": [
      "Jiaxing Huang",
      "Jingyi Zhang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-08-28T13:05:55Z",
    "pdf_url": "https://arxiv.org/pdf/2408.15769v1"
  },
  {
    "arxiv_id": "2408.14772v2",
    "entry_id": "http://arxiv.org/abs/2408.14772v2",
    "title": "A global AI community requires language-diverse publishing",
    "summary": "In this provocation, we discuss the English dominance of the AI research community, arguing that the requirement for English language publishing upholds and reinforces broader regimes of extraction in AI. While large language models and machine translation have been celebrated as a way to break down barriers, we regard their use as a symptom of linguistic exclusion of scientists and potential readers. We propose alternative futures for a healthier publishing culture, organized around three themes: administering conferences in the languages of the country in which they are held, instructing peer reviewers not to adjudicate the language appropriateness of papers, and offering opportunities to publish and present in multiple languages. We welcome new translations of this piece. Please contact the authors if you would like to contribute one.",
    "authors": [
      "Haley Lepp",
      "Parth Sarin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-27T04:20:10Z",
    "pdf_url": "https://arxiv.org/pdf/2408.14772v2"
  },
  {
    "arxiv_id": "2409.00097v3",
    "entry_id": "http://arxiv.org/abs/2409.00097v3",
    "title": "Large Language Models for Disease Diagnosis: A Scoping Review",
    "summary": "Automatic disease diagnosis has become increasingly valuable in clinical practice. The advent of large language models (LLMs) has catalyzed a paradigm shift in artificial intelligence, with growing evidence supporting the efficacy of LLMs in diagnostic tasks. Despite the increasing attention in this field, a holistic view is still lacking. Many critical aspects remain unclear, such as the diseases and clinical data to which LLMs have been applied, the LLM techniques employed, and the evaluation methods used. In this article, we perform a comprehensive review of LLM-based methods for disease diagnosis. Our review examines the existing literature across various dimensions, including disease types and associated clinical specialties, clinical data, LLM techniques, and evaluation methods. Additionally, we offer recommendations for applying and evaluating LLMs for diagnostic tasks. Furthermore, we assess the limitations of current research and discuss future directions. To our knowledge, this is the first comprehensive review for LLM-based disease diagnosis.",
    "authors": [
      "Shuang Zhou",
      "Zidu Xu",
      "Mian Zhang",
      "Chunpu Xu",
      "Yawen Guo",
      "Zaifu Zhan",
      "Yi Fang",
      "Sirui Ding",
      "Jiashuo Wang",
      "Kaishuai Xu",
      "Liqiao Xia",
      "Jeremy Yeung",
      "Daochen Zha",
      "Dongming Cai",
      "Genevieve B. Melton",
      "Mingquan Lin",
      "Rui Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-27T02:06:45Z",
    "pdf_url": "https://arxiv.org/pdf/2409.00097v3"
  },
  {
    "arxiv_id": "2409.06721v1",
    "entry_id": "http://arxiv.org/abs/2409.06721v1",
    "title": "Students' Perceived Roles, Opportunities, and Challenges of a Generative AI-powered Teachable Agent: A Case of Middle School Math Class",
    "summary": "Ongoing advancements in Generative AI (GenAI) have boosted the potential of applying long-standing learning-by-teaching practices in the form of a teachable agent (TA). Despite the recognized roles and opportunities of TAs, less is known about how GenAI could create synergy or introduce challenges in TAs and how students perceived the application of GenAI in TAs. This study explored middle school students perceived roles, benefits, and challenges of GenAI-powered TAs in an authentic mathematics classroom. Through classroom observation, focus-group interviews, and open-ended surveys of 108 sixth-grade students, we found that students expected the GenAI-powered TA to serve as a learning companion, facilitator, and collaborative problem-solver. Students also expressed the benefits and challenges of GenAI-powered TAs. This study provides implications for the design of educational AI and AI-assisted instruction.",
    "authors": [
      "Yukyeong Song",
      "Jinhee Kim",
      "Zifeng Liu",
      "Chenglu Li",
      "Wanli Xing"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-08-26T18:54:20Z",
    "pdf_url": "https://arxiv.org/pdf/2409.06721v1"
  },
  {
    "arxiv_id": "2408.14340v3",
    "entry_id": "http://arxiv.org/abs/2408.14340v3",
    "title": "Foundation Models for Music: A Survey",
    "summary": "In recent years, foundation models (FMs) such as large language models (LLMs) and latent diffusion models (LDMs) have profoundly impacted diverse sectors, including music. This comprehensive review examines state-of-the-art (SOTA) pre-trained models and foundation models in music, spanning from representation learning, generative learning and multimodal learning. We first contextualise the significance of music in various industries and trace the evolution of AI in music. By delineating the modalities targeted by foundation models, we discover many of the music representations are underexplored in FM development. Then, emphasis is placed on the lack of versatility of previous methods on diverse music applications, along with the potential of FMs in music understanding, generation and medical application. By comprehensively exploring the details of the model pre-training paradigm, architectural choices, tokenisation, finetuning methodologies and controllability, we emphasise the important topics that should have been well explored, like instruction tuning and in-context learning, scaling law and emergent ability, as well as long-sequence modelling etc. A dedicated section presents insights into music agents, accompanied by a thorough analysis of datasets and evaluations essential for pre-training and downstream tasks. Finally, by underscoring the vital importance of ethical considerations, we advocate that following research on FM for music should focus more on such issues as interpretability, transparency, human responsibility, and copyright issues. The paper offers insights into future challenges and trends on FMs for music, aiming to shape the trajectory of human-AI collaboration in the music realm.",
    "authors": [
      "Yinghao Ma",
      "Anders Øland",
      "Anton Ragni",
      "Bleiz MacSen Del Sette",
      "Charalampos Saitis",
      "Chris Donahue",
      "Chenghua Lin",
      "Christos Plachouras",
      "Emmanouil Benetos",
      "Elona Shatri",
      "Fabio Morreale",
      "Ge Zhang",
      "György Fazekas",
      "Gus Xia",
      "Huan Zhang",
      "Ilaria Manco",
      "Jiawen Huang",
      "Julien Guinot",
      "Liwei Lin",
      "Luca Marinelli",
      "Max W. Y. Lam",
      "Megha Sharma",
      "Qiuqiang Kong",
      "Roger B. Dannenberg",
      "Ruibin Yuan",
      "Shangda Wu",
      "Shih-Lun Wu",
      "Shuqi Dai",
      "Shun Lei",
      "Shiyin Kang",
      "Simon Dixon",
      "Wenhu Chen",
      "Wenhao Huang",
      "Xingjian Du",
      "Xingwei Qu",
      "Xu Tan",
      "Yizhi Li",
      "Zeyue Tian",
      "Zhiyong Wu",
      "Zhizheng Wu",
      "Ziyang Ma",
      "Ziyu Wang"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "eess.AS"
    ],
    "published": "2024-08-26T15:13:14Z",
    "pdf_url": "https://arxiv.org/pdf/2408.14340v3"
  },
  {
    "arxiv_id": "2408.14317v2",
    "entry_id": "http://arxiv.org/abs/2408.14317v2",
    "title": "Claim Verification in the Age of Large Language Models: A Survey",
    "summary": "The large and ever-increasing amount of data available on the Internet coupled with the laborious task of manual claim and fact verification has sparked the interest in the development of automated claim verification systems. Several deep learning and transformer-based models have been proposed for this task over the years. With the introduction of Large Language Models (LLMs) and their superior performance in several NLP tasks, we have seen a surge of LLM-based approaches to claim verification along with the use of novel methods such as Retrieval Augmented Generation (RAG). In this survey, we present a comprehensive account of recent claim verification frameworks using LLMs. We describe the different components of the claim verification pipeline used in these frameworks in detail including common approaches to retrieval, prompting, and fine-tuning. Finally, we describe publicly available English datasets created for this task.",
    "authors": [
      "Alphaeus Dmonte",
      "Roland Oruche",
      "Marcos Zampieri",
      "Prasad Calyam",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-26T14:45:03Z",
    "pdf_url": "https://arxiv.org/pdf/2408.14317v2"
  },
  {
    "arxiv_id": "2409.00091v1",
    "entry_id": "http://arxiv.org/abs/2409.00091v1",
    "title": "Classification of Safety Events at Nuclear Sites using Large Language Models",
    "summary": "This paper proposes the development of a Large Language Model (LLM) based machine learning classifier designed to categorize Station Condition Records (SCRs) at nuclear power stations into safety-related and non-safety-related categories. The primary objective is to augment the existing manual review process by enhancing the efficiency and accuracy of the safety classification process at nuclear stations. The paper discusses experiments performed to classify a labeled SCR dataset and evaluates the performance of the classifier. It explores the construction of several prompt variations and their observed effects on the LLM's decision-making process. Additionally, it introduces a numerical scoring mechanism that could offer a more nuanced and flexible approach to SCR safety classification. This method represents an innovative step in nuclear safety management, providing a scalable tool for the identification of safety events.",
    "authors": [
      "Mishca de Costa",
      "Muhammad Anwar",
      "Daniel Lau",
      "Issam Hammad"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-08-26T08:21:21Z",
    "pdf_url": "https://arxiv.org/pdf/2409.00091v1"
  },
  {
    "arxiv_id": "2409.00089v1",
    "entry_id": "http://arxiv.org/abs/2409.00089v1",
    "title": "Watermarking Techniques for Large Language Models: A Survey",
    "summary": "With the rapid advancement and extensive application of artificial intelligence technology, large language models (LLMs) are extensively used to enhance production, creativity, learning, and work efficiency across various domains. However, the abuse of LLMs also poses potential harm to human society, such as intellectual property rights issues, academic misconduct, false content, and hallucinations. Relevant research has proposed the use of LLM watermarking to achieve IP protection for LLMs and traceability of multimedia data output by LLMs. To our knowledge, this is the first thorough review that investigates and analyzes LLM watermarking technology in detail. This review begins by recounting the history of traditional watermarking technology, then analyzes the current state of LLM watermarking research, and thoroughly examines the inheritance and relevance of these techniques. By analyzing their inheritance and relevance, this review can provide research with ideas for applying traditional digital watermarking techniques to LLM watermarking, to promote the cross-integration and innovation of watermarking technology. In addition, this review examines the pros and cons of LLM watermarking. Considering the current multimodal development trend of LLMs, it provides a detailed analysis of emerging multimodal LLM watermarking, such as visual and audio data, to offer more reference ideas for relevant research. This review delves into the challenges and future prospects of current watermarking technologies, offering valuable insights for future LLM watermarking research and applications.",
    "authors": [
      "Yuqing Liang",
      "Jiancheng Xiao",
      "Wensheng Gan",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-08-26T06:50:11Z",
    "pdf_url": "https://arxiv.org/pdf/2409.00089v1"
  },
  {
    "arxiv_id": "2408.14520v3",
    "entry_id": "http://arxiv.org/abs/2408.14520v3",
    "title": "Towards Graph Prompt Learning: A Survey and Beyond",
    "summary": "Large-scale \"pre-train and prompt learning\" paradigms have demonstrated remarkable adaptability, enabling broad applications across diverse domains such as question answering, image recognition, and multimodal retrieval. This approach fully leverages the potential of large-scale pre-trained models, reducing downstream data requirements and computational costs while enhancing model applicability across various tasks. Graphs, as versatile data structures that capture relationships between entities, play pivotal roles in fields such as social network analysis, recommender systems, and biological graphs. Despite the success of pre-train and prompt learning paradigms in Natural Language Processing (NLP) and Computer Vision (CV), their application in graph domains remains nascent. In graph-structured data, not only do the node and edge features often have disparate distributions, but the topological structures also differ significantly. This diversity in graph data can lead to incompatible patterns or gaps between pre-training and fine-tuning on downstream graphs. We aim to bridge this gap by summarizing methods for alleviating these disparities. This includes exploring prompt design methodologies, comparing related techniques, assessing application scenarios and datasets, and identifying unresolved problems and challenges. This survey categorizes over 100 relevant works in this field, summarizing general design principles and the latest applications, including text-attributed graphs, molecules, proteins, and recommendation systems. Through this extensive review, we provide a foundational understanding of graph prompt learning, aiming to impact not only the graph mining community but also the broader Artificial General Intelligence (AGI) community.",
    "authors": [
      "Qingqing Long",
      "Yuchen Yan",
      "Peiyan Zhang",
      "Chen Fang",
      "Wentao Cui",
      "Zhiyuan Ning",
      "Meng Xiao",
      "Ning Cao",
      "Xiao Luo",
      "Lingjun Xu",
      "Shiyue Jiang",
      "Zheng Fang",
      "Chong Chen",
      "Xian-Sheng Hua",
      "Yuanchun Zhou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2024-08-26T06:36:42Z",
    "pdf_url": "https://arxiv.org/pdf/2408.14520v3"
  },
  {
    "arxiv_id": "2408.14518v2",
    "entry_id": "http://arxiv.org/abs/2408.14518v2",
    "title": "A Survey on Reinforcement Learning Applications in SLAM",
    "summary": "The emergence of mobile robotics, particularly in the automotive industry, introduces a promising era of enriched user experiences and adept handling of complex navigation challenges. The realization of these advancements necessitates a focused technological effort and the successful execution of numerous intricate tasks, particularly in the critical domain of Simultaneous Localization and Mapping (SLAM). Various artificial intelligence (AI) methodologies, such as deep learning and reinforcement learning, present viable solutions to address the challenges in SLAM. This study specifically explores the application of reinforcement learning in the context of SLAM. By enabling the agent (the robot) to iteratively interact with and receive feedback from its environment, reinforcement learning facilitates the acquisition of navigation and mapping skills, thereby enhancing the robot's decision-making capabilities. This approach offers several advantages, including improved navigation proficiency, increased resilience, reduced dependence on sensor precision, and refinement of the decision-making process. The findings of this study, which provide an overview of reinforcement learning's utilization in SLAM, reveal significant advancements in the field. The investigation also highlights the evolution and innovative integration of these techniques.",
    "authors": [
      "Mohammad Dehghani Tezerjani",
      "Mohammad Khoshnazar",
      "Mohammadhamed Tangestanizadeh",
      "Arman Kiani",
      "Qing Yang"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2024-08-26T00:13:14Z",
    "pdf_url": "https://arxiv.org/pdf/2408.14518v2"
  },
  {
    "arxiv_id": "2408.13960v2",
    "entry_id": "http://arxiv.org/abs/2408.13960v2",
    "title": "Time Series Analysis for Education: Methods, Applications, and Future Directions",
    "summary": "Recent advancements in the collection and analysis of sequential educational data have brought time series analysis to a pivotal position in educational research, highlighting its essential role in facilitating data-driven decision-making. However, there is a lack of comprehensive summaries that consolidate these advancements. To the best of our knowledge, this paper is the first to provide a comprehensive review of time series analysis techniques specifically within the educational context. We begin by exploring the landscape of educational data analytics, categorizing various data sources and types relevant to education. We then review four prominent time series methods-forecasting, classification, clustering, and anomaly detection-illustrating their specific application points in educational settings. Subsequently, we present a range of educational scenarios and applications, focusing on how these methods are employed to address diverse educational tasks, which highlights the practical integration of multiple time series methods to solve complex educational problems. Finally, we conclude with a discussion on future directions, including personalized learning analytics, multimodal data fusion, and the role of large language models (LLMs) in educational time series. The contributions of this paper include a detailed taxonomy of educational data, a synthesis of time series techniques with specific educational applications, and a forward-looking perspective on emerging trends and future research opportunities in educational analysis. The related papers and resources are available and regularly updated at the project page.",
    "authors": [
      "Shengzhong Mao",
      "Chaoli Zhang",
      "Yichi Song",
      "Jindong Wang",
      "Xiao-Jun Zeng",
      "Zenglin Xu",
      "Qingsong Wen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-08-25T23:48:11Z",
    "pdf_url": "https://arxiv.org/pdf/2408.13960v2"
  },
  {
    "arxiv_id": "2408.13630v2",
    "entry_id": "http://arxiv.org/abs/2408.13630v2",
    "title": "DeepVoting: Learning and Fine-Tuning Voting Rules with Canonical Embeddings",
    "summary": "Aggregating agent preferences into a collective decision is an important step in many problems (e.g., hiring, elections, peer review) and across areas of computer science (e.g., reinforcement learning, recommender systems). As Social Choice Theory has shown, the problem of designing aggregation rules with specific sets of properties (axioms) can be difficult, or provably impossible in some cases. Instead of designing algorithms by hand, one can learn aggregation rules, particularly voting rules, from data. However, prior work in this area has required extremely large models or been limited by the choice of preference representation, i.e., embedding. We recast the problem of designing voting rules with desirable properties into one of learning probabilistic functions that output distributions over a set of candidates. Specifically, we use neural networks to learn probabilistic social choice functions. Using standard embeddings from the social choice literature we show that preference profile encoding has significant impact on the efficiency and ability of neural networks to learn rules, allowing us to learn rules faster and with smaller networks than previous work. Moreover, we show that our learned rules can be fine-tuned using axiomatic properties to create novel voting rules and make them resistant to specific types of \"attack\". Namely, we fine-tune rules to resist a probabilistic version of the No Show Paradox.",
    "authors": [
      "Leonardo Matone",
      "Ben Abramowitz",
      "Ben Armstrong",
      "Avinash Balakrishnan",
      "Nicholas Mattei"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.LG",
      "econ.GN"
    ],
    "published": "2024-08-24T17:15:20Z",
    "pdf_url": "https://arxiv.org/pdf/2408.13630v2"
  },
  {
    "arxiv_id": "2408.13406v2",
    "entry_id": "http://arxiv.org/abs/2408.13406v2",
    "title": "Collaboration Dynamics and Reliability Challenges of Multi-Agent LLM Systems in Finite Element Analysis",
    "summary": "Large Language Model (LLM)-based multi-agent systems are increasingly applied to automate computational workflows in science and engineering. However, how inter-agent dynamics influence reasoning quality and verification reliability remains unclear. We study these mechanisms using an AutoGen-based multi-agent framework for linear-elastic Finite Element Analysis (FEA), evaluating seven role configurations across four tasks under a fixed 12-turn conversation limit. From 1,120 controlled trials, we find that collaboration effectiveness depends more on functional complementarity than team size: the three-agent Coder-Executor-Critic configuration uniquely produced physically and visually correct solutions, while adding redundant reviewers reduced success rates. Yet three systematic failure modes persist: (1) affirmation bias, where the Rebuttal agent endorsed rather than challenged outputs (85-92% agreement, including errors); (2) premature consensus caused by redundant reviewers; and (3) a verification-validation gap where executable but physically incorrect code passed undetected. No agent combination successfully validated constitutive relations in complex tasks. Building on theories of functional diversity, role differentiation, and computational validation, we propose actionable design principles: (i) assign complementary agent roles, (ii) enforce multi-level validation (execution, specification, physics), and (iii) prevent early consensus through adversarial or trigger-based interaction control. These findings establish a principled foundation for designing trustworthy LLM collaborations in engineering workflows.",
    "authors": [
      "Chuan Tian",
      "Yilei Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.MA"
    ],
    "published": "2024-08-23T23:11:08Z",
    "pdf_url": "https://arxiv.org/pdf/2408.13406v2"
  },
  {
    "arxiv_id": "2408.13296v3",
    "entry_id": "http://arxiv.org/abs/2408.13296v3",
    "title": "The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities",
    "summary": "This report examines the fine-tuning of Large Language Models (LLMs), integrating theoretical insights with practical applications. It outlines the historical evolution of LLMs from traditional Natural Language Processing (NLP) models to their pivotal role in AI. A comparison of fine-tuning methodologies, including supervised, unsupervised, and instruction-based approaches, highlights their applicability to different tasks. The report introduces a structured seven-stage pipeline for fine-tuning LLMs, spanning data preparation, model initialization, hyperparameter tuning, and model deployment. Emphasis is placed on managing imbalanced datasets and optimization techniques. Parameter-efficient methods like Low-Rank Adaptation (LoRA) and Half Fine-Tuning are explored for balancing computational efficiency with performance. Advanced techniques such as memory fine-tuning, Mixture of Experts (MoE), and Mixture of Agents (MoA) are discussed for leveraging specialized networks and multi-agent collaboration. The report also examines novel approaches like Proximal Policy Optimization (PPO) and Direct Preference Optimization (DPO), which align LLMs with human preferences, alongside pruning and routing optimizations to improve efficiency. Further sections cover validation frameworks, post-deployment monitoring, and inference optimization, with attention to deploying LLMs on distributed and cloud-based platforms. Emerging areas such as multimodal LLMs, fine-tuning for audio and speech, and challenges related to scalability, privacy, and accountability are also addressed. This report offers actionable insights for researchers and practitioners navigating LLM fine-tuning in an evolving landscape.",
    "authors": [
      "Venkatesh Balavadhani Parthasarathy",
      "Ahtsham Zafar",
      "Aafaq Khan",
      "Arsalan Shahid"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-08-23T14:48:02Z",
    "pdf_url": "https://arxiv.org/pdf/2408.13296v3"
  },
  {
    "arxiv_id": "2408.12963v1",
    "entry_id": "http://arxiv.org/abs/2408.12963v1",
    "title": "Open Llama2 Model for the Lithuanian Language",
    "summary": "In this paper, we propose and describe the first open Llama2 large language models (LLMs) for the Lithuanian language, including an accompanying question/answer (Q/A) dataset and translations of popular LLM benchmarks. We provide a brief review of open regional LLMs and detailed information on the proposed LLMs and their training process. We also conduct an empirical evaluation, comparing the perplexities of the proposed LLMs with those of other modern open LLMs. In addition, benchmarking the proposed LLMs against language understanding tasks reveals that high-quality pretraining datasets may be essential for achieving models that perform efficiently on these benchmarks. The full realisations of the described LLMs are available in the accompanying open repository~\\url{https://huggingface.co/neurotechnology}.",
    "authors": [
      "Artūras Nakvosas",
      "Povilas Daniušis",
      "Vytas Mulevičius"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-08-23T10:18:39Z",
    "pdf_url": "https://arxiv.org/pdf/2408.12963v1"
  },
  {
    "arxiv_id": "2408.12935v3",
    "entry_id": "http://arxiv.org/abs/2408.12935v3",
    "title": "Trustworthy, Responsible, and Safe AI: A Comprehensive Architectural Framework for AI Safety with Challenges and Mitigations",
    "summary": "AI Safety is an emerging area of critical importance to the safe adoption and deployment of AI systems. With the rapid proliferation of AI and especially with the recent advancement of Generative AI (or GAI), the technology ecosystem behind the design, development, adoption, and deployment of AI systems has drastically changed, broadening the scope of AI Safety to address impacts on public safety and national security. In this paper, we propose a novel architectural framework for understanding and analyzing AI Safety; defining its characteristics from three perspectives: Trustworthy AI, Responsible AI, and Safe AI. We provide an extensive review of current research and advancements in AI safety from these perspectives, highlighting their key challenges and mitigation approaches. Through examples from state-of-the-art technologies, particularly Large Language Models (LLMs), we present innovative mechanism, methodologies, and techniques for designing and testing AI safety. Our goal is to promote advancement in AI safety research, and ultimately enhance people's trust in digital transformation.",
    "authors": [
      "Chen Chen",
      "Xueluan Gong",
      "Ziyao Liu",
      "Weifeng Jiang",
      "Si Qi Goh",
      "Kwok-Yan Lam"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-23T09:33:48Z",
    "pdf_url": "https://arxiv.org/pdf/2408.12935v3"
  },
  {
    "arxiv_id": "2408.12157v1",
    "entry_id": "http://arxiv.org/abs/2408.12157v1",
    "title": "Implicit Sentiment Analysis Based on Chain of Thought Prompting",
    "summary": "Implicit Sentiment Analysis (ISA) is a crucial research area in natural language processing. Inspired by the idea of large language model Chain of Thought (CoT), this paper introduces a Sentiment Analysis of Thinking (SAoT) framework. The framework first analyzes the implicit aspects and opinions in the text using common sense and thinking chain capabilities. Then, it reflects on the process of implicit sentiment analysis and finally deduces the polarity of sentiment. The model is evaluated on the SemEval 2014 dataset, consisting of 1120 restaurant reviews and 638 laptop reviews. The experimental results demonstrate that the utilization of the ERNIE-Bot-4+SAoT model yields a notable performance improvement. Specifically, on the restaurant dataset, the F1 score reaches 75.27, accompanied by an ISA score of 66.29. Similarly, on the computer dataset, the F1 score achieves 76.50, while the ISA score amounts to 73.46. Comparatively, the ERNIE-Bot-4+SAoT model surpasses the BERTAsp + SCAPt baseline by an average margin of 47.99%.",
    "authors": [
      "Zhihua Duan",
      "Jialin Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-22T06:55:29Z",
    "pdf_url": "https://arxiv.org/pdf/2408.12157v1"
  },
  {
    "arxiv_id": "2408.11943v1",
    "entry_id": "http://arxiv.org/abs/2408.11943v1",
    "title": "Advances in Preference-based Reinforcement Learning: A Review",
    "summary": "Reinforcement Learning (RL) algorithms suffer from the dependency on accurately engineered reward functions to properly guide the learning agents to do the required tasks. Preference-based reinforcement learning (PbRL) addresses that by utilizing human preferences as feedback from the experts instead of numeric rewards. Due to its promising advantage over traditional RL, PbRL has gained more focus in recent years with many significant advances. In this survey, we present a unified PbRL framework to include the newly emerging approaches that improve the scalability and efficiency of PbRL. In addition, we give a detailed overview of the theoretical guarantees and benchmarking work done in the field, while presenting its recent applications in complex real-world tasks. Lastly, we go over the limitations of the current approaches and the proposed future research directions.",
    "authors": [
      "Youssef Abdelkareem",
      "Shady Shehata",
      "Fakhri Karray"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-21T18:57:12Z",
    "pdf_url": "https://arxiv.org/pdf/2408.11943v1"
  },
  {
    "arxiv_id": "2408.11735v3",
    "entry_id": "http://arxiv.org/abs/2408.11735v3",
    "title": "Clinical Insights: A Comprehensive Review of Language Models in Medicine",
    "summary": "This paper explores the advancements and applications of language models in healthcare, focusing on their clinical use cases. It examines the evolution from early encoder-based systems requiring extensive fine-tuning to state-of-the-art large language and multimodal models capable of integrating text and visual data through in-context learning. The analysis emphasizes locally deployable models, which enhance data privacy and operational autonomy, and their applications in tasks such as text generation, classification, information extraction, and conversational systems. The paper also highlights a structured organization of tasks and a tiered ethical approach, providing a valuable resource for researchers and practitioners, while discussing key challenges related to ethics, evaluation, and implementation.",
    "authors": [
      "Nikita Neveditsin",
      "Pawan Lingras",
      "Vijay Mago"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-21T15:59:33Z",
    "pdf_url": "https://arxiv.org/pdf/2408.11735v3"
  },
  {
    "arxiv_id": "2408.11415v2",
    "entry_id": "http://arxiv.org/abs/2408.11415v2",
    "title": "Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations",
    "summary": "Contemporary research in social sciences increasingly utilizes state-of-the-art generative language models to annotate or generate content. While these models achieve benchmark-leading performance on common language tasks, their application to novel out-of-domain tasks remains insufficiently explored. To address this gap, we investigate how personalized language models align with human responses on the Moral Foundation Theory Questionnaire. We adapt open-source generative language models to different political personas and repeatedly survey these models to generate synthetic data sets where model-persona combinations define our sub-populations. Our analysis reveals that models produce inconsistent results across multiple repetitions, yielding high response variance. Furthermore, the alignment between synthetic data and corresponding human data from psychological studies shows a weak correlation, with conservative persona-prompted models particularly failing to align with actual conservative populations. These results suggest that language models struggle to coherently represent ideologies through in-context prompting due to their alignment process. Thus, using language models to simulate social interactions requires measurable improvements in in-context optimization or parameter manipulation to align with psychological and sociological stereotypes properly.",
    "authors": [
      "Simon Münker"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-21T08:20:41Z",
    "pdf_url": "https://arxiv.org/pdf/2408.11415v2"
  },
  {
    "arxiv_id": "2408.11288v1",
    "entry_id": "http://arxiv.org/abs/2408.11288v1",
    "title": "Applying and Evaluating Large Language Models in Mental Health Care: A Scoping Review of Human-Assessed Generative Tasks",
    "summary": "Large language models (LLMs) are emerging as promising tools for mental health care, offering scalable support through their ability to generate human-like responses. However, the effectiveness of these models in clinical settings remains unclear. This scoping review aimed to assess the current generative applications of LLMs in mental health care, focusing on studies where these models were tested with human participants in real-world scenarios. A systematic search across APA PsycNet, Scopus, PubMed, and Web of Science identified 726 unique articles, of which 17 met the inclusion criteria. These studies encompassed applications such as clinical assistance, counseling, therapy, and emotional support. However, the evaluation methods were often non-standardized, with most studies relying on ad hoc scales that limit comparability and robustness. Privacy, safety, and fairness were also frequently underexplored. Moreover, reliance on proprietary models, such as OpenAI's GPT series, raises concerns about transparency and reproducibility. While LLMs show potential in expanding mental health care access, especially in underserved areas, the current evidence does not fully support their use as standalone interventions. More rigorous, standardized evaluations and ethical oversight are needed to ensure these tools can be safely and effectively integrated into clinical practice.",
    "authors": [
      "Yining Hua",
      "Hongbin Na",
      "Zehan Li",
      "Fenglin Liu",
      "Xiao Fang",
      "David Clifton",
      "John Torous"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-21T02:21:59Z",
    "pdf_url": "https://arxiv.org/pdf/2408.11288v1"
  },
  {
    "arxiv_id": "2408.10946v2",
    "entry_id": "http://arxiv.org/abs/2408.10946v2",
    "title": "Large Language Model Driven Recommendation",
    "summary": "While previous chapters focused on recommendation systems (RSs) based on standardized, non-verbal user feedback such as purchases, views, and clicks -- the advent of LLMs has unlocked the use of natural language (NL) interactions for recommendation. This chapter discusses how LLMs' abilities for general NL reasoning present novel opportunities to build highly personalized RSs -- which can effectively connect nuanced and diverse user preferences to items, potentially via interactive dialogues. To begin this discussion, we first present a taxonomy of the key data sources for language-driven recommendation, covering item descriptions, user-system interactions, and user profiles. We then proceed to fundamental techniques for LLM recommendation, reviewing the use of encoder-only and autoregressive LLM recommendation in both tuned and untuned settings. Afterwards, we move to multi-module recommendation architectures in which LLMs interact with components such as retrievers and RSs in multi-stage pipelines. This brings us to architectures for conversational recommender systems (CRSs), in which LLMs facilitate multi-turn dialogues where each turn presents an opportunity not only to make recommendations, but also to engage with the user in interactive preference elicitation, critiquing, and question-answering.",
    "authors": [
      "Anton Korikov",
      "Scott Sanner",
      "Yashar Deldjoo",
      "Zhankui He",
      "Julian McAuley",
      "Arnau Ramisa",
      "Rene Vidal",
      "Mahesh Sathiamoorthy",
      "Atoosa Kasrizadeh",
      "Silvia Milano",
      "Francesco Ricci"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-20T15:36:24Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10946v2"
  },
  {
    "arxiv_id": "2408.10932v3",
    "entry_id": "http://arxiv.org/abs/2408.10932v3",
    "title": "The Evolution of Reinforcement Learning in Quantitative Finance: A Survey",
    "summary": "Reinforcement Learning (RL) has experienced significant advancement over the past decade, prompting a growing interest in applications within finance. This survey critically evaluates 167 publications, exploring diverse RL applications and frameworks in finance. Financial markets, marked by their complexity, multi-agent nature, information asymmetry, and inherent randomness, serve as an intriguing test-bed for RL. Traditional finance offers certain solutions, and RL advances these with a more dynamic approach, incorporating machine learning methods, including transfer learning, meta-learning, and multi-agent solutions. This survey dissects key RL components through the lens of Quantitative Finance. We uncover emerging themes, propose areas for future research, and critique the strengths and weaknesses of existing methods.",
    "authors": [
      "Nikolaos Pippas",
      "Elliot A. Ludvig",
      "Cagatay Turkay"
    ],
    "categories": [
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "published": "2024-08-20T15:15:10Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10932v3"
  },
  {
    "arxiv_id": "2408.10729v1",
    "entry_id": "http://arxiv.org/abs/2408.10729v1",
    "title": "Towards Efficient Large Language Models for Scientific Text: A Review",
    "summary": "Large language models (LLMs) have ushered in a new era for processing complex information in various fields, including science. The increasing amount of scientific literature allows these models to acquire and understand scientific knowledge effectively, thus improving their performance in a wide range of tasks. Due to the power of LLMs, they require extremely expensive computational resources, intense amounts of data, and training time. Therefore, in recent years, researchers have proposed various methodologies to make scientific LLMs more affordable. The most well-known approaches align in two directions. It can be either focusing on the size of the models or enhancing the quality of data. To date, a comprehensive review of these two families of methods has not yet been undertaken. In this paper, we (I) summarize the current advances in the emerging abilities of LLMs into more accessible AI solutions for science, and (II) investigate the challenges and opportunities of developing affordable solutions for scientific domains using LLMs.",
    "authors": [
      "Huy Quoc To",
      "Ming Liu",
      "Guangyan Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-20T10:57:34Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10729v1"
  },
  {
    "arxiv_id": "2408.10715v1",
    "entry_id": "http://arxiv.org/abs/2408.10715v1",
    "title": "Fine-Tuning a Local LLaMA-3 Large Language Model for Automated Privacy-Preserving Physician Letter Generation in Radiation Oncology",
    "summary": "Generating physician letters is a time-consuming task in daily clinical practice. This study investigates local fine-tuning of large language models (LLMs), specifically LLaMA models, for physician letter generation in a privacy-preserving manner within the field of radiation oncology. Our findings demonstrate that base LLaMA models, without fine-tuning, are inadequate for effectively generating physician letters. The QLoRA algorithm provides an efficient method for local intra-institutional fine-tuning of LLMs with limited computational resources (i.e., a single 48 GB GPU workstation within the hospital). The fine-tuned LLM successfully learns radiation oncology-specific information and generates physician letters in an institution-specific style. ROUGE scores of the generated summary reports highlight the superiority of the 8B LLaMA-3 model over the 13B LLaMA-2 model. Further multidimensional physician evaluations of 10 cases reveal that, although the fine-tuned LLaMA-3 model has limited capacity to generate content beyond the provided input data, it successfully generates salutations, diagnoses and treatment histories, recommendations for further treatment, and planned schedules. Overall, clinical benefit was rated highly by the clinical experts (average score of 3.44 on a 4-point scale). With careful physician review and correction, automated LLM-based physician letter generation has significant practical value.",
    "authors": [
      "Yihao Hou",
      "Christoph Bert",
      "Ahmed Gomaa",
      "Godehard Lahmer",
      "Daniel Hoefler",
      "Thomas Weissmann",
      "Raphaela Voigt",
      "Philipp Schubert",
      "Charlotte Schmitter",
      "Alina Depardon",
      "Sabine Semrau",
      "Andreas Maier",
      "Rainer Fietkau",
      "Yixing Huang",
      "Florian Putz"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-20T10:31:36Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10715v1"
  },
  {
    "arxiv_id": "2408.10691v3",
    "entry_id": "http://arxiv.org/abs/2408.10691v3",
    "title": "Fine-Tuning and Deploying Large Language Models Over Edges: Issues and Approaches",
    "summary": "Since the release of GPT2-1.5B in 2019, the large language models (LLMs) have evolved from specialized deep models to versatile foundation models. While demonstrating remarkable zero-shot ability, the LLMs still require fine-tuning on local datasets and substantial memory for deployment over the network edges. Traditional first-order fine-tuning techniques require significant GPU memory that exceeds the capacity of mainstream hardware. Besides, the LLMs have been expanded beyond text generation to create images, audio, video, and multi-modal content, necessitating careful investigation of efficient deployment strategies for large-scale foundation models. In response to these challenges, model fine-tuning and model-compression techniques have been developed to support the sustainable growth of LLMs by reducing both operational and capital expenditures. In this work, we provide a comprehensive overview of prevalent memory-efficient fine-tuning methods for deployment at the network edge. We also review state-of-the-art literature on model compression, offering insights into the deployment of LLMs at network edges.",
    "authors": [
      "Yanjie Dong",
      "Haijun Zhang",
      "Chengming Li",
      "Song Guo",
      "Victor C. M. Leung",
      "Xiping Hu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-20T09:42:17Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10691v3"
  },
  {
    "arxiv_id": "2408.10641v3",
    "entry_id": "http://arxiv.org/abs/2408.10641v3",
    "title": "A Review of Human-Object Interaction Detection",
    "summary": "Human-object interaction (HOI) detection plays a key role in high-level visual understanding, facilitating a deep comprehension of human activities. Specifically, HOI detection aims to locate the humans and objects involved in interactions within images or videos and classify the specific interactions between them. The success of this task is influenced by several key factors, including the accurate localization of human and object instances, as well as the correct classification of object categories and interaction relationships. This paper systematically summarizes and discusses the recent work in image-based HOI detection. First, the mainstream datasets involved in HOI relationship detection are introduced. Furthermore, starting with two-stage methods and end-to-end one-stage detection approaches, this paper comprehensively discusses the current developments in image-based HOI detection, analyzing the strengths and weaknesses of these two methods. Additionally, the advancements of zero-shot learning, weakly supervised learning, and the application of large-scale language models in HOI detection are discussed. Finally, the current challenges in HOI detection are outlined, and potential research directions and future trends are explored.",
    "authors": [
      "Yuxiao Wang",
      "Yu Lei",
      "Li Cui",
      "Weiying Xue",
      "Qi Liu",
      "Zhenao Wei"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-08-20T08:32:39Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10641v3"
  },
  {
    "arxiv_id": "2408.10495v2",
    "entry_id": "http://arxiv.org/abs/2408.10495v2",
    "title": "How Well Do Large Language Models Serve as End-to-End Secure Code Agents for Python?",
    "summary": "The rapid advancement of large language models (LLMs) such as GPT-4 has revolutionized the landscape of software engineering, positioning these models at the core of modern development practices. As we anticipate these models to evolve into the primary and trustworthy tools used in software development, ensuring the security of the code they produce becomes paramount. How well can LLMs serve as end-to-end secure code producers? This paper presents a systematic investigation into LLMs' inherent potential to generate code with fewer vulnerabilities. Specifically, We studied GPT-3.5 and GPT-4's capability to identify and repair vulnerabilities in the code generated by four popular LLMs including themselves (GPT-3.5, GPT-4, Code Llama, and CodeGeeX2). By manually or automatically reviewing 4,900 pieces of code, our study reveals that: (1) large language models lack awareness of scenario-relevant security risks, which leads to the generation of over 75% vulnerable code on the SecurityEval benchmark; (2) LLMs such as GPT-3.5 and GPT-4 are unable to precisely identify vulnerabilities in the code they generated; (3) GPT-3.5 and GPT-4 can achieve 33.2%~59.6% success rates in repairing the insecure code produced by the 4 LLMs, but they both perform poorly when repairing self-produced code, indicating self-repair \"blind spots\". To address the limitation of a single round of repair, we developed a lightweight tool that prompts LLMs to construct safer source code through an iterative repair procedure based on the insights gained from our study. Experiments show that assisted by semantic analysis engines, our tool significantly improves the success rates of repair to 65.9%~85.5%.",
    "authors": [
      "Jianian Gong",
      "Nachuan Duan",
      "Ziheng Tao",
      "Zhaohui Gong",
      "Yuan Yuan",
      "Minlie Huang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-08-20T02:42:29Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10495v2"
  },
  {
    "arxiv_id": "2408.10428v1",
    "entry_id": "http://arxiv.org/abs/2408.10428v1",
    "title": "Are LLMs Any Good for High-Level Synthesis?",
    "summary": "The increasing complexity and demand for faster, energy-efficient hardware designs necessitate innovative High-Level Synthesis (HLS) methodologies. This paper explores the potential of Large Language Models (LLMs) to streamline or replace the HLS process, leveraging their ability to understand natural language specifications and refactor code. We survey the current research and conduct experiments comparing Verilog designs generated by a standard HLS tool (Vitis HLS) with those produced by LLMs translating C code or natural language specifications. Our evaluation focuses on quantifying the impact on performance, power, and resource utilization, providing an assessment of the efficiency of LLM-based approaches. This study aims to illuminate the role of LLMs in HLS, identifying promising directions for optimized hardware design in applications such as AI acceleration, embedded systems, and high-performance computing.",
    "authors": [
      "Yuchao Liao",
      "Tosiron Adegbija",
      "Roman Lysecky"
    ],
    "categories": [
      "cs.AR",
      "cs.AI"
    ],
    "published": "2024-08-19T21:40:28Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10428v1"
  },
  {
    "arxiv_id": "2408.10365v1",
    "entry_id": "http://arxiv.org/abs/2408.10365v1",
    "title": "AI-Driven Review Systems: Evaluating LLMs in Scalable and Bias-Aware Academic Reviews",
    "summary": "Automatic reviewing helps handle a large volume of papers, provides early feedback and quality control, reduces bias, and allows the analysis of trends. We evaluate the alignment of automatic paper reviews with human reviews using an arena of human preferences by pairwise comparisons. Gathering human preference may be time-consuming; therefore, we also use an LLM to automatically evaluate reviews to increase sample efficiency while reducing bias. In addition to evaluating human and LLM preferences among LLM reviews, we fine-tune an LLM to predict human preferences, predicting which reviews humans will prefer in a head-to-head battle between LLMs. We artificially introduce errors into papers and analyze the LLM's responses to identify limitations, use adaptive review questions, meta prompting, role-playing, integrate visual and textual analysis, use venue-specific reviewing materials, and predict human preferences, improving upon the limitations of the traditional review processes. We make the reviews of publicly available arXiv and open-access Nature journal papers available online, along with a free service which helps authors review and revise their research papers and improve their quality. This work develops proof-of-concept LLM reviewing systems that quickly deliver consistent, high-quality reviews and evaluate their quality. We mitigate the risks of misuse, inflated review scores, overconfident ratings, and skewed score distributions by augmenting the LLM with multiple documents, including the review form, reviewer guide, code of ethics and conduct, area chair guidelines, and previous year statistics, by finding which errors and shortcomings of the paper may be detected by automated reviews, and evaluating pairwise reviewer preferences. This work identifies and addresses the limitations of using LLMs as reviewers and evaluators and enhances the quality of the reviewing process.",
    "authors": [
      "Keith Tyser",
      "Ben Segev",
      "Gaston Longhitano",
      "Xin-Yu Zhang",
      "Zachary Meeks",
      "Jason Lee",
      "Uday Garg",
      "Nicholas Belsten",
      "Avi Shporer",
      "Madeleine Udell",
      "Dov Te'eni",
      "Iddo Drori"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-19T19:10:38Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10365v1"
  },
  {
    "arxiv_id": "2409.02111v1",
    "entry_id": "http://arxiv.org/abs/2409.02111v1",
    "title": "Toward Large-scale Spiking Neural Networks: A Comprehensive Survey and Future Directions",
    "summary": "Deep learning has revolutionized artificial intelligence (AI), achieving remarkable progress in fields such as computer vision, speech recognition, and natural language processing. Moreover, the recent success of large language models (LLMs) has fueled a surge in research on large-scale neural networks. However, the escalating demand for computing resources and energy consumption has prompted the search for energy-efficient alternatives. Inspired by the human brain, spiking neural networks (SNNs) promise energy-efficient computation with event-driven spikes. To provide future directions toward building energy-efficient large SNN models, we present a survey of existing methods for developing deep spiking neural networks, with a focus on emerging Spiking Transformers. Our main contributions are as follows: (1) an overview of learning methods for deep spiking neural networks, categorized by ANN-to-SNN conversion and direct training with surrogate gradients; (2) an overview of network architectures for deep spiking neural networks, categorized by deep convolutional neural networks (DCNNs) and Transformer architecture; and (3) a comprehensive comparison of state-of-the-art deep SNNs with a focus on emerging Spiking Transformers. We then further discuss and outline future directions toward large-scale SNNs.",
    "authors": [
      "Yangfan Hu",
      "Qian Zheng",
      "Guoqi Li",
      "Huajin Tang",
      "Gang Pan"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-08-19T13:07:48Z",
    "pdf_url": "https://arxiv.org/pdf/2409.02111v1"
  },
  {
    "arxiv_id": "2408.09675v1",
    "entry_id": "http://arxiv.org/abs/2408.09675v1",
    "title": "Multi-Agent Reinforcement Learning for Autonomous Driving: A Survey",
    "summary": "Reinforcement Learning (RL) is a potent tool for sequential decision-making and has achieved performance surpassing human capabilities across many challenging real-world tasks. As the extension of RL in the multi-agent system domain, multi-agent RL (MARL) not only need to learn the control policy but also requires consideration regarding interactions with all other agents in the environment, mutual influences among different system components, and the distribution of computational resources. This augments the complexity of algorithmic design and poses higher requirements on computational resources. Simultaneously, simulators are crucial to obtain realistic data, which is the fundamentals of RL. In this paper, we first propose a series of metrics of simulators and summarize the features of existing benchmarks. Second, to ease comprehension, we recall the foundational knowledge and then synthesize the recently advanced studies of MARL-related autonomous driving and intelligent transportation systems. Specifically, we examine their environmental modeling, state representation, perception units, and algorithm design. Conclusively, we discuss open challenges as well as prospects and opportunities. We hope this paper can help the researchers integrate MARL technologies and trigger more insightful ideas toward the intelligent and autonomous driving.",
    "authors": [
      "Ruiqi Zhang",
      "Jing Hou",
      "Florian Walter",
      "Shangding Gu",
      "Jiayi Guan",
      "Florian Röhrbein",
      "Yali Du",
      "Panpan Cai",
      "Guang Chen",
      "Alois Knoll"
    ],
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "published": "2024-08-19T03:31:20Z",
    "pdf_url": "https://arxiv.org/pdf/2408.09675v1"
  },
  {
    "arxiv_id": "2408.08972v1",
    "entry_id": "http://arxiv.org/abs/2408.08972v1",
    "title": "ASGM-KG: Unveiling Alluvial Gold Mining Through Knowledge Graphs",
    "summary": "Artisanal and Small-Scale Gold Mining (ASGM) is a low-cost yet highly destructive mining practice, leading to environmental disasters across the world's tropical watersheds. The topic of ASGM spans multiple domains of research and information, including natural and social systems, and knowledge is often atomized across a diversity of media and documents. We therefore introduce a knowledge graph (ASGM-KG) that consolidates and provides crucial information about ASGM practices and their environmental effects. The current version of ASGM-KG consists of 1,899 triples extracted using a large language model (LLM) from documents and reports published by both non-governmental and governmental organizations. These documents were carefully selected by a group of tropical ecologists with expertise in ASGM. This knowledge graph was validated using two methods. First, a small team of ASGM experts reviewed and labeled triples as factual or non-factual. Second, we devised and applied an automated factual reduction framework that relies on a search engine and an LLM for labeling triples. Our framework performs as well as five baselines on a publicly available knowledge graph and achieves over 90 accuracy on our ASGM-KG validated by domain experts. ASGM-KG demonstrates an advancement in knowledge aggregation and representation for complex, interdisciplinary environmental crises such as ASGM.",
    "authors": [
      "Debashis Gupta",
      "Aditi Golder",
      "Luis Fernendez",
      "Miles Silman",
      "Greg Lersen",
      "Fan Yang",
      "Bob Plemmons",
      "Sarra Alqahtani",
      "Paul Victor Pauca"
    ],
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2024-08-16T18:48:15Z",
    "pdf_url": "https://arxiv.org/pdf/2408.08972v1"
  },
  {
    "arxiv_id": "2408.08694v1",
    "entry_id": "http://arxiv.org/abs/2408.08694v1",
    "title": "Quantifying the Effectiveness of Student Organization Activities using Natural Language Processing",
    "summary": "Student extracurricular activities play an important role in enriching the students' educational experiences. With the increasing popularity of Machine Learning and Natural Language Processing, it becomes a logical step that incorporating ML-NLP in improving extracurricular activities is a potential focus of study in Artificial Intelligence (AI). This research study aims to develop a machine learning workflow that will quantify the effectiveness of student-organized activities based on student emotional responses using sentiment analysis. The study uses the Bidirectional Encoder Representations from Transformers (BERT) Large Language Model (LLM) called via the pysentimiento toolkit, as a Transformer pipeline in Hugging Face. A sample data set from Organization C, a Recognized Student Organization (RSO) of a higher educational institute in the Philippines, College X, was used to develop the workflow. The workflow consisted of data preprocessing, key feature selection, LLM feature processing, and score aggregation, resulting in an Event Score for each data set. The results show that the BERT LLM can also be used effectively in analyzing sentiment beyond product reviews and post comments. For the student affairs offices of educational institutions, this study can provide a practical example of how NLP can be applied to real-world scenarios, showcasing the potential impact of data-driven decision making.",
    "authors": [
      "Lyberius Ennio F. Taruc",
      "Arvin R. De La Cruz"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET"
    ],
    "published": "2024-08-16T12:16:59Z",
    "pdf_url": "https://arxiv.org/pdf/2408.08694v1"
  },
  {
    "arxiv_id": "2408.08688v7",
    "entry_id": "http://arxiv.org/abs/2408.08688v7",
    "title": "The Fellowship of the LLMs: Multi-Model Workflows for Synthetic Preference Optimization Dataset Generation",
    "summary": "This paper presents a novel methodology for generating synthetic Preference Optimization (PO) datasets using multi-model workflows. We evaluate the effectiveness and potential of these workflows in automating and enhancing the dataset generation process. PO dataset generation requires two modules: (1) $\\textit{response evaluation}$, and (2) $\\textit{response generation}$. In the $\\textit{response evaluation}$ module, the responses from Large Language Models (LLMs) are evaluated and ranked - a task typically carried out by human annotators that we automate using LLMs. We assess the response evaluation module in a 2 step process. In step 1, we assess LLMs as evaluators using three distinct prompting strategies. In step 2, we apply the winning prompting strategy to compare the performance of LLM-as-a-Judge, LLMs-as-a-Jury, and LLM Debate. Our evaluation shows that GPT-4o-as-a-Judge is more consistent across all datasets. For the $\\textit{response generation}$ module, we use the identified LLM evaluator configuration and compare different configurations of the LLM Feedback Loop. We use the win rate to determine the best multi-model configuration for generation. Experimenting with various configurations, we find that the LLM Feedback Loop, with Llama as the generator and Gemma as the reviewer, achieves a notable 71.8% and 73.8% win rate over single-model Llama and Gemma, respectively. After identifying the best configurations for both modules, we generate our PO datasets using the above pipeline.",
    "authors": [
      "Samee Arif",
      "Sualeha Farid",
      "Abdul Hameed Azeemi",
      "Awais Athar",
      "Agha Ali Raza"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-16T12:01:55Z",
    "pdf_url": "https://arxiv.org/pdf/2408.08688v7"
  },
  {
    "arxiv_id": "2408.08632v2",
    "entry_id": "http://arxiv.org/abs/2408.08632v2",
    "title": "A Survey on Benchmarks of Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) are gaining increasing popularity in both academia and industry due to their remarkable performance in various applications such as visual question answering, visual perception, understanding, and reasoning. Over the past few years, significant efforts have been made to examine MLLMs from multiple perspectives. This paper presents a comprehensive review of 200 benchmarks and evaluations for MLLMs, focusing on (1)perception and understanding, (2)cognition and reasoning, (3)specific domains, (4)key capabilities, and (5)other modalities. Finally, we discuss the limitations of the current evaluation methods for MLLMs and explore promising future directions. Our key argument is that evaluation should be regarded as a crucial discipline to support the development of MLLMs better. For more details, please visit our GitHub repository: https://github.com/swordlidev/Evaluation-Multimodal-LLMs-Survey.",
    "authors": [
      "Jian Li",
      "Weiheng Lu",
      "Hao Fei",
      "Meng Luo",
      "Ming Dai",
      "Min Xia",
      "Yizhang Jin",
      "Zhenye Gan",
      "Ding Qi",
      "Chaoyou Fu",
      "Ying Tai",
      "Wankou Yang",
      "Yabiao Wang",
      "Chengjie Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-08-16T09:52:02Z",
    "pdf_url": "https://arxiv.org/pdf/2408.08632v2"
  },
  {
    "arxiv_id": "2408.08628v1",
    "entry_id": "http://arxiv.org/abs/2408.08628v1",
    "title": "A survey on secure decentralized optimization and learning",
    "summary": "Decentralized optimization has become a standard paradigm for solving large-scale decision-making problems and training large machine learning models without centralizing data. However, this paradigm introduces new privacy and security risks, with malicious agents potentially able to infer private data or impair the model accuracy. Over the past decade, significant advancements have been made in developing secure decentralized optimization and learning frameworks and algorithms. This survey provides a comprehensive tutorial on these advancements. We begin with the fundamentals of decentralized optimization and learning, highlighting centralized aggregation and distributed consensus as key modules exposed to security risks in federated and distributed optimization, respectively. Next, we focus on privacy-preserving algorithms, detailing three cryptographic tools and their integration into decentralized optimization and learning systems. Additionally, we examine resilient algorithms, exploring the design and analysis of resilient aggregation and consensus protocols that support these systems. We conclude the survey by discussing current trends and potential future directions.",
    "authors": [
      "Changxin Liu",
      "Nicola Bastianello",
      "Wei Huo",
      "Yang Shi",
      "Karl H. Johansson"
    ],
    "categories": [
      "cs.LG",
      "math.OC"
    ],
    "published": "2024-08-16T09:42:19Z",
    "pdf_url": "https://arxiv.org/pdf/2408.08628v1"
  },
  {
    "arxiv_id": "2408.16961v2",
    "entry_id": "http://arxiv.org/abs/2408.16961v2",
    "title": "The Future of Open Human Feedback",
    "summary": "Human feedback on conversations with language language models (LLMs) is central to how these systems learn about the world, improve their capabilities, and are steered toward desirable and safe behaviors. However, this feedback is mostly collected by frontier AI labs and kept behind closed doors. In this work, we bring together interdisciplinary experts to assess the opportunities and challenges to realizing an open ecosystem of human feedback for AI. We first look for successful practices in peer production, open source, and citizen science communities. We then characterize the main challenges for open human feedback. For each, we survey current approaches and offer recommendations. We end by envisioning the components needed to underpin a sustainable and open human feedback ecosystem. In the center of this ecosystem are mutually beneficial feedback loops, between users and specialized models, incentivizing a diverse stakeholders community of model trainers and feedback providers to support a general open feedback pool.",
    "authors": [
      "Shachar Don-Yehiya",
      "Ben Burtenshaw",
      "Ramon Fernandez Astudillo",
      "Cailean Osborne",
      "Mimansa Jaiswal",
      "Tzu-Sheng Kuo",
      "Wenting Zhao",
      "Idan Shenfeld",
      "Andi Peng",
      "Mikhail Yurochkin",
      "Atoosa Kasirzadeh",
      "Yangsibo Huang",
      "Tatsunori Hashimoto",
      "Yacine Jernite",
      "Daniel Vila-Suero",
      "Omri Abend",
      "Jennifer Ding",
      "Sara Hooker",
      "Hannah Rose Kirk",
      "Leshem Choshen"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-08-15T17:59:14Z",
    "pdf_url": "https://arxiv.org/pdf/2408.16961v2"
  },
  {
    "arxiv_id": "2408.08921v2",
    "entry_id": "http://arxiv.org/abs/2408.08921v2",
    "title": "Graph Retrieval-Augmented Generation: A Survey",
    "summary": "Recently, Retrieval-Augmented Generation (RAG) has achieved remarkable success in addressing the challenges of Large Language Models (LLMs) without necessitating retraining. By referencing an external knowledge base, RAG refines LLM outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for RAG systems. In response, GraphRAG leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of GraphRAG, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of GraphRAG methodologies. We formalize the GraphRAG workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of GraphRAG. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at \\url{https://github.com/pengboci/GraphRAG-Survey}.",
    "authors": [
      "Boci Peng",
      "Yun Zhu",
      "Yongchao Liu",
      "Xiaohe Bo",
      "Haizhou Shi",
      "Chuntao Hong",
      "Yan Zhang",
      "Siliang Tang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2024-08-15T12:20:24Z",
    "pdf_url": "https://arxiv.org/pdf/2408.08921v2"
  },
  {
    "arxiv_id": "2408.07666v4",
    "entry_id": "http://arxiv.org/abs/2408.07666v4",
    "title": "Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities",
    "summary": "Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and 10+ machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at \\url{https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications}.",
    "authors": [
      "Enneng Yang",
      "Li Shen",
      "Guibing Guo",
      "Xingwei Wang",
      "Xiaochun Cao",
      "Jie Zhang",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-08-14T16:58:48Z",
    "pdf_url": "https://arxiv.org/pdf/2408.07666v4"
  },
  {
    "arxiv_id": "2408.07583v2",
    "entry_id": "http://arxiv.org/abs/2408.07583v2",
    "title": "Transformers and Large Language Models for Efficient Intrusion Detection Systems: A Comprehensive Survey",
    "summary": "With significant advancements in Transformers LLMs, NLP has extended its reach into many research fields due to its enhanced capabilities in text generation and user interaction. One field benefiting greatly from these advancements is cybersecurity. In cybersecurity, many parameters that need to be protected and exchanged between senders and receivers are in the form of text and tabular data, making NLP a valuable tool in enhancing the security measures of communication protocols. This survey paper provides a comprehensive analysis of the utilization of Transformers and LLMs in cyber-threat detection systems. The methodology of paper selection and bibliometric analysis is outlined to establish a rigorous framework for evaluating existing research. The fundamentals of Transformers are discussed, including background information on various cyber-attacks and datasets commonly used in this field. The survey explores the application of Transformers in IDSs, focusing on different architectures such as Attention-based models, LLMs like BERT and GPT, CNN/LSTM-Transformer hybrids, emerging approaches like ViTs, among others. Furthermore, it explores the diverse environments and applications where Transformers and LLMs-based IDS have been implemented, including computer networks, IoT devices, critical infrastructure protection, cloud computing, SDN, as well as in autonomous vehicles. The paper also addresses research challenges and future directions in this area, identifying key issues such as interpretability, scalability, and adaptability to evolving threats, and more. Finally, the conclusion summarizes the findings and highlights the significance of Transformers and LLMs in enhancing cyber-threat detection capabilities, while also outlining potential avenues for further research and development.",
    "authors": [
      "Hamza Kheddar"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "eess.AS"
    ],
    "published": "2024-08-14T14:28:11Z",
    "pdf_url": "https://arxiv.org/pdf/2408.07583v2"
  },
  {
    "arxiv_id": "2408.06292v3",
    "entry_id": "http://arxiv.org/abs/2408.06292v3",
    "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
    "summary": "One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist",
    "authors": [
      "Chris Lu",
      "Cong Lu",
      "Robert Tjarko Lange",
      "Jakob Foerster",
      "Jeff Clune",
      "David Ha"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-08-12T16:58:11Z",
    "pdf_url": "https://arxiv.org/pdf/2408.06292v3"
  },
  {
    "arxiv_id": "2408.05715v1",
    "entry_id": "http://arxiv.org/abs/2408.05715v1",
    "title": "Top Pass: Improve Code Generation by Pass@k-Maximized Code Ranking",
    "summary": "Code generation has been greatly enhanced by the profound advancements in Large Language Models (LLMs) recently. Nevertheless, such LLM-based code generation approaches still struggle to generate error-free code in a few tries when faced with complex problems. To address this, the prevailing strategy is to sample a huge number of candidate programs, with the hope of any one in them could work. However, users of code generation systems usually expect to find a correct program by reviewing or testing only a small number of code candidates. Otherwise, the system would be unhelpful. In this paper, we propose Top Pass, a code ranking approach that identifies potential correct solutions from a large number of candidates. Top Pass directly optimizes the pass@k loss function, enhancing the quality at the top of the candidate list. This enables the user to find the correct solution within as few tries as possible. Experimental results on four benchmarks indicate that our Top Pass method enhances the usability of code generation models by producing better ranking results, particularly achieving a 32.9\\% relative improvement in pass@1 on CodeContests when compared to the state-of-the-art ranking method.",
    "authors": [
      "Zhi-Cun Lyu",
      "Xin-Ye Li",
      "Zheng Xie",
      "Ming Li"
    ],
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-08-11T07:53:51Z",
    "pdf_url": "https://arxiv.org/pdf/2408.05715v1"
  },
  {
    "arxiv_id": "2408.05212v2",
    "entry_id": "http://arxiv.org/abs/2408.05212v2",
    "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions",
    "summary": "Large Language Models (LLMs) represent a significant advancement in artificial intelligence, finding applications across various domains. However, their reliance on massive internet-sourced datasets for training brings notable privacy issues, which are exacerbated in critical domains (e.g., healthcare). Moreover, certain application-specific scenarios may require fine-tuning these models on private data. This survey critically examines the privacy threats associated with LLMs, emphasizing the potential for these models to memorize and inadvertently reveal sensitive information. We explore current threats by reviewing privacy attacks on LLMs and propose comprehensive solutions for integrating privacy mechanisms throughout the entire learning pipeline. These solutions range from anonymizing training datasets to implementing differential privacy during training or inference and machine unlearning after training. Our comprehensive review of existing literature highlights ongoing challenges, available tools, and future directions for preserving privacy in LLMs. This work aims to guide the development of more secure and trustworthy AI systems by providing a thorough understanding of privacy preservation methods and their effectiveness in mitigating risks.",
    "authors": [
      "Michele Miranda",
      "Elena Sofia Ruzzetti",
      "Andrea Santilli",
      "Fabio Massimo Zanzotto",
      "Sébastien Bratières",
      "Emanuele Rodolà"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-08-10T05:41:19Z",
    "pdf_url": "https://arxiv.org/pdf/2408.05212v2"
  },
  {
    "arxiv_id": "2408.05109v5",
    "entry_id": "http://arxiv.org/abs/2408.05109v5",
    "title": "A Survey of Text-to-SQL in the Era of LLMs: Where are we, and where are we going?",
    "summary": "Translating users' natural language queries (NL) into SQL queries (i.e., Text-to-SQL, a.k.a. NL2SQL) can significantly reduce barriers to accessing relational databases and support various commercial applications. The performance of Text-to-SQL has been greatly enhanced with the emergence of Large Language Models (LLMs). In this survey, we provide a comprehensive review of Text-to-SQL techniques powered by LLMs, covering its entire lifecycle from the following four aspects: (1) Model: Text-to-SQL translation techniques that tackle not only NL ambiguity and under-specification, but also properly map NL with database schema and instances; (2) Data: From the collection of training data, data synthesis due to training data scarcity, to Text-to-SQL benchmarks; (3) Evaluation: Evaluating Text-to-SQL methods from multiple angles using different metrics and granularities; and (4) Error Analysis: analyzing Text-to-SQL errors to find the root cause and guiding Text-to-SQL models to evolve. Moreover, we offer a rule of thumb for developing Text-to-SQL solutions. Finally, we discuss the research challenges and open problems of Text-to-SQL in the LLMs era.",
    "authors": [
      "Xinyu Liu",
      "Shuyu Shen",
      "Boyan Li",
      "Peixian Ma",
      "Runzhi Jiang",
      "Yuxin Zhang",
      "Ju Fan",
      "Guoliang Li",
      "Nan Tang",
      "Yuyu Luo"
    ],
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "published": "2024-08-09T14:59:36Z",
    "pdf_url": "https://arxiv.org/pdf/2408.05109v5"
  },
  {
    "arxiv_id": "2408.05025v2",
    "entry_id": "http://arxiv.org/abs/2408.05025v2",
    "title": "Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks",
    "summary": "Retrieval Augmented Generation (RAG) is a technique commonly used to equip models with out of distribution knowledge. This process involves collecting, indexing, retrieving, and providing information to an LLM for generating responses. Despite its growing popularity due to its flexibility and low cost, the security implications of RAG have not been extensively studied. The data for such systems are often collected from public sources, providing an attacker a gateway for indirect prompt injections to manipulate the responses of the model. In this paper, we investigate the security of RAG systems against end-to-end indirect prompt manipulations. First, we review existing RAG framework pipelines, deriving a prototypical architecture and identifying critical parameters. We then examine prior works searching for techniques that attackers can use to perform indirect prompt manipulations. Finally, we implemented Rag 'n Roll, a framework to determine the effectiveness of attacks against end-to-end RAG applications. Our results show that existing attacks are mostly optimized to boost the ranking of malicious documents during the retrieval phase. However, a higher rank does not immediately translate into a reliable attack. Most attacks, against various configurations, settle around a 40% success rate, which could rise to 60% when considering ambiguous answers as successful attacks (those that include the expected benign one as well). Additionally, when using unoptimized documents, attackers deploying two of them (or more) for a target query can achieve similar results as those using optimized ones. Finally, exploration of the configuration space of a RAG showed limited impact in thwarting the attacks, where the most successful combination severely undermines functionality.",
    "authors": [
      "Gianluca De Stefano",
      "Lea Schönherr",
      "Giancarlo Pellegrino"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-08-09T12:26:05Z",
    "pdf_url": "https://arxiv.org/pdf/2408.05025v2"
  },
  {
    "arxiv_id": "2408.04820v4",
    "entry_id": "http://arxiv.org/abs/2408.04820v4",
    "title": "Natural Language Outlines for Code: Literate Programming in the LLM Era",
    "summary": "We propose using natural language outlines as a novel modality and interaction surface for providing AI assistance to developers throughout the software development process. An NL outline for a code function comprises multiple statements written in concise prose, which partition the code and summarize its main ideas in the style of literate programming. Crucially, we find that modern LLMs can generate accurate and high-quality NL outlines in practice. Moreover, NL outlines enable a bidirectional sync between code and NL, where a developer can change either code or NL and have the LLM automatically update the other. We discuss many use cases for NL outlines: they can accelerate understanding and navigation of code and diffs, simplify code maintenance, augment code search, steer code generation, and more. We then propose and compare multiple LLM prompting techniques for generating outlines and ask professional developers to judge outline quality. Finally, we present two case studies applying NL outlines toward code review and malware detection.",
    "authors": [
      "Kensen Shi",
      "Deniz Altınbüken",
      "Saswat Anand",
      "Mihai Christodorescu",
      "Katja Grünwedel",
      "Alexa Koenings",
      "Sai Naidu",
      "Anurag Pathak",
      "Marc Rasi",
      "Fredde Ribeiro",
      "Brandon Ruffin",
      "Siddhant Sanyam",
      "Maxim Tabachnyk",
      "Sara Toth",
      "Roy Tu",
      "Tobias Welp",
      "Pengcheng Yin",
      "Manzil Zaheer",
      "Satish Chandra",
      "Charles Sutton"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-08-09T02:22:51Z",
    "pdf_url": "https://arxiv.org/pdf/2408.04820v4"
  },
  {
    "arxiv_id": "2408.04304v1",
    "entry_id": "http://arxiv.org/abs/2408.04304v1",
    "title": "Learning with Digital Agents: An Analysis based on the Activity Theory",
    "summary": "Digital agents are considered a general-purpose technology. They spread quickly in private and organizational contexts, including education. Yet, research lacks a conceptual framing to describe interaction with such agents in a holistic manner. While focusing on the interaction with a pedagogical agent, i.e., a digital agent capable of natural-language interaction with a learner, we propose a model of learning activity based on activity theory. We use this model and a review of prior research on digital agents in education to analyze how various characteristics of the activity, including features of a pedagogical agent or learner, influence learning outcomes. The analysis leads to identification of IS research directions and guidance for developers of pedagogical agents and digital agents in general. We conclude by extending the activity theory-based model beyond the context of education and show how it helps designers and researchers ask the right questions when creating a digital agent.",
    "authors": [
      "Mateusz Dolata",
      "Dzmitry Katsiuba",
      "Natalie Wellnhammer",
      "Gerhard Schwabe"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-08-08T08:38:02Z",
    "pdf_url": "https://arxiv.org/pdf/2408.04304v1"
  },
  {
    "arxiv_id": "2408.04681v1",
    "entry_id": "http://arxiv.org/abs/2408.04681v1",
    "title": "Conversational AI Powered by Large Language Models Amplifies False Memories in Witness Interviews",
    "summary": "This study examines the impact of AI on human false memories -- recollections of events that did not occur or deviate from actual occurrences. It explores false memory induction through suggestive questioning in Human-AI interactions, simulating crime witness interviews. Four conditions were tested: control, survey-based, pre-scripted chatbot, and generative chatbot using a large language model (LLM). Participants (N=200) watched a crime video, then interacted with their assigned AI interviewer or survey, answering questions including five misleading ones. False memories were assessed immediately and after one week. Results show the generative chatbot condition significantly increased false memory formation, inducing over 3 times more immediate false memories than the control and 1.7 times more than the survey method. 36.4% of users' responses to the generative chatbot were misled through the interaction. After one week, the number of false memories induced by generative chatbots remained constant. However, confidence in these false memories remained higher than the control after one week. Moderating factors were explored: users who were less familiar with chatbots but more familiar with AI technology, and more interested in crime investigations, were more susceptible to false memories. These findings highlight the potential risks of using advanced AI in sensitive contexts, like police interviews, emphasizing the need for ethical considerations.",
    "authors": [
      "Samantha Chan",
      "Pat Pataranutaporn",
      "Aditya Suri",
      "Wazeer Zulfikar",
      "Pattie Maes",
      "Elizabeth F. Loftus"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2024-08-08T04:55:03Z",
    "pdf_url": "https://arxiv.org/pdf/2408.04681v1"
  },
  {
    "arxiv_id": "2408.03827v1",
    "entry_id": "http://arxiv.org/abs/2408.03827v1",
    "title": "Automated Code Fix Suggestions for Accessibility Issues in Mobile Apps",
    "summary": "Accessibility is crucial for inclusive app usability, yet developers often struggle to identify and fix app accessibility issues due to a lack of awareness, expertise, and inadequate tools. Current accessibility testing tools can identify accessibility issues but may not always provide guidance on how to address them. We introduce FixAlly, an automated tool designed to suggest source code fixes for accessibility issues detected by automated accessibility scanners. FixAlly employs a multi-agent LLM architecture to generate fix strategies, localize issues within the source code, and propose code modification suggestions to fix the accessibility issue. Our empirical study demonstrates FixAlly's capability in suggesting fixes that resolve issues found by accessibility scanners -- with an effectiveness of 77% in generating plausible fix suggestions -- and our survey of 12 iOS developers finds they would be willing to accept 69.4% of evaluated fix suggestions.",
    "authors": [
      "Forough Mehralian",
      "Titus Barik",
      "Jeff Nichols",
      "Amanda Swearngin"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-08-07T15:06:07Z",
    "pdf_url": "https://arxiv.org/pdf/2408.03827v1"
  },
  {
    "arxiv_id": "2408.03359v1",
    "entry_id": "http://arxiv.org/abs/2408.03359v1",
    "title": "LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal Classification",
    "summary": "We introduce LAMPO, a novel paradigm that leverages Large Language Models (LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike conventional methods, which concatenate all demonstration examples with the test instance and prompt LLMs to produce the pointwise prediction, our framework uses the LLM as a preference machine that makes a relative comparative decision between the test instance and each demonstration. A self-supervised method is then introduced to aggregate these binary comparisons into the final ordinal decision. LAMPO addresses several limitations inherent in previous methods, including context length constraints, ordering biases, and challenges associated with absolute point-wise estimation. Extensive experiments on seven public datasets demonstrate LAMPO's remarkably competitive performance across a diverse spectrum of applications (e.g., movie review analysis and hate speech detection). Notably, in certain applications, the improvement can be substantial, exceeding 20% in an absolute term. Moreover, we believe LAMPO represents an interesting addition to the non-parametric application layered on top of LLMs, as it supports black-box LLMs without necessitating the outputting of LLM's internal states (e.g., embeddings), as seen in previous approaches.",
    "authors": [
      "Zhen Qin",
      "Junru Wu",
      "Jiaming Shen",
      "Tianqi Liu",
      "Xuanhui Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-08-06T15:55:05Z",
    "pdf_url": "https://arxiv.org/pdf/2408.03359v1"
  },
  {
    "arxiv_id": "2408.03354v3",
    "entry_id": "http://arxiv.org/abs/2408.03354v3",
    "title": "The Use of Large Language Models (LLM) for Cyber Threat Intelligence (CTI) in Cybercrime Forums",
    "summary": "Large language models (LLMs) can be used to analyze cyber threat intelligence (CTI) data from cybercrime forums, which contain extensive information and key discussions about emerging cyber threats. However, to date, the level of accuracy and efficiency of LLMs for such critical tasks has yet to be thoroughly evaluated. Hence, this study assesses the performance of an LLM system built on the OpenAI GPT-3.5-turbo model [8] to extract CTI information. To do so, a random sample of more than 700 daily conversations from three cybercrime forums - XSS, Exploit_in, and RAMP - was extracted, and the LLM system was instructed to summarize the conversations and predict 10 key CTI variables, such as whether a large organization and/or a critical infrastructure is being targeted, with only simple human-language instructions. Then, two coders reviewed each conversation and evaluated whether the information extracted by the LLM was accurate. The LLM system performed well, with an average accuracy score of 96.23%, an average precision of 90% and an average recall of 88.2%. Various ways to enhance the model were uncovered, such as the need to help the LLM distinguish between stories and past events, as well as being careful with verb tenses in prompts. Nevertheless, the results of this study highlight the relevance of using LLMs for cyber threat intelligence.",
    "authors": [
      "Vanessa Clairoux-Trepanier",
      "Isa-May Beauchamp",
      "Estelle Ruellan",
      "Masarah Paquet-Clouston",
      "Serge-Olivier Paquette",
      "Eric Clay"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-08-06T09:15:25Z",
    "pdf_url": "https://arxiv.org/pdf/2408.03354v3"
  },
  {
    "arxiv_id": "2408.02479v2",
    "entry_id": "http://arxiv.org/abs/2408.02479v2",
    "title": "From LLMs to LLM-based Agents for Software Engineering: A Survey of Current, Challenges and Future",
    "summary": "With the rise of large language models (LLMs), researchers are increasingly exploring their applications in var ious vertical domains, such as software engineering. LLMs have achieved remarkable success in areas including code generation and vulnerability detection. However, they also exhibit numerous limitations and shortcomings. LLM-based agents, a novel tech nology with the potential for Artificial General Intelligence (AGI), combine LLMs as the core for decision-making and action-taking, addressing some of the inherent limitations of LLMs such as lack of autonomy and self-improvement. Despite numerous studies and surveys exploring the possibility of using LLMs in software engineering, it lacks a clear distinction between LLMs and LLM based agents. It is still in its early stage for a unified standard and benchmarking to qualify an LLM solution as an LLM-based agent in its domain. In this survey, we broadly investigate the current practice and solutions for LLMs and LLM-based agents for software engineering. In particular we summarise six key topics: requirement engineering, code generation, autonomous decision-making, software design, test generation, and software maintenance. We review and differentiate the work of LLMs and LLM-based agents from these six topics, examining their differences and similarities in tasks, benchmarks, and evaluation metrics. Finally, we discuss the models and benchmarks used, providing a comprehensive analysis of their applications and effectiveness in software engineering. We anticipate this work will shed some lights on pushing the boundaries of LLM-based agents in software engineering for future research.",
    "authors": [
      "Haolin Jin",
      "Linghan Huang",
      "Haipeng Cai",
      "Jun Yan",
      "Bo Li",
      "Huaming Chen"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-08-05T14:01:15Z",
    "pdf_url": "https://arxiv.org/pdf/2408.02479v2"
  },
  {
    "arxiv_id": "2408.02248v2",
    "entry_id": "http://arxiv.org/abs/2408.02248v2",
    "title": "ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems",
    "summary": "Recently, there has been increasing interest in using Large Language Models (LLMs) to construct complex multi-agent systems to perform tasks such as compiling literature reviews, drafting consumer reports, and planning vacations. Many tools and libraries exist for helping create such systems, however none support recursive multi-agent systems -- where the models themselves flexibly decide when to delegate tasks and how to organize their delegation structure. In this work, we introduce ReDel: a toolkit for recursive multi-agent systems that supports custom tool-use, delegation schemes, event-based logging, and interactive replay in an easy-to-use web interface. We show that, using ReDel, we are able to easily identify potential areas of improvements through the visualization and debugging tools. Our code, documentation, and PyPI package are open-source and free to use under the MIT license at https://github.com/zhudotexe/redel.",
    "authors": [
      "Andrew Zhu",
      "Liam Dugan",
      "Chris Callison-Burch"
    ],
    "categories": [
      "cs.CL",
      "cs.MA",
      "cs.SE"
    ],
    "published": "2024-08-05T05:43:23Z",
    "pdf_url": "https://arxiv.org/pdf/2408.02248v2"
  },
  {
    "arxiv_id": "2408.02232v4",
    "entry_id": "http://arxiv.org/abs/2408.02232v4",
    "title": "SpecRover: Code Intent Extraction via LLMs",
    "summary": "Autonomous program improvement typically involves automatically producing bug fixes and feature additions. Such program improvement can be accomplished by a combination of large language model (LLM) and program analysis capabilities, in the form of an LLM agent. Since program repair or program improvement typically requires a specification of intended behavior - specification inference can be useful for producing high quality program patches. In this work, we examine efficient and low-cost workflows for iterative specification inference within an LLM agent. Given a GitHub issue to be resolved in a software project, our goal is to conduct iterative code search accompanied by specification inference - thereby inferring intent from both the project structure and behavior. The intent thus captured is examined by a reviewer agent with the goal of vetting the patches as well as providing a measure of confidence in the vetted patches. Our approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent AutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub issues, it shows more than 50% improvement in efficacy over AutoCodeRover. Compared to the open-source agents available, our work shows modest cost ($0.65 per issue) in resolving an average GitHub issue in SWE-Bench lite. The production of explanation by SpecRover allows for a better \"signal\" to be given to the developer, on when the suggested patches can be accepted with confidence. SpecRover also seeks to demonstrate the continued importance of specification inference in automated program repair, even as program repair technologies enter the LLM era.",
    "authors": [
      "Haifeng Ruan",
      "Yuntong Zhang",
      "Abhik Roychoudhury"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-08-05T04:53:01Z",
    "pdf_url": "https://arxiv.org/pdf/2408.02232v4"
  },
  {
    "arxiv_id": "2408.02205v4",
    "entry_id": "http://arxiv.org/abs/2408.02205v4",
    "title": "Swiss Cheese Model for AI Safety: A Taxonomy and Reference Architecture for Multi-Layered Guardrails of Foundation Model Based Agents",
    "summary": "Foundation Model (FM)-based agents are revolutionizing application development across various domains. However, their rapidly growing capabilities and autonomy have raised significant concerns about AI safety. Researchers are exploring better ways to design guardrails to ensure that the runtime behavior of FM-based agents remains within specific boundaries. Nevertheless, designing effective runtime guardrails is challenging due to the agents' autonomous and non-deterministic behavior. The involvement of multiple pipeline stages and agent artifacts, such as goals, plans, tools, at runtime further complicates these issues. Addressing these challenges at runtime requires multi-layered guardrails that operate effectively at various levels of the agent architecture. Therefore, in this paper, based on the results of a systematic literature review, we present a comprehensive taxonomy of runtime guardrails for FM-based agents to identify the key quality attributes for guardrails and design dimensions. Inspired by the Swiss Cheese Model, we also propose a reference architecture for designing multi-layered runtime guardrails for FM-based agents, which includes three dimensions: quality attributes, pipelines, and artifacts. The proposed taxonomy and reference architecture provide concrete and robust guidance for researchers and practitioners to build AI-safety-by-design from a software architecture perspective.",
    "authors": [
      "Md Shamsujjoha",
      "Qinghua Lu",
      "Dehai Zhao",
      "Liming Zhu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-08-05T03:08:51Z",
    "pdf_url": "https://arxiv.org/pdf/2408.02205v4"
  },
  {
    "arxiv_id": "2408.02143v1",
    "entry_id": "http://arxiv.org/abs/2408.02143v1",
    "title": "Analyzing Cultural Representations of Emotions in LLMs through Mixed Emotion Survey",
    "summary": "Large Language Models (LLMs) have gained widespread global adoption, showcasing advanced linguistic capabilities across multiple of languages. There is a growing interest in academia to use these models to simulate and study human behaviors. However, it is crucial to acknowledge that an LLM's proficiency in a specific language might not fully encapsulate the norms and values associated with its culture. Concerns have emerged regarding potential biases towards Anglo-centric cultures and values due to the predominance of Western and US-based training data. This study focuses on analyzing the cultural representations of emotions in LLMs, in the specific case of mixed-emotion situations. Our methodology is based on the studies of Miyamoto et al. (2010), which identified distinctive emotional indicators in Japanese and American human responses. We first administer their mixed emotion survey to five different LLMs and analyze their outputs. Second, we experiment with contextual variables to explore variations in responses considering both language and speaker origin. Thirdly, we expand our investigation to encompass additional East Asian and Western European origin languages to gauge their alignment with their respective cultures, anticipating a closer fit. We find that (1) models have limited alignment with the evidence in the literature; (2) written language has greater effect on LLMs' response than information on participants origin; and (3) LLMs responses were found more similar for East Asian languages than Western European languages.",
    "authors": [
      "Shiran Dudy",
      "Ibrahim Said Ahmad",
      "Ryoko Kitajima",
      "Agata Lapedriza"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-08-04T20:56:05Z",
    "pdf_url": "https://arxiv.org/pdf/2408.02143v1"
  },
  {
    "arxiv_id": "2408.02085v5",
    "entry_id": "http://arxiv.org/abs/2408.02085v5",
    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
    "summary": "Instruction tuning plays a critical role in aligning large language models (LLMs) with human preference. Despite the vast amount of open instruction datasets, naively training a LLM on all existing instructions may not be optimal and practical. To pinpoint the most beneficial datapoints, data assessment and selection methods have been proposed in the fields of natural language processing (NLP) and deep learning. However, under the context of instruction tuning, there still exists a gap in knowledge on what kind of data evaluation metrics can be employed and how they can be integrated into the selection mechanism. To bridge this gap, we present a comprehensive review on existing literature of data assessment and selection especially for instruction tuning of LLMs. We systematically categorize all applicable methods into quality-based, diversity-based, and importance-based ones where a unified, fine-grained taxonomy is structured. For each category, representative methods are elaborated to describe the landscape of relevant research. In addition, comparison between the latest methods is conducted on their officially reported results to provide in-depth discussions on their limitations. Finally, we summarize the open challenges and propose the promosing avenues for future studies. All related contents are available at https://github.com/yuleiqin/fantastic-data-engineering.",
    "authors": [
      "Yulei Qin",
      "Yuncheng Yang",
      "Pengcheng Guo",
      "Gang Li",
      "Hang Shao",
      "Yuchen Shi",
      "Zihan Xu",
      "Yun Gu",
      "Ke Li",
      "Xing Sun"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "eess.SP"
    ],
    "published": "2024-08-04T16:50:07Z",
    "pdf_url": "https://arxiv.org/pdf/2408.02085v5"
  },
  {
    "arxiv_id": "2408.01916v2",
    "entry_id": "http://arxiv.org/abs/2408.01916v2",
    "title": "MAO: A Framework for Process Model Generation with Multi-Agent Orchestration",
    "summary": "Process models are frequently used in software engineering to describe business requirements, guide software testing and control system improvement. However, traditional process modeling methods often require the participation of numerous experts, which is expensive and time-consuming. Therefore, the exploration of a more efficient and cost-effective automated modeling method has emerged as a focal point in current research. This article explores a framework for automatically generating process models with multi-agent orchestration (MAO), aiming to enhance the efficiency of process modeling and offer valuable insights for domain experts. Our framework MAO leverages large language models as the cornerstone for multi-agent, employing an innovative prompt strategy to ensure efficient collaboration among multi-agent. Specifically, 1) generation. The first phase of MAO is to generate a slightly rough process model from the text description; 2) refinement. The agents would continuously refine the initial process model through multiple rounds of dialogue; 3) reviewing. Large language models are prone to hallucination phenomena among multi-turn dialogues, so the agents need to review and repair semantic hallucinations in process models; 4) testing. The representation of process models is diverse. Consequently, the agents utilize external tools to test whether the generated process model contains format errors, namely format hallucinations, and then adjust the process model to conform to the output paradigm. The experiments demonstrate that the process models generated by our framework outperform existing methods and surpass manual modeling by 89%, 61%, 52%, and 75% on four different datasets, respectively.",
    "authors": [
      "Leilei Lin",
      "Yumeng Jin",
      "Yingming Zhou",
      "Wenlong Chen",
      "Chen Qian"
    ],
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-08-04T03:32:17Z",
    "pdf_url": "https://arxiv.org/pdf/2408.01916v2"
  },
  {
    "arxiv_id": "2408.04646v2",
    "entry_id": "http://arxiv.org/abs/2408.04646v2",
    "title": "Efficacy of Large Language Models in Systematic Reviews",
    "summary": "This study investigates the effectiveness of Large Language Models (LLMs) in interpreting existing literature through a systematic review of the relationship between Environmental, Social, and Governance (ESG) factors and financial performance. The primary objective is to assess how LLMs can replicate a systematic review on a corpus of ESG-focused papers. We compiled and hand-coded a database of 88 relevant papers published from March 2020 to May 2024. Additionally, we used a set of 238 papers from a previous systematic review of ESG literature from January 2015 to February 2020. We evaluated two current state-of-the-art LLMs, Meta AI's Llama 3 8B and OpenAI's GPT-4o, on the accuracy of their interpretations relative to human-made classifications on both sets of papers. We then compared these results to a \"Custom GPT\" and a fine-tuned GPT-4o Mini model using the corpus of 238 papers as training data. The fine-tuned GPT-4o Mini model outperformed the base LLMs by 28.3% on average in overall accuracy on prompt 1. At the same time, the \"Custom GPT\" showed a 3.0% and 15.7% improvement on average in overall accuracy on prompts 2 and 3, respectively. Our findings reveal promising results for investors and agencies to leverage LLMs to summarize complex evidence related to ESG investing, thereby enabling quicker decision-making and a more efficient market.",
    "authors": [
      "Aaditya Shah",
      "Shridhar Mehendale",
      "Siddha Kanthi"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-08-03T00:01:13Z",
    "pdf_url": "https://arxiv.org/pdf/2408.04646v2"
  },
  {
    "arxiv_id": "2408.01527v2",
    "entry_id": "http://arxiv.org/abs/2408.01527v2",
    "title": "Using LLMs to Establish Implicit User Sentiment of Software Desirability",
    "summary": "This study explores the use of LLMs for providing quantitative zero-shot sentiment analysis of implicit software desirability, addressing a critical challenge in product evaluation where traditional review scores, though convenient, fail to capture the richness of qualitative user feedback. Innovations include establishing a method that 1) works with qualitative user experience data without the need for explicit review scores, 2) focuses on implicit user satisfaction, and 3) provides scaled numerical sentiment analysis, offering a more nuanced understanding of user sentiment, instead of simply classifying sentiment as positive, neutral, or negative.\n  Data is collected using the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user experience analysis tool. For initial exploration, the PDT metric was given to users of two software systems. PDT data was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment, and Vader, a leading sentiment analysis tool. Each system was asked to evaluate the data in two ways, by looking at the sentiment expressed in the PDT word/explanation pairs; and by looking at the sentiment expressed by the users in their grouped selection of five words and explanations, as a whole. Each LLM provided a sentiment score, its confidence (low, medium, high) in the score, and an explanation of the score.\n  All LLMs tested were able to statistically detect user sentiment from the users' grouped data, whereas TRBS and Vader were not. The confidence and explanation of confidence provided by the LLMs assisted in understanding user sentiment. This study adds deeper understanding of evaluating user experiences, toward the goal of creating a universal tool that quantifies implicit sentiment.",
    "authors": [
      "Sherri Weitl-Harms",
      "John D. Hastings",
      "Jonah Lum"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.SE"
    ],
    "published": "2024-08-02T18:40:10Z",
    "pdf_url": "https://arxiv.org/pdf/2408.01527v2"
  },
  {
    "arxiv_id": "2408.01319v1",
    "entry_id": "http://arxiv.org/abs/2408.01319v1",
    "title": "A Comprehensive Review of Multimodal Large Language Models: Performance and Challenges Across Different Tasks",
    "summary": "In an era defined by the explosive growth of data and rapid technological advancements, Multimodal Large Language Models (MLLMs) stand at the forefront of artificial intelligence (AI) systems. Designed to seamlessly integrate diverse data types-including text, images, videos, audio, and physiological sequences-MLLMs address the complexities of real-world applications far beyond the capabilities of single-modality systems. In this paper, we systematically sort out the applications of MLLM in multimodal tasks such as natural language, vision, and audio. We also provide a comparative analysis of the focus of different MLLMs in the tasks, and provide insights into the shortcomings of current MLLMs, and suggest potential directions for future research. Through these discussions, this paper hopes to provide valuable insights for the further development and application of MLLM.",
    "authors": [
      "Jiaqi Wang",
      "Hanqi Jiang",
      "Yiheng Liu",
      "Chong Ma",
      "Xu Zhang",
      "Yi Pan",
      "Mengyuan Liu",
      "Peiran Gu",
      "Sichen Xia",
      "Wenjun Li",
      "Yutong Zhang",
      "Zihao Wu",
      "Zhengliang Liu",
      "Tianyang Zhong",
      "Bao Ge",
      "Tuo Zhang",
      "Ning Qiang",
      "Xintao Hu",
      "Xi Jiang",
      "Xin Zhang",
      "Wei Zhang",
      "Dinggang Shen",
      "Tianming Liu",
      "Shu Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-02T15:14:53Z",
    "pdf_url": "https://arxiv.org/pdf/2408.01319v1"
  },
  {
    "arxiv_id": "2408.01129v6",
    "entry_id": "http://arxiv.org/abs/2408.01129v6",
    "title": "A Survey of Mamba",
    "summary": "As one of the most representative DL techniques, Transformer architecture has empowered numerous advanced models, especially the large language models (LLMs) that comprise billions of parameters, becoming a cornerstone in deep learning. Despite the impressive achievements, Transformers still face inherent limitations, particularly the time-consuming inference resulting from the quadratic computation complexity of attention calculation. Recently, a novel architecture named Mamba, drawing inspiration from classical state space models (SSMs), has emerged as a promising alternative for building foundation models, delivering comparable modeling abilities to Transformers while preserving near-linear scalability concerning sequence length. This has sparked an increasing number of studies actively exploring Mamba's potential to achieve impressive performance across diverse domains. Given such rapid evolution, there is a critical need for a systematic review that consolidates existing Mamba-empowered models, offering a comprehensive understanding of this emerging model architecture. In this survey, we therefore conduct an in-depth investigation of recent Mamba-associated studies, covering three main aspects: the advancements of Mamba-based models, the techniques of adapting Mamba to diverse data, and the applications where Mamba can excel. Specifically, we first review the foundational knowledge of various representative deep learning models and the details of Mamba-1&2 as preliminaries. Then, to showcase the significance of Mamba for AI, we comprehensively review the related studies focusing on Mamba models' architecture design, data adaptability, and applications. Finally, we present a discussion of current limitations and explore various promising research directions to provide deeper insights for future investigations.",
    "authors": [
      "Haohao Qu",
      "Liangbo Ning",
      "Rui An",
      "Wenqi Fan",
      "Tyler Derr",
      "Hui Liu",
      "Xin Xu",
      "Qing Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-08-02T09:18:41Z",
    "pdf_url": "https://arxiv.org/pdf/2408.01129v6"
  },
  {
    "arxiv_id": "2408.01072v4",
    "entry_id": "http://arxiv.org/abs/2408.01072v4",
    "title": "A Survey on Self-play Methods in Reinforcement Learning",
    "summary": "Self-play, a learning paradigm where agents iteratively refine their policies by interacting with historical or concurrent versions of themselves or other evolving agents, has shown remarkable success in solving complex non-cooperative multi-agent tasks. Despite its growing prominence in multi-agent reinforcement learning (MARL), such as Go, poker, and video games, a comprehensive and structured understanding of self-play remains lacking. This survey fills this gap by offering a comprehensive roadmap to the diverse landscape of self-play methods. We begin by introducing the necessary preliminaries, including the MARL framework and basic game theory concepts. Then, it provides a unified framework and classifies existing self-play algorithms within this framework. Moreover, the paper bridges the gap between the algorithms and their practical implications by illustrating the role of self-play in different non-cooperative scenarios. Finally, the survey highlights open challenges and future research directions in self-play.",
    "authors": [
      "Ruize Zhang",
      "Zelai Xu",
      "Chengdong Ma",
      "Chao Yu",
      "Wei-Wei Tu",
      "Wenhao Tang",
      "Shiyu Huang",
      "Deheng Ye",
      "Wenbo Ding",
      "Yaodong Yang",
      "Yu Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-02T07:47:51Z",
    "pdf_url": "https://arxiv.org/pdf/2408.01072v4"
  },
  {
    "arxiv_id": "2408.00989v4",
    "entry_id": "http://arxiv.org/abs/2408.00989v4",
    "title": "On the Resilience of LLM-Based Multi-Agent Collaboration with Faulty Agents",
    "summary": "Large language model-based multi-agent systems have shown great abilities across various tasks due to the collaboration of expert agents, each focusing on a specific domain. However, the impact of clumsy or even malicious agents--those who frequently make errors in their tasks--on the overall performance of the system remains underexplored. This paper investigates: (1) What is the resilience of various system structures (e.g., A$\\rightarrow$B$\\rightarrow$C, A$\\leftrightarrow$B$\\leftrightarrow$C) under faulty agents, on different downstream tasks? (2) How can we increase system resilience to defend against these agents? To simulate faulty agents, we propose two approaches--AutoTransform and AutoInject--which introduce mistakes into the agents' responses. Experiments on four downstream tasks using six systems show that the \"hierarchical\" structure, i.e., A$\\rightarrow$(B$\\leftrightarrow$C), exhibits superior resilience with the lowest performance drop of 5.5%, compared to 10.5% and 23.7% of other two structures. To further improve resilience, we introduce (1) Challenger, that introduces a mechanism for each agent to challenge others' outputs, and (2) Inspector, an additional agent to review and correct messages, recovering up to 96.4% errors made by faulty agents. Our code and data are available at https://github.com/CUHK-ARISE/MAS-Resilience.",
    "authors": [
      "Jen-tse Huang",
      "Jiaxu Zhou",
      "Tailin Jin",
      "Xuhui Zhou",
      "Zixi Chen",
      "Wenxuan Wang",
      "Youliang Yuan",
      "Michael R. Lyu",
      "Maarten Sap"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-08-02T03:25:20Z",
    "pdf_url": "https://arxiv.org/pdf/2408.00989v4"
  },
  {
    "arxiv_id": "2408.00921v1",
    "entry_id": "http://arxiv.org/abs/2408.00921v1",
    "title": "Automatic Pull Request Description Generation Using LLMs: A T5 Model Approach",
    "summary": "Developers create pull request (PR) descriptions to provide an overview of their changes and explain the motivations behind them. These descriptions help reviewers and fellow developers quickly understand the updates. Despite their importance, some developers omit these descriptions. To tackle this problem, we propose an automated method for generating PR descriptions based on commit messages and source code comments. This method frames the task as a text summarization problem, for which we utilized the T5 text-to-text transfer model. We fine-tuned a pre-trained T5 model using a dataset containing 33,466 PRs. The model's effectiveness was assessed using ROUGE metrics, which are recognized for their strong alignment with human evaluations. Our findings reveal that the T5 model significantly outperforms LexRank, which served as our baseline for comparison.",
    "authors": [
      "Md Nazmus Sakib",
      "Md Athikul Islam",
      "Md Mashrur Arifin"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.SE"
    ],
    "published": "2024-08-01T21:22:16Z",
    "pdf_url": "https://arxiv.org/pdf/2408.00921v1"
  },
  {
    "arxiv_id": "2408.04643v1",
    "entry_id": "http://arxiv.org/abs/2408.04643v1",
    "title": "Risks, Causes, and Mitigations of Widespread Deployments of Large Language Models (LLMs): A Survey",
    "summary": "Recent advancements in Large Language Models (LLMs), such as ChatGPT and LLaMA, have significantly transformed Natural Language Processing (NLP) with their outstanding abilities in text generation, summarization, and classification. Nevertheless, their widespread adoption introduces numerous challenges, including issues related to academic integrity, copyright, environmental impacts, and ethical considerations such as data bias, fairness, and privacy. The rapid evolution of LLMs also raises concerns regarding the reliability and generalizability of their evaluations. This paper offers a comprehensive survey of the literature on these subjects, systematically gathered and synthesized from Google Scholar. Our study provides an in-depth analysis of the risks associated with specific LLMs, identifying sub-risks, their causes, and potential solutions. Furthermore, we explore the broader challenges related to LLMs, detailing their causes and proposing mitigation strategies. Through this literature analysis, our survey aims to deepen the understanding of the implications and complexities surrounding these powerful models.",
    "authors": [
      "Md Nazmus Sakib",
      "Md Athikul Islam",
      "Royal Pathak",
      "Md Mashrur Arifin"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-08-01T21:21:18Z",
    "pdf_url": "https://arxiv.org/pdf/2408.04643v1"
  },
  {
    "arxiv_id": "2408.00914v1",
    "entry_id": "http://arxiv.org/abs/2408.00914v1",
    "title": "Granting GPT-4 License and Opportunity: Enhancing Accuracy and Confidence Estimation for Few-Shot Event Detection",
    "summary": "Large Language Models (LLMs) such as GPT-4 have shown enough promise in the few-shot learning context to suggest use in the generation of \"silver\" data and refinement of new ontologies through iterative application and review. Such workflows become more effective with reliable confidence estimation. Unfortunately, confidence estimation is a documented weakness of models such as GPT-4, and established methods to compensate require significant additional complexity and computation. The present effort explores methods for effective confidence estimation with GPT-4 with few-shot learning for event detection in the BETTER ontology as a vehicle. The key innovation is expanding the prompt and task presented to GPT-4 to provide License to speculate when unsure and Opportunity to quantify and explain its uncertainty (L&O). This approach improves accuracy and provides usable confidence measures (0.759 AUC) with no additional machinery.",
    "authors": [
      "Steven Fincke",
      "Adrien Bibal",
      "Elizabeth Boschee"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-08-01T21:08:07Z",
    "pdf_url": "https://arxiv.org/pdf/2408.00914v1"
  },
  {
    "arxiv_id": "2408.00161v2",
    "entry_id": "http://arxiv.org/abs/2408.00161v2",
    "title": "Automatic Generation of Behavioral Test Cases For Natural Language Processing Using Clustering and Prompting",
    "summary": "Recent work in behavioral testing for natural language processing (NLP) models, such as Checklist, is inspired by related paradigms in software engineering testing. They allow evaluation of general linguistic capabilities and domain understanding, hence can help evaluate conceptual soundness and identify model weaknesses. However, a major challenge is the creation of test cases. The current packages rely on semi-automated approach using manual development which requires domain expertise and can be time consuming. This paper introduces an automated approach to develop test cases by exploiting the power of large language models and statistical techniques. It clusters the text representations to carefully construct meaningful groups and then apply prompting techniques to automatically generate Minimal Functionality Tests (MFT). The well-known Amazon Reviews corpus is used to demonstrate our approach. We analyze the behavioral test profiles across four different classification algorithms and discuss the limitations and strengths of those models.",
    "authors": [
      "Ying Li",
      "Rahul Singh",
      "Tarun Joshi",
      "Agus Sudjianto"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "published": "2024-07-31T21:12:21Z",
    "pdf_url": "https://arxiv.org/pdf/2408.00161v2"
  },
  {
    "arxiv_id": "2407.21794v2",
    "entry_id": "http://arxiv.org/abs/2407.21794v2",
    "title": "Generalized Out-of-Distribution Detection and Beyond in Vision Language Model Era: A Survey",
    "summary": "Detecting out-of-distribution (OOD) samples is crucial for ensuring the safety of machine learning systems and has shaped the field of OOD detection. Meanwhile, several other problems are closely related to OOD detection, including anomaly detection (AD), novelty detection (ND), open set recognition (OSR), and outlier detection (OD). To unify these problems, a generalized OOD detection framework was proposed, taxonomically categorizing these five problems. However, Vision Language Models (VLMs) such as CLIP have significantly changed the paradigm and blurred the boundaries between these fields, again confusing researchers. In this survey, we first present a generalized OOD detection v2, encapsulating the evolution of these fields in the VLM era. Our framework reveals that, with some field inactivity and integration, the demanding challenges have become OOD detection and AD. Then, we highlight the significant shift in the definition, problem settings, and benchmarks; we thus feature a comprehensive review of the methodology for OOD detection and related tasks to clarify their relationship to OOD detection. Finally, we explore the advancements in the emerging Large Vision Language Model (LVLM) era, such as GPT-4V. We conclude with open challenges and future directions. The resource is available at https://github.com/AtsuMiyai/Awesome-OOD-VLM.",
    "authors": [
      "Atsuyuki Miyai",
      "Jingkang Yang",
      "Jingyang Zhang",
      "Yifei Ming",
      "Yueqian Lin",
      "Qing Yu",
      "Go Irie",
      "Shafiq Joty",
      "Yixuan Li",
      "Hai Li",
      "Ziwei Liu",
      "Toshihiko Yamasaki",
      "Kiyoharu Aizawa"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-07-31T17:59:58Z",
    "pdf_url": "https://arxiv.org/pdf/2407.21794v2"
  },
  {
    "arxiv_id": "2407.21726v1",
    "entry_id": "http://arxiv.org/abs/2407.21726v1",
    "title": "Artificial Intelligence Approaches for Energy Efficiency: A Review",
    "summary": "United Nations set Sustainable Development Goals and this paper focuses on 7th (Affordable and Clean Energy), 9th (Industries, Innovation and Infrastructure), and 13th (Climate Action) goals. Climate change is a major concern in our society; for this reason, a current global objective is to reduce energy waste. This work summarizes all main approaches towards energy efficiency using Artificial Intelligence with a particular focus on multi-agent systems to create smart buildings. It mentions the tight relationship between AI, especially IoT, and Big Data. It explains the application of AI to anomaly detection in smart buildings and a possible classification of Intelligent Energy Management Systems: Direct and Indirect. Finally, some drawbacks of AI approaches and some possible future research focuses are proposed.",
    "authors": [
      "Alberto Pasqualetto",
      "Lorenzo Serafini",
      "Michele Sprocatti"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-07-31T16:24:52Z",
    "pdf_url": "https://arxiv.org/pdf/2407.21726v1"
  },
  {
    "arxiv_id": "2407.21713v2",
    "entry_id": "http://arxiv.org/abs/2407.21713v2",
    "title": "Social Learning through Interactions with Other Agents: A Survey",
    "summary": "Social learning plays an important role in the development of human intelligence. As children, we imitate our parents' speech patterns until we are able to produce sounds; we learn from them praising us and scolding us; and as adults, we learn by working with others. In this work, we survey the degree to which this paradigm -- social learning -- has been mirrored in machine learning. In particular, since learning socially requires interacting with others, we are interested in how embodied agents can and have utilised these techniques. This is especially in light of the degree to which recent advances in natural language processing (NLP) enable us to perform new forms of social learning. We look at how behavioural cloning and next-token prediction mirror human imitation, how learning from human feedback mirrors human education, and how we can go further to enable fully communicative agents that learn from each other. We find that while individual social learning techniques have been used successfully, there has been little unifying work showing how to bring them together into socially embodied agents.",
    "authors": [
      "Dylan Hillier",
      "Cheston Tan",
      "Jing Jiang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-07-31T16:06:34Z",
    "pdf_url": "https://arxiv.org/pdf/2407.21713v2"
  },
  {
    "arxiv_id": "2407.21459v1",
    "entry_id": "http://arxiv.org/abs/2407.21459v1",
    "title": "KemenkeuGPT: Leveraging a Large Language Model on Indonesia's Government Financial Data and Regulations to Enhance Decision Making",
    "summary": "Data is crucial for evidence-based policymaking and enhancing public services, including those at the Ministry of Finance of the Republic of Indonesia. However, the complexity and dynamic nature of governmental financial data and regulations can hinder decision-making. This study investigates the potential of Large Language Models (LLMs) to address these challenges, focusing on Indonesia's financial data and regulations. While LLMs are effective in the financial sector, their use in the public sector in Indonesia is unexplored. This study undertakes an iterative process to develop KemenkeuGPT using the LangChain with Retrieval-Augmented Generation (RAG), prompt engineering and fine-tuning. The dataset from 2003 to 2023 was collected from the Ministry of Finance, Statistics Indonesia and the International Monetary Fund (IMF). Surveys and interviews with Ministry officials informed, enhanced and fine-tuned the model. We evaluated the model using human feedback, LLM-based evaluation and benchmarking. The model's accuracy improved from 35% to 61%, with correctness increasing from 48% to 64%. The Retrieval-Augmented Generation Assessment (RAGAS) framework showed that KemenkeuGPT achieved 44% correctness with 73% faithfulness, 40% precision and 60% recall, outperforming several other base models. An interview with an expert from the Ministry of Finance indicated that KemenkeuGPT has the potential to become an essential tool for decision-making. These results are expected to improve with continuous human feedback.",
    "authors": [
      "Gilang Fajar Febrian",
      "Grazziela Figueredo"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-07-31T09:16:33Z",
    "pdf_url": "https://arxiv.org/pdf/2407.21459v1"
  },
  {
    "arxiv_id": "2407.21307v1",
    "entry_id": "http://arxiv.org/abs/2407.21307v1",
    "title": "Modeling Urban Transport Choices: Incorporating Sociocultural Aspects",
    "summary": "This paper introduces an agent-based simulation model aimed at understanding urban commuters mode choices and evaluating the impacts of transport policies to promote sustainable mobility. Crafted for developing countries, where utilitarian travel heavily relies on motorcycles, the model integrates sociocultural factors that influence transport behavior. Multinomial models and inferential statistics applied to survey data from Cali, Colombia, inform the model, revealing significant influences of sociodemographic factors and travel attributes on mode choice. Findings highlight the importance of cost, time, safety, comfort, and personal security, with disparities across socioeconomic groups. Policy simulations demonstrate positive responses to interventions like free public transportation, increased bus frequency, and enhanced security, yet with modest shifts in mode choice. Multifaceted policy approaches are deemed more effective, addressing diverse user preferences. Outputs can be extended to cities with similar sociocultural characteristics and transport dynamics. The methodology applied in this work can be replicated for other territories.",
    "authors": [
      "Kathleen Salazar-Serna",
      "Lorena Cadavid",
      "Carlos J. Franco"
    ],
    "categories": [
      "cs.MA",
      "stat.AP"
    ],
    "published": "2024-07-31T03:19:56Z",
    "pdf_url": "https://arxiv.org/pdf/2407.21307v1"
  },
  {
    "arxiv_id": "2407.20906v5",
    "entry_id": "http://arxiv.org/abs/2407.20906v5",
    "title": "Automated Review Generation Method Based on Large Language Models",
    "summary": "Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties. Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5\\% with 95\\% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.",
    "authors": [
      "Shican Wu",
      "Xiao Ma",
      "Dehui Luo",
      "Lulu Li",
      "Xiangcheng Shi",
      "Xin Chang",
      "Xiaoyun Lin",
      "Ran Luo",
      "Chunlei Pei",
      "Changying Du",
      "Zhi-Jian Zhao",
      "Jinlong Gong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "physics.data-an"
    ],
    "published": "2024-07-30T15:26:36Z",
    "pdf_url": "https://arxiv.org/pdf/2407.20906v5"
  },
  {
    "arxiv_id": "2407.20578v2",
    "entry_id": "http://arxiv.org/abs/2407.20578v2",
    "title": "Comparison of Large Language Models for Generating Contextually Relevant Questions",
    "summary": "This study explores the effectiveness of Large Language Models (LLMs) for Automatic Question Generation in educational settings. Three LLMs are compared in their ability to create questions from university slide text without fine-tuning. Questions were obtained in a two-step pipeline: first, answer phrases were extracted from slides using Llama 2-Chat 13B; then, the three models generated questions for each answer. To analyze whether the questions would be suitable in educational applications for students, a survey was conducted with 46 students who evaluated a total of 246 questions across five metrics: clarity, relevance, difficulty, slide relation, and question-answer alignment. Results indicate that GPT-3.5 and Llama 2-Chat 13B outperform Flan T5 XXL by a small margin, particularly in terms of clarity and question-answer alignment. GPT-3.5 especially excels at tailoring questions to match the input answers. The contribution of this research is the analysis of the capacity of LLMs for Automatic Question Generation in education.",
    "authors": [
      "Ivo Lodovico Molina",
      "Valdemar Švábenský",
      "Tsubasa Minematsu",
      "Li Chen",
      "Fumiya Okubo",
      "Atsushi Shimada"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-07-30T06:23:59Z",
    "pdf_url": "https://arxiv.org/pdf/2407.20578v2"
  },
  {
    "arxiv_id": "2407.20516v1",
    "entry_id": "http://arxiv.org/abs/2407.20516v1",
    "title": "Machine Unlearning in Generative AI: A Survey",
    "summary": "Generative AI technologies have been deployed in many places, such as (multimodal) large language models and vision generative models. Their remarkable performance should be attributed to massive training data and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or dangerous information originated from the training data especially those from web crawl. New machine unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects from the models, because those that were designed for traditional classification tasks could not be applied for Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of different kinds of MU techniques. It also presents several critical challenges and promising directions in MU research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.",
    "authors": [
      "Zheyuan Liu",
      "Guangyao Dou",
      "Zhaoxuan Tan",
      "Yijun Tian",
      "Meng Jiang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-07-30T03:26:09Z",
    "pdf_url": "https://arxiv.org/pdf/2407.20516v1"
  },
  {
    "arxiv_id": "2407.19914v1",
    "entry_id": "http://arxiv.org/abs/2407.19914v1",
    "title": "Sentiment Analysis of Lithuanian Online Reviews Using Large Language Models",
    "summary": "Sentiment analysis is a widely researched area within Natural Language Processing (NLP), attracting significant interest due to the advent of automated solutions. Despite this, the task remains challenging because of the inherent complexity of languages and the subjective nature of sentiments. It is even more challenging for less-studied and less-resourced languages such as Lithuanian. Our review of existing Lithuanian NLP research reveals that traditional machine learning methods and classification algorithms have limited effectiveness for the task. In this work, we address sentiment analysis of Lithuanian five-star-based online reviews from multiple domains that we collect and clean. We apply transformer models to this task for the first time, exploring the capabilities of pre-trained multilingual Large Language Models (LLMs), specifically focusing on fine-tuning BERT and T5 models. Given the inherent difficulty of the task, the fine-tuned models perform quite well, especially when the sentiments themselves are less ambiguous: 80.74% and 89.61% testing recognition accuracy of the most popular one- and five-star reviews respectively. They significantly outperform current commercial state-of-the-art general-purpose LLM GPT-4. We openly share our fine-tuned LLMs online.",
    "authors": [
      "Brigita Vileikytė",
      "Mantas Lukoševičius",
      "Lukas Stankevičius"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-07-29T11:44:21Z",
    "pdf_url": "https://arxiv.org/pdf/2407.19914v1"
  },
  {
    "arxiv_id": "2407.19679v1",
    "entry_id": "http://arxiv.org/abs/2407.19679v1",
    "title": "Harnessing Large Vision and Language Models in Agriculture: A Review",
    "summary": "Large models can play important roles in many domains. Agriculture is another key factor affecting the lives of people around the world. It provides food, fabric, and coal for humanity. However, facing many challenges such as pests and diseases, soil degradation, global warming, and food security, how to steadily increase the yield in the agricultural sector is a problem that humans still need to solve. Large models can help farmers improve production efficiency and harvest by detecting a series of agricultural production tasks such as pests and diseases, soil quality, and seed quality. It can also help farmers make wise decisions through a variety of information, such as images, text, etc. Herein, we delve into the potential applications of large models in agriculture, from large language model (LLM) and large vision model (LVM) to large vision-language models (LVLM). After gaining a deeper understanding of multimodal large language models (MLLM), it can be recognized that problems such as agricultural image processing, agricultural question answering systems, and agricultural machine automation can all be solved by large models. Large models have great potential in the field of agriculture. We outline the current applications of agricultural large models, and aims to emphasize the importance of large models in the domain of agriculture. In the end, we envisage a future in which famers use MLLM to accomplish many tasks in agriculture, which can greatly improve agricultural production efficiency and yield.",
    "authors": [
      "Hongyan Zhu",
      "Shuai Qin",
      "Min Su",
      "Chengzhi Lin",
      "Anjie Li",
      "Junfeng Gao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-07-29T03:47:54Z",
    "pdf_url": "https://arxiv.org/pdf/2407.19679v1"
  },
  {
    "arxiv_id": "2407.19422v1",
    "entry_id": "http://arxiv.org/abs/2407.19422v1",
    "title": "A Generic Review of Integrating Artificial Intelligence in Cognitive Behavioral Therapy",
    "summary": "Cognitive Behavioral Therapy (CBT) is a well-established intervention for mitigating psychological issues by modifying maladaptive cognitive and behavioral patterns. However, delivery of CBT is often constrained by resource limitations and barriers to access. Advancements in artificial intelligence (AI) have provided technical support for the digital transformation of CBT. Particularly, the emergence of pre-training models (PTMs) and large language models (LLMs) holds immense potential to support, augment, optimize and automate CBT delivery. This paper reviews the literature on integrating AI into CBT interventions. We begin with an overview of CBT. Then, we introduce the integration of AI into CBT across various stages: pre-treatment, therapeutic process, and post-treatment. Next, we summarized the datasets relevant to some CBT-related tasks. Finally, we discuss the benefits and current limitations of applying AI to CBT. We suggest key areas for future research, highlighting the need for further exploration and validation of the long-term efficacy and clinical utility of AI-enhanced CBT. The transformative potential of AI in reshaping the practice of CBT heralds a new era of more accessible, efficient, and personalized mental health interventions.",
    "authors": [
      "Meng Jiang",
      "Qing Zhao",
      "Jianqiang Li",
      "Fan Wang",
      "Tianyu He",
      "Xinyan Cheng",
      "Bing Xiang Yang",
      "Grace W. K. Ho",
      "Guanghui Fu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-07-28T08:09:46Z",
    "pdf_url": "https://arxiv.org/pdf/2407.19422v1"
  },
  {
    "arxiv_id": "2407.19280v1",
    "entry_id": "http://arxiv.org/abs/2407.19280v1",
    "title": "Large Language Models for Human-like Autonomous Driving: A Survey",
    "summary": "Large Language Models (LLMs), AI models trained on massive text corpora with remarkable language understanding and generation capabilities, are transforming the field of Autonomous Driving (AD). As AD systems evolve from rule-based and optimization-based methods to learning-based techniques like deep reinforcement learning, they are now poised to embrace a third and more advanced category: knowledge-based AD empowered by LLMs. This shift promises to bring AD closer to human-like AD. However, integrating LLMs into AD systems poses challenges in real-time inference, safety assurance, and deployment costs. This survey provides a comprehensive and critical review of recent progress in leveraging LLMs for AD, focusing on their applications in modular AD pipelines and end-to-end AD systems. We highlight key advancements, identify pressing challenges, and propose promising research directions to bridge the gap between LLMs and AD, thereby facilitating the development of more human-like AD systems. The survey first introduces LLMs' key features and common training schemes, then delves into their applications in modular AD pipelines and end-to-end AD, respectively, followed by discussions on open challenges and future directions. Through this in-depth analysis, we aim to provide insights and inspiration for researchers and practitioners working at the intersection of AI and autonomous vehicles, ultimately contributing to safer, smarter, and more human-centric AD technologies.",
    "authors": [
      "Yun Li",
      "Kai Katsumata",
      "Ehsan Javanmardi",
      "Manabu Tsukada"
    ],
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "published": "2024-07-27T15:24:11Z",
    "pdf_url": "https://arxiv.org/pdf/2407.19280v1"
  },
  {
    "arxiv_id": "2407.19256v1",
    "entry_id": "http://arxiv.org/abs/2407.19256v1",
    "title": "Stochastic Parrots or ICU Experts? Large Language Models in Critical Care Medicine: A Scoping Review",
    "summary": "With the rapid development of artificial intelligence (AI), large language models (LLMs) have shown strong capabilities in natural language understanding, reasoning, and generation, attracting amounts of research interest in applying LLMs to health and medicine. Critical care medicine (CCM) provides diagnosis and treatment for critically ill patients who often require intensive monitoring and interventions in intensive care units (ICUs). Can LLMs be applied to CCM? Are LLMs just like stochastic parrots or ICU experts in assisting clinical decision-making? This scoping review aims to provide a panoramic portrait of the application of LLMs in CCM. Literature in seven databases, including PubMed, Embase, Scopus, Web of Science, CINAHL, IEEE Xplore, and ACM Digital Library, were searched from January 1, 2019, to June 10, 2024. Peer-reviewed journal and conference articles that discussed the application of LLMs in critical care settings were included. From an initial 619 articles, 24 were selected for final review. This review grouped applications of LLMs in CCM into three categories: clinical decision support, medical documentation and reporting, and medical education and doctor-patient communication. LLMs have advantages in handling unstructured data and do not require manual feature engineering. Meanwhile, applying LLMs to CCM faces challenges, including hallucinations, poor interpretability, bias and alignment challenges, and privacy and ethics issues. Future research should enhance model reliability and interpretability, integrate up-to-date medical knowledge, and strengthen privacy and ethical guidelines. As LLMs evolve, they could become key tools in CCM to help improve patient outcomes and optimize healthcare delivery. This study is the first review of LLMs in CCM, aiding researchers, clinicians, and policymakers to understand the current status and future potentials of LLMs in CCM.",
    "authors": [
      "Tongyue Shi",
      "Jun Ma",
      "Zihan Yu",
      "Haowei Xu",
      "Minqi Xiong",
      "Meirong Xiao",
      "Yilin Li",
      "Huiying Zhao",
      "Guilan Kong"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-07-27T13:41:43Z",
    "pdf_url": "https://arxiv.org/pdf/2407.19256v1"
  },
  {
    "arxiv_id": "2407.19200v2",
    "entry_id": "http://arxiv.org/abs/2407.19200v2",
    "title": "On Behalf of the Stakeholders: Trends in NLP Model Interpretability in the Era of LLMs",
    "summary": "Recent advancements in NLP systems, particularly with the introduction of LLMs, have led to widespread adoption of these systems by a broad spectrum of users across various domains, impacting decision-making, the job market, society, and scientific research. This surge in usage has led to an explosion in NLP model interpretability and analysis research, accompanied by numerous technical surveys. Yet, these surveys often overlook the needs and perspectives of explanation stakeholders. In this paper, we address three fundamental questions: Why do we need interpretability, what are we interpreting, and how? By exploring these questions, we examine existing interpretability paradigms, their properties, and their relevance to different stakeholders. We further explore the practical implications of these paradigms by analyzing trends from the past decade across multiple research fields. To this end, we retrieved thousands of papers and employed an LLM to characterize them. Our analysis reveals significant disparities between NLP developers and non-developer users, as well as between research fields, underscoring the diverse needs of stakeholders. For example, explanations of internal model components are rarely used outside the NLP field. We hope this paper informs the future design, development, and application of methods that align with the objectives and requirements of various stakeholders.",
    "authors": [
      "Nitay Calderon",
      "Roi Reichart"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-27T08:00:27Z",
    "pdf_url": "https://arxiv.org/pdf/2407.19200v2"
  },
  {
    "arxiv_id": "2408.01459v1",
    "entry_id": "http://arxiv.org/abs/2408.01459v1",
    "title": "AgentPeerTalk: Empowering Students through Agentic-AI-Driven Discernment of Bullying and Joking in Peer Interactions in Schools",
    "summary": "Addressing school bullying effectively and promptly is crucial for the mental health of students. This study examined the potential of large language models (LLMs) to empower students by discerning between bullying and joking in school peer interactions. We employed ChatGPT-4, Gemini 1.5 Pro, and Claude 3 Opus, evaluating their effectiveness through human review. Our results revealed that not all LLMs were suitable for an agentic approach, with ChatGPT-4 showing the most promise. We observed variations in LLM outputs, possibly influenced by political overcorrectness, context window limitations, and pre-existing bias in their training data. ChatGPT-4 excelled in context-specific accuracy after implementing the agentic approach, highlighting its potential to provide continuous, real-time support to vulnerable students. This study underlines the significant social impact of using agentic AI in educational settings, offering a new avenue for reducing the negative consequences of bullying and enhancing student well-being.",
    "authors": [
      "Aditya Paul",
      "Chi Lok Yu",
      "Eva Adelina Susanto",
      "Nicholas Wai Long Lau",
      "Gwenyth Isobel Meadows"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-07-27T05:50:02Z",
    "pdf_url": "https://arxiv.org/pdf/2408.01459v1"
  },
  {
    "arxiv_id": "2407.18827v1",
    "entry_id": "http://arxiv.org/abs/2407.18827v1",
    "title": "Human-artificial intelligence teaming for scientific information extraction from data-driven additive manufacturing research using large language models",
    "summary": "Data-driven research in Additive Manufacturing (AM) has gained significant success in recent years. This has led to a plethora of scientific literature to emerge. The knowledge in these works consists of AM and Artificial Intelligence (AI) contexts that have not been mined and formalized in an integrated way. It requires substantial effort and time to extract scientific information from these works. AM domain experts have contributed over two dozen review papers to summarize these works. However, information specific to AM and AI contexts still requires manual effort to extract. The recent success of foundation models such as BERT (Bidirectional Encoder Representations for Transformers) or GPT (Generative Pre-trained Transformers) on textual data has opened the possibility of expediting scientific information extraction. We propose a framework that enables collaboration between AM and AI experts to continuously extract scientific information from data-driven AM literature. A demonstration tool is implemented based on the proposed framework and a case study is conducted to extract information relevant to the datasets, modeling, sensing, and AM system categories. We show the ability of LLMs (Large Language Models) to expedite the extraction of relevant information from data-driven AM literature. In the future, the framework can be used to extract information from the broader design and manufacturing literature in the engineering discipline.",
    "authors": [
      "Mutahar Safdar",
      "Jiarui Xie",
      "Andrei Mircea",
      "Yaoyao Fiona Zhao"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-07-26T15:43:52Z",
    "pdf_url": "https://arxiv.org/pdf/2407.18827v1"
  },
  {
    "arxiv_id": "2407.20181v2",
    "entry_id": "http://arxiv.org/abs/2407.20181v2",
    "title": "Blockchain for Large Language Model Security and Safety: A Holistic Survey",
    "summary": "With the growing development and deployment of large language models (LLMs) in both industrial and academic fields, their security and safety concerns have become increasingly critical. However, recent studies indicate that LLMs face numerous vulnerabilities, including data poisoning, prompt injections, and unauthorized data exposure, which conventional methods have struggled to address fully. In parallel, blockchain technology, known for its data immutability and decentralized structure, offers a promising foundation for safeguarding LLMs. In this survey, we aim to comprehensively assess how to leverage blockchain technology to enhance LLMs' security and safety. Besides, we propose a new taxonomy of blockchain for large language models (BC4LLMs) to systematically categorize related works in this emerging field. Our analysis includes novel frameworks and definitions to delineate security and safety in the context of BC4LLMs, highlighting potential research directions and challenges at this intersection. Through this study, we aim to stimulate targeted advancements in blockchain-integrated LLM security.",
    "authors": [
      "Caleb Geren",
      "Amanda Board",
      "Gaby G. Dagher",
      "Tim Andersen",
      "Jun Zhuang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "published": "2024-07-26T15:24:01Z",
    "pdf_url": "https://arxiv.org/pdf/2407.20181v2"
  },
  {
    "arxiv_id": "2407.18597v1",
    "entry_id": "http://arxiv.org/abs/2407.18597v1",
    "title": "Reinforcement Learning for Sustainable Energy: A Survey",
    "summary": "The transition to sustainable energy is a key challenge of our time, requiring modifications in the entire pipeline of energy production, storage, transmission, and consumption. At every stage, new sequential decision-making challenges emerge, ranging from the operation of wind farms to the management of electrical grids or the scheduling of electric vehicle charging stations. All such problems are well suited for reinforcement learning, the branch of machine learning that learns behavior from data. Therefore, numerous studies have explored the use of reinforcement learning for sustainable energy. This paper surveys this literature with the intention of bridging both the underlying research communities: energy and machine learning. After a brief introduction of both fields, we systematically list relevant sustainability challenges, how they can be modeled as a reinforcement learning problem, and what solution approaches currently exist in the literature. Afterwards, we zoom out and identify overarching reinforcement learning themes that appear throughout sustainability, such as multi-agent, offline, and safe reinforcement learning. Lastly, we also cover standardization of environments, which will be crucial for connecting both research fields, and highlight potential directions for future work. In summary, this survey provides an extensive overview of reinforcement learning methods for sustainable energy, which may play a vital role in the energy transition.",
    "authors": [
      "Koen Ponse",
      "Felix Kleuker",
      "Márton Fejér",
      "Álvaro Serra-Gómez",
      "Aske Plaat",
      "Thomas Moerland"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "eess.SY",
      "stat.ML"
    ],
    "published": "2024-07-26T08:37:14Z",
    "pdf_url": "https://arxiv.org/pdf/2407.18597v1"
  },
  {
    "arxiv_id": "2407.18454v2",
    "entry_id": "http://arxiv.org/abs/2407.18454v2",
    "title": "Fairness Definitions in Language Models Explained",
    "summary": "Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as gender and race, limiting their adoption in real-world applications. Therefore, fairness has been extensively explored in LMs, leading to the proposal of various fairness notions. However, the lack of clear agreement on which fairness definition to apply in specific contexts and the complexity of understanding the distinctions between these definitions can create confusion and impede further progress. To this end, this paper proposes a systematic survey that clarifies the definitions of fairness as they apply to LMs. Specifically, we begin with a brief introduction to LMs and fairness in LMs, followed by a comprehensive, up-to-date overview of existing fairness notions in LMs and the introduction of a novel taxonomy that categorizes these concepts based on their transformer architecture: encoder-only, decoder-only, and encoder-decoder LMs. We further illustrate each definition through experiments, showcasing their practical implications and outcomes. Finally, we discuss current research challenges and open questions, aiming to foster innovative ideas and advance the field. The repository is publicly available online at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/definitions.",
    "authors": [
      "Avash Palikhe",
      "Zichong Wang",
      "Zhipeng Yin",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-07-26T01:21:25Z",
    "pdf_url": "https://arxiv.org/pdf/2407.18454v2"
  },
  {
    "arxiv_id": "2408.05354v2",
    "entry_id": "http://arxiv.org/abs/2408.05354v2",
    "title": "Trusting Your AI Agent Emotionally and Cognitively: Development and Validation of a Semantic Differential Scale for AI Trust",
    "summary": "Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLMs-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person's overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.",
    "authors": [
      "Ruoxi Shang",
      "Gary Hsieh",
      "Chirag Shah"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-07-25T18:55:33Z",
    "pdf_url": "https://arxiv.org/pdf/2408.05354v2"
  },
  {
    "arxiv_id": "2408.00588v1",
    "entry_id": "http://arxiv.org/abs/2408.00588v1",
    "title": "Closing the gap between open-source and commercial large language models for medical evidence summarization",
    "summary": "Large language models (LLMs) hold great promise in summarizing medical evidence. Most recent studies focus on the application of proprietary LLMs. Using proprietary LLMs introduces multiple risk factors, including a lack of transparency and vendor dependency. While open-source LLMs allow better transparency and customization, their performance falls short compared to proprietary ones. In this study, we investigated to what extent fine-tuning open-source LLMs can further improve their performance in summarizing medical evidence. Utilizing a benchmark dataset, MedReview, consisting of 8,161 pairs of systematic reviews and summaries, we fine-tuned three broadly-used, open-sourced LLMs, namely PRIMERA, LongT5, and Llama-2. Overall, the fine-tuned LLMs obtained an increase of 9.89 in ROUGE-L (95% confidence interval: 8.94-10.81), 13.21 in METEOR score (95% confidence interval: 12.05-14.37), and 15.82 in CHRF score (95% confidence interval: 13.89-16.44). The performance of fine-tuned LongT5 is close to GPT-3.5 with zero-shot settings. Furthermore, smaller fine-tuned models sometimes even demonstrated superior performance compared to larger zero-shot models. The above trends of improvement were also manifested in both human and GPT4-simulated evaluations. Our results can be applied to guide model selection for tasks demanding particular domain knowledge, such as medical evidence summarization.",
    "authors": [
      "Gongbo Zhang",
      "Qiao Jin",
      "Yiliang Zhou",
      "Song Wang",
      "Betina R. Idnay",
      "Yiming Luo",
      "Elizabeth Park",
      "Jordan G. Nestor",
      "Matthew E. Spotnitz",
      "Ali Soroush",
      "Thomas Campion",
      "Zhiyong Lu",
      "Chunhua Weng",
      "Yifan Peng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-25T05:03:01Z",
    "pdf_url": "https://arxiv.org/pdf/2408.00588v1"
  },
  {
    "arxiv_id": "2408.06352v1",
    "entry_id": "http://arxiv.org/abs/2408.06352v1",
    "title": "Using Large Language Models to Compare Explainable Models for Smart Home Human Activity Recognition",
    "summary": "Recognizing daily activities with unobtrusive sensors in smart environments enables various healthcare applications. Monitoring how subjects perform activities at home and their changes over time can reveal early symptoms of health issues, such as cognitive decline. Most approaches in this field use deep learning models, which are often seen as black boxes mapping sensor data to activities. However, non-expert users like clinicians need to trust and understand these models' outputs. Thus, eXplainable AI (XAI) methods for Human Activity Recognition have emerged to provide intuitive natural language explanations from these models. Different XAI methods generate different explanations, and their effectiveness is typically evaluated through user surveys, that are often challenging in terms of costs and fairness. This paper proposes an automatic evaluation method using Large Language Models (LLMs) to identify, in a pool of candidates, the best XAI approach for non-expert users. Our preliminary results suggest that LLM evaluation aligns with user surveys.",
    "authors": [
      "Michele Fiori",
      "Gabriele Civitarese",
      "Claudio Bettini"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-07-24T12:15:07Z",
    "pdf_url": "https://arxiv.org/pdf/2408.06352v1"
  },
  {
    "arxiv_id": "2407.16867v2",
    "entry_id": "http://arxiv.org/abs/2407.16867v2",
    "title": "From Text to Insight: Large Language Models for Materials Science Data Extraction",
    "summary": "The vast majority of materials science knowledge exists in unstructured natural language, yet structured data is crucial for innovative and systematic materials design. Traditionally, the field has relied on manual curation and partial automation for data extraction for specific use cases. The advent of large language models (LLMs) represents a significant shift, potentially enabling efficient extraction of structured, actionable data from unstructured text by non-experts. While applying LLMs to materials science data extraction presents unique challenges, domain knowledge offers opportunities to guide and validate LLM outputs. This review provides a comprehensive overview of LLM-based structured data extraction in materials science, synthesizing current knowledge and outlining future directions. We address the lack of standardized guidelines and present frameworks for leveraging the synergy between LLMs and materials science expertise. This work serves as a foundational resource for researchers aiming to harness LLMs for data-driven materials research. The insights presented here could significantly enhance how researchers across disciplines access and utilize scientific information, potentially accelerating the development of novel materials for critical societal needs.",
    "authors": [
      "Mara Schilling-Wilhelmi",
      "Martiño Ríos-García",
      "Sherjeel Shabih",
      "María Victoria Gil",
      "Santiago Miret",
      "Christoph T. Koch",
      "José A. Márquez",
      "Kevin Maik Jablonka"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "published": "2024-07-23T22:23:47Z",
    "pdf_url": "https://arxiv.org/pdf/2407.16867v2"
  },
  {
    "arxiv_id": "2407.16557v3",
    "entry_id": "http://arxiv.org/abs/2407.16557v3",
    "title": "Patched RTC: evaluating LLMs for diverse software development tasks",
    "summary": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation technique for Large Language Models (LLMs) applied to diverse software development tasks, particularly focusing on \"outer loop\" activities such as bug fixing, code review, and documentation updates. Patched RTC extends the original Round-Trip Correctness method to work with any LLM and downstream task, offering a self-evaluating framework that measures consistency and robustness of model responses without human intervention. The study demonstrates a correlation between Patched RTC scores and task-specific accuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm for open-domain task evaluation. We implement Patched RTC in an open-source framework called patchwork, allowing for transparent evaluation during inference across various patchflows. Experiments comparing GPT-3.5 and GPT-4 models across different software development tasks reveal that Patched RTC effectively distinguishes model performance and task difficulty. The paper also explores the impact of consistency prompts on improving model accuracy, suggesting that Patched RTC can guide prompt refinement and model selection for complex software development workflows.",
    "authors": [
      "Asankhaya Sharma"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-07-23T15:12:14Z",
    "pdf_url": "https://arxiv.org/pdf/2407.16557v3"
  },
  {
    "arxiv_id": "2407.15017v4",
    "entry_id": "http://arxiv.org/abs/2407.15017v4",
    "title": "Knowledge Mechanisms in Large Language Models: A Survey and Perspective",
    "summary": "Understanding knowledge mechanisms in Large Language Models (LLMs) is crucial for advancing towards trustworthy AGI. This paper reviews knowledge mechanism analysis from a novel taxonomy including knowledge utilization and evolution. Knowledge utilization delves into the mechanism of memorization, comprehension and application, and creation. Knowledge evolution focuses on the dynamic progression of knowledge within individual and group LLMs. Moreover, we discuss what knowledge LLMs have learned, the reasons for the fragility of parametric knowledge, and the potential dark knowledge (hypothesis) that will be challenging to address. We hope this work can help understand knowledge in LLMs and provide insights for future research.",
    "authors": [
      "Mengru Wang",
      "Yunzhi Yao",
      "Ziwen Xu",
      "Shuofei Qiao",
      "Shumin Deng",
      "Peng Wang",
      "Xiang Chen",
      "Jia-Chen Gu",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Huajun Chen",
      "Ningyu Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-07-22T06:15:59Z",
    "pdf_url": "https://arxiv.org/pdf/2407.15017v4"
  },
  {
    "arxiv_id": "2407.14116v1",
    "entry_id": "http://arxiv.org/abs/2407.14116v1",
    "title": "AuditNet: A Conversational AI-based Security Assistant [DEMO]",
    "summary": "In the age of information overload, professionals across various fields face the challenge of navigating vast amounts of documentation and ever-evolving standards. Ensuring compliance with standards, regulations, and contractual obligations is a critical yet complex task across various professional fields. We propose a versatile conversational AI assistant framework designed to facilitate compliance checking on the go, in diverse domains, including but not limited to network infrastructure, legal contracts, educational standards, environmental regulations, and government policies. By leveraging retrieval-augmented generation using large language models, our framework automates the review, indexing, and retrieval of relevant, context-aware information, streamlining the process of verifying adherence to established guidelines and requirements. This AI assistant not only reduces the manual effort involved in compliance checks but also enhances accuracy and efficiency, supporting professionals in maintaining high standards of practice and ensuring regulatory compliance in their respective fields. We propose and demonstrate AuditNet, the first conversational AI security assistant designed to assist IoT network security experts by providing instant access to security standards, policies, and regulations.",
    "authors": [
      "Shohreh Deldari",
      "Mohammad Goudarzi",
      "Aditya Joshi",
      "Arash Shaghaghi",
      "Simon Finn",
      "Flora D. Salim",
      "Sanjay Jha"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-07-19T08:33:07Z",
    "pdf_url": "https://arxiv.org/pdf/2407.14116v1"
  },
  {
    "arxiv_id": "2407.14567v3",
    "entry_id": "http://arxiv.org/abs/2407.14567v3",
    "title": "Integrating Artificial Intelligence into Operating Systems: A Survey on Techniques, Applications, and Future Directions",
    "summary": "Heterogeneous hardware and dynamic workloads worsen long-standing OS bottlenecks in scalability, adaptability, and manageability. At the same time, advances in machine learning (ML), large language models (LLMs), and agent-based methods enable automation and self-optimization, but current efforts lack a unifying view. This survey reviews techniques, architectures, applications, challenges, and future directions at the AI-OS intersection. We chart the shift from heuristic- and rule-based designs to AI-enhanced systems, outlining the strengths of ML, LLMs, and agents across the OS stack. We summarize progress in AI for OS (core components and the wider ecosystem) and in OS for AI (component- and architecture-level support for short- and long-context inference, distributed training, and edge inference). For practice, we consolidate evaluation dimensions, methodological pipelines, and patterns that balance real-time constraints with predictive accuracy. We identify key challenges, such as complexity, overhead, model drift, limited explainability, and privacy and safety risks, and recommend modular, AI-ready kernel interfaces; unified toolchains and benchmarks; hybrid rules-plus-AI decisions with guardrails; and verifiable in-kernel inference. Finally, we propose a three-stage roadmap including AI-powered, AI-refactored, and AI-driven OSs, to bridge prototypes and production and to enable scalable, reliable AI deployment.",
    "authors": [
      "Yifan Zhang",
      "Xinkui Zhao",
      "Ziying Li",
      "Guanjie Cheng",
      "Jianwei Yin",
      "Lufei Zhang",
      "Zuoning Chen"
    ],
    "categories": [
      "cs.OS",
      "cs.AI"
    ],
    "published": "2024-07-19T05:29:34Z",
    "pdf_url": "https://arxiv.org/pdf/2407.14567v3"
  },
  {
    "arxiv_id": "2407.13993v3",
    "entry_id": "http://arxiv.org/abs/2407.13993v3",
    "title": "LLAssist: Simple Tools for Automating Literature Review Using Large Language Models",
    "summary": "This paper introduces LLAssist, an open-source tool designed to streamline literature reviews in academic research. In an era of exponential growth in scientific publications, researchers face mounting challenges in efficiently processing vast volumes of literature. LLAssist addresses this issue by leveraging Large Language Models (LLMs) and Natural Language Processing (NLP) techniques to automate key aspects of the review process. Specifically, it extracts important information from research articles and evaluates their relevance to user-defined research questions. The goal of LLAssist is to significantly reduce the time and effort required for comprehensive literature reviews, allowing researchers to focus more on analyzing and synthesizing information rather than on initial screening tasks. By automating parts of the literature review workflow, LLAssist aims to help researchers manage the growing volume of academic publications more efficiently.",
    "authors": [
      "Christoforus Yoga Haryanto"
    ],
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "published": "2024-07-19T02:48:54Z",
    "pdf_url": "https://arxiv.org/pdf/2407.13993v3"
  },
  {
    "arxiv_id": "2407.14561v4",
    "entry_id": "http://arxiv.org/abs/2407.14561v4",
    "title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals",
    "summary": "We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. The National Deep Inference Fabric (NDIF) is a scalable inference service that executes NNsight requests, allowing users to share GPU resources and pretrained models. These technologies are enabled by the Intervention Graph, an architecture developed to decouple experimental design from model runtime. Together, this framework provides transparent and efficient access to the internals of deep neural networks such as very large language models (LLMs) without imposing the cost or complexity of hosting customized models individually. We conduct a quantitative survey of the machine learning literature that reveals a growing gap in the study of the internals of large-scale AI. We demonstrate the design and use of our framework to address this gap by enabling a range of research methods on huge models. Finally, we conduct benchmarks to compare performance with previous approaches.\n  Code, documentation, and tutorials are available at https://nnsight.net/.",
    "authors": [
      "Jaden Fiotto-Kaufman",
      "Alexander R. Loftus",
      "Eric Todd",
      "Jannik Brinkmann",
      "Koyena Pal",
      "Dmitrii Troitskii",
      "Michael Ripa",
      "Adam Belfki",
      "Can Rager",
      "Caden Juang",
      "Aaron Mueller",
      "Samuel Marks",
      "Arnab Sen Sharma",
      "Francesca Lucchetti",
      "Nikhil Prakash",
      "Carla Brodley",
      "Arjun Guha",
      "Jonathan Bell",
      "Byron C. Wallace",
      "David Bau"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-07-18T17:59:01Z",
    "pdf_url": "https://arxiv.org/pdf/2407.14561v4"
  },
  {
    "arxiv_id": "2407.13699v4",
    "entry_id": "http://arxiv.org/abs/2407.13699v4",
    "title": "A Comprehensive Review of Recommender Systems: Transitioning from Theory to Practice",
    "summary": "Recommender Systems (RS) play an integral role in enhancing user experiences by providing personalized item suggestions. This survey reviews the progress in RS inclusively from 2017 to 2024, effectively connecting theoretical advances with practical applications. We explore the development from traditional RS techniques like content-based and collaborative filtering to advanced methods involving deep learning, graph-based models, reinforcement learning, and large language models. We also discuss specialized systems such as context-aware, review-based, and fairness-aware RS. The primary goal of this survey is to bridge theory with practice. It addresses challenges across various sectors, including e-commerce, healthcare, and finance, emphasizing the need for scalable, real-time, and trustworthy solutions. Through this survey, we promote stronger partnerships between academic research and industry practices. The insights offered by this survey aim to guide industry professionals in optimizing RS deployment and to inspire future research directions, especially in addressing emerging technological and societal trends\\footnote. The survey resources are available in the public GitHub repository https://github.com/VectorInstitute/Recommender-Systems-Survey. (Recommender systems, large language models, chatgpt, responsible AI)",
    "authors": [
      "Shaina Raza",
      "Mizanur Rahman",
      "Safiullah Kamawal",
      "Armin Toroghi",
      "Ananya Raval",
      "Farshad Navah",
      "Amirmohammad Kazemeini"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-07-18T17:00:53Z",
    "pdf_url": "https://arxiv.org/pdf/2407.13699v4"
  },
  {
    "arxiv_id": "2407.12994v2",
    "entry_id": "http://arxiv.org/abs/2407.12994v2",
    "title": "A Survey of Prompt Engineering Methods in Large Language Models for Different NLP Tasks",
    "summary": "Large language models (LLMs) have shown remarkable performance on many different Natural Language Processing (NLP) tasks. Prompt engineering plays a key role in adding more to the already existing abilities of LLMs to achieve significant performance gains on various NLP tasks. Prompt engineering requires composing natural language instructions called prompts to elicit knowledge from LLMs in a structured way. Unlike previous state-of-the-art (SoTA) models, prompt engineering does not require extensive parameter re-training or fine-tuning based on the given NLP task and thus solely operates on the embedded knowledge of LLMs. Additionally, LLM enthusiasts can intelligently extract LLMs' knowledge through a basic natural language conversational exchange or prompt engineering, allowing more and more people even without deep mathematical machine learning background to experiment with LLMs. With prompt engineering gaining popularity in the last two years, researchers have come up with numerous engineering techniques around designing prompts to improve accuracy of information extraction from the LLMs. In this paper, we summarize different prompting techniques and club them together based on different NLP tasks that they have been used for. We further granularly highlight the performance of these prompting strategies on various datasets belonging to that NLP task, talk about the corresponding LLMs used, present a taxonomy diagram and discuss the possible SoTA for specific datasets. In total, we read and present a survey of 44 research papers which talk about 39 different prompting methods on 29 different NLP tasks of which most of them have been published in the last two years.",
    "authors": [
      "Shubham Vatsal",
      "Harsh Dubey"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-17T20:23:19Z",
    "pdf_url": "https://arxiv.org/pdf/2407.12994v2"
  },
  {
    "arxiv_id": "2407.12438v1",
    "entry_id": "http://arxiv.org/abs/2407.12438v1",
    "title": "Semantic-Aware Representation of Multi-Modal Data for Data Ingress: A Literature Review",
    "summary": "Machine Learning (ML) is continuously permeating a growing amount of application domains. Generative AI such as Large Language Models (LLMs) also sees broad adoption to process multi-modal data such as text, images, audio, and video. While the trend is to use ever-larger datasets for training, managing this data efficiently has become a significant practical challenge in the industry-double as much data is certainly not double as good. Rather the opposite is important since getting an understanding of the inherent quality and diversity of the underlying data lakes is a growing challenge for application-specific ML as well as for fine-tuning foundation models. Furthermore, information retrieval (IR) from expanding data lakes is complicated by the temporal dimension inherent in time-series data which must be considered to determine its semantic value. This study focuses on the different semantic-aware techniques to extract embeddings from mono-modal, multi-modal, and cross-modal data to enhance IR capabilities in a growing data lake. Articles were collected to summarize information about the state-of-the-art techniques focusing on applications of embedding for three different categories of data modalities.",
    "authors": [
      "Pierre Lamart",
      "Yinan Yu",
      "Christian Berger"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-07-17T09:49:11Z",
    "pdf_url": "https://arxiv.org/pdf/2407.12438v1"
  },
  {
    "arxiv_id": "2407.12391v1",
    "entry_id": "http://arxiv.org/abs/2407.12391v1",
    "title": "LLM Inference Serving: Survey of Recent Advances and Opportunities",
    "summary": "This survey offers a comprehensive overview of recent advancements in Large Language Model (LLM) serving systems, focusing on research since the year 2023. We specifically examine system-level enhancements that improve performance and efficiency without altering the core LLM decoding mechanisms. By selecting and reviewing high-quality papers from prestigious ML and system venues, we highlight key innovations and practical considerations for deploying and scaling LLMs in real-world production environments. This survey serves as a valuable resource for LLM practitioners seeking to stay abreast of the latest developments in this rapidly evolving field.",
    "authors": [
      "Baolin Li",
      "Yankai Jiang",
      "Vijay Gadepally",
      "Devesh Tiwari"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2024-07-17T08:11:47Z",
    "pdf_url": "https://arxiv.org/pdf/2407.12391v1"
  },
  {
    "arxiv_id": "2407.12888v1",
    "entry_id": "http://arxiv.org/abs/2407.12888v1",
    "title": "Explainable Biomedical Hypothesis Generation via Retrieval Augmented Generation enabled Large Language Models",
    "summary": "The vast amount of biomedical information available today presents a significant challenge for investigators seeking to digest, process, and understand these findings effectively. Large Language Models (LLMs) have emerged as powerful tools to navigate this complex and challenging data landscape. However, LLMs may lead to hallucinatory responses, making Retrieval Augmented Generation (RAG) crucial for achieving accurate information. In this protocol, we present RUGGED (Retrieval Under Graph-Guided Explainable disease Distinction), a comprehensive workflow designed to support investigators with knowledge integration and hypothesis generation, identifying validated paths forward. Relevant biomedical information from publications and knowledge bases are reviewed, integrated, and extracted via text-mining association analysis and explainable graph prediction models on disease nodes, forecasting potential links among drugs and diseases. These analyses, along with biomedical texts, are integrated into a framework that facilitates user-directed mechanism elucidation as well as hypothesis exploration through RAG-enabled LLMs. A clinical use-case demonstrates RUGGED's ability to evaluate and recommend therapeutics for Arrhythmogenic Cardiomyopathy (ACM) and Dilated Cardiomyopathy (DCM), analyzing prescribed drugs for molecular interactions and unexplored uses. The platform minimizes LLM hallucinations, offers actionable insights, and improves the investigation of novel therapeutics.",
    "authors": [
      "Alexander R. Pelletier",
      "Joseph Ramirez",
      "Irsyad Adam",
      "Simha Sankar",
      "Yu Yan",
      "Ding Wang",
      "Dylan Steinecke",
      "Wei Wang",
      "Peipei Ping"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-17T07:44:18Z",
    "pdf_url": "https://arxiv.org/pdf/2407.12888v1"
  },
  {
    "arxiv_id": "2407.11919v1",
    "entry_id": "http://arxiv.org/abs/2407.11919v1",
    "title": "What's Wrong? Refining Meeting Summaries with LLM Feedback",
    "summary": "Meeting summarization has become a critical task since digital encounters have become a common practice. Large language models (LLMs) show great potential in summarization, offering enhanced coherence and context understanding compared to traditional methods. However, they still struggle to maintain relevance and avoid hallucination. We introduce a multi-LLM correction approach for meeting summarization using a two-phase process that mimics the human review process: mistake identification and summary refinement. We release QMSum Mistake, a dataset of 200 automatically generated meeting summaries annotated by humans on nine error types, including structural, omission, and irrelevance errors. Our experiments show that these errors can be identified with high accuracy by an LLM. We transform identified mistakes into actionable feedback to improve the quality of a given summary measured by relevance, informativeness, conciseness, and coherence. This post-hoc refinement effectively improves summary quality by leveraging multiple LLMs to validate output quality. Our multi-LLM approach for meeting summarization shows potential for similar complex text generation tasks requiring robustness, action planning, and discussion towards a goal.",
    "authors": [
      "Frederic Kirstein",
      "Terry Ruas",
      "Bela Gipp"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-16T17:10:16Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11919v1"
  },
  {
    "arxiv_id": "2407.11511v3",
    "entry_id": "http://arxiv.org/abs/2407.11511v3",
    "title": "Multi-Step Reasoning with Large Language Models, a Survey",
    "summary": "Large language models (LLMs) with billions of parameters exhibit in-context learning abilities, enabling few-shot learning on tasks that the model was not specifically trained for. Traditional models achieve breakthrough performance on language tasks, but do not perform well on basic reasoning benchmarks. However, a new in-context learning approach, Chain-of-thought, has demonstrated strong multi-step reasoning abilities on these benchmarks. The research on LLM reasoning abilities started with the question whether LLMs can solve grade school math word problems, and has expanded to other tasks in the past few years. This article reviews the field of multi-step reasoning with LLMs. We propose a taxonomy that identifies different ways to generate, evaluate, and control multi-step reasoning. We provide an in-depth coverage of core approaches and open problems, and we propose a research agenda for the near future. We find that multi-step reasoning approaches have progressed beyond math word problems, and can now successfully solve challenges in logic, combinatorial games, and robotics, sometimes by first generating code that is then executed by external tools. Many studies in multi-step methods use reinforcement learning for finetuning, external optimization loops, in-context reinforcement learning, and self-reflection.",
    "authors": [
      "Aske Plaat",
      "Annie Wong",
      "Suzan Verberne",
      "Joost Broekens",
      "Niki van Stein",
      "Thomas Back"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-07-16T08:49:35Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11511v3"
  },
  {
    "arxiv_id": "2407.11484v9",
    "entry_id": "http://arxiv.org/abs/2407.11484v9",
    "title": "The Oscars of AI Theater: A Survey on Role-Playing with Language Models",
    "summary": "This survey explores the burgeoning field of role-playing with language models, focusing on their development from early persona-based models to advanced character-driven simulations facilitated by Large Language Models (LLMs). Initially confined to simple persona consistency due to limited model capabilities, role-playing tasks have now expanded to embrace complex character portrayals involving character consistency, behavioral alignment, and overall attractiveness. We provide a comprehensive taxonomy of the critical components in designing these systems, including data, models and alignment, agent architecture and evaluation. This survey not only outlines the current methodologies and challenges, such as managing dynamic personal profiles and achieving high-level persona consistency but also suggests avenues for future research in improving the depth and realism of role-playing applications. The goal is to guide future research by offering a structured overview of current methodologies and identifying potential areas for improvement. Related resources and papers are available at https://github.com/nuochenpku/Awesome-Role-Play-Papers.",
    "authors": [
      "Nuo Chen",
      "Yan Wang",
      "Yang Deng",
      "Jia Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-07-16T08:20:39Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11484v9"
  },
  {
    "arxiv_id": "2407.11194v2",
    "entry_id": "http://arxiv.org/abs/2407.11194v2",
    "title": "AstroMLab 1: Who Wins Astronomy Jeopardy!?",
    "summary": "We present a comprehensive evaluation of proprietary and open-weights large language models using the first astronomy-specific benchmarking dataset. This dataset comprises 4,425 multiple-choice questions curated from the Annual Review of Astronomy and Astrophysics, covering a broad range of astrophysical topics. Our analysis examines model performance across various astronomical subfields and assesses response calibration, crucial for potential deployment in research environments. Claude-3.5-Sonnet outperforms competitors by up to 4.6 percentage points, achieving 85.0% accuracy. For proprietary models, we observed a universal reduction in cost every 3-to-12 months to achieve similar score in this particular astronomy benchmark. open-weights models have rapidly improved, with LLaMA-3-70b (80.6%) and Qwen-2-72b (77.7%) now competing with some of the best proprietary models. We identify performance variations across topics, with non-English-focused models generally struggling more in exoplanet-related fields, stellar astrophysics, and instrumentation related questions. These challenges likely stem from less abundant training data, limited historical context, and rapid recent developments in these areas. This pattern is observed across both open-weights and proprietary models, with regional dependencies evident, highlighting the impact of training data diversity on model performance in specialized scientific domains. Top-performing models demonstrate well-calibrated confidence, with correlations above 0.9 between confidence and correctness, though they tend to be slightly underconfident. The development for fast, low-cost inference of open-weights models presents new opportunities for affordable deployment in astronomy. The rapid progress observed suggests that LLM-driven research in astronomy may become feasible in the near future.",
    "authors": [
      "Yuan-Sen Ting",
      "Tuan Dung Nguyen",
      "Tirthankar Ghosal",
      "Rui Pan",
      "Hardik Arora",
      "Zechang Sun",
      "Tijmen de Haan",
      "Nesar Ramachandra",
      "Azton Wells",
      "Sandeep Madireddy",
      "Alberto Accomazzi"
    ],
    "categories": [
      "astro-ph.IM",
      "astro-ph.EP",
      "astro-ph.GA",
      "astro-ph.SR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-07-15T19:28:14Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11194v2"
  },
  {
    "arxiv_id": "2407.10652v2",
    "entry_id": "http://arxiv.org/abs/2407.10652v2",
    "title": "Cutting Through the Clutter: The Potential of LLMs for Efficient Filtration in Systematic Literature Reviews",
    "summary": "Systematic literature reviews (SLRs) are essential but labor-intensive due to high publication volumes and inefficient keyword-based filtering. To streamline this process, we evaluate Large Language Models (LLMs) for enhancing efficiency and accuracy in corpus filtration while minimizing manual effort. Our open-source tool LLMSurver presents a visual interface to utilize LLMs for literature filtration, evaluate the results, and refine queries in an interactive way. We assess the real-world performance of our approach in filtering over 8.3k articles during a recent survey construction, comparing results with human efforts. The findings show that recent LLM models can reduce filtering time from weeks to minutes. A consensus scheme ensures recall rates >98.8%, surpassing typical human error thresholds and improving selection accuracy. This work advances literature review methodologies and highlights the potential of responsible human-AI collaboration in academic research.",
    "authors": [
      "Lucas Joos",
      "Daniel A. Keim",
      "Maximilian T. Fischer"
    ],
    "categories": [
      "cs.LG",
      "cs.DL",
      "cs.HC"
    ],
    "published": "2024-07-15T12:13:53Z",
    "pdf_url": "https://arxiv.org/pdf/2407.10652v2"
  },
  {
    "arxiv_id": "2407.11100v3",
    "entry_id": "http://arxiv.org/abs/2407.11100v3",
    "title": "Building Intelligence Identification System via Large Language Model Watermarking: A Survey and Beyond",
    "summary": "Large Language Models (LLMs) are increasingly integrated into diverse industries, posing substantial security risks due to unauthorized replication and misuse. To mitigate these concerns, robust identification mechanisms are widely acknowledged as an effective strategy. Identification systems for LLMs now rely heavily on watermarking technology to manage and protect intellectual property and ensure data security. However, previous studies have primarily concentrated on the basic principles of algorithms and lacked a comprehensive analysis of watermarking theory and practice from the perspective of intelligent identification. To bridge this gap, firstly, we explore how a robust identity recognition system can be effectively implemented and managed within LLMs by various participants using watermarking technology. Secondly, we propose a mathematical framework based on mutual information theory, which systematizes the identification process to achieve more precise and customized watermarking. Additionally, we present a comprehensive evaluation of performance metrics for LLM watermarking, reflecting participant preferences and advancing discussions on its identification applications. Lastly, we outline the existing challenges in current watermarking technologies and theoretical frameworks, and provide directional guidance to address these challenges. Our systematic classification and detailed exposition aim to enhance the comparison and evaluation of various methods, fostering further research and development toward a transparent, secure, and equitable LLM ecosystem.",
    "authors": [
      "Xuhong Wang",
      "Haoyu Jiang",
      "Yi Yu",
      "Jingru Yu",
      "Yilun Lin",
      "Ping Yi",
      "Yingchun Wang",
      "Yu Qiao",
      "Li Li",
      "Fei-Yue Wang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-07-15T07:20:02Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11100v3"
  },
  {
    "arxiv_id": "2407.11069v2",
    "entry_id": "http://arxiv.org/abs/2407.11069v2",
    "title": "Combining Federated Learning and Control: A Survey",
    "summary": "This survey provides an overview of combining Federated Learning (FL) and control to enhance adaptability, scalability, generalization, and privacy in (nonlinear) control applications. Traditional control methods rely on controller design models, but real-world scenarios often require online model retuning or learning. FL offers a distributed approach to model training, enabling collaborative learning across distributed devices while preserving data privacy. By keeping data localized, FL mitigates concerns regarding privacy and security while reducing network bandwidth requirements for communication. This survey summarizes the state-of-the-art concepts and ideas of combining FL and control. The methodical benefits are further discussed, culminating in a detailed overview of expected applications, from dynamical system modeling over controller design, focusing on adaptive control, to knowledge transfer in multi-agent decision-making systems.",
    "authors": [
      "Jakob Weber",
      "Markus Gurtner",
      "Amadeus Lobe",
      "Adrian Trachte",
      "Andreas Kugi"
    ],
    "categories": [
      "cs.LG",
      "eess.SY",
      "stat.ML"
    ],
    "published": "2024-07-12T14:29:17Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11069v2"
  },
  {
    "arxiv_id": "2408.10210v1",
    "entry_id": "http://arxiv.org/abs/2408.10210v1",
    "title": "A Survey on Symbolic Knowledge Distillation of Large Language Models",
    "summary": "This survey paper delves into the emerging and critical area of symbolic knowledge distillation in Large Language Models (LLMs). As LLMs like Generative Pre-trained Transformer-3 (GPT-3) and Bidirectional Encoder Representations from Transformers (BERT) continue to expand in scale and complexity, the challenge of effectively harnessing their extensive knowledge becomes paramount. This survey concentrates on the process of distilling the intricate, often implicit knowledge contained within these models into a more symbolic, explicit form. This transformation is crucial for enhancing the interpretability, efficiency, and applicability of LLMs. We categorize the existing research based on methodologies and applications, focusing on how symbolic knowledge distillation can be used to improve the transparency and functionality of smaller, more efficient Artificial Intelligence (AI) models. The survey discusses the core challenges, including maintaining the depth of knowledge in a comprehensible format, and explores the various approaches and techniques that have been developed in this field. We identify gaps in current research and potential opportunities for future advancements. This survey aims to provide a comprehensive overview of symbolic knowledge distillation in LLMs, spotlighting its significance in the progression towards more accessible and efficient AI systems.",
    "authors": [
      "Kamal Acharya",
      "Alvaro Velasquez",
      "Houbing Herbert Song"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-07-12T12:18:19Z",
    "pdf_url": "https://arxiv.org/pdf/2408.10210v1"
  },
  {
    "arxiv_id": "2407.08583v2",
    "entry_id": "http://arxiv.org/abs/2407.08583v2",
    "title": "The Synergy between Data and Multi-Modal Large Language Models: A Survey from Co-Development Perspective",
    "summary": "The rapid development of large language models (LLMs) has been witnessed in recent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the modality from text to a broader spectrum of domains, attracting widespread attention due to the broader range of application scenarios. As LLMs and MLLMs rely on vast amounts of model parameters and data to achieve emergent capabilities, the importance of data is receiving increasingly widespread attention and recognition. Tracing and analyzing recent data-oriented works for MLLMs, we find that the development of models and data is not two separate paths but rather interconnected. On the one hand, vaster and higher-quality data contribute to better performance of MLLMs; on the other hand, MLLMs can facilitate the development of data. The co-development of multi-modal data and MLLMs requires a clear view of 1) at which development stages of MLLMs specific data-centric approaches can be employed to enhance certain MLLM capabilities, and 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal data in specific roles. To promote the data-model co-development for MLLM community, we systematically review existing works related to MLLMs from the data-model co-development perspective. A regularly maintained project associated with this survey is accessible at https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.",
    "authors": [
      "Zhen Qin",
      "Daoyuan Chen",
      "Wenhao Zhang",
      "Liuyi Yao",
      "Yilun Huang",
      "Bolin Ding",
      "Yaliang Li",
      "Shuiguang Deng"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-07-11T15:08:11Z",
    "pdf_url": "https://arxiv.org/pdf/2407.08583v2"
  },
  {
    "arxiv_id": "2407.08563v1",
    "entry_id": "http://arxiv.org/abs/2407.08563v1",
    "title": "Vox Populi, Vox AI? Using Language Models to Estimate German Public Opinion",
    "summary": "The recent development of large language models (LLMs) has spurred discussions about whether LLM-generated \"synthetic samples\" could complement or replace traditional surveys, considering their training data potentially reflects attitudes and behaviors prevalent in the population. A number of mostly US-based studies have prompted LLMs to mimic survey respondents, with some of them finding that the responses closely match the survey data. However, several contextual factors related to the relationship between the respective target population and LLM training data might affect the generalizability of such findings. In this study, we investigate the extent to which LLMs can estimate public opinion in Germany, using the example of vote choice. We generate a synthetic sample of personas matching the individual characteristics of the 2017 German Longitudinal Election Study respondents. We ask the LLM GPT-3.5 to predict each respondent's vote choice and compare these predictions to the survey-based estimates on the aggregate and subgroup levels. We find that GPT-3.5 does not predict citizens' vote choice accurately, exhibiting a bias towards the Green and Left parties. While the LLM captures the tendencies of \"typical\" voter subgroups, such as partisans, it misses the multifaceted factors swaying individual voter choices. By examining the LLM-based prediction of voting behavior in a new context, our study contributes to the growing body of research about the conditions under which LLMs can be leveraged for studying public opinion. The findings point to disparities in opinion representation in LLMs and underscore the limitations in applying them for public opinion estimation.",
    "authors": [
      "Leah von der Heyde",
      "Anna-Carolina Haensch",
      "Alexander Wenz"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "stat.AP"
    ],
    "published": "2024-07-11T14:52:18Z",
    "pdf_url": "https://arxiv.org/pdf/2407.08563v1"
  },
  {
    "arxiv_id": "2407.08029v1",
    "entry_id": "http://arxiv.org/abs/2407.08029v1",
    "title": "A Critical Review of Causal Reasoning Benchmarks for Large Language Models",
    "summary": "Numerous benchmarks aim to evaluate the capabilities of Large Language Models (LLMs) for causal inference and reasoning. However, many of them can likely be solved through the retrieval of domain knowledge, questioning whether they achieve their purpose. In this review, we present a comprehensive overview of LLM benchmarks for causality. We highlight how recent benchmarks move towards a more thorough definition of causal reasoning by incorporating interventional or counterfactual reasoning. We derive a set of criteria that a useful benchmark or set of benchmarks should aim to satisfy. We hope this work will pave the way towards a general framework for the assessment of causal understanding in LLMs and the design of novel benchmarks.",
    "authors": [
      "Linying Yang",
      "Vik Shirvaikar",
      "Oscar Clivio",
      "Fabian Falck"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-07-10T20:11:51Z",
    "pdf_url": "https://arxiv.org/pdf/2407.08029v1"
  },
  {
    "arxiv_id": "2407.07966v1",
    "entry_id": "http://arxiv.org/abs/2407.07966v1",
    "title": "A Comprehensive Survey on the Security of Smart Grid: Challenges, Mitigations, and Future Research Opportunities",
    "summary": "In this study, we conduct a comprehensive review of smart grid security, exploring system architectures, attack methodologies, defense strategies, and future research opportunities. We provide an in-depth analysis of various attack vectors, focusing on new attack surfaces introduced by advanced components in smart grids. The review particularly includes an extensive analysis of coordinated attacks that incorporate multiple attack strategies and exploit vulnerabilities across various smart grid components to increase their adverse impact, demonstrating the complexity and potential severity of these threats. Following this, we examine innovative detection and mitigation strategies, including game theory, graph theory, blockchain, and machine learning, discussing their advancements in counteracting evolving threats and associated research challenges. In particular, our review covers a thorough examination of widely used machine learning-based mitigation strategies, analyzing their applications and research challenges spanning across supervised, unsupervised, semi-supervised, ensemble, and reinforcement learning. Further, we outline future research directions and explore new techniques and concerns. We first discuss the research opportunities for existing and emerging strategies, and then explore the potential role of new techniques, such as large language models (LLMs), and the emerging threat of adversarial machine learning in the future of smart grid security.",
    "authors": [
      "Arastoo Zibaeirad",
      "Farnoosh Koleini",
      "Shengping Bi",
      "Tao Hou",
      "Tao Wang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-07-10T18:03:24Z",
    "pdf_url": "https://arxiv.org/pdf/2407.07966v1"
  },
  {
    "arxiv_id": "2407.18932v2",
    "entry_id": "http://arxiv.org/abs/2407.18932v2",
    "title": "Be More Real: Travel Diary Generation Using LLM Agents and Individual Profiles",
    "summary": "Human mobility is inextricably linked to social issues such as traffic congestion, energy consumption, and public health; however, privacy concerns restrict access to mobility data. Recently, research have utilized Large Language Models (LLMs) for human mobility generation, in which the challenge is how LLMs can understand individuals' mobility behavioral differences to generate realistic trajectories conforming to real world contexts. This study handles this problem by presenting an LLM agent-based framework (MobAgent) composing two phases: understanding-based mobility pattern extraction and reasoning-based trajectory generation, which enables generate more real travel diaries at urban scale, considering different individual profiles. MobAgent extracts reasons behind specific mobility trendiness and attribute influences to provide reliable patterns; infers the relationships between contextual factors and underlying motivations of mobility; and based on the patterns and the recursive reasoning process, MobAgent finally generates more authentic and personalized mobilities that reflect both individual differences and real-world constraints. We validate our framework with 0.2 million travel survey data, demonstrating its effectiveness in producing personalized and accurate travel diaries. This study highlights the capacity of LLMs to provide detailed and sophisticated understanding of human mobility through the real-world mobility data.",
    "authors": [
      "Xuchuan Li",
      "Fei Huang",
      "Jianrong Lv",
      "Zhixiong Xiao",
      "Guolong Li",
      "Yang Yue"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-07-10T09:11:57Z",
    "pdf_url": "https://arxiv.org/pdf/2407.18932v2"
  },
  {
    "arxiv_id": "2407.12858v1",
    "entry_id": "http://arxiv.org/abs/2407.12858v1",
    "title": "Grounding and Evaluation for Large Language Models: Practical Challenges and Lessons Learned (Survey)",
    "summary": "With the ongoing rapid adoption of Artificial Intelligence (AI)-based systems in high-stakes domains, ensuring the trustworthiness, safety, and observability of these systems has become crucial. It is essential to evaluate and monitor AI systems not only for accuracy and quality-related metrics but also for robustness, bias, security, interpretability, and other responsible AI dimensions. We focus on large language models (LLMs) and other generative AI models, which present additional challenges such as hallucinations, harmful and manipulative content, and copyright infringement. In this survey article accompanying our KDD 2024 tutorial, we highlight a wide range of harms associated with generative AI systems, and survey state of the art approaches (along with open challenges) to address these harms.",
    "authors": [
      "Krishnaram Kenthapadi",
      "Mehrnoosh Sameki",
      "Ankur Taly"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-07-10T01:23:10Z",
    "pdf_url": "https://arxiv.org/pdf/2407.12858v1"
  },
  {
    "arxiv_id": "2407.07064v2",
    "entry_id": "http://arxiv.org/abs/2407.07064v2",
    "title": "Prompting Techniques for Secure Code Generation: A Systematic Investigation",
    "summary": "Large Language Models (LLMs) are gaining momentum in software development with prompt-driven programming enabling developers to create code from natural language (NL) instructions. However, studies have questioned their ability to produce secure code and, thereby, the quality of prompt-generated software. Alongside, various prompting techniques that carefully tailor prompts have emerged to elicit optimal responses from LLMs. Still, the interplay between such prompting strategies and secure code generation remains under-explored and calls for further investigations. OBJECTIVE: In this study, we investigate the impact of different prompting techniques on the security of code generated from NL instructions by LLMs. METHOD: First we perform a systematic literature review to identify the existing prompting techniques that can be used for code generation tasks. A subset of these techniques are evaluated on GPT-3, GPT-3.5, and GPT-4 models for secure code generation. For this, we used an existing dataset consisting of 150 NL security-relevant code-generation prompts. RESULTS: Our work (i) classifies potential prompting techniques for code generation (ii) adapts and evaluates a subset of the identified techniques for secure code generation tasks and (iii) observes a reduction in security weaknesses across the tested LLMs, especially after using an existing technique called Recursive Criticism and Improvement (RCI), contributing valuable insights to the ongoing discourse on LLM-generated code security.",
    "authors": [
      "Catherine Tony",
      "Nicolás E. Díaz Ferreyra",
      "Markus Mutas",
      "Salem Dhiff",
      "Riccardo Scandariato"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-07-09T17:38:03Z",
    "pdf_url": "https://arxiv.org/pdf/2407.07064v2"
  },
  {
    "arxiv_id": "2407.06992v2",
    "entry_id": "http://arxiv.org/abs/2407.06992v2",
    "title": "Robust Neural Information Retrieval: An Adversarial and Out-of-distribution Perspective",
    "summary": "Recent advances in neural information retrieval (IR) models have significantly enhanced their effectiveness over various IR tasks. The robustness of these models, essential for ensuring their reliability in practice, has also garnered significant attention. With a wide array of research on robust IR being proposed, we believe it is the opportune moment to consolidate the current status, glean insights from existing methodologies, and lay the groundwork for future development. We view the robustness of IR to be a multifaceted concept, emphasizing its necessity against adversarial attacks, out-of-distribution (OOD) scenarios and performance variance. With a focus on adversarial and OOD robustness, we dissect robustness solutions for dense retrieval models (DRMs) and neural ranking models (NRMs), respectively, recognizing them as pivotal components of the neural IR pipeline. We provide an in-depth discussion of existing methods, datasets, and evaluation metrics, shedding light on challenges and future directions in the era of large language models. To the best of our knowledge, this is the first comprehensive survey on the robustness of neural IR models, and we will also be giving our first tutorial presentation at SIGIR 2024 \\url{https://sigir2024-robust-information-retrieval.github.io}. Along with the organization of existing work, we introduce a Benchmark for robust IR (BestIR), a heterogeneous evaluation benchmark for robust neural information retrieval, which is publicly available at \\url{https://github.com/Davion-Liu/BestIR}. We hope that this study provides useful clues for future research on the robustness of IR models and helps to develop trustworthy search engines \\url{https://github.com/Davion-Liu/Awesome-Robustness-in-Information-Retrieval}.",
    "authors": [
      "Yu-An Liu",
      "Ruqing Zhang",
      "Jiafeng Guo",
      "Maarten de Rijke",
      "Yixing Fan",
      "Xueqi Cheng"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-07-09T16:07:01Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06992v2"
  },
  {
    "arxiv_id": "2407.06985v4",
    "entry_id": "http://arxiv.org/abs/2407.06985v4",
    "title": "PEER: Expertizing Domain-Specific Tasks with a Multi-Agent Framework and Tuning Methods",
    "summary": "In domain-specific applications, GPT-4, augmented with precise prompts or Retrieval-Augmented Generation (RAG), shows notable potential but faces the critical tri-lemma of performance, cost, and data privacy. High performance requires sophisticated processing techniques, yet managing multiple agents within a complex workflow often proves costly and challenging. To address this, we introduce the PEER (Plan, Execute, Express, Review) multi-agent framework. This systematizes domain-specific tasks by integrating precise question decomposition, advanced information retrieval, comprehensive summarization, and rigorous self-assessment. Given the concerns of cost and data privacy, enterprises are shifting from proprietary models like GPT-4 to custom models, striking a balance between cost, security, and performance. We developed industrial practices leveraging online data and user feedback for efficient model tuning. This study provides best practice guidelines for applying multi-agent systems in domain-specific problem-solving and implementing effective agent tuning strategies. Our empirical studies, particularly in the financial question-answering domain, demonstrate that our approach achieves 95.0% of GPT-4's performance, while effectively managing costs and ensuring data privacy.",
    "authors": [
      "Yiying Wang",
      "Xiaojing Li",
      "Binzhu Wang",
      "Yueyang Zhou",
      "Yingru Lin",
      "Han Ji",
      "Hong Chen",
      "Jinshi Zhang",
      "Fei Yu",
      "Zewei Zhao",
      "Song Jin",
      "Renji Gong",
      "Wanqing Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-07-09T15:59:28Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06985v4"
  },
  {
    "arxiv_id": "2407.19096v1",
    "entry_id": "http://arxiv.org/abs/2407.19096v1",
    "title": "AI Companions Reduce Loneliness",
    "summary": "Chatbots are now able to engage in sophisticated conversations with consumers in the domain of relationships, providing a potential coping solution to widescale societal loneliness. Behavioral research provides little insight into whether these applications are effective at alleviating loneliness. We address this question by focusing on AI companions applications designed to provide consumers with synthetic interaction partners. Studies 1 and 2 find suggestive evidence that consumers use AI companions to alleviate loneliness, by employing a novel methodology for fine tuning large language models to detect loneliness in conversations and reviews. Study 3 finds that AI companions successfully alleviate loneliness on par only with interacting with another person, and more than other activities such watching YouTube videos. Moreover, consumers underestimate the degree to which AI companions improve their loneliness. Study 4 uses a longitudinal design and finds that an AI companion consistently reduces loneliness over the course of a week. Study 5 provides evidence that both the chatbots' performance and, especially, whether it makes users feel heard, explain reductions in loneliness. Study 6 provides an additional robustness check for the loneliness alleviating benefits of AI companions.",
    "authors": [
      "Julian De Freitas",
      "Ahmet K Uguralp",
      "Zeliha O Uguralp",
      "Puntoni Stefano"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-07-09T15:04:08Z",
    "pdf_url": "https://arxiv.org/pdf/2407.19096v1"
  },
  {
    "arxiv_id": "2407.06902v2",
    "entry_id": "http://arxiv.org/abs/2407.06902v2",
    "title": "Learning From Crowdsourced Noisy Labels: A Signal Processing Perspective",
    "summary": "One of the primary catalysts fueling advances in artificial intelligence (AI) and machine learning (ML) is the availability of massive, curated datasets. A commonly used technique to curate such massive datasets is crowdsourcing, where data are dispatched to multiple annotators. The annotator-produced labels are then fused to serve downstream learning and inference tasks. This annotation process often creates noisy labels due to various reasons, such as the limited expertise, or unreliability of annotators, among others. Therefore, a core objective in crowdsourcing is to develop methods that effectively mitigate the negative impact of such label noise on learning tasks. This feature article introduces advances in learning from noisy crowdsourced labels. The focus is on key crowdsourcing models and their methodological treatments, from classical statistical models to recent deep learning-based approaches, emphasizing analytical insights and algorithmic developments. In particular, this article reviews the connections between signal processing (SP) theory and methods, such as identifiability of tensor and nonnegative matrix factorization, and novel, principled solutions of longstanding challenges in crowdsourcing -- showing how SP perspectives drive the advancements of this field. Furthermore, this article touches upon emerging topics that are critical for developing cutting-edge AI/ML systems, such as crowdsourcing in reinforcement learning with human feedback (RLHF) and direct preference optimization (DPO) that are key techniques for fine-tuning large language models (LLMs).",
    "authors": [
      "Shahana Ibrahim",
      "Panagiotis A. Traganitis",
      "Xiao Fu",
      "Georgios B. Giannakis"
    ],
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-07-09T14:34:40Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06902v2"
  },
  {
    "arxiv_id": "2407.06886v8",
    "entry_id": "http://arxiv.org/abs/2407.06886v8",
    "title": "Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI",
    "summary": "Embodied Artificial Intelligence (Embodied AI) is crucial for achieving Artificial General Intelligence (AGI) and serves as a foundation for various applications (e.g., intelligent mechatronics systems, smart manufacturing) that bridge cyberspace and the physical world. Recently, the emergence of Multi-modal Large Models (MLMs) and World Models (WMs) have attracted significant attention due to their remarkable perception, interaction, and reasoning capabilities, making them a promising architecture for embodied agents. In this survey, we give a comprehensive exploration of the latest advancements in Embodied AI. Our analysis firstly navigates through the forefront of representative works of embodied robots and simulators, to fully understand the research focuses and their limitations. Then, we analyze four main research targets: 1) embodied perception, 2) embodied interaction, 3) embodied agent, and 4) sim-to-real adaptation, covering state-of-the-art methods, essential paradigms, and comprehensive datasets. Additionally, we explore the complexities of MLMs in virtual and real embodied agents, highlighting their significance in facilitating interactions in digital and physical environments. Finally, we summarize the challenges and limitations of embodied AI and discuss potential future directions. We hope this survey will serve as a foundational reference for the research community. The associated project can be found at https://github.com/HCPLab-SYSU/Embodied_AI_Paper_List.",
    "authors": [
      "Yang Liu",
      "Weixing Chen",
      "Yongjie Bai",
      "Xiaodan Liang",
      "Guanbin Li",
      "Wen Gao",
      "Liang Lin"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.RO"
    ],
    "published": "2024-07-09T14:14:47Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06886v8"
  },
  {
    "arxiv_id": "2407.18921v2",
    "entry_id": "http://arxiv.org/abs/2407.18921v2",
    "title": "Mobile Edge Intelligence for Large Language Models: A Contemporary Survey",
    "summary": "On-device large language models (LLMs), referring to running LLMs on edge devices, have raised considerable interest since they are more cost-effective, latency-efficient, and privacy-preserving compared with the cloud paradigm. Nonetheless, the performance of on-device LLMs is intrinsically constrained by resource limitations on edge devices. Sitting between cloud and on-device AI, mobile edge intelligence (MEI) presents a viable solution by provisioning AI capabilities at the edge of mobile networks, enabling end users to offload heavy AI computation to capable edge servers nearby. This article provides a contemporary survey on harnessing MEI for LLMs. We begin by illustrating several killer applications to demonstrate the urgent need for deploying LLMs at the network edge. Next, we present the preliminaries of LLMs and MEI, followed by resource-efficient LLM techniques. We then present an architectural overview of MEI for LLMs (MEI4LLM), outlining its core components and how it supports the deployment of LLMs. Subsequently, we delve into various aspects of MEI4LLM, extensively covering edge LLM caching and delivery, edge LLM training, and edge LLM inference. Finally, we identify future research opportunities. We hope this article inspires researchers in the field to leverage mobile edge computing to facilitate LLM deployment, thereby unleashing the potential of LLMs across various privacy- and delay-sensitive applications.",
    "authors": [
      "Guanqiao Qu",
      "Qiyuan Chen",
      "Wei Wei",
      "Zheng Lin",
      "Xianhao Chen",
      "Kaibin Huang"
    ],
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-07-09T13:47:05Z",
    "pdf_url": "https://arxiv.org/pdf/2407.18921v2"
  },
  {
    "arxiv_id": "2407.11054v3",
    "entry_id": "http://arxiv.org/abs/2407.11054v3",
    "title": "Generative AI for Health Technology Assessment: Opportunities, Challenges, and Policy Considerations",
    "summary": "This review introduces the transformative potential of generative Artificial Intelligence (AI) and foundation models, including large language models (LLMs), for health technology assessment (HTA). We explore their applications in four critical areas, evidence synthesis, evidence generation, clinical trials and economic modeling: (1) Evidence synthesis: Generative AI has the potential to assist in automating literature reviews and meta-analyses by proposing search terms, screening abstracts, and extracting data with notable accuracy; (2) Evidence generation: These models can potentially facilitate automating the process and analyze the increasingly available large collections of real-world data (RWD), including unstructured clinical notes and imaging, enhancing the speed and quality of real-world evidence (RWE) generation; (3) Clinical trials: Generative AI can be used to optimize trial design, improve patient matching, and manage trial data more efficiently; and (4) Economic modeling: Generative AI can also aid in the development of health economic models, from conceptualization to validation, thus streamlining the overall HTA process. Despite their promise, these technologies, while rapidly improving, are still nascent and continued careful evaluation in their applications to HTA is required. To ensure their responsible use and implementation, both developers and users of research incorporating these tools, should familiarize themselves with their current limitations, including the issues related to scientific validity, risk of bias, and consider equity and ethical implications. We also surveyed the current policy landscape and provide suggestions for HTA agencies on responsibly integrating generative AI into their workflows, emphasizing the importance of human oversight and the fast-evolving nature of these tools.",
    "authors": [
      "Rachael Fleurence",
      "Jiang Bian",
      "Xiaoyan Wang",
      "Hua Xu",
      "Dalia Dawoud",
      "Mitch Higashi",
      "Jagpreet Chhatwal"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-07-09T09:25:27Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11054v3"
  },
  {
    "arxiv_id": "2407.06576v3",
    "entry_id": "http://arxiv.org/abs/2407.06576v3",
    "title": "Virtual Personas for Language Models via an Anthology of Backstories",
    "summary": "Large language models (LLMs) are trained from vast repositories of text authored by millions of distinct authors, reflecting an enormous diversity of human traits. While these models bear the potential to be used as approximations of human subjects in behavioral studies, prior efforts have been limited in steering model responses to match individual human users. In this work, we introduce \"Anthology\", a method for conditioning LLMs to particular virtual personas by harnessing open-ended life narratives, which we refer to as \"backstories.\" We show that our methodology enhances the consistency and reliability of experimental outcomes while ensuring better representation of diverse sub-populations. Across three nationally representative human surveys conducted as part of Pew Research Center's American Trends Panel (ATP), we demonstrate that Anthology achieves up to 18% improvement in matching the response distributions of human respondents and 27% improvement in consistency metrics. Our code and generated backstories are available at https://github.com/CannyLab/anthology.",
    "authors": [
      "Suhong Moon",
      "Marwa Abdulhai",
      "Minwoo Kang",
      "Joseph Suh",
      "Widyadewi Soedarmadji",
      "Eran Kohen Behar",
      "David M. Chan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-09T06:11:18Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06576v3"
  },
  {
    "arxiv_id": "2407.06485v1",
    "entry_id": "http://arxiv.org/abs/2407.06485v1",
    "title": "CrowdTransfer: Enabling Crowd Knowledge Transfer in AIoT Community",
    "summary": "Artificial Intelligence of Things (AIoT) is an emerging frontier based on the deep fusion of Internet of Things (IoT) and Artificial Intelligence (AI) technologies. Although advanced deep learning techniques enhance the efficient data processing and intelligent analysis of complex IoT data, they still suffer from notable challenges when deployed to practical AIoT applications, such as constrained resources, and diverse task requirements. Knowledge transfer is an effective method to enhance learning performance by avoiding the exorbitant costs associated with data recollection and model retraining. Notably, although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances of various knowledge transfer techniques for AIoT field. This survey endeavors to introduce a new concept of knowledge transfer, referred to as Crowd Knowledge Transfer (CrowdTransfer), which aims to transfer prior knowledge learned from a crowd of agents to reduce the training cost and as well as improve the performance of the model in real-world complicated scenarios. Particularly, we present four transfer modes from the perspective of crowd intelligence, including derivation, sharing, evolution and fusion modes. Building upon conventional transfer learning methods, we further delve into advanced crowd knowledge transfer models from three perspectives for various AIoT applications. Furthermore, we explore some applications of AIoT areas, such as human activity recognition, urban computing, multi-robot system, and smart factory. Finally, we discuss the open issues and outline future research directions of knowledge transfer in AIoT community.",
    "authors": [
      "Yan Liu",
      "Bin Guo",
      "Nuo Li",
      "Yasan Ding",
      "Zhouyangzi Zhang",
      "Zhiwen Yu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-07-09T01:20:37Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06485v1"
  },
  {
    "arxiv_id": "2407.06177v1",
    "entry_id": "http://arxiv.org/abs/2407.06177v1",
    "title": "Vision-Language Models under Cultural and Inclusive Considerations",
    "summary": "Large vision-language models (VLMs) can assist visually impaired people by describing images from their daily lives. Current evaluation datasets may not reflect diverse cultural user backgrounds or the situational context of this use case. To address this problem, we create a survey to determine caption preferences and propose a culture-centric evaluation benchmark by filtering VizWiz, an existing dataset with images taken by people who are blind. We then evaluate several VLMs, investigating their reliability as visual assistants in a culturally diverse setting. While our results for state-of-the-art models are promising, we identify challenges such as hallucination and misalignment of automatic evaluation metrics with human judgment. We make our survey, data, code, and model outputs publicly available.",
    "authors": [
      "Antonia Karamolegkou",
      "Phillip Rust",
      "Yong Cao",
      "Ruixiang Cui",
      "Anders Søgaard",
      "Daniel Hershcovich"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2024-07-08T17:50:00Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06177v1"
  },
  {
    "arxiv_id": "2407.05965v3",
    "entry_id": "http://arxiv.org/abs/2407.05965v3",
    "title": "T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models",
    "summary": "The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its security risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover fewer aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, a new benchmark designed for conducting safety-critical assessments of text-to-video models. We define 12 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts and jailbreak attack-based prompts. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AI.",
    "authors": [
      "Yibo Miao",
      "Yifan Zhu",
      "Yinpeng Dong",
      "Lijia Yu",
      "Jun Zhu",
      "Xiao-Shan Gao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-07-08T14:04:58Z",
    "pdf_url": "https://arxiv.org/pdf/2407.05965v3"
  },
  {
    "arxiv_id": "2407.11046v4",
    "entry_id": "http://arxiv.org/abs/2407.11046v4",
    "title": "A Survey on LoRA of Large Language Models",
    "summary": "Low-Rank Adaptation~(LoRA), which updates the dense neural network layers with pluggable low-rank matrices, is one of the best performed parameter efficient fine-tuning paradigms. Furthermore, it has significant advantages in cross-task generalization and privacy-preserving. Hence, LoRA has gained much attention recently, and the number of related literature demonstrates exponential growth. It is necessary to conduct a comprehensive overview of the current progress on LoRA. This survey categorizes and reviews the progress from the perspectives of (1) downstream adaptation improving variants that improve LoRA's performance on downstream tasks; (2) cross-task generalization methods that mix multiple LoRA plugins to achieve cross-task generalization; (3) efficiency-improving methods that boost the computation-efficiency of LoRA; (4) data privacy-preserving methods that use LoRA in federated learning; (5) application. Besides, this survey also discusses the future directions in this field. At last, we provide a Github page~\\footnote{\\href{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}{https://github.com/ZJU-LLMs/Awesome-LoRAs.git}} for readers to check the updates and initiate discussions on this survey paper.",
    "authors": [
      "Yuren Mao",
      "Yuhang Ge",
      "Yijiang Fan",
      "Wenyi Xu",
      "Yu Mi",
      "Zhonghao Hu",
      "Yunjun Gao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-07-08T12:32:10Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11046v4"
  },
  {
    "arxiv_id": "2407.04903v3",
    "entry_id": "http://arxiv.org/abs/2407.04903v3",
    "title": "MMSci: A Dataset for Graduate-Level Multi-Discipline Multimodal Scientific Understanding",
    "summary": "Scientific figure interpretation is a crucial capability for AI-driven scientific assistants built on advanced Large Vision Language Models. However, current datasets and benchmarks primarily focus on simple charts or other relatively straightforward figures from limited science domains. To address this gap, we present a comprehensive dataset compiled from peer-reviewed Nature Communications articles covering 72 scientific fields, encompassing complex visualizations such as schematic diagrams, microscopic images, and experimental data which require graduate-level expertise to interpret. We evaluated 19 proprietary and open-source models on two benchmark tasks, figure captioning and multiple-choice, and conducted human expert annotation. Our analysis revealed significant task challenges and performance gaps among models. Beyond serving as a benchmark, this dataset serves as a valuable resource for large-scale training. Fine-tuning Qwen2-VL-7B with our task-specific data achieved better performance than GPT-4o and even human experts in multiple-choice evaluations. Furthermore, continuous pre-training on our interleaved article and figure data substantially enhanced the model's downstream task performance in materials science. We have released our dataset to support further research.",
    "authors": [
      "Zekun Li",
      "Xianjun Yang",
      "Kyuri Choi",
      "Wanrong Zhu",
      "Ryan Hsieh",
      "HyeonJung Kim",
      "Jin Hyuk Lim",
      "Sungyoung Ji",
      "Byungju Lee",
      "Xifeng Yan",
      "Linda Ruth Petzold",
      "Stephen D. Wilson",
      "Woosang Lim",
      "William Yang Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-07-06T00:40:53Z",
    "pdf_url": "https://arxiv.org/pdf/2407.04903v3"
  },
  {
    "arxiv_id": "2407.04295v2",
    "entry_id": "http://arxiv.org/abs/2407.04295v2",
    "title": "Jailbreak Attacks and Defenses Against Large Language Models: A Survey",
    "summary": "Large Language Models (LLMs) have performed exceptionally in various text-generative tasks, including question answering, translation, code completion, etc. However, the over-assistance of LLMs has raised the challenge of \"jailbreaking\", which induces the model to generate malicious responses against the usage policy and society by designing adversarial prompts. With the emergence of jailbreak attack methods exploiting different vulnerabilities in LLMs, the corresponding safety alignment measures are also evolving. In this paper, we propose a comprehensive and detailed taxonomy of jailbreak attack and defense methods. For instance, the attack methods are divided into black-box and white-box attacks based on the transparency of the target model. Meanwhile, we classify defense methods into prompt-level and model-level defenses. Additionally, we further subdivide these attack and defense methods into distinct sub-classes and present a coherent diagram illustrating their relationships. We also conduct an investigation into the current evaluation methods and compare them from different perspectives. Our findings aim to inspire future research and practical implementations in safeguarding LLMs against adversarial attacks. Above all, although jailbreak remains a significant concern within the community, we believe that our work enhances the understanding of this domain and provides a foundation for developing more secure LLMs.",
    "authors": [
      "Sibo Yi",
      "Yule Liu",
      "Zhen Sun",
      "Tianshuo Cong",
      "Xinlei He",
      "Jiaxing Song",
      "Ke Xu",
      "Qi Li"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-07-05T06:57:30Z",
    "pdf_url": "https://arxiv.org/pdf/2407.04295v2"
  },
  {
    "arxiv_id": "2407.04069v2",
    "entry_id": "http://arxiv.org/abs/2407.04069v2",
    "title": "A Systematic Survey and Critical Review on Evaluating Large Language Models: Challenges, Limitations, and Recommendations",
    "summary": "Large Language Models (LLMs) have recently gained significant attention due to their remarkable capabilities in performing diverse tasks across various domains. However, a thorough evaluation of these models is crucial before deploying them in real-world applications to ensure they produce reliable performance. Despite the well-established importance of evaluating LLMs in the community, the complexity of the evaluation process has led to varied evaluation setups, causing inconsistencies in findings and interpretations. To address this, we systematically review the primary challenges and limitations causing these inconsistencies and unreliable evaluations in various steps of LLM evaluation. Based on our critical review, we present our perspectives and recommendations to ensure LLM evaluations are reproducible, reliable, and robust.",
    "authors": [
      "Md Tahmid Rahman Laskar",
      "Sawsan Alqahtani",
      "M Saiful Bari",
      "Mizanur Rahman",
      "Mohammad Abdullah Matin Khan",
      "Haidar Khan",
      "Israt Jahan",
      "Amran Bhuiyan",
      "Chee Wei Tan",
      "Md Rizwan Parvez",
      "Enamul Hoque",
      "Shafiq Joty",
      "Jimmy Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-07-04T17:15:37Z",
    "pdf_url": "https://arxiv.org/pdf/2407.04069v2"
  },
  {
    "arxiv_id": "2407.06083v3",
    "entry_id": "http://arxiv.org/abs/2407.06083v3",
    "title": "A Survey of Controllable Learning: Methods and Applications in Information Retrieval",
    "summary": "Controllability has become a crucial aspect of trustworthy machine learning, enabling learners to meet predefined targets and adapt dynamically at test time without requiring retraining as the targets shift. We provide a formal definition of controllable learning (CL), and discuss its applications in information retrieval (IR) where information needs are often complex and dynamic. The survey categorizes CL according to what is controllable (e.g., multiple objectives, user portrait, scenario adaptation), who controls (users or platforms), how control is implemented (e.g., rule-based method, Pareto optimization, hypernetwork and others), and where to implement control (e.g., pre-processing, in-processing, post-processing methods). Then, we identify challenges faced by CL across training, evaluation, task setting, and deployment in online environments. Additionally, we outline promising directions for CL in theoretical analysis, efficient computation, empowering large language models, application scenarios and evaluation frameworks.",
    "authors": [
      "Chenglei Shen",
      "Xiao Zhang",
      "Teng Shi",
      "Changshuo Zhang",
      "Guofu Xie",
      "Jun Xu"
    ],
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "published": "2024-07-04T09:50:50Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06083v3"
  },
  {
    "arxiv_id": "2407.03640v1",
    "entry_id": "http://arxiv.org/abs/2407.03640v1",
    "title": "Generative Technology for Human Emotion Recognition: A Scope Review",
    "summary": "Affective computing stands at the forefront of artificial intelligence (AI), seeking to imbue machines with the ability to comprehend and respond to human emotions. Central to this field is emotion recognition, which endeavors to identify and interpret human emotional states from different modalities, such as speech, facial images, text, and physiological signals. In recent years, important progress has been made in generative models, including Autoencoder, Generative Adversarial Network, Diffusion Model, and Large Language Model. These models, with their powerful data generation capabilities, emerge as pivotal tools in advancing emotion recognition. However, up to now, there remains a paucity of systematic efforts that review generative technology for emotion recognition. This survey aims to bridge the gaps in the existing literature by conducting a comprehensive analysis of over 320 research papers until June 2024. Specifically, this survey will firstly introduce the mathematical principles of different generative models and the commonly used datasets. Subsequently, through a taxonomy, it will provide an in-depth analysis of how generative techniques address emotion recognition based on different modalities in several aspects, including data augmentation, feature extraction, semi-supervised learning, cross-domain, etc. Finally, the review will outline future research directions, emphasizing the potential of generative models to advance the field of emotion recognition and enhance the emotional intelligence of AI systems.",
    "authors": [
      "Fei Ma",
      "Yucheng Yuan",
      "Yifan Xie",
      "Hongwei Ren",
      "Ivan Liu",
      "Ying He",
      "Fuji Ren",
      "Fei Richard Yu",
      "Shiguang Ni"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-07-04T05:22:55Z",
    "pdf_url": "https://arxiv.org/pdf/2407.03640v1"
  },
  {
    "arxiv_id": "2407.02996v2",
    "entry_id": "http://arxiv.org/abs/2407.02996v2",
    "title": "Are Large Language Models Consistent over Value-laden Questions?",
    "summary": "Large language models (LLMs) appear to bias their survey answers toward certain values. Nonetheless, some argue that LLMs are too inconsistent to simulate particular values. Are they? To answer, we first define value consistency as the similarity of answers across (1) paraphrases of one question, (2) related questions under one topic, (3) multiple-choice and open-ended use-cases of one question, and (4) multilingual translations of a question to English, Chinese, German, and Japanese. We apply these measures to small and large, open LLMs including llama-3, as well as gpt-4o, using 8,000 questions spanning more than 300 topics. Unlike prior work, we find that models are relatively consistent across paraphrases, use-cases, translations, and within a topic. Still, some inconsistencies remain. Models are more consistent on uncontroversial topics (e.g., in the U.S., \"Thanksgiving\") than on controversial ones (\"euthanasia\"). Base models are both more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics (\"euthanasia\") than others (\"women's rights\") like our human subjects (n=165).",
    "authors": [
      "Jared Moore",
      "Tanvi Deshpande",
      "Diyi Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-03T10:53:54Z",
    "pdf_url": "https://arxiv.org/pdf/2407.02996v2"
  },
  {
    "arxiv_id": "2407.02425v2",
    "entry_id": "http://arxiv.org/abs/2407.02425v2",
    "title": "Reinforcement Learning and Machine ethics:a systematic review",
    "summary": "Machine ethics is the field that studies how ethical behaviour can be accomplished by autonomous systems. While there exist some systematic reviews aiming to consolidate the state of the art in machine ethics prior to 2020, these tend to not include work that uses reinforcement learning agents as entities whose ethical behaviour is to be achieved. The reason for this is that only in the last years we have witnessed an increase in machine ethics studies within reinforcement learning. We present here a systematic review of reinforcement learning for machine ethics and machine ethics within reinforcement learning. Additionally, we highlight trends in terms of ethics specifications, components and frameworks of reinforcement learning, and environments used to result in ethical behaviour. Our systematic review aims to consolidate the work in machine ethics and reinforcement learning thus completing the gap in the state of the art machine ethics landscape",
    "authors": [
      "Ajay Vishwanath",
      "Louise A. Dennis",
      "Marija Slavkovik"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-07-02T16:54:00Z",
    "pdf_url": "https://arxiv.org/pdf/2407.02425v2"
  },
  {
    "arxiv_id": "2407.02354v1",
    "entry_id": "http://arxiv.org/abs/2407.02354v1",
    "title": "Talking to Machines: do you read me?",
    "summary": "In this dissertation I would like to guide the reader to the research on dialogue but more precisely the research I have conducted during my career since my PhD thesis. Starting from modular architectures with machine learning/deep learning and reinforcement learning to end-to-end deep neural networks. Besides my work as research associate, I also present the work I have supervised in the last years.\n  I review briefly the state of the art and highlight the open research problems on conversational agents. Afterwards, I present my contribution to Task-Oriented Dialogues (TOD), both as research associate and as the industrial supervisor of CIFRE theses.  I discuss conversational QA. Particularly, I present the work of two PhD candidates Thibault Cordier and Sebastien Montella; as well as the work of the young researcher Quentin Brabant. Finally, I present the scientific project, where I discuss about Large Language Models (LLMs) for Task-Oriented Dialogue and Multimodal Task-Oriented Dialogue.",
    "authors": [
      "Lina M. Rojas-Barahona"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-07-02T15:19:46Z",
    "pdf_url": "https://arxiv.org/pdf/2407.02354v1"
  },
  {
    "arxiv_id": "2407.12826v1",
    "entry_id": "http://arxiv.org/abs/2407.12826v1",
    "title": "Assessing the Effectiveness of GPT-4o in Climate Change Evidence Synthesis and Systematic Assessments: Preliminary Insights",
    "summary": "In this research short, we examine the potential of using GPT-4o, a state-of-the-art large language model (LLM) to undertake evidence synthesis and systematic assessment tasks. Traditional workflows for such tasks involve large groups of domain experts who manually review and synthesize vast amounts of literature. The exponential growth of scientific literature and recent advances in LLMs provide an opportunity to complementing these traditional workflows with new age tools. We assess the efficacy of GPT-4o to do these tasks on a sample from the dataset created by the Global Adaptation Mapping Initiative (GAMI) where we check the accuracy of climate change adaptation related feature extraction from the scientific literature across three levels of expertise. Our results indicate that while GPT-4o can achieve high accuracy in low-expertise tasks like geographic location identification, their performance in intermediate and high-expertise tasks, such as stakeholder identification and assessment of depth of the adaptation response, is less reliable. The findings motivate the need for designing assessment workflows that utilize the strengths of models like GPT-4o while also providing refinements to improve their performance on these tasks.",
    "authors": [
      "Elphin Tom Joe",
      "Sai Dileep Koneru",
      "Christine J Kirchhoff"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-02T13:14:57Z",
    "pdf_url": "https://arxiv.org/pdf/2407.12826v1"
  },
  {
    "arxiv_id": "2407.02209v1",
    "entry_id": "http://arxiv.org/abs/2407.02209v1",
    "title": "Generative Monoculture in Large Language Models",
    "summary": "We introduce {\\em generative monoculture}, a behavior observed in large language models (LLMs) characterized by a significant narrowing of model output diversity relative to available training data for a given task: for example, generating only positive book reviews for books with a mixed reception. While in some cases, generative monoculture enhances performance (e.g., LLMs more often produce efficient code), the dangers are exacerbated in others (e.g., LLMs refuse to share diverse opinions). As LLMs are increasingly used in high-impact settings such as education and web search, careful maintenance of LLM output diversity is essential to ensure a variety of facts and perspectives are preserved over time. We experimentally demonstrate the prevalence of generative monoculture through analysis of book review and code generation tasks, and find that simple countermeasures such as altering sampling or prompting strategies are insufficient to mitigate the behavior. Moreover, our results suggest that the root causes of generative monoculture are likely embedded within the LLM's alignment processes, suggesting a need for developing fine-tuning paradigms that preserve or promote diversity.",
    "authors": [
      "Fan Wu",
      "Emily Black",
      "Varun Chandrasekaran"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-02T12:17:07Z",
    "pdf_url": "https://arxiv.org/pdf/2407.02209v1"
  },
  {
    "arxiv_id": "2407.01885v1",
    "entry_id": "http://arxiv.org/abs/2407.01885v1",
    "title": "Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application",
    "summary": "Large Language Models (LLMs) have showcased exceptional capabilities in various domains, attracting significant interest from both academia and industry. Despite their impressive performance, the substantial size and computational demands of LLMs pose considerable challenges for practical deployment, particularly in environments with limited resources. The endeavor to compress language models while maintaining their accuracy has become a focal point of research. Among the various methods, knowledge distillation has emerged as an effective technique to enhance inference speed without greatly compromising performance. This paper presents a thorough survey from three aspects: method, evaluation, and application, exploring knowledge distillation techniques tailored specifically for LLMs. Specifically, we divide the methods into white-box KD and black-box KD to better illustrate their differences. Furthermore, we also explored the evaluation tasks and distillation effects between different distillation methods, and proposed directions for future research. Through in-depth understanding of the latest advancements and practical applications, this survey provides valuable resources for researchers, paving the way for sustained progress in this field.",
    "authors": [
      "Chuanpeng Yang",
      "Wang Lu",
      "Yao Zhu",
      "Yidong Wang",
      "Qian Chen",
      "Chenlong Gao",
      "Bingjie Yan",
      "Yiqiang Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-02T02:14:42Z",
    "pdf_url": "https://arxiv.org/pdf/2407.01885v1"
  },
  {
    "arxiv_id": "2407.00942v1",
    "entry_id": "http://arxiv.org/abs/2407.00942v1",
    "title": "ProductAgent: Benchmarking Conversational Product Search Agent with Asking Clarification Questions",
    "summary": "This paper introduces the task of product demand clarification within an e-commercial scenario, where the user commences the conversation with ambiguous queries and the task-oriented agent is designed to achieve more accurate and tailored product searching by asking clarification questions. To address this task, we propose ProductAgent, a conversational information seeking agent equipped with abilities of strategic clarification question generation and dynamic product retrieval. Specifically, we develop the agent with strategies for product feature summarization, query generation, and product retrieval. Furthermore, we propose the benchmark called PROCLARE to evaluate the agent's performance both automatically and qualitatively with the aid of a LLM-driven user simulator. Experiments show that ProductAgent interacts positively with the user and enhances retrieval performance with increasing dialogue turns, where user demands become gradually more explicit and detailed. All the source codes will be released after the review anonymity period.",
    "authors": [
      "Jingheng Ye",
      "Yong Jiang",
      "Xiaobin Wang",
      "Yinghui Li",
      "Yangning Li",
      "Hai-Tao Zheng",
      "Pengjun Xie",
      "Fei Huang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-07-01T03:50:23Z",
    "pdf_url": "https://arxiv.org/pdf/2407.00942v1"
  },
  {
    "arxiv_id": "2407.00936v5",
    "entry_id": "http://arxiv.org/abs/2407.00936v5",
    "title": "Large Language Model Enhanced Knowledge Representation Learning: A Survey",
    "summary": "Knowledge Representation Learning (KRL) is crucial for enabling applications of symbolic knowledge from Knowledge Graphs (KGs) to downstream tasks by projecting knowledge facts into vector spaces. Despite their effectiveness in modeling KG structural information, KRL methods are suffering from the sparseness of KGs. The rise of Large Language Models (LLMs) built on the Transformer architecture presents promising opportunities for enhancing KRL by incorporating textual information to address information sparsity in KGs. LLM-enhanced KRL methods, including three key approaches, encoder-based methods that leverage detailed contextual information, encoder-decoder-based methods that utilize a unified Seq2Seq model for comprehensive encoding and decoding, and decoder-based methods that utilize extensive knowledge from large corpora, have significantly advanced the effectiveness and generalization of KRL in addressing a wide range of downstream tasks. This work provides a broad overview of downstream tasks while simultaneously identifying emerging research directions in these evolving domains.",
    "authors": [
      "Xin Wang",
      "Zirui Chen",
      "Haofen Wang",
      "Leong Hou U",
      "Zhao Li",
      "Wenbin Guo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-07-01T03:37:35Z",
    "pdf_url": "https://arxiv.org/pdf/2407.00936v5"
  },
  {
    "arxiv_id": "2407.00541v1",
    "entry_id": "http://arxiv.org/abs/2407.00541v1",
    "title": "Answering real-world clinical questions using large language model based systems",
    "summary": "Evidence to guide healthcare decisions is often limited by a lack of relevant and trustworthy literature as well as difficulty in contextualizing existing research for a specific patient. Large language models (LLMs) could potentially address both challenges by either summarizing published literature or generating new studies based on real-world data (RWD). We evaluated the ability of five LLM-based systems in answering 50 clinical questions and had nine independent physicians review the responses for relevance, reliability, and actionability. As it stands, general-purpose LLMs (ChatGPT-4, Claude 3 Opus, Gemini Pro 1.5) rarely produced answers that were deemed relevant and evidence-based (2% - 10%). In contrast, retrieval augmented generation (RAG)-based and agentic LLM systems produced relevant and evidence-based answers for 24% (OpenEvidence) to 58% (ChatRWD) of questions. Only the agentic ChatRWD was able to answer novel questions compared to other LLMs (65% vs. 0-9%). These results suggest that while general-purpose LLMs should not be used as-is, a purpose-built system for evidence summarization based on RAG and one for generating novel evidence working synergistically would improve availability of pertinent evidence for patient care.",
    "authors": [
      "Yen Sia Low",
      "Michael L. Jackson",
      "Rebecca J. Hyde",
      "Robert E. Brown",
      "Neil M. Sanghavi",
      "Julian D. Baldwin",
      "C. William Pike",
      "Jananee Muralidharan",
      "Gavin Hui",
      "Natasha Alexander",
      "Hadeel Hassan",
      "Rahul V. Nene",
      "Morgan Pike",
      "Courtney J. Pokrzywa",
      "Shivam Vedak",
      "Adam Paul Yan",
      "Dong-han Yao",
      "Amy R. Zipursky",
      "Christina Dinh",
      "Philip Ballentine",
      "Dan C. Derieg",
      "Vladimir Polony",
      "Rehan N. Chawdry",
      "Jordan Davies",
      "Brigham B. Hyde",
      "Nigam H. Shah",
      "Saurabh Gombar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-06-29T22:39:20Z",
    "pdf_url": "https://arxiv.org/pdf/2407.00541v1"
  },
  {
    "arxiv_id": "2407.00215v1",
    "entry_id": "http://arxiv.org/abs/2407.00215v1",
    "title": "LLM Critics Help Catch LLM Bugs",
    "summary": "Reinforcement learning from human feedback (RLHF) is fundamentally limited by the capacity of humans to correctly evaluate model output. To improve human evaluation ability and overcome that limitation this work trains \"critic\" models that help humans to more accurately evaluate model-written code. These critics are themselves LLMs trained with RLHF to write natural language feedback highlighting problems in code from real-world assistant tasks. On code containing naturally occurring LLM errors model-written critiques are preferred over human critiques in 63% of cases, and human evaluation finds that models catch more bugs than human contractors paid for code review. We further confirm that our fine-tuned LLM critics can successfully identify hundreds of errors in ChatGPT training data rated as \"flawless\", even though the majority of those tasks are non-code tasks and thus out-of-distribution for the critic model. Critics can have limitations of their own, including hallucinated bugs that could mislead humans into making mistakes they might have otherwise avoided, but human-machine teams of critics and contractors catch similar numbers of bugs to LLM critics while hallucinating less than LLMs alone.",
    "authors": [
      "Nat McAleese",
      "Rai Michael Pokorny",
      "Juan Felipe Ceron Uribe",
      "Evgenia Nitishinskaya",
      "Maja Trebacz",
      "Jan Leike"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2024-06-28T19:53:17Z",
    "pdf_url": "https://arxiv.org/pdf/2407.00215v1"
  },
  {
    "arxiv_id": "2406.19614v1",
    "entry_id": "http://arxiv.org/abs/2406.19614v1",
    "title": "A Survey on Data Quality Dimensions and Tools for Machine Learning",
    "summary": "Machine learning (ML) technologies have become substantial in practically all aspects of our society, and data quality (DQ) is critical for the performance, fairness, robustness, safety, and scalability of ML models. With the large and complex data in data-centric AI, traditional methods like exploratory data analysis (EDA) and cross-validation (CV) face challenges, highlighting the importance of mastering DQ tools. In this survey, we review 17 DQ evaluation and improvement tools in the last 5 years. By introducing the DQ dimensions, metrics, and main functions embedded in these tools, we compare their strengths and limitations and propose a roadmap for developing open-source DQ tools for ML. Based on the discussions on the challenges and emerging trends, we further highlight the potential applications of large language models (LLMs) and generative AI in DQ evaluation and improvement for ML. We believe this comprehensive survey can enhance understanding of DQ in ML and could drive progress in data-centric AI. A complete list of the literature investigated in this survey is available on GitHub at: https://github.com/haihua0913/awesome-dq4ml.",
    "authors": [
      "Yuhan Zhou",
      "Fengjiao Tu",
      "Kewei Sha",
      "Junhua Ding",
      "Haihua Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-06-28T02:41:33Z",
    "pdf_url": "https://arxiv.org/pdf/2406.19614v1"
  },
  {
    "arxiv_id": "2407.00125v1",
    "entry_id": "http://arxiv.org/abs/2407.00125v1",
    "title": "A Survey on Failure Analysis and Fault Injection in AI Systems",
    "summary": "The rapid advancement of Artificial Intelligence (AI) has led to its integration into various areas, especially with Large Language Models (LLMs) significantly enhancing capabilities in Artificial Intelligence Generated Content (AIGC). However, the complexity of AI systems has also exposed their vulnerabilities, necessitating robust methods for failure analysis (FA) and fault injection (FI) to ensure resilience and reliability. Despite the importance of these techniques, there lacks a comprehensive review of FA and FI methodologies in AI systems. This study fills this gap by presenting a detailed survey of existing FA and FI approaches across six layers of AI systems. We systematically analyze 160 papers and repositories to answer three research questions including (1) what are the prevalent failures in AI systems, (2) what types of faults can current FI tools simulate, (3) what gaps exist between the simulated faults and real-world failures. Our findings reveal a taxonomy of AI system failures, assess the capabilities of existing FI tools, and highlight discrepancies between real-world and simulated failures. Moreover, this survey contributes to the field by providing a framework for fault diagnosis, evaluating the state-of-the-art in FI, and identifying areas for improvement in FI techniques to enhance the resilience of AI systems.",
    "authors": [
      "Guangba Yu",
      "Gou Tan",
      "Haojia Huang",
      "Zhenyu Zhang",
      "Pengfei Chen",
      "Roberto Natella",
      "Zibin Zheng"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2024-06-28T00:32:03Z",
    "pdf_url": "https://arxiv.org/pdf/2407.00125v1"
  },
  {
    "arxiv_id": "2406.19512v1",
    "entry_id": "http://arxiv.org/abs/2406.19512v1",
    "title": "Captioning Visualizations with Large Language Models (CVLLM): A Tutorial",
    "summary": "Automatically captioning visualizations is not new, but recent advances in large language models(LLMs) open exciting new possibilities. In this tutorial, after providing a brief review of Information Visualization (InfoVis) principles and past work in captioning, we introduce neural models and the transformer architecture used in generic LLMs. We then discuss their recent applications in InfoVis, with a focus on captioning. Additionally, we explore promising future directions in this field.",
    "authors": [
      "Giuseppe Carenini",
      "Jordon Johnson",
      "Ali Salamatian"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-06-27T20:18:18Z",
    "pdf_url": "https://arxiv.org/pdf/2406.19512v1"
  },
  {
    "arxiv_id": "2406.19317v2",
    "entry_id": "http://arxiv.org/abs/2406.19317v2",
    "title": "Jump Starting Bandits with LLM-Generated Prior Knowledge",
    "summary": "We present substantial evidence demonstrating the benefits of integrating Large Language Models (LLMs) with a Contextual Multi-Armed Bandit framework. Contextual bandits have been widely used in recommendation systems to generate personalized suggestions based on user-specific contexts. We show that LLMs, pre-trained on extensive corpora rich in human knowledge and preferences, can simulate human behaviours well enough to jump-start contextual multi-armed bandits to reduce online learning regret. We propose an initialization algorithm for contextual bandits by prompting LLMs to produce a pre-training dataset of approximate human preferences for the bandit. This significantly reduces online learning regret and data-gathering costs for training such models. Our approach is validated empirically through two sets of experiments with different bandit setups: one which utilizes LLMs to serve as an oracle and a real-world experiment utilizing data from a conjoint survey experiment.",
    "authors": [
      "Parand A. Alamdari",
      "Yanshuai Cao",
      "Kevin H. Wilson"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-06-27T16:52:19Z",
    "pdf_url": "https://arxiv.org/pdf/2406.19317v2"
  },
  {
    "arxiv_id": "2407.00118v1",
    "entry_id": "http://arxiv.org/abs/2407.00118v1",
    "title": "From Efficient Multimodal Models to World Models: A Survey",
    "summary": "Multimodal Large Models (MLMs) are becoming a significant research focus, combining powerful large language models with multimodal learning to perform complex tasks across different data modalities. This review explores the latest developments and challenges in MLMs, emphasizing their potential in achieving artificial general intelligence and as a pathway to world models. We provide an overview of key techniques such as Multimodal Chain of Thought (M-COT), Multimodal Instruction Tuning (M-IT), and Multimodal In-Context Learning (M-ICL). Additionally, we discuss both the fundamental and specific technologies of multimodal models, highlighting their applications, input/output modalities, and design characteristics. Despite significant advancements, the development of a unified multimodal model remains elusive. We discuss the integration of 3D generation and embodied intelligence to enhance world simulation capabilities and propose incorporating external rule systems for improved reasoning and decision-making. Finally, we outline future research directions to address these challenges and advance the field.",
    "authors": [
      "Xinji Mai",
      "Zeng Tao",
      "Junxiong Lin",
      "Haoran Wang",
      "Yang Chang",
      "Yanlan Kang",
      "Yan Wang",
      "Wenqiang Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-06-27T15:36:43Z",
    "pdf_url": "https://arxiv.org/pdf/2407.00118v1"
  },
  {
    "arxiv_id": "2406.19238v3",
    "entry_id": "http://arxiv.org/abs/2406.19238v3",
    "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
    "summary": "Uncovering latent values and opinions embedded in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by prompting LLMs with survey questions and quantifying the stances in the outputs towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing natural patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs. open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances.",
    "authors": [
      "Dustin Wright",
      "Arnav Arora",
      "Nadav Borenstein",
      "Srishti Yadav",
      "Serge Belongie",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-06-27T15:01:53Z",
    "pdf_url": "https://arxiv.org/pdf/2406.19238v3"
  },
  {
    "arxiv_id": "2406.19228v1",
    "entry_id": "http://arxiv.org/abs/2406.19228v1",
    "title": "Tools Fail: Detecting Silent Errors in Faulty Tools",
    "summary": "Tools have become a mainstay of LLMs, allowing them to retrieve knowledge not in their weights, to perform tasks on the web, and even to control robots. However, most ontologies and surveys of tool-use have assumed the core challenge for LLMs is choosing the tool. Instead, we introduce a framework for tools more broadly which guides us to explore a model's ability to detect \"silent\" tool errors, and reflect on how to plan. This more directly aligns with the increasingly popular use of models as tools. We provide an initial approach to failure recovery with promising results both on a controlled calculator setting and embodied agent planning.",
    "authors": [
      "Jimin Sun",
      "So Yeon Min",
      "Yingshan Chang",
      "Yonatan Bisk"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-27T14:52:34Z",
    "pdf_url": "https://arxiv.org/pdf/2406.19228v1"
  },
  {
    "arxiv_id": "2407.00116v2",
    "entry_id": "http://arxiv.org/abs/2407.00116v2",
    "title": "Generative AI for Synthetic Data Across Multiple Medical Modalities: A Systematic Review of Recent Developments and Challenges",
    "summary": "This paper presents a comprehensive systematic review of generative models (GANs, VAEs, DMs, and LLMs) used to synthesize various medical data types, including imaging (dermoscopic, mammographic, ultrasound, CT, MRI, and X-ray), text, time-series, and tabular data (EHR). Unlike previous narrowly focused reviews, our study encompasses a broad array of medical data modalities and explores various generative models. Our search strategy queries databases such as Scopus, PubMed, and ArXiv, focusing on recent works from January 2021 to November 2023, excluding reviews and perspectives. This period emphasizes recent advancements beyond GANs, which have been extensively covered previously.\n  The survey reveals insights from three key aspects: (1) Synthesis applications and purpose of synthesis, (2) generation techniques, and (3) evaluation methods. It highlights clinically valid synthesis applications, demonstrating the potential of synthetic data to tackle diverse clinical requirements. While conditional models incorporating class labels, segmentation masks and image translations are prevalent, there is a gap in utilizing prior clinical knowledge and patient-specific context, suggesting a need for more personalized synthesis approaches and emphasizing the importance of tailoring generative approaches to the unique characteristics of medical data. Additionally, there is a significant gap in using synthetic data beyond augmentation, such as for validation and evaluation of downstream medical AI models. The survey uncovers that the lack of standardized evaluation methodologies tailored to medical images is a barrier to clinical application, underscoring the need for in-depth evaluation approaches, benchmarking, and comparative studies to promote openness and collaboration.",
    "authors": [
      "Mahmoud Ibrahim",
      "Yasmina Al Khalil",
      "Sina Amirrajab",
      "Chang Sun",
      "Marcel Breeuwer",
      "Josien Pluim",
      "Bart Elen",
      "Gokhan Ertaylan",
      "Michel Dumontier"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-06-27T14:00:11Z",
    "pdf_url": "https://arxiv.org/pdf/2407.00116v2"
  },
  {
    "arxiv_id": "2407.12812v1",
    "entry_id": "http://arxiv.org/abs/2407.12812v1",
    "title": "Building Understandable Messaging for Policy and Evidence Review (BUMPER) with AI",
    "summary": "We introduce a framework for the use of large language models (LLMs) in Building Understandable Messaging for Policy and Evidence Review (BUMPER). LLMs are proving capable of providing interfaces for understanding and synthesizing large databases of diverse media. This presents an exciting opportunity to supercharge the translation of scientific evidence into policy and action, thereby improving livelihoods around the world. However, these models also pose challenges related to access, trust-worthiness, and accountability. The BUMPER framework is built atop a scientific knowledge base (e.g., documentation, code, survey data) by the same scientists (e.g., individual contributor, lab, consortium). We focus on a solution that builds trustworthiness through transparency, scope-limiting, explicit-checks, and uncertainty measures. LLMs are rapidly being adopted and consequences are poorly understood. The framework addresses open questions regarding the reliability of LLMs and their use in high-stakes applications. We provide a worked example in health policy for a model designed to inform measles control programs. We argue that this framework can facilitate accessibility of and confidence in scientific evidence for policymakers, drive a focus on policy-relevance and translatability for researchers, and ultimately increase and accelerate the impact of scientific knowledge used for policy decisions.",
    "authors": [
      "Katherine A. Rosenfeld",
      "Maike Sonnewald",
      "Sonia J. Jindal",
      "Kevin A. McCarthy",
      "Joshua L. Proctor"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-27T05:03:03Z",
    "pdf_url": "https://arxiv.org/pdf/2407.12812v1"
  },
  {
    "arxiv_id": "2407.11016v3",
    "entry_id": "http://arxiv.org/abs/2407.11016v3",
    "title": "LongLaMP: A Benchmark for Personalized Long-form Text Generation",
    "summary": "Long-text generation is seemingly ubiquitous in real-world applications of large language models such as generating an email or writing a review. Despite the fundamental importance and prevalence of long-text generation in many practical applications, existing work on personalized generation has focused on the generation of very short text. To overcome these limitations, we study the problem of personalized long-text generation, that is, generating long-text that is personalized for a specific user while being practically useful for the vast majority of real-world applications that naturally require the generation of longer text. In this work, we demonstrate the importance of user-specific personalization for long-text generation tasks and develop the Long-text Language Model Personalization (LongLaMP) Benchmark. LongLaMP provides a comprehensive and diverse evaluation framework for personalized long-text generation. Extensive experiments on LongLaMP for zero-shot and fine-tuned language tasks demonstrate the effectiveness of the proposed benchmark and its utility for developing and evaluating techniques for personalized long-text generation across a wide variety of long-text generation tasks. The results highlight the importance of personalization across a wide variety of long-text generation tasks. Finally, we release the benchmark for others to use for this important problem.",
    "authors": [
      "Ishita Kumar",
      "Snigdha Viswanathan",
      "Sushrita Yerra",
      "Alireza Salemi",
      "Ryan A. Rossi",
      "Franck Dernoncourt",
      "Hanieh Deilamsalehy",
      "Xiang Chen",
      "Ruiyi Zhang",
      "Shubham Agarwal",
      "Nedim Lipka",
      "Chien Van Nguyen",
      "Thien Huu Nguyen",
      "Hamed Zamani"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-27T01:52:05Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11016v3"
  },
  {
    "arxiv_id": "2407.11015v1",
    "entry_id": "http://arxiv.org/abs/2407.11015v1",
    "title": "Does ChatGPT Have a Mind?",
    "summary": "This paper examines the question of whether Large Language Models (LLMs) like ChatGPT possess minds, focusing specifically on whether they have a genuine folk psychology encompassing beliefs, desires, and intentions. We approach this question by investigating two key aspects: internal representations and dispositions to act. First, we survey various philosophical theories of representation, including informational, causal, structural, and teleosemantic accounts, arguing that LLMs satisfy key conditions proposed by each. We draw on recent interpretability research in machine learning to support these claims. Second, we explore whether LLMs exhibit robust dispositions to perform actions, a necessary component of folk psychology. We consider two prominent philosophical traditions, interpretationism and representationalism, to assess LLM action dispositions. While we find evidence suggesting LLMs may satisfy some criteria for having a mind, particularly in game-theoretic environments, we conclude that the data remains inconclusive. Additionally, we reply to several skeptical challenges to LLM folk psychology, including issues of sensory grounding, the \"stochastic parrots\" argument, and concerns about memorization. Our paper has three main upshots. First, LLMs do have robust internal representations. Second, there is an open question to answer about whether LLMs have robust action dispositions. Third, existing skeptical challenges to LLM representation do not survive philosophical scrutiny.",
    "authors": [
      "Simon Goldstein",
      "Benjamin A. Levinstein"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-27T00:21:16Z",
    "pdf_url": "https://arxiv.org/pdf/2407.11015v1"
  },
  {
    "arxiv_id": "2407.01603v3",
    "entry_id": "http://arxiv.org/abs/2407.01603v3",
    "title": "A Review of Large Language Models and Autonomous Agents in Chemistry",
    "summary": "Large language models (LLMs) have emerged as powerful tools in chemistry, significantly impacting molecule design, property prediction, and synthesis optimization. This review highlights LLM capabilities in these domains and their potential to accelerate scientific discovery through automation. We also review LLM-based autonomous agents: LLMs with a broader set of tools to interact with their surrounding environment. These agents perform diverse tasks such as paper scraping, interfacing with automated laboratories, and synthesis planning. As agents are an emerging topic, we extend the scope of our review of agents beyond chemistry and discuss across any scientific domains. This review covers the recent history, current capabilities, and design of LLMs and autonomous agents, addressing specific challenges, opportunities, and future directions in chemistry. Key challenges include data quality and integration, model interpretability, and the need for standard benchmarks, while future directions point towards more sophisticated multi-modal agents and enhanced collaboration between agents and experimental methods. Due to the quick pace of this field, a repository has been built to keep track of the latest studies: https://github.com/ur-whitelab/LLMs-in-science.",
    "authors": [
      "Mayk Caldas Ramos",
      "Christopher J. Collison",
      "Andrew D. White"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "physics.chem-ph"
    ],
    "published": "2024-06-26T17:33:21Z",
    "pdf_url": "https://arxiv.org/pdf/2407.01603v3"
  },
  {
    "arxiv_id": "2407.06204v3",
    "entry_id": "http://arxiv.org/abs/2407.06204v3",
    "title": "A Survey on Mixture of Experts in Large Language Models",
    "summary": "Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE research, we have established a resource repository at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.",
    "authors": [
      "Weilin Cai",
      "Juyong Jiang",
      "Fan Wang",
      "Jing Tang",
      "Sunghun Kim",
      "Jiayi Huang"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-06-26T16:34:33Z",
    "pdf_url": "https://arxiv.org/pdf/2407.06204v3"
  },
  {
    "arxiv_id": "2406.18078v1",
    "entry_id": "http://arxiv.org/abs/2406.18078v1",
    "title": "Self-Training with Pseudo-Label Scorer for Aspect Sentiment Quad Prediction",
    "summary": "Aspect Sentiment Quad Prediction (ASQP) aims to predict all quads (aspect term, aspect category, opinion term, sentiment polarity) for a given review, which is the most representative and challenging task in aspect-based sentiment analysis. A key challenge in the ASQP task is the scarcity of labeled data, which limits the performance of existing methods. To tackle this issue, we propose a self-training framework with a pseudo-label scorer, wherein a scorer assesses the match between reviews and their pseudo-labels, aiming to filter out mismatches and thereby enhance the effectiveness of self-training. We highlight two critical aspects to ensure the scorer's effectiveness and reliability: the quality of the training dataset and its model architecture. To this end, we create a human-annotated comparison dataset and train a generative model on it using ranking-based objectives. Extensive experiments on public ASQP datasets reveal that using our scorer can greatly and consistently improve the effectiveness of self-training. Moreover, we explore the possibility of replacing humans with large language models for comparison dataset annotation, and experiments demonstrate its feasibility. We release our code and data at https://github.com/HITSZ-HLT/ST-w-Scorer-ABSA .",
    "authors": [
      "Yice Zhang",
      "Jie Zeng",
      "Weiming Hu",
      "Ziyi Wang",
      "Shiwei Chen",
      "Ruifeng Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-26T05:30:21Z",
    "pdf_url": "https://arxiv.org/pdf/2406.18078v1"
  },
  {
    "arxiv_id": "2407.01599v3",
    "entry_id": "http://arxiv.org/abs/2407.01599v3",
    "title": "JailbreakZoo: Survey, Landscapes, and Horizons in Jailbreaking Large Language and Vision-Language Models",
    "summary": "The rapid evolution of artificial intelligence (AI) through developments in Large Language Models (LLMs) and Vision-Language Models (VLMs) has brought significant advancements across various technological domains. While these models enhance capabilities in natural language processing and visual interactive tasks, their growing adoption raises critical concerns regarding security and ethical alignment. This survey provides an extensive review of the emerging field of jailbreaking--deliberately circumventing the ethical and operational boundaries of LLMs and VLMs--and the consequent development of defense mechanisms. Our study categorizes jailbreaks into seven distinct types and elaborates on defense strategies that address these vulnerabilities. Through this comprehensive examination, we identify research gaps and propose directions for future studies to enhance the security frameworks of LLMs and VLMs. Our findings underscore the necessity for a unified perspective that integrates both jailbreak strategies and defensive solutions to foster a robust, secure, and reliable environment for the next generation of language models. More details can be found on our website: https://chonghan-chen.com/llm-jailbreak-zoo-survey/.",
    "authors": [
      "Haibo Jin",
      "Leyang Hu",
      "Xinnuo Li",
      "Peiyan Zhang",
      "Chonghan Chen",
      "Jun Zhuang",
      "Haohan Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-06-26T02:20:23Z",
    "pdf_url": "https://arxiv.org/pdf/2407.01599v3"
  },
  {
    "arxiv_id": "2406.17975v3",
    "entry_id": "http://arxiv.org/abs/2406.17975v3",
    "title": "SoK: Membership Inference Attacks on LLMs are Rushing Nowhere (and How to Fix It)",
    "summary": "Whether LLMs memorize their training data and what this means, from measuring privacy leakage to detecting copyright violations, has become a rapidly growing area of research. In the last few months, more than 10 new methods have been proposed to perform Membership Inference Attacks (MIAs) against LLMs. Contrary to traditional MIAs which rely on fixed-but randomized-records or models, these methods are mostly trained and tested on datasets collected post-hoc. Sets of members and non-members, used to evaluate the MIA, are constructed using informed guesses after the release of a model. This lack of randomization raises concerns of a distribution shift between members and non-members. In this work, we first extensively review the literature on MIAs against LLMs and show that, while most work focuses on sequence-level MIAs evaluated in post-hoc setups, a range of target models, motivations and units of interest are considered. We then quantify distribution shifts present in 6 datasets used in the literature using a model-less bag of word classifier and show that all datasets constructed post-hoc suffer from strong distribution shifts. These shifts invalidate the claims of LLMs memorizing strongly in real-world scenarios and, potentially, also the methodological contributions of the recent papers based on these datasets. Yet, all hope might not be lost. We introduce important considerations to properly evaluate MIAs against LLMs and discuss, in turn, potential ways forwards: randomized test splits, injections of randomized (unique) sequences, randomized fine-tuning, and several post-hoc control methods. While each option comes with its advantages and limitations, we believe they collectively provide solid grounds to guide MIA development and study LLM memorization. We conclude with an overview of recommended approaches to benchmark sequence-level and document-level MIAs against LLMs.",
    "authors": [
      "Matthieu Meeus",
      "Igor Shilov",
      "Shubham Jain",
      "Manuel Faysse",
      "Marek Rei",
      "Yves-Alexandre de Montjoye"
    ],
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2024-06-25T23:12:07Z",
    "pdf_url": "https://arxiv.org/pdf/2406.17975v3"
  },
  {
    "arxiv_id": "2406.17972v3",
    "entry_id": "http://arxiv.org/abs/2406.17972v3",
    "title": "LABOR-LLM: Language-Based Occupational Representations with Large Language Models",
    "summary": "Vafa et al. (2024) introduced a transformer-based econometric model, CAREER, that predicts a worker's next job as a function of career history (an \"occupation model\"). CAREER was initially estimated (\"pre-trained\") using a large, unrepresentative resume dataset, which served as a \"foundation model,\" and parameter estimation was continued (\"fine-tuned\") using data from a representative survey. CAREER had better predictive performance than benchmarks. This paper considers an alternative where the resume-based foundation model is replaced by a large language model (LLM). We convert tabular data from the survey into text files that resemble resumes and fine-tune the LLMs using these text files with the objective to predict the next token (word). The resulting fine-tuned LLM is used as an input to an occupation model. Its predictive performance surpasses all prior models. We demonstrate the value of fine-tuning and further show that by adding more career data from a different population, fine-tuning smaller LLMs surpasses the performance of fine-tuning larger models.",
    "authors": [
      "Susan Athey",
      "Herman Brunborg",
      "Tianyu Du",
      "Ayush Kanodia",
      "Keyon Vafa"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "econ.EM"
    ],
    "published": "2024-06-25T23:07:18Z",
    "pdf_url": "https://arxiv.org/pdf/2406.17972v3"
  },
  {
    "arxiv_id": "2406.17624v1",
    "entry_id": "http://arxiv.org/abs/2406.17624v1",
    "title": "Self-assessment, Exhibition, and Recognition: a Review of Personality in Large Language Models",
    "summary": "As large language models (LLMs) appear to behave increasingly human-like in text-based interactions, more and more researchers become interested in investigating personality in LLMs. However, the diversity of psychological personality research and the rapid development of LLMs have led to a broad yet fragmented landscape of studies in this interdisciplinary field. Extensive studies across different research focuses, different personality psychometrics, and different LLMs make it challenging to have a holistic overview and further pose difficulties in applying findings to real-world applications. In this paper, we present a comprehensive review by categorizing current studies into three research problems: self-assessment, exhibition, and recognition, based on the intrinsic characteristics and external manifestations of personality in LLMs. For each problem, we provide a thorough analysis and conduct in-depth comparisons of their corresponding solutions. Besides, we summarize research findings and open challenges from current studies and further discuss their underlying causes. We also collect extensive publicly available resources to facilitate interested researchers and developers. Lastly, we discuss the potential future research directions and application scenarios. Our paper is the first comprehensive survey of up-to-date literature on personality in LLMs. By presenting a clear taxonomy, in-depth analysis, promising future directions, and extensive resource collections, we aim to provide a better understanding and facilitate further advancements in this emerging field.",
    "authors": [
      "Zhiyuan Wen",
      "Yu Yang",
      "Jiannong Cao",
      "Haoming Sun",
      "Ruosong Yang",
      "Shuaiqi Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-25T15:08:44Z",
    "pdf_url": "https://arxiv.org/pdf/2406.17624v1"
  },
  {
    "arxiv_id": "2406.17189v1",
    "entry_id": "http://arxiv.org/abs/2406.17189v1",
    "title": "Hierarchical Framework for Optimizing Wildfire Surveillance and Suppression using Human-Autonomous Teaming",
    "summary": "The integration of manned and unmanned aircraft can help improve wildfire response. Wildfire containment failures occur when resources available to first responders, who execute the initial stages of wildfire management referred to as the initial attack, are ineffective or insufficient. Initial attack surveillance and suppression models have linked action spaces and objectives, making their optimization computationally challenging. The initial attack may be formulated as a multi-agent partially observable Markov decision process (MPOMDP). We divide the initial attack MPOMDP into surveillance and suppression processes with their respective planners operating on different, but constant, time scales. A hierarchical framework iterates between surveillance and suppression planners while also providing collision avoidance. This framework is exemplified by a set of multi-rotor unmanned aircraft surveying an initial attack fire while a manned helicopter suppresses the fire with a water bucket. Wildfire-specific solver extensions are formulated to reduce the otherwise vast action spaces. The hierarchical framework outperforms firefighting techniques and a myopic baseline by up to 242% for moderate wildfires and 60% for rapid wildfires when simulated in abstracted and actual case studies. We also validate the early dispatching of additional suppression assets using regression models to ensure wildfire containment to thresholds established by wildfire agencies.",
    "authors": [
      "Mahdi Al-Husseini",
      "Kyle Wray",
      "Mykel Kochenderfer"
    ],
    "categories": [
      "eess.SY",
      "cs.MA"
    ],
    "published": "2024-06-25T00:07:33Z",
    "pdf_url": "https://arxiv.org/pdf/2406.17189v1"
  },
  {
    "arxiv_id": "2406.16838v2",
    "entry_id": "http://arxiv.org/abs/2406.16838v2",
    "title": "From Decoding to Meta-Generation: Inference-time Algorithms for Large Language Models",
    "summary": "One of the most striking findings in modern research on large language models (LLMs) is that scaling up compute during training leads to better results. However, less attention has been given to the benefits of scaling compute during inference. This survey focuses on these inference-time approaches. We explore three areas under a unified mathematical formalism: token-level generation algorithms, meta-generation algorithms, and efficient generation. Token-level generation algorithms, often called decoding algorithms, operate by sampling a single token at a time or constructing a token-level search space and then selecting an output. These methods typically assume access to a language model's logits, next-token distributions, or probability scores. Meta-generation algorithms work on partial or full sequences, incorporating domain knowledge, enabling backtracking, and integrating external information. Efficient generation methods aim to reduce token costs and improve the speed of generation. Our survey unifies perspectives from three research communities: traditional natural language processing, modern LLMs, and machine learning systems.",
    "authors": [
      "Sean Welleck",
      "Amanda Bertsch",
      "Matthew Finlayson",
      "Hailey Schoelkopf",
      "Alex Xie",
      "Graham Neubig",
      "Ilia Kulikov",
      "Zaid Harchaoui"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-24T17:45:59Z",
    "pdf_url": "https://arxiv.org/pdf/2406.16838v2"
  },
  {
    "arxiv_id": "2406.17809v2",
    "entry_id": "http://arxiv.org/abs/2406.17809v2",
    "title": "Towards a Science Exocortex",
    "summary": "Artificial intelligence (AI) methods are poised to revolutionize intellectual work, with generative AI enabling automation of text analysis, text generation, and simple decision making or reasoning. The impact to science is only just beginning, but the opportunity is significant since scientific research relies fundamentally on extended chains of cognitive work. Here, we review the state of the art in agentic AI systems, and discuss how these methods could be extended to have even greater impact on science. We propose the development of an exocortex, a synthetic extension of a person's cognition. A science exocortex could be designed as a swarm of AI agents, with each agent individually streamlining specific researcher tasks, and whose inter-communication leads to emergent behavior that greatly extend the researcher's cognition and volition.",
    "authors": [
      "Kevin G. Yager"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-06-24T14:32:32Z",
    "pdf_url": "https://arxiv.org/pdf/2406.17809v2"
  },
  {
    "arxiv_id": "2407.09522v2",
    "entry_id": "http://arxiv.org/abs/2407.09522v2",
    "title": "UQE: A Query Engine for Unstructured Databases",
    "summary": "Analytics on structured data is a mature field with many successful methods. However, most real world data exists in unstructured form, such as images and conversations. We investigate the potential of Large Language Models (LLMs) to enable unstructured data analytics. In particular, we propose a new Universal Query Engine (UQE) that directly interrogates and draws insights from unstructured data collections. This engine accepts queries in a Universal Query Language (UQL), a dialect of SQL that provides full natural language flexibility in specifying conditions and operators. The new engine leverages the ability of LLMs to conduct analysis of unstructured data, while also allowing us to exploit advances in sampling and optimization techniques to achieve efficient and accurate query execution. In addition, we borrow techniques from classical compiler theory to better orchestrate the workflow between sampling methods and foundation model calls. We demonstrate the efficiency of UQE on data analytics across different modalities, including images, dialogs and reviews, across a range of useful query types, including conditional aggregation, semantic retrieval and abstraction aggregation.",
    "authors": [
      "Hanjun Dai",
      "Bethany Yixin Wang",
      "Xingchen Wan",
      "Bo Dai",
      "Sherry Yang",
      "Azade Nova",
      "Pengcheng Yin",
      "Phitchaya Mangpo Phothilimthana",
      "Charles Sutton",
      "Dale Schuurmans"
    ],
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2024-06-23T06:58:55Z",
    "pdf_url": "https://arxiv.org/pdf/2407.09522v2"
  },
  {
    "arxiv_id": "2406.17806v1",
    "entry_id": "http://arxiv.org/abs/2406.17806v1",
    "title": "MOSSBench: Is Your Multimodal Language Model Oversensitive to Safe Queries?",
    "summary": "Humans are prone to cognitive distortions -- biased thinking patterns that lead to exaggerated responses to specific stimuli, albeit in very different contexts. This paper demonstrates that advanced Multimodal Large Language Models (MLLMs) exhibit similar tendencies. While these models are designed to respond queries under safety mechanism, they sometimes reject harmless queries in the presence of certain visual stimuli, disregarding the benign nature of their contexts. As the initial step in investigating this behavior, we identify three types of stimuli that trigger the oversensitivity of existing MLLMs: Exaggerated Risk, Negated Harm, and Counterintuitive Interpretation. To systematically evaluate MLLMs' oversensitivity to these stimuli, we propose the Multimodal OverSenSitivity Benchmark (MOSSBench). This toolkit consists of 300 manually collected benign multimodal queries, cross-verified by third-party reviewers (AMT). Empirical studies using MOSSBench on 20 MLLMs reveal several insights: (1). Oversensitivity is prevalent among SOTA MLLMs, with refusal rates reaching up to 76% for harmless queries. (2). Safer models are more oversensitive: increasing safety may inadvertently raise caution and conservatism in the model's responses. (3). Different types of stimuli tend to cause errors at specific stages -- perception, intent reasoning, and safety judgement -- in the response process of MLLMs. These findings highlight the need for refined safety mechanisms that balance caution with contextually appropriate responses, improving the reliability of MLLMs in real-world applications. We make our project available at https://turningpoint-ai.github.io/MOSSBench/.",
    "authors": [
      "Xirui Li",
      "Hengguang Zhou",
      "Ruochen Wang",
      "Tianyi Zhou",
      "Minhao Cheng",
      "Cho-Jui Hsieh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-06-22T23:26:07Z",
    "pdf_url": "https://arxiv.org/pdf/2406.17806v1"
  },
  {
    "arxiv_id": "2406.15859v2",
    "entry_id": "http://arxiv.org/abs/2406.15859v2",
    "title": "LLM-Powered Explanations: Unraveling Recommendations Through Subgraph Reasoning",
    "summary": "Recommender systems are pivotal in enhancing user experiences across various web applications by analyzing the complicated relationships between users and items. Knowledge graphs(KGs) have been widely used to enhance the performance of recommender systems. However, KGs are known to be noisy and incomplete, which are hard to provide reliable explanations for recommendation results. An explainable recommender system is crucial for the product development and subsequent decision-making. To address these challenges, we introduce a novel recommender that synergies Large Language Models (LLMs) and KGs to enhance the recommendation and provide interpretable results. Specifically, we first harness the power of LLMs to augment KG reconstruction. LLMs comprehend and decompose user reviews into new triples that are added into KG. In this way, we can enrich KGs with explainable paths that express user preferences. To enhance the recommendation on augmented KGs, we introduce a novel subgraph reasoning module that effectively measures the importance of nodes and discovers reasoning for recommendation. Finally, these reasoning paths are fed into the LLMs to generate interpretable explanations of the recommendation results. Our approach significantly enhances both the effectiveness and interpretability of recommender systems, especially in cross-selling scenarios where traditional methods falter. The effectiveness of our approach has been rigorously tested on four open real-world datasets, with our methods demonstrating a superior performance over contemporary state-of-the-art techniques by an average improvement of 12%. The application of our model in a multinational engineering and technology company cross-selling recommendation system further underscores its practical utility and potential to redefine recommendation practices through improved accuracy and user trust.",
    "authors": [
      "Guangsi Shi",
      "Xiaofeng Deng",
      "Linhao Luo",
      "Lijuan Xia",
      "Lei Bao",
      "Bei Ye",
      "Fei Du",
      "Shirui Pan",
      "Yuxiao Li"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-06-22T14:14:03Z",
    "pdf_url": "https://arxiv.org/pdf/2406.15859v2"
  },
  {
    "arxiv_id": "2406.15809v4",
    "entry_id": "http://arxiv.org/abs/2406.15809v4",
    "title": "LaMSUM: Amplifying Voices Against Harassment through LLM Guided Extractive Summarization of User Incident Reports",
    "summary": "Citizen reporting platforms like Safe City in India help the public and authorities stay informed about sexual harassment incidents. However, the high volume of data shared on these platforms makes reviewing each individual case challenging. Therefore, a summarization algorithm capable of processing and understanding various Indian code-mixed languages is essential. In recent years, Large Language Models (LLMs) have shown exceptional performance in NLP tasks, including summarization. LLMs inherently produce abstractive summaries by paraphrasing the original text, while the generation of extractive summaries - selecting specific subsets from the original text - through LLMs remains largely unexplored. Moreover, LLMs have a limited context window size, restricting the amount of data that can be processed at once. We tackle these challenge by introducing LaMSUM, a novel multi-level framework designed to generate extractive summaries for large collections of Safe City posts using LLMs. LaMSUM integrates summarization with different voting methods to achieve robust summaries. Extensive evaluation using three popular LLMs (Llama, Mistral and GPT-4o) demonstrates that LaMSUM outperforms state-of-the-art extractive summarization methods for Safe City posts. Overall, this work represents one of the first attempts to achieve extractive summarization through LLMs, and is likely to support stakeholders by offering a comprehensive overview and enabling them to develop effective policies to minimize incidents of unwarranted harassment.",
    "authors": [
      "Garima Chhikara",
      "Anurag Sharma",
      "V. Gurucharan",
      "Kripabandhu Ghosh",
      "Abhijnan Chakraborty"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-22T10:25:55Z",
    "pdf_url": "https://arxiv.org/pdf/2406.15809v4"
  },
  {
    "arxiv_id": "2407.04722v1",
    "entry_id": "http://arxiv.org/abs/2407.04722v1",
    "title": "A GPT-based Code Review System for Programming Language Learning",
    "summary": "The increasing demand for programming language education and growing class sizes require immediate and personalized feedback. However, traditional code review methods have limitations in providing this level of feedback. As the capabilities of Large Language Models (LLMs) like GPT for generating accurate solutions and timely code reviews are verified, this research proposes a system that employs GPT-4 to offer learner-friendly code reviews and minimize the risk of AI-assist cheating.\n  To provide learner-friendly code reviews, a dataset was collected from an online judge system, and this dataset was utilized to develop and enhance the system's prompts. In addition, to minimize AI-assist cheating, the system flow was designed to provide code reviews only for code submitted by a learner, and a feature that highlights code lines to fix was added. After the initial system was deployed on the web, software education experts conducted usability test. Based on the results, improvement strategies were developed to improve code review and code correctness check module, thereby enhancing the system.\n  The improved system underwent evaluation by software education experts based on four criteria: strict code correctness checks, response time, lower API call costs, and the quality of code reviews. The results demonstrated a performance to accurately identify error types, shorten response times, lower API call costs, and maintain high-quality code reviews without major issues. Feedback from participants affirmed the tool's suitability for teaching programming to primary and secondary school students. Given these benefits, the system is anticipated to be a efficient learning tool in programming language learning for educational settings.",
    "authors": [
      "Lee Dong-Kyu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-06-21T12:16:01Z",
    "pdf_url": "https://arxiv.org/pdf/2407.04722v1"
  },
  {
    "arxiv_id": "2406.15527v1",
    "entry_id": "http://arxiv.org/abs/2406.15527v1",
    "title": "Data Efficient Evaluation of Large Language Models and Text-to-Image Models via Adaptive Sampling",
    "summary": "Evaluating LLMs and text-to-image models is a computationally intensive task often overlooked. Efficient evaluation is crucial for understanding the diverse capabilities of these models and enabling comparisons across a growing number of new models and benchmarks. To address this, we introduce SubLIME, a data-efficient evaluation framework that employs adaptive sampling techniques, such as clustering and quality-based methods, to create representative subsets of benchmarks. Our approach ensures statistically aligned model rankings compared to full datasets, evidenced by high Pearson correlation coefficients. Empirical analysis across six NLP benchmarks reveals that: (1) quality-based sampling consistently achieves strong correlations (0.85 to 0.95) with full datasets at a 10\\% sampling rate such as Quality SE and Quality CPD (2) clustering methods excel in specific benchmarks such as MMLU (3) no single method universally outperforms others across all metrics. Extending this framework, we leverage the HEIM leaderboard to cover 25 text-to-image models on 17 different benchmarks. SubLIME dynamically selects the optimal technique for each benchmark, significantly reducing evaluation costs while preserving ranking integrity and score distribution. Notably, a minimal sampling rate of 1% proves effective for benchmarks like MMLU. Additionally, we demonstrate that employing difficulty-based sampling to target more challenging benchmark segments enhances model differentiation with broader score distributions. We also combine semantic search, tool use, and GPT-4 review to identify redundancy across benchmarks within specific LLM categories, such as coding benchmarks. This allows us to further reduce the number of samples needed to maintain targeted rank preservation. Overall, SubLIME offers a versatile and cost-effective solution for the robust evaluation of LLMs and text-to-image models.",
    "authors": [
      "Cong Xu",
      "Gayathri Saranathan",
      "Mahammad Parwez Alam",
      "Arpit Shah",
      "James Lim",
      "Soon Yee Wong",
      "Foltin Martin",
      "Suparna Bhattacharya"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-06-21T07:38:55Z",
    "pdf_url": "https://arxiv.org/pdf/2406.15527v1"
  },
  {
    "arxiv_id": "2406.14871v2",
    "entry_id": "http://arxiv.org/abs/2406.14871v2",
    "title": "I don't trust you (anymore)! -- The effect of students' LLM use on Lecturer-Student-Trust in Higher Education",
    "summary": "Trust plays a pivotal role in Lecturer-Student-Collaboration, encompassing teaching and research aspects. The advent of Large Language Models (LLMs) in platforms like Open AI's ChatGPT, coupled with their cost-effectiveness and high-quality results, has led to their rapid adoption among university students. However, discerning genuine student input from LLM-generated output poses a challenge for lecturers. This dilemma jeopardizes the trust relationship between lecturers and students, potentially impacting university downstream activities, particularly collaborative research initiatives. Despite attempts to establish guidelines for student LLM use, a clear framework mutually beneficial for lecturers and students in higher education remains elusive. This study addresses the research question: How does the use of LLMs by students impact Informational and Procedural Justice, influencing Team Trust and Expected Team Performance? Methodically, we applied a quantitative construct-based survey, evaluated using techniques of Structural Equation Modelling (PLS- SEM) to examine potential relationships among these constructs. Our findings based on 23 valid respondents from Ndejje University indicate that lecturers are less concerned about the fairness of LLM use per se but are more focused on the transparency of student utilization, which significantly influences Team Trust positively. This research contributes to the global discourse on integrating and regulating LLMs and subsequent models in education. We propose that guidelines should support LLM use while enforcing transparency in Lecturer-Student- Collaboration to foster Team Trust and Performance. The study contributes valuable insights for shaping policies enabling ethical and transparent LLMs usage in education to ensure effectiveness of collaborative learning environments.",
    "authors": [
      "Simon Kloker",
      "Matthew Bazanya",
      "Twaha Kateete"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-06-21T05:35:57Z",
    "pdf_url": "https://arxiv.org/pdf/2406.14871v2"
  },
  {
    "arxiv_id": "2406.14508v1",
    "entry_id": "http://arxiv.org/abs/2406.14508v1",
    "title": "Evidence of a log scaling law for political persuasion with large language models",
    "summary": "Large language models can now generate political messages as persuasive as those written by humans, raising concerns about how far this persuasiveness may continue to increase with model size. Here, we generate 720 persuasive messages on 10 U.S. political issues from 24 language models spanning several orders of magnitude in size. We then deploy these messages in a large-scale randomized survey experiment (N = 25,982) to estimate the persuasive capability of each model. Our findings are twofold. First, we find evidence of a log scaling law: model persuasiveness is characterized by sharply diminishing returns, such that current frontier models are barely more persuasive than models smaller in size by an order of magnitude or more. Second, mere task completion (coherence, staying on topic) appears to account for larger models' persuasive advantage. These findings suggest that further scaling model size will not much increase the persuasiveness of static LLM-generated messages.",
    "authors": [
      "Kobi Hackenburg",
      "Ben M. Tappin",
      "Paul Röttger",
      "Scott Hale",
      "Jonathan Bright",
      "Helen Margetts"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2024-06-20T17:12:38Z",
    "pdf_url": "https://arxiv.org/pdf/2406.14508v1"
  },
  {
    "arxiv_id": "2406.14284v2",
    "entry_id": "http://arxiv.org/abs/2406.14284v2",
    "title": "Leveraging LLMs for Bangla Grammar Error Correction:Error Categorization, Synthetic Data, and Model Evaluation",
    "summary": "Large Language Models (LLMs) perform exceedingly well in Natural Language Understanding (NLU) tasks for many languages including English. However, despite being the fifth most-spoken language globally, Grammatical Error Correction (GEC) in Bangla remains underdeveloped. In this work, we investigate how LLMs can be leveraged for improving Bangla GEC. For that, we first do an extensive categorization of 12 error classes in Bangla, and take a survey of native Bangla speakers to collect real-world errors. We next devise a rule-based noise injection method to create grammatically incorrect sentences corresponding to correct ones. The Vaiyakarana dataset, thus created, consists of 5,67,422 sentences of which 2,27,119 are erroneous. This dataset is then used to instruction-tune LLMs for the task of GEC in Bangla. Evaluations show that instruction-tuning with \\name improves GEC performance of LLMs by 3-7 percentage points as compared to the zero-shot setting, and makes them achieve human-like performance in grammatical error identification. Humans, though, remain superior in error correction.",
    "authors": [
      "Pramit Bhattacharyya",
      "Arnab Bhattacharya"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-20T13:09:29Z",
    "pdf_url": "https://arxiv.org/pdf/2406.14284v2"
  },
  {
    "arxiv_id": "2406.14169v1",
    "entry_id": "http://arxiv.org/abs/2406.14169v1",
    "title": "Optimizing Novelty of Top-k Recommendations using Large Language Models and Reinforcement Learning",
    "summary": "Given an input query, a recommendation model is trained using user feedback data (e.g., click data) to output a ranked list of items. In real-world systems, besides accuracy, an important consideration for a new model is novelty of its top-k recommendations w.r.t. an existing deployed model. However, novelty of top-k items is a difficult goal to optimize a model for, since it involves a non-differentiable sorting operation on the model's predictions. Moreover, novel items, by definition, do not have any user feedback data. Given the semantic capabilities of large language models, we address these problems using a reinforcement learning (RL) formulation where large language models provide feedback for the novel items. However, given millions of candidate items, the sample complexity of a standard RL algorithm can be prohibitively high. To reduce sample complexity, we reduce the top-k list reward to a set of item-wise rewards and reformulate the state space to consist of <query, item> tuples such that the action space is reduced to a binary decision; and show that this reformulation results in a significantly lower complexity when the number of items is large. We evaluate the proposed algorithm on improving novelty for a query-ad recommendation task on a large-scale search engine. Compared to supervised finetuning on recent <query, ad> pairs, the proposed RL-based algorithm leads to significant novelty gains with minimal loss in recall. We obtain similar results on the ORCAS query-webpage matching dataset and a product recommendation dataset based on Amazon reviews.",
    "authors": [
      "Amit Sharma",
      "Hua Li",
      "Xue Li",
      "Jian Jiao"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-06-20T10:20:02Z",
    "pdf_url": "https://arxiv.org/pdf/2406.14169v1"
  },
  {
    "arxiv_id": "2406.13993v2",
    "entry_id": "http://arxiv.org/abs/2406.13993v2",
    "title": "Exploring Changes in Nation Perception with Nationality-Assigned Personas in LLMs",
    "summary": "Persona assignment has become a common strategy for customizing LLM use to particular tasks and contexts. In this study, we explore how evaluation of different nations change when LLMs are assigned specific nationality personas. We assign 193 different nationality personas (e.g., an American person) to four LLMs and examine how the LLM evaluations (or ''perceptions'')of countries change. We find that all LLM-persona combinations tend to favor Western European nations, though nation-personas push LLM behaviors to focus more on and treat the nation-persona's own region more favorably. Eastern European, Latin American, and African nations are treated more negatively by different nationality personas. We additionally find that evaluations by nation-persona LLMs of other nations correlate with human survey responses but fail to match the values closely. Our study provides insight into how biases and stereotypes are realized within LLMs when adopting different national personas. In line with the ''Blueprint for an AI Bill of Rights'', our findings underscore the critical need for developing mechanisms to ensure that LLM outputs promote fairness and avoid over-generalization.",
    "authors": [
      "Mahammed Kamruzzaman",
      "Gene Louis Kim"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-20T04:44:20Z",
    "pdf_url": "https://arxiv.org/pdf/2406.13993v2"
  },
  {
    "arxiv_id": "2406.13966v1",
    "entry_id": "http://arxiv.org/abs/2406.13966v1",
    "title": "Causal Inference with Latent Variables: Recent Advances and Future Prospectives",
    "summary": "Causality lays the foundation for the trajectory of our world. Causal inference (CI), which aims to infer intrinsic causal relations among variables of interest, has emerged as a crucial research topic. Nevertheless, the lack of observation of important variables (e.g., confounders, mediators, exogenous variables, etc.) severely compromises the reliability of CI methods. The issue may arise from the inherent difficulty in measuring the variables. Additionally, in observational studies where variables are passively recorded, certain covariates might be inadvertently omitted by the experimenter. Depending on the type of unobserved variables and the specific CI task, various consequences can be incurred if these latent variables are carelessly handled, such as biased estimation of causal effects, incomplete understanding of causal mechanisms, lack of individual-level causal consideration, etc. In this survey, we provide a comprehensive review of recent developments in CI with latent variables. We start by discussing traditional CI techniques when variables of interest are assumed to be fully observed. Afterward, under the taxonomy of circumvention and inference-based methods, we provide an in-depth discussion of various CI strategies to handle latent variables, covering the tasks of causal effect estimation, mediation analysis, counterfactual reasoning, and causal discovery. Furthermore, we generalize the discussion to graph data where interference among units may exist. Finally, we offer fresh aspects for further advancement of CI with latent variables, especially new opportunities in the era of large language models (LLMs).",
    "authors": [
      "Yaochen Zhu",
      "Yinhan He",
      "Jing Ma",
      "Mengxuan Hu",
      "Sheng Li",
      "Jundong Li"
    ],
    "categories": [
      "cs.LG",
      "stat.ME"
    ],
    "published": "2024-06-20T03:15:53Z",
    "pdf_url": "https://arxiv.org/pdf/2406.13966v1"
  },
  {
    "arxiv_id": "2406.13558v2",
    "entry_id": "http://arxiv.org/abs/2406.13558v2",
    "title": "Enhancing Travel Choice Modeling with Large Language Models: A Prompt-Learning Approach",
    "summary": "Travel choice analysis is crucial for understanding individual travel behavior to develop appropriate transport policies and recommendation systems in Intelligent Transportation Systems (ITS). Despite extensive research, this domain faces two critical challenges: a) modeling with limited survey data, and b) simultaneously achieving high model explainability and accuracy. In this paper, we introduce a novel prompt-learning-based Large Language Model(LLM) framework that significantly improves prediction accuracy and provides explicit explanations for individual predictions. This framework involves three main steps: transforming input variables into textual form; building of demonstrations similar to the object, and applying these to a well-trained LLM. We tested the framework's efficacy using two widely used choice datasets: London Passenger Mode Choice (LPMC) and Optima-Mode collected in Switzerland. The results indicate that the LLM significantly outperforms state-of-the-art deep learning methods and discrete choice models in predicting people's choices. Additionally, we present a case of explanation illustrating how the LLM framework generates understandable and explicit explanations at the individual level.",
    "authors": [
      "Xuehao Zhai",
      "Hanlin Tian",
      "Lintong Li",
      "Tianyu Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-06-19T13:46:08Z",
    "pdf_url": "https://arxiv.org/pdf/2406.13558v2"
  },
  {
    "arxiv_id": "2406.13415v1",
    "entry_id": "http://arxiv.org/abs/2406.13415v1",
    "title": "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators",
    "summary": "Large Language Models (LLMs) tend to be unreliable in the factuality of their answers. To address this problem, NLP researchers have proposed a range of techniques to estimate LLM's confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one another. To fill this gap, we present a survey and empirical comparison of estimators of factual confidence. We define an experimental framework allowing for fair comparison, covering both fact-verification and question answering. Our experiments across a series of LLMs indicate that trained hidden-state probes provide the most reliable confidence estimates, albeit at the expense of requiring access to weights and training data. We also conduct a deeper assessment of factual confidence by measuring the consistency of model behavior under meaning-preserving variations in the input. We find that the confidence of LLMs is often unstable across semantically equivalent inputs, suggesting that there is much room for improvement of the stability of models' parametric knowledge. Our code is available at (https://github.com/amazon-science/factual-confidence-of-llms).",
    "authors": [
      "Matéo Mahaut",
      "Laura Aina",
      "Paula Czarnowska",
      "Momchil Hardalov",
      "Thomas Müller",
      "Lluís Màrquez"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-19T10:11:37Z",
    "pdf_url": "https://arxiv.org/pdf/2406.13415v1"
  },
  {
    "arxiv_id": "2406.13232v1",
    "entry_id": "http://arxiv.org/abs/2406.13232v1",
    "title": "Towards Robust Evaluation: A Comprehensive Taxonomy of Datasets and Metrics for Open Domain Question Answering in the Era of Large Language Models",
    "summary": "Open Domain Question Answering (ODQA) within natural language processing involves building systems that answer factual questions using large-scale knowledge corpora. Recent advances stem from the confluence of several factors, such as large-scale training datasets, deep learning techniques, and the rise of large language models. High-quality datasets are used to train models on realistic scenarios and enable the evaluation of the system on potentially unseen data. Standardized metrics facilitate comparisons between different ODQA systems, allowing researchers to objectively track advancements in the field. Our study presents a thorough examination of the current landscape of ODQA benchmarking by reviewing 52 datasets and 20 evaluation techniques across textual and multimodal modalities. We introduce a novel taxonomy for ODQA datasets that incorporates both the modality and difficulty of the question types. Additionally, we present a structured organization of ODQA evaluation metrics along with a critical analysis of their inherent trade-offs. Our study aims to empower researchers by providing a framework for the robust evaluation of modern question-answering systems. We conclude by identifying the current challenges and outlining promising avenues for future research and development.",
    "authors": [
      "Akchay Srivastava",
      "Atif Memon"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-06-19T05:43:02Z",
    "pdf_url": "https://arxiv.org/pdf/2406.13232v1"
  },
  {
    "arxiv_id": "2406.13009v1",
    "entry_id": "http://arxiv.org/abs/2406.13009v1",
    "title": "Detecting Errors through Ensembling Prompts (DEEP): An End-to-End LLM Framework for Detecting Factual Errors",
    "summary": "Accurate text summarization is one of the most common and important tasks performed by Large Language Models, where the costs of human review for an entire document may be high, but the costs of errors in summarization may be even greater. We propose Detecting Errors through Ensembling Prompts (DEEP) - an end-to-end large language model framework for detecting factual errors in text summarization. Our framework uses a diverse set of LLM prompts to identify factual inconsistencies, treating their outputs as binary features, which are then fed into ensembling models. We then calibrate the ensembled models to produce empirically accurate probabilities that a text is factually consistent or free of hallucination. We demonstrate that prior models for detecting factual errors in summaries perform significantly worse without optimizing the thresholds on subsets of the evaluated dataset. Our framework achieves state-of-the-art (SOTA) balanced accuracy on the AggreFact-XSUM FTSOTA, TofuEval Summary-Level, and HaluEval Summarization benchmarks in detecting factual errors within transformer-generated text summaries. It does so without any fine-tuning of the language model or reliance on thresholding techniques not available in practical settings.",
    "authors": [
      "Alex Chandler",
      "Devesh Surve",
      "Hui Su"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-18T18:59:37Z",
    "pdf_url": "https://arxiv.org/pdf/2406.13009v1"
  },
  {
    "arxiv_id": "2406.12844v1",
    "entry_id": "http://arxiv.org/abs/2406.12844v1",
    "title": "Synergizing Foundation Models and Federated Learning: A Survey",
    "summary": "The recent development of Foundation Models (FMs), represented by large language models, vision transformers, and multimodal models, has been making a significant impact on both academia and industry. Compared with small-scale models, FMs have a much stronger demand for high-volume data during the pre-training phase. Although general FMs can be pre-trained on data collected from open sources such as the Internet, domain-specific FMs need proprietary data, posing a practical challenge regarding the amount of data available due to privacy concerns. Federated Learning (FL) is a collaborative learning paradigm that breaks the barrier of data availability from different participants. Therefore, it provides a promising solution to customize and adapt FMs to a wide range of domain-specific tasks using distributed datasets whilst preserving privacy. This survey paper discusses the potentials and challenges of synergizing FL and FMs and summarizes core techniques, future directions, and applications. A periodically updated paper collection on FM-FL is available at https://github.com/lishenghui/awesome-fm-fl.",
    "authors": [
      "Shenghui Li",
      "Fanghua Ye",
      "Meng Fang",
      "Jiaxu Zhao",
      "Yun-Hin Chan",
      "Edith C. -H. Ngai",
      "Thiemo Voigt"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-06-18T17:58:09Z",
    "pdf_url": "https://arxiv.org/pdf/2406.12844v1"
  },
  {
    "arxiv_id": "2406.12655v1",
    "entry_id": "http://arxiv.org/abs/2406.12655v1",
    "title": "Benchmarks and Metrics for Evaluations of Code Generation: A Critical Review",
    "summary": "With the rapid development of Large Language Models (LLMs), a large number of machine learning models have been developed to assist programming tasks including the generation of program code from natural language input. However, how to evaluate such LLMs for this task is still an open problem despite of the great amount of research efforts that have been made and reported to evaluate and compare them. This paper provides a critical review of the existing work on the testing and evaluation of these tools with a focus on two key aspects: the benchmarks and the metrics used in the evaluations. Based on the review, further research directions are discussed.",
    "authors": [
      "Debalina Ghosh Paul",
      "Hong Zhu",
      "Ian Bayley"
    ],
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-06-18T14:25:34Z",
    "pdf_url": "https://arxiv.org/pdf/2406.12655v1"
  },
  {
    "arxiv_id": "2406.12036v4",
    "entry_id": "http://arxiv.org/abs/2406.12036v4",
    "title": "MedCalc-Bench: Evaluating Large Language Models for Medical Calculations",
    "summary": "As opposed to evaluating computation and logic-based reasoning, current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks.",
    "authors": [
      "Nikhil Khandekar",
      "Qiao Jin",
      "Guangzhi Xiong",
      "Soren Dunn",
      "Serina S Applebaum",
      "Zain Anwar",
      "Maame Sarfo-Gyamfi",
      "Conrad W Safranek",
      "Abid A Anwar",
      "Andrew Zhang",
      "Aidan Gilson",
      "Maxwell B Singer",
      "Amisha Dave",
      "Andrew Taylor",
      "Aidong Zhang",
      "Qingyu Chen",
      "Zhiyong Lu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-17T19:07:21Z",
    "pdf_url": "https://arxiv.org/pdf/2406.12036v4"
  },
  {
    "arxiv_id": "2406.16937v2",
    "entry_id": "http://arxiv.org/abs/2406.16937v2",
    "title": "A Complete Survey on LLM-based AI Chatbots",
    "summary": "The past few decades have witnessed an upsurge in data, forming the foundation for data-hungry, learning-based AI technology. Conversational agents, often referred to as AI chatbots, rely heavily on such data to train large language models (LLMs) and generate new content (knowledge) in response to user prompts. With the advent of OpenAI's ChatGPT, LLM-based chatbots have set new standards in the AI community. This paper presents a complete survey of the evolution and deployment of LLM-based chatbots in various sectors. We first summarize the development of foundational chatbots, followed by the evolution of LLMs, and then provide an overview of LLM-based chatbots currently in use and those in the development phase. Recognizing AI chatbots as tools for generating new knowledge, we explore their diverse applications across various industries. We then discuss the open challenges, considering how the data used to train the LLMs and the misuse of the generated knowledge can cause several issues. Finally, we explore the future outlook to augment their efficiency and reliability in numerous applications. By addressing key milestones and the present-day context of LLM-based chatbots, our survey invites readers to delve deeper into this realm, reflecting on how their next generation will reshape conversational AI.",
    "authors": [
      "Sumit Kumar Dam",
      "Choong Seon Hong",
      "Yu Qiao",
      "Chaoning Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-17T09:39:34Z",
    "pdf_url": "https://arxiv.org/pdf/2406.16937v2"
  },
  {
    "arxiv_id": "2406.11106v2",
    "entry_id": "http://arxiv.org/abs/2406.11106v2",
    "title": "From Intentions to Techniques: A Comprehensive Taxonomy and Challenges in Text Watermarking for Large Language Models",
    "summary": "With the rapid growth of Large Language Models (LLMs), safeguarding textual content against unauthorized use is crucial. Watermarking offers a vital solution, protecting both - LLM-generated and plain text sources. This paper presents a unified overview of different perspectives behind designing watermarking techniques through a comprehensive survey of the research literature. Our work has two key advantages: (1) We analyze research based on the specific intentions behind different watermarking techniques, evaluation datasets used, and watermarking addition and removal methods to construct a cohesive taxonomy. (2) We highlight the gaps and open challenges in text watermarking to promote research protecting text authorship. This extensive coverage and detailed analysis sets our work apart, outlining the evolving landscape of text watermarking in Language Models.",
    "authors": [
      "Harsh Nishant Lalai",
      "Aashish Anantha Ramakrishnan",
      "Raj Sanjay Shah",
      "Dongwon Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-17T00:09:31Z",
    "pdf_url": "https://arxiv.org/pdf/2406.11106v2"
  },
  {
    "arxiv_id": "2406.10903v1",
    "entry_id": "http://arxiv.org/abs/2406.10903v1",
    "title": "New Solutions on LLM Acceleration, Optimization, and Application",
    "summary": "Large Language Models (LLMs) have become extremely potent instruments with exceptional capacities for comprehending and producing human-like text in a wide range of applications. However, the increasing size and complexity of LLMs present significant challenges in both training and deployment, leading to substantial computational and storage costs as well as heightened energy consumption. In this paper, we provide a review of recent advancements and research directions aimed at addressing these challenges and enhancing the efficiency of LLM-based systems. We begin by discussing algorithm-level acceleration techniques focused on optimizing LLM inference speed and resource utilization. We also explore LLM-hardware co-design strategies with a vision to improve system efficiency by tailoring hardware architectures to LLM requirements. Further, we delve into LLM-to-accelerator compilation approaches, which involve customizing hardware accelerators for efficient LLM deployment. Finally, as a case study to leverage LLMs for assisting circuit design, we examine LLM-aided design methodologies for an important task: High-Level Synthesis (HLS) functional verification, by creating a new dataset that contains a large number of buggy and bug-free codes, which can be essential for training LLMs to specialize on HLS verification and debugging. For each aspect mentioned above, we begin with a detailed background study, followed by the presentation of several novel solutions proposed to overcome specific challenges. We then outline future research directions to drive further advancements. Through these efforts, we aim to pave the way for more efficient and scalable deployment of LLMs across a diverse range of applications.",
    "authors": [
      "Yingbing Huang",
      "Lily Jiaxin Wan",
      "Hanchen Ye",
      "Manvi Jha",
      "Jinghua Wang",
      "Yuhong Li",
      "Xiaofan Zhang",
      "Deming Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.SE"
    ],
    "published": "2024-06-16T11:56:50Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10903v1"
  },
  {
    "arxiv_id": "2406.10886v1",
    "entry_id": "http://arxiv.org/abs/2406.10886v1",
    "title": "Distilling Opinions at Scale: Incremental Opinion Summarization using XL-OPSUMM",
    "summary": "Opinion summarization in e-commerce encapsulates the collective views of numerous users about a product based on their reviews. Typically, a product on an e-commerce platform has thousands of reviews, each review comprising around 10-15 words. While Large Language Models (LLMs) have shown proficiency in summarization tasks, they struggle to handle such a large volume of reviews due to context limitations. To mitigate, we propose a scalable framework called Xl-OpSumm that generates summaries incrementally. However, the existing test set, AMASUM has only 560 reviews per product on average. Due to the lack of a test set with thousands of reviews, we created a new test set called Xl-Flipkart by gathering data from the Flipkart website and generating summaries using GPT-4. Through various automatic evaluations and extensive analysis, we evaluated the framework's efficiency on two datasets, AMASUM and Xl-Flipkart. Experimental results show that our framework, Xl-OpSumm powered by Llama-3-8B-8k, achieves an average ROUGE-1 F1 gain of 4.38% and a ROUGE-L F1 gain of 3.70% over the next best-performing model.",
    "authors": [
      "Sri Raghava Muddu",
      "Rupasai Rangaraju",
      "Tejpalsingh Siledar",
      "Swaroop Nath",
      "Pushpak Bhattacharyya",
      "Swaprava Nath",
      "Suman Banerjee",
      "Amey Patil",
      "Muthusamy Chelliah",
      "Sudhanshu Shekhar Singh",
      "Nikesh Garera"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-16T10:36:41Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10886v1"
  },
  {
    "arxiv_id": "2406.10729v3",
    "entry_id": "http://arxiv.org/abs/2406.10729v3",
    "title": "A Comprehensive Survey of Foundation Models in Medicine",
    "summary": "Foundation models (FMs) are large-scale deep learning models trained on massive datasets, often using self-supervised learning techniques. These models serve as a versatile base for a wide range of downstream tasks, including those in medicine and healthcare. FMs have demonstrated remarkable success across multiple healthcare domains. However, existing surveys in this field do not comprehensively cover all areas where FMs have made significant strides. In this survey, we present a comprehensive review of FMs in medicine, focusing on their evolution, learning strategies, flagship models, applications, and associated challenges. We examine how prominent FMs, such as the BERT and GPT families, are transforming various aspects of healthcare, including clinical large language models, medical image analysis, and omics research. Additionally, we provide a detailed taxonomy of FM-enabled healthcare applications, spanning clinical natural language processing, medical computer vision, graph learning, and other biology- and omics- related tasks. Despite the transformative potentials of FMs, they also pose unique challenges. This survey delves into these challenges and highlights open research questions and lessons learned to guide researchers and practitioners. Our goal is to provide valuable insights into the capabilities of FMs in health, facilitating responsible deployment and mitigating associated risks.",
    "authors": [
      "Wasif Khan",
      "Seowung Leem",
      "Kyle B. See",
      "Joshua K. Wong",
      "Shaoting Zhang",
      "Ruogu Fang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-06-15T20:04:06Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10729v3"
  },
  {
    "arxiv_id": "2406.11903v1",
    "entry_id": "http://arxiv.org/abs/2406.11903v1",
    "title": "A Survey of Large Language Models for Financial Applications: Progress, Prospects and Challenges",
    "summary": "Recent advances in large language models (LLMs) have unlocked novel opportunities for machine learning applications in the financial domain. These models have demonstrated remarkable capabilities in understanding context, processing vast amounts of data, and generating human-preferred contents. In this survey, we explore the application of LLMs on various financial tasks, focusing on their potential to transform traditional practices and drive innovation. We provide a discussion of the progress and advantages of LLMs in financial contexts, analyzing their advanced technologies as well as prospective capabilities in contextual understanding, transfer learning flexibility, complex emotion detection, etc. We then highlight this survey for categorizing the existing literature into key application areas, including linguistic tasks, sentiment analysis, financial time series, financial reasoning, agent-based modeling, and other applications. For each application area, we delve into specific methodologies, such as textual analysis, knowledge-based analysis, forecasting, data augmentation, planning, decision support, and simulations. Furthermore, a comprehensive collection of datasets, model assets, and useful codes associated with mainstream applications are presented as resources for the researchers and practitioners. Finally, we outline the challenges and opportunities for future research, particularly emphasizing a number of distinctive aspects in this field. We hope our work can help facilitate the adoption and further development of LLMs in the financial sector.",
    "authors": [
      "Yuqi Nie",
      "Yaxuan Kong",
      "Xiaowen Dong",
      "John M. Mulvey",
      "H. Vincent Poor",
      "Qingsong Wen",
      "Stefan Zohren"
    ],
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "q-fin.CP"
    ],
    "published": "2024-06-15T16:11:35Z",
    "pdf_url": "https://arxiv.org/pdf/2406.11903v1"
  },
  {
    "arxiv_id": "2406.10540v1",
    "entry_id": "http://arxiv.org/abs/2406.10540v1",
    "title": "Generating and Evolving Reward Functions for Highway Driving with Large Language Models",
    "summary": "Reinforcement Learning (RL) plays a crucial role in advancing autonomous driving technologies by maximizing reward functions to achieve the optimal policy. However, crafting these reward functions has been a complex, manual process in many practices. To reduce this complexity, we introduce a novel framework that integrates Large Language Models (LLMs) with RL to improve reward function design in autonomous driving. This framework utilizes the coding capabilities of LLMs, proven in other areas, to generate and evolve reward functions for highway scenarios. The framework starts with instructing LLMs to create an initial reward function code based on the driving environment and task descriptions. This code is then refined through iterative cycles involving RL training and LLMs' reflection, which benefits from their ability to review and improve the output. We have also developed a specific prompt template to improve LLMs' understanding of complex driving simulations, ensuring the generation of effective and error-free code. Our experiments in a highway driving simulator across three traffic configurations show that our method surpasses expert handcrafted reward functions, achieving a 22% higher average success rate. This not only indicates safer driving but also suggests significant gains in development productivity.",
    "authors": [
      "Xu Han",
      "Qiannan Yang",
      "Xianda Chen",
      "Xiaowen Chu",
      "Meixin Zhu"
    ],
    "categories": [
      "cs.AI",
      "cs.NE",
      "cs.RO"
    ],
    "published": "2024-06-15T07:50:10Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10540v1"
  },
  {
    "arxiv_id": "2406.09831v2",
    "entry_id": "http://arxiv.org/abs/2406.09831v2",
    "title": "Recent Advances in Federated Learning Driven Large Language Models: A Survey on Architecture, Performance, and Security",
    "summary": "Federated Learning (FL) offers a promising paradigm for training Large Language Models (LLMs) in a decentralized manner while preserving data privacy and minimizing communication overhead. This survey examines recent advancements in FL-driven LLMs, with a particular emphasis on architectural designs, performance optimization, and security concerns, including the emerging area of machine unlearning. In this context, machine unlearning refers to the systematic removal of specific data contributions from trained models to comply with privacy regulations such as the Right to be Forgotten. We review a range of strategies enabling unlearning in federated LLMs, including perturbation-based methods, model decomposition, and incremental retraining, while evaluating their trade-offs in terms of efficiency, privacy guarantees, and model utility. Through selected case studies and empirical evaluations, we analyze how these methods perform in practical FL scenarios. This survey identifies critical research directions toward developing secure, adaptable, and high-performing federated LLM systems for real-world deployment.",
    "authors": [
      "Youyang Qu",
      "Ming Liu",
      "Tianqing Zhu",
      "Longxiang Gao",
      "Shui Yu",
      "Wanlei Zhou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.NE"
    ],
    "published": "2024-06-14T08:40:58Z",
    "pdf_url": "https://arxiv.org/pdf/2406.09831v2"
  },
  {
    "arxiv_id": "2406.10307v1",
    "entry_id": "http://arxiv.org/abs/2406.10307v1",
    "title": "What is the best model? Application-driven Evaluation for Large Language Models",
    "summary": "General large language models enhanced with supervised fine-tuning and reinforcement learning from human feedback are increasingly popular in academia and industry as they generalize foundation models to various practical tasks in a prompt manner. To assist users in selecting the best model in practical application scenarios, i.e., choosing the model that meets the application requirements while minimizing cost, we introduce A-Eval, an application-driven LLMs evaluation benchmark for general large language models. First, we categorize evaluation tasks into five main categories and 27 sub-categories from a practical application perspective. Next, we construct a dataset comprising 678 question-and-answer pairs through a process of collecting, annotating, and reviewing. Then, we design an objective and effective evaluation method and evaluate a series of LLMs of different scales on A-Eval. Finally, we reveal interesting laws regarding model scale and task difficulty level and propose a feasible method for selecting the best model. Through A-Eval, we provide clear empirical and engineer guidance for selecting the best model, reducing barriers to selecting and using LLMs and promoting their application and development. Our benchmark is publicly available at https://github.com/UnicomAI/DataSet/tree/main/TestData/GeneralAbility.",
    "authors": [
      "Shiguo Lian",
      "Kaikai Zhao",
      "Xinhui Liu",
      "Xuejiao Lei",
      "Bikun Yang",
      "Wenjing Zhang",
      "Kai Wang",
      "Zhaoxiang Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-14T04:52:15Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10307v1"
  },
  {
    "arxiv_id": "2406.09671v1",
    "entry_id": "http://arxiv.org/abs/2406.09671v1",
    "title": "Evaluating ChatGPT-4 Vision on Brazil's National Undergraduate Computer Science Exam",
    "summary": "The recent integration of visual capabilities into Large Language Models (LLMs) has the potential to play a pivotal role in science and technology education, where visual elements such as diagrams, charts, and tables are commonly used to improve the learning experience. This study investigates the performance of ChatGPT-4 Vision, OpenAI's most advanced visual model at the time the study was conducted, on the Bachelor in Computer Science section of Brazil's 2021 National Undergraduate Exam (ENADE). By presenting the model with the exam's open and multiple-choice questions in their original image format and allowing for reassessment in response to differing answer keys, we were able to evaluate the model's reasoning and self-reflecting capabilities in a large-scale academic assessment involving textual and visual content. ChatGPT-4 Vision significantly outperformed the average exam participant, positioning itself within the top 10 best score percentile. While it excelled in questions that incorporated visual elements, it also encountered challenges with question interpretation, logical reasoning, and visual acuity. The involvement of an independent expert panel to review cases of disagreement between the model and the answer key revealed some poorly constructed questions containing vague or ambiguous statements, calling attention to the critical need for improved question design in future exams. Our findings suggest that while ChatGPT-4 Vision shows promise in multimodal academic evaluations, human oversight remains crucial for verifying the model's accuracy and ensuring the fairness of high-stakes educational exams. The paper's research materials are publicly available at https://github.com/nabormendonca/gpt-4v-enade-cs-2021.",
    "authors": [
      "Nabor C. Mendonça"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-06-14T02:42:30Z",
    "pdf_url": "https://arxiv.org/pdf/2406.09671v1"
  },
  {
    "arxiv_id": "2406.10303v2",
    "entry_id": "http://arxiv.org/abs/2406.10303v2",
    "title": "A Survey on Large Language Models from General Purpose to Medical Applications: Datasets, Methodologies, and Evaluations",
    "summary": "Large Language Models (LLMs) have demonstrated surprising performance across various natural language processing tasks. Recently, medical LLMs enhanced with domain-specific knowledge have exhibited excellent capabilities in medical consultation and diagnosis. These models can smoothly simulate doctor-patient dialogues and provide professional medical advice. Most medical LLMs are developed through continued training of open-source general LLMs, which require significantly fewer computational resources than training LLMs from scratch. Additionally, this approach offers better patient privacy protection than API-based solutions. Given the above advantages, this survey systematically summarizes how to train medical LLMs based on open-source general LLMs from a more fine-grained perspective. It covers (a) how to acquire training corpus and construct customized medical training sets, (b) how to choose an appropriate training paradigm, (c) how to choose a suitable evaluation benchmark, and (d) existing challenges and promising research directions are discussed. This survey can provide guidance for the development of LLMs focused on various medical applications, such as medical education, diagnostic planning, and clinical assistants. Related resources and supplemental information can be found on the GitHub repository.",
    "authors": [
      "Jinqiang Wang",
      "Huansheng Ning",
      "Yi Peng",
      "Qikai Wei",
      "Daniel Tesfai",
      "Wenwei Mao",
      "Tao Zhu",
      "Runhe Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-14T02:42:20Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10303v2"
  },
  {
    "arxiv_id": "2406.09646v1",
    "entry_id": "http://arxiv.org/abs/2406.09646v1",
    "title": "A Survey of Video Datasets for Grounded Event Understanding",
    "summary": "While existing video benchmarks largely consider specialized downstream tasks like retrieval or question-answering (QA), contemporary multimodal AI systems must be capable of well-rounded common-sense reasoning akin to human visual understanding. A critical component of human temporal-visual perception is our ability to identify and cognitively model \"things happening\", or events. Historically, video benchmark tasks have implicitly tested for this ability (e.g., video captioning, in which models describe visual events with natural language), but they do not consider video event understanding as a task in itself. Recent work has begun to explore video analogues to textual event extraction but consists of competing task definitions and datasets limited to highly specific event types. Therefore, while there is a rich domain of event-centric video research spanning the past 10+ years, it is unclear how video event understanding should be framed and what resources we have to study it. In this paper, we survey 105 video datasets that require event understanding capability, consider how they contribute to the study of robust event understanding in video, and assess proposed video event extraction tasks in the context of this body of research. We propose suggestions informed by this survey for dataset curation and task framing, with an emphasis on the uniquely temporal nature of video events and ambiguity in visual content.",
    "authors": [
      "Kate Sanders",
      "Benjamin Van Durme"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-06-14T00:36:55Z",
    "pdf_url": "https://arxiv.org/pdf/2406.09646v1"
  },
  {
    "arxiv_id": "2406.10300v1",
    "entry_id": "http://arxiv.org/abs/2406.10300v1",
    "title": "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
    "summary": "Large Language Models (LLMs) have become widely adopted recently. Research explores their use both as autonomous agents and as tools for software engineering. LLM-integrated applications, on the other hand, are software systems that leverage an LLM to perform tasks that would otherwise be impossible or require significant coding effort. While LLM-integrated application engineering is emerging as new discipline, its terminology, concepts and methods need to be established. This study provides a taxonomy for LLM-integrated applications, offering a framework for analyzing and describing these systems. It also demonstrates various ways to utilize LLMs in applications, as well as options for implementing such integrations.\n  Following established methods, we analyze a sample of recent LLM-integrated applications to identify relevant dimensions. We evaluate the taxonomy by applying it to additional cases. This review shows that applications integrate LLMs in numerous ways for various purposes. Frequently, they comprise multiple LLM integrations, which we term ``LLM components''. To gain a clear understanding of an application's architecture, we examine each LLM component separately. We identify thirteen dimensions along which to characterize an LLM component, including the LLM skills leveraged, the format of the output, and more. LLM-integrated applications are described as combinations of their LLM components. We suggest a concise representation using feature vectors for visualization.\n  The taxonomy is effective for describing LLM-integrated applications. It can contribute to theory building in the nascent field of LLM-integrated application engineering and aid in developing such systems. Researchers and practitioners explore numerous creative ways to leverage LLMs in applications. Though challenges persist, integrating LLMs may revolutionize the way software systems are built.",
    "authors": [
      "Irene Weber"
    ],
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-13T21:32:56Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10300v1"
  },
  {
    "arxiv_id": "2406.09559v1",
    "entry_id": "http://arxiv.org/abs/2406.09559v1",
    "title": "Decoding the Diversity: A Review of the Indic AI Research Landscape",
    "summary": "This review paper provides a comprehensive overview of large language model (LLM) research directions within Indic languages. Indic languages are those spoken in the Indian subcontinent, including India, Pakistan, Bangladesh, Sri Lanka, Nepal, and Bhutan, among others. These languages have a rich cultural and linguistic heritage and are spoken by over 1.5 billion people worldwide. With the tremendous market potential and growing demand for natural language processing (NLP) based applications in diverse languages, generative applications for Indic languages pose unique challenges and opportunities for research. Our paper deep dives into the recent advancements in Indic generative modeling, contributing with a taxonomy of research directions, tabulating 84 recent publications. Research directions surveyed in this paper include LLM development, fine-tuning existing LLMs, development of corpora, benchmarking and evaluation, as well as publications around specific techniques, tools, and applications. We found that researchers across the publications emphasize the challenges associated with limited data availability, lack of standardization, and the peculiar linguistic complexities of Indic languages. This work aims to serve as a valuable resource for researchers and practitioners working in the field of NLP, particularly those focused on Indic languages, and contributes to the development of more accurate and efficient LLM applications for these languages.",
    "authors": [
      "Sankalp KJ",
      "Vinija Jain",
      "Sreyoshi Bhaduri",
      "Tamoghna Roy",
      "Aman Chadha"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-13T19:55:20Z",
    "pdf_url": "https://arxiv.org/pdf/2406.09559v1"
  },
  {
    "arxiv_id": "2406.09363v3",
    "entry_id": "http://arxiv.org/abs/2406.09363v3",
    "title": "ElicitationGPT: Text Elicitation Mechanisms via Language Models",
    "summary": "Scoring rules evaluate probabilistic forecasts of an unknown state against the realized state and are a fundamental building block in the incentivized elicitation of information. This paper develops mechanisms for scoring elicited text against ground truth text by reducing the textual information elicitation problem to a forecast elicitation problem, via domain-knowledge-free queries to a large language model (specifically ChatGPT), and empirically evaluates their alignment with human preferences. Our theoretical analysis shows that the reduction achieves provable properness via black-box language models. The empirical evaluation is conducted on peer reviews from a peer-grading dataset, in comparison to manual instructor scores for the peer reviews.\n  Our results suggest a paradigm of algorithmic artificial intelligence that may be useful for developing artificial intelligence technologies with provable guarantees.",
    "authors": [
      "Yifan Wu",
      "Jason Hartline"
    ],
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "published": "2024-06-13T17:49:10Z",
    "pdf_url": "https://arxiv.org/pdf/2406.09363v3"
  },
  {
    "arxiv_id": "2406.09105v2",
    "entry_id": "http://arxiv.org/abs/2406.09105v2",
    "title": "INS-MMBench: A Comprehensive Benchmark for Evaluating LVLMs' Performance in Insurance",
    "summary": "Large Vision-Language Models (LVLMs) and Multimodal Large Language Models (MLLMs) have demonstrated outstanding performance in various general multimodal applications and have shown increasing promise in specialized domains. However, their potential in the insurance domain-characterized by diverse application scenarios and rich multimodal data-remains largely underexplored. To date, there is no systematic review of multimodal tasks, nor a benchmark specifically designed to assess the capabilities of LVLMs in insurance. This gap hinders the development of LVLMs within the insurance industry. This study systematically reviews and categorizes multimodal tasks for 4 representative types of insurance: auto, property, health, and agricultural. We introduce INS-MMBench, the first hierarchical benchmark tailored for the insurance domain. INS-MMBench encompasses 22 fundamental tasks, 12 meta-tasks and 5 scenario tasks, enabling a comprehensive and progressive assessment from basic capabilities to real-world use cases. We benchmark 11 leading LVLMs, including closed-source models such as GPT-4o and open-source models like LLaVA. Our evaluation validates the effectiveness of INS-MMBench and offers detailed insights into the strengths and limitations of current LVLMs on a variety of insurance-related multimodal tasks. We hope that INS-MMBench will accelerate the integration of LVLMs into the insurance industry and foster interdisciplinary research. Our dataset and evaluation code are available at https://github.com/FDU-INS/INS-MMBench.",
    "authors": [
      "Chenwei Lin",
      "Hanjia Lyu",
      "Xian Xu",
      "Jiebo Luo"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-13T13:31:49Z",
    "pdf_url": "https://arxiv.org/pdf/2406.09105v2"
  },
  {
    "arxiv_id": "2406.09062v2",
    "entry_id": "http://arxiv.org/abs/2406.09062v2",
    "title": "State-Space Modeling in Long Sequence Processing: A Survey on Recurrence in the Transformer Era",
    "summary": "Effectively learning from sequential data is a longstanding goal of Artificial Intelligence, especially in the case of long sequences. From the dawn of Machine Learning, several researchers have pursued algorithms and architectures capable of processing sequences of patterns, retaining information about past inputs while still leveraging future data, without losing precious long-term dependencies and correlations. While such an ultimate goal is inspired by the human hallmark of continuous real-time processing of sensory information, several solutions have simplified the learning paradigm by artificially limiting the processed context or dealing with sequences of limited length, given in advance. These solutions were further emphasized by the ubiquity of Transformers, which initially overshadowed the role of Recurrent Neural Nets. However, recurrent networks are currently experiencing a strong recent revival due to the growing popularity of (deep) State-Space models and novel instances of large-context Transformers, which are both based on recurrent computations that aim to go beyond several limits of currently ubiquitous technologies. The fast development of Large Language Models has renewed the interest in efficient solutions to process data over time. This survey provides an in-depth summary of the latest approaches that are based on recurrent models for sequential data processing. A complete taxonomy of recent trends in architectural and algorithmic solutions is reported and discussed, guiding researchers in this appealing research field. The emerging picture suggests that there is room for exploring novel routes, constituted by learning algorithms that depart from the standard Backpropagation Through Time, towards a more realistic scenario where patterns are effectively processed online, leveraging local-forward computations, and opening new directions for research on this topic.",
    "authors": [
      "Matteo Tiezzi",
      "Michele Casoni",
      "Alessandro Betti",
      "Marco Gori",
      "Stefano Melacci"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-06-13T12:51:22Z",
    "pdf_url": "https://arxiv.org/pdf/2406.09062v2"
  },
  {
    "arxiv_id": "2406.10294v1",
    "entry_id": "http://arxiv.org/abs/2406.10294v1",
    "title": "RelevAI-Reviewer: A Benchmark on AI Reviewers for Survey Paper Relevance",
    "summary": "Recent advancements in Artificial Intelligence (AI), particularly the widespread adoption of Large Language Models (LLMs), have significantly enhanced text analysis capabilities. This technological evolution offers considerable promise for automating the review of scientific papers, a task traditionally managed through peer review by fellow researchers. Despite its critical role in maintaining research quality, the conventional peer-review process is often slow and subject to biases, potentially impeding the swift propagation of scientific knowledge. In this paper, we propose RelevAI-Reviewer, an automatic system that conceptualizes the task of survey paper review as a classification problem, aimed at assessing the relevance of a paper in relation to a specified prompt, analogous to a \"call for papers\". To address this, we introduce a novel dataset comprised of 25,164 instances. Each instance contains one prompt and four candidate papers, each varying in relevance to the prompt. The objective is to develop a machine learning (ML) model capable of determining the relevance of each paper and identifying the most pertinent one. We explore various baseline approaches, including traditional ML classifiers like Support Vector Machine (SVM) and advanced language models such as BERT. Preliminary findings indicate that the BERT-based end-to-end classifier surpasses other conventional ML methods in performance. We present this problem as a public challenge to foster engagement and interest in this area of research.",
    "authors": [
      "Paulo Henrique Couto",
      "Quang Phuoc Ho",
      "Nageeta Kumari",
      "Benedictus Kent Rachmat",
      "Thanh Gia Hieu Khuong",
      "Ihsan Ullah",
      "Lisheng Sun-Hosoya"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-13T06:42:32Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10294v1"
  },
  {
    "arxiv_id": "2406.09464v1",
    "entry_id": "http://arxiv.org/abs/2406.09464v1",
    "title": "GPT-ology, Computational Models, Silicon Sampling: How should we think about LLMs in Cognitive Science?",
    "summary": "Large Language Models have taken the cognitive science world by storm. It is perhaps timely now to take stock of the various research paradigms that have been used to make scientific inferences about ``cognition\" in these models or about human cognition. We review several emerging research paradigms -- GPT-ology, LLMs-as-computational-models, and ``silicon sampling\" -- and review recent papers that have used LLMs under these paradigms. In doing so, we discuss their claims as well as challenges to scientific inference under these various paradigms. We highlight several outstanding issues about LLMs that have to be addressed to push our science forward: closed-source vs open-sourced models; (the lack of visibility of) training data; and reproducibility in LLM research, including forming conventions on new task ``hyperparameters\" like instructions and prompts.",
    "authors": [
      "Desmond C. Ong"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-06-13T04:19:17Z",
    "pdf_url": "https://arxiv.org/pdf/2406.09464v1"
  },
  {
    "arxiv_id": "2406.08787v2",
    "entry_id": "http://arxiv.org/abs/2406.08787v2",
    "title": "A Survey on Compositional Learning of AI Models: Theoretical and Experimental Practices",
    "summary": "Compositional learning, mastering the ability to combine basic concepts and construct more intricate ones, is crucial for human cognition, especially in human language comprehension and visual perception. This notion is tightly connected to generalization over unobserved situations. Despite its integral role in intelligence, there is a lack of systematic theoretical and experimental research methodologies, making it difficult to analyze the compositional learning abilities of computational models. In this paper, we survey the literature on compositional learning of AI models and the connections made to cognitive studies. We identify abstract concepts of compositionality in cognitive and linguistic studies and connect these to the computational challenges faced by language and vision models in compositional reasoning. We overview the formal definitions, tasks, evaluation benchmarks, various computational models, and theoretical findings. Our primary focus is on linguistic benchmarks and combining language and vision, though there is a large amount of research on compositional concept learning in the computer vision community alone. We cover modern studies on large language models to provide a deeper understanding of the cutting-edge compositional capabilities exhibited by state-of-the-art AI models and pinpoint important directions for future research.",
    "authors": [
      "Sania Sinha",
      "Tanawan Premsri",
      "Parisa Kordjamshidi"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-06-13T03:46:21Z",
    "pdf_url": "https://arxiv.org/pdf/2406.08787v2"
  },
  {
    "arxiv_id": "2406.10291v3",
    "entry_id": "http://arxiv.org/abs/2406.10291v3",
    "title": "ResearchArena: Benchmarking Large Language Models' Ability to Collect and Organize Information as Research Agents",
    "summary": "Large language models (LLMs) excel across many natural language processing tasks but face challenges in domain-specific, analytical tasks such as conducting research surveys. This study introduces ResearchArena, a benchmark designed to evaluate LLMs' capabilities in conducting academic surveys -- a foundational step in academic research. ResearchArena models the process in three stages: (1) information discovery, identifying relevant literature; (2) information selection, evaluating papers' relevance and impact; and (3) information organization, structuring knowledge into hierarchical frameworks such as mind-maps. Notably, mind-map construction is treated as a bonus task, reflecting its supplementary role in survey-writing. To support these evaluations, we construct an offline environment of 12M full-text academic papers and 7.9K survey papers. To ensure ethical compliance, we do not redistribute copyrighted materials; instead, we provide code to construct the environment from the Semantic Scholar Open Research Corpus (S2ORC). Preliminary evaluations reveal that LLM-based approaches underperform compared to simpler keyword-based retrieval methods, though recent reasoning models such as DeepSeek-R1 show slightly better zero-shot performance. These results underscore significant opportunities for advancing LLMs in autonomous research. We open-source the code to construct the ResearchArena benchmark at https://github.com/cxcscmu/ResearchArena.",
    "authors": [
      "Hao Kang",
      "Chenyan Xiong"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2024-06-13T03:26:30Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10291v3"
  },
  {
    "arxiv_id": "2406.08446v2",
    "entry_id": "http://arxiv.org/abs/2406.08446v2",
    "title": "OLMES: A Standard for Language Model Evaluations",
    "summary": "Progress in AI is often demonstrated by new models claiming improved performance on tasks measuring model capabilities. Evaluating language models can be particularly challenging, as choices of how a model is evaluated on a task can lead to large changes in measured performance. There is no common standard setup, so different models are evaluated on the same tasks in different ways, leading to claims about which models perform best not being reproducible. We propose OLMES, a completely documented, practical, open standard for reproducible LLM evaluations. In developing this standard, we identify and review the varying factors in evaluation practices adopted by the community - such as details of prompt formatting, choice of in-context examples, probability normalizations, and task formulation. In particular, OLMES supports meaningful comparisons between smaller base models that require the unnatural \"cloze\" formulation of multiple-choice questions against larger models that can utilize the original formulation. OLMES includes well-considered, documented recommendations guided by results from existing literature as well as new experiments resolving open questions.",
    "authors": [
      "Yuling Gu",
      "Oyvind Tafjord",
      "Bailey Kuehl",
      "Dany Haddad",
      "Jesse Dodge",
      "Hannaneh Hajishirzi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-12T17:37:09Z",
    "pdf_url": "https://arxiv.org/pdf/2406.08446v2"
  },
  {
    "arxiv_id": "2406.08426v6",
    "entry_id": "http://arxiv.org/abs/2406.08426v6",
    "title": "Next-Generation Database Interfaces: A Survey of LLM-based Text-to-SQL",
    "summary": "Generating accurate SQL from users' natural language questions (text-to-SQL) remains a long-standing challenge due to the complexities involved in user question understanding, database schema comprehension, and SQL generation. Traditional text-to-SQL systems, which combine human engineering and deep neural networks, have made significant progress. Subsequently, pre-trained language models (PLMs) have been developed for text-to-SQL tasks, achieving promising results. However, as modern databases and user questions grow more complex, PLMs with a limited parameter size often produce incorrect SQL. This necessitates more sophisticated and tailored optimization methods, which restricts the application of PLM-based systems. Recently, large language models (LLMs) have shown significant capabilities in natural language understanding as model scale increases. Thus, integrating LLM-based solutions can bring unique opportunities, improvements, and solutions to text-to-SQL research. In this survey, we provide a comprehensive review of existing LLM-based text-to-SQL studies. Specifically, we offer a brief overview of the technical challenges and evolutionary process of text-to-SQL. Next, we introduce the datasets and metrics designed to evaluate text-to-SQL systems. Subsequently, we present a systematic analysis of recent advances in LLM-based text-to-SQL. Finally, we make a summarization and discuss the remaining challenges in this field and suggest expectations for future research directions. All the related resources of LLM-based, including research papers, benchmarks, and open-source projects, are collected for the community in our repository: https://github.com/DEEP-PolyU/Awesome-LLM-based-Text2SQL.",
    "authors": [
      "Zijin Hong",
      "Zheng Yuan",
      "Qinggang Zhang",
      "Hao Chen",
      "Junnan Dong",
      "Feiran Huang",
      "Xiao Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "published": "2024-06-12T17:13:17Z",
    "pdf_url": "https://arxiv.org/pdf/2406.08426v6"
  },
  {
    "arxiv_id": "2406.08413v1",
    "entry_id": "http://arxiv.org/abs/2406.08413v1",
    "title": "Memory Is All You Need: An Overview of Compute-in-Memory Architectures for Accelerating Large Language Model Inference",
    "summary": "Large language models (LLMs) have recently transformed natural language processing, enabling machines to generate human-like text and engage in meaningful conversations. This development necessitates speed, efficiency, and accessibility in LLM inference as the computational and memory requirements of these systems grow exponentially. Meanwhile, advancements in computing and memory capabilities are lagging behind, exacerbated by the discontinuation of Moore's law. With LLMs exceeding the capacity of single GPUs, they require complex, expert-level configurations for parallel processing. Memory accesses become significantly more expensive than computation, posing a challenge for efficient scaling, known as the memory wall. Here, compute-in-memory (CIM) technologies offer a promising solution for accelerating AI inference by directly performing analog computations in memory, potentially reducing latency and power consumption. By closely integrating memory and compute elements, CIM eliminates the von Neumann bottleneck, reducing data movement and improving energy efficiency. This survey paper provides an overview and analysis of transformer-based models, reviewing various CIM architectures and exploring how they can address the imminent challenges of modern AI computing systems. We discuss transformer-related operators and their hardware acceleration schemes and highlight challenges, trends, and insights in corresponding CIM designs.",
    "authors": [
      "Christopher Wolters",
      "Xiaoxuan Yang",
      "Ulf Schlichtmann",
      "Toyotaro Suzumura"
    ],
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "published": "2024-06-12T16:57:58Z",
    "pdf_url": "https://arxiv.org/pdf/2406.08413v1"
  },
  {
    "arxiv_id": "2406.08398v1",
    "entry_id": "http://arxiv.org/abs/2406.08398v1",
    "title": "cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers",
    "summary": "An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers. Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists. This work introduces Conversational Papers (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv. We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from LaTeX source files. Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset.",
    "authors": [
      "Anirudh Sundar",
      "Jin Xu",
      "William Gay",
      "Christopher Richardson",
      "Larry Heck"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-12T16:46:12Z",
    "pdf_url": "https://arxiv.org/pdf/2406.08398v1"
  },
  {
    "arxiv_id": "2406.08223v2",
    "entry_id": "http://arxiv.org/abs/2406.08223v2",
    "title": "Research Trends for the Interplay between Large Language Models and Knowledge Graphs",
    "summary": "This survey investigates the synergistic relationship between Large Language Models (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's capabilities in understanding, reasoning, and language processing. It aims to address gaps in current research by exploring areas such as KG Question Answering, ontology generation, KG validation, and the enhancement of KG accuracy and consistency through LLMs. The paper further examines the roles of LLMs in generating descriptive texts and natural language queries for KGs. Through a structured analysis that includes categorizing LLM-KG interactions, examining methodologies, and investigating collaborative uses and potential biases, this study seeks to provide new insights into the combined potential of LLMs and KGs. It highlights the importance of their interaction for improving AI applications and outlines future research directions.",
    "authors": [
      "Hanieh Khorashadizadeh",
      "Fatima Zahra Amara",
      "Morteza Ezzabady",
      "Frédéric Ieng",
      "Sanju Tiwari",
      "Nandana Mihindukulasooriya",
      "Jinghua Groppe",
      "Soror Sahri",
      "Farah Benamara",
      "Sven Groppe"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-06-12T13:52:38Z",
    "pdf_url": "https://arxiv.org/pdf/2406.08223v2"
  },
  {
    "arxiv_id": "2406.08115v1",
    "entry_id": "http://arxiv.org/abs/2406.08115v1",
    "title": "Resource Allocation and Workload Scheduling for Large-Scale Distributed Deep Learning: A Survey",
    "summary": "With rapidly increasing distributed deep learning workloads in large-scale data centers, efficient distributed deep learning framework strategies for resource allocation and workload scheduling have become the key to high-performance deep learning. The large-scale environment with large volumes of datasets, models, and computational and communication resources raises various unique challenges for resource allocation and workload scheduling in distributed deep learning, such as scheduling complexity, resource and workload heterogeneity, and fault tolerance. To uncover these challenges and corresponding solutions, this survey reviews the literature, mainly from 2019 to 2024, on efficient resource allocation and workload scheduling strategies for large-scale distributed DL. We explore these strategies by focusing on various resource types, scheduling granularity levels, and performance goals during distributed training and inference processes. We highlight critical challenges for each topic and discuss key insights of existing technologies. To illustrate practical large-scale resource allocation and workload scheduling in real distributed deep learning scenarios, we use a case study of training large language models. This survey aims to encourage computer science, artificial intelligence, and communications researchers to understand recent advances and explore future research directions for efficient framework strategies for large-scale distributed deep learning.",
    "authors": [
      "Feng Liang",
      "Zhen Zhang",
      "Haifeng Lu",
      "Chengming Li",
      "Victor C. M. Leung",
      "Yanyi Guo",
      "Xiping Hu"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2024-06-12T11:51:44Z",
    "pdf_url": "https://arxiv.org/pdf/2406.08115v1"
  },
  {
    "arxiv_id": "2406.08035v3",
    "entry_id": "http://arxiv.org/abs/2406.08035v3",
    "title": "LVBench: An Extreme Long Video Understanding Benchmark",
    "summary": "Recent progress in multimodal large language models has markedly enhanced the understanding of short videos (typically under one minute), and several evaluation datasets have emerged accordingly. However, these advancements fall short of meeting the demands of real-world applications such as embodied intelligence for long-term decision-making, in-depth movie reviews and discussions, and live sports commentary, all of which require comprehension of long videos spanning several hours. To address this gap, we introduce LVBench, a benchmark specifically designed for long video understanding. Our dataset comprises publicly sourced videos and encompasses a diverse set of tasks aimed at long video comprehension and information extraction. LVBench is designed to challenge multimodal models to demonstrate long-term memory and extended comprehension capabilities. Our extensive evaluations reveal that current multimodal models still underperform on these demanding long video understanding tasks. Through LVBench, we aim to spur the development of more advanced models capable of tackling the complexities of long video comprehension. Our data and code are publicly available at: https://lvbench.github.io.",
    "authors": [
      "Weihan Wang",
      "Zehai He",
      "Wenyi Hong",
      "Yean Cheng",
      "Xiaohan Zhang",
      "Ji Qi",
      "Xiaotao Gu",
      "Shiyu Huang",
      "Bin Xu",
      "Yuxiao Dong",
      "Ming Ding",
      "Jie Tang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-06-12T09:36:52Z",
    "pdf_url": "https://arxiv.org/pdf/2406.08035v3"
  },
  {
    "arxiv_id": "2406.07962v2",
    "entry_id": "http://arxiv.org/abs/2406.07962v2",
    "title": "Toward a Method to Generate Capability Ontologies from Natural Language Descriptions",
    "summary": "To achieve a flexible and adaptable system, capability ontologies are increasingly leveraged to describe functions in a machine-interpretable way. However, modeling such complex ontological descriptions is still a manual and error-prone task that requires a significant amount of effort and ontology expertise. This contribution presents an innovative method to automate capability ontology modeling using Large Language Models (LLMs), which have proven to be well suited for such tasks. Our approach requires only a natural language description of a capability, which is then automatically inserted into a predefined prompt using a few-shot prompting technique. After prompting an LLM, the resulting capability ontology is automatically verified through various steps in a loop with the LLM to check the overall correctness of the capability ontology. First, a syntax check is performed, then a check for contradictions, and finally a check for hallucinations and missing ontology elements. Our method greatly reduces manual effort, as only the initial natural language description and a final human review and possible correction are necessary, thereby streamlining the capability ontology generation process.",
    "authors": [
      "Luis Miguel Vieira da Silva",
      "Aljosha Köcher",
      "Felix Gehlhoff",
      "Alexander Fay"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-06-12T07:41:44Z",
    "pdf_url": "https://arxiv.org/pdf/2406.07962v2"
  },
  {
    "arxiv_id": "2406.07778v2",
    "entry_id": "http://arxiv.org/abs/2406.07778v2",
    "title": "A Study of Backdoors in Instruction Fine-tuned Language Models",
    "summary": "Backdoor data poisoning, inserted within instruction examples used to fine-tune a foundation Large Language Model (LLM) for downstream tasks (\\textit{e.g.,} sentiment prediction), is a serious security concern due to the evasive nature of such attacks. The poisoning is usually in the form of a (seemingly innocuous) trigger word or phrase inserted into a very small fraction of the fine-tuning samples from a target class. Such backdoor attacks can: alter response sentiment, violate censorship, over-refuse (invoke censorship for legitimate queries), inject false content, or trigger nonsense responses (hallucinations). In this work we investigate the efficacy of instruction fine-tuning backdoor attacks as attack \"hyperparameters\" are varied under a variety of scenarios, considering: the trigger location in the poisoned examples; robustness to change in the trigger location, partial triggers, and synonym substitutions at test time; attack transfer from one (fine-tuning) domain to a related test domain; and clean-label vs. dirty-label poisoning. Based on our observations, we propose and evaluate two defenses against these attacks: i) a \\textit{during-fine-tuning defense} based on word-frequency counts that assumes the (possibly poisoned) fine-tuning dataset is available and identifies the backdoor trigger tokens; and ii) a \\textit{post-fine-tuning defense} based on downstream clean fine-tuning of the backdoored LLM with a small defense dataset. Finally, we provide a brief survey of related work on backdoor attacks and defenses.",
    "authors": [
      "Jayaram Raghuram",
      "George Kesidis",
      "David J. Miller"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-12T00:01:32Z",
    "pdf_url": "https://arxiv.org/pdf/2406.07778v2"
  },
  {
    "arxiv_id": "2406.10273v5",
    "entry_id": "http://arxiv.org/abs/2406.10273v5",
    "title": "Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis",
    "summary": "Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks.\n  Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis.\n  Method. We manually curated 193 unique scenarios leading to 1283 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other human experts to review the models and the former human experts' analysis. The reviewers analyzed 5,000 scenario analyses.\n  Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs as an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.",
    "authors": [
      "Matteo Esposito",
      "Francesco Palagiano",
      "Valentina Lenarduzzi",
      "Davide Taibi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.HC"
    ],
    "published": "2024-06-11T19:20:27Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10273v5"
  },
  {
    "arxiv_id": "2406.07494v3",
    "entry_id": "http://arxiv.org/abs/2406.07494v3",
    "title": "CADS: A Systematic Literature Review on the Challenges of Abstractive Dialogue Summarization",
    "summary": "Abstractive dialogue summarization is the task of distilling conversations into informative and concise summaries. Although reviews have been conducted on this topic, there is a lack of comprehensive work detailing the challenges of dialogue summarization, unifying the differing understanding of the task, and aligning proposed techniques, datasets, and evaluation metrics with the challenges. This article summarizes the research on Transformer-based abstractive summarization for English dialogues by systematically reviewing 1262 unique research papers published between 2019 and 2024, relying on the Semantic Scholar and DBLP databases. We cover the main challenges present in dialog summarization (i.e., language, structure, comprehension, speaker, salience, and factuality) and link them to corresponding techniques such as graph-based approaches, additional training tasks, and planning strategies, which typically overly rely on BART-based encoder-decoder models. We find that while some challenges, like language, have seen considerable progress, mainly due to training methods, others, such as comprehension, factuality, and salience, remain difficult and hold significant research opportunities. We investigate how these approaches are typically assessed, covering the datasets for the subdomains of dialogue (e.g., meeting, medical), the established automatic metrics and human evaluation approaches for assessing scores and annotator agreement. We observe that only a few datasets span across all subdomains. The ROUGE metric is the most used, while human evaluation is frequently reported without sufficient detail on inner-annotator agreement and annotation guidelines. Additionally, we discuss the possible implications of the recently explored large language models and conclude that despite a potential shift in relevance and difficulty, our described challenge taxonomy remains relevant.",
    "authors": [
      "Frederic Kirstein",
      "Jan Philip Wahle",
      "Bela Gipp",
      "Terry Ruas"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-11T17:30:22Z",
    "pdf_url": "https://arxiv.org/pdf/2406.07494v3"
  },
  {
    "arxiv_id": "2406.07353v1",
    "entry_id": "http://arxiv.org/abs/2406.07353v1",
    "title": "Toxic Memes: A Survey of Computational Perspectives on the Detection and Explanation of Meme Toxicities",
    "summary": "Internet memes, channels for humor, social commentary, and cultural expression, are increasingly used to spread toxic messages. Studies on the computational analyses of toxic memes have significantly grown over the past five years, and the only three surveys on computational toxic meme analysis cover only work published until 2022, leading to inconsistent terminology and unexplored trends. Our work fills this gap by surveying content-based computational perspectives on toxic memes, and reviewing key developments until early 2024. Employing the PRISMA methodology, we systematically extend the previously considered papers, achieving a threefold result. First, we survey 119 new papers, analyzing 158 computational works focused on content-based toxic meme analysis. We identify over 30 datasets used in toxic meme analysis and examine their labeling systems. Second, after observing the existence of unclear definitions of meme toxicity in computational works, we introduce a new taxonomy for categorizing meme toxicity types. We also note an expansion in computational tasks beyond the simple binary classification of memes as toxic or non-toxic, indicating a shift towards achieving a nuanced comprehension of toxicity. Third, we identify three content-based dimensions of meme toxicity under automatic study: target, intent, and conveyance tactics. We develop a framework illustrating the relationships between these dimensions and meme toxicities. The survey analyzes key challenges and recent trends, such as enhanced cross-modal reasoning, integrating expert and cultural knowledge, the demand for automatic toxicity explanations, and handling meme toxicity in low-resource languages. Also, it notes the rising use of Large Language Models (LLMs) and generative AI for detecting and generating toxic memes. Finally, it proposes pathways for advancing toxic meme detection and interpretation.",
    "authors": [
      "Delfina Sol Martinez Pandiani",
      "Erik Tjong Kim Sang",
      "Davide Ceolin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.SI"
    ],
    "published": "2024-06-11T15:22:48Z",
    "pdf_url": "https://arxiv.org/pdf/2406.07353v1"
  },
  {
    "arxiv_id": "2406.07259v1",
    "entry_id": "http://arxiv.org/abs/2406.07259v1",
    "title": "Scientific Computing with Large Language Models",
    "summary": "We provide an overview of the emergence of large language models for scientific computing applications. We highlight use cases that involve natural language processing of scientific documents and specialized languages designed to describe physical systems. For the former, chatbot style applications appear in medicine, mathematics and physics and can be used iteratively with domain experts for problem solving. We also review specialized languages within molecular biology, the languages of molecules, proteins, and DNA where language models are being used to predict properties and even create novel physical systems at much faster rates than traditional computing methods.",
    "authors": [
      "Christopher Culver",
      "Peter Hicks",
      "Mihailo Milenkovic",
      "Sanjif Shanmugavelu",
      "Tobias Becker"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-11T13:39:07Z",
    "pdf_url": "https://arxiv.org/pdf/2406.07259v1"
  },
  {
    "arxiv_id": "2406.06863v1",
    "entry_id": "http://arxiv.org/abs/2406.06863v1",
    "title": "Ollabench: Evaluating LLMs' Reasoning for Human-centric Interdependent Cybersecurity",
    "summary": "Large Language Models (LLMs) have the potential to enhance Agent-Based Modeling by better representing complex interdependent cybersecurity systems, improving cybersecurity threat modeling and risk management. However, evaluating LLMs in this context is crucial for legal compliance and effective application development. Existing LLM evaluation frameworks often overlook the human factor and cognitive computing capabilities essential for interdependent cybersecurity. To address this gap, I propose OllaBench, a novel evaluation framework that assesses LLMs' accuracy, wastefulness, and consistency in answering scenario-based information security compliance and non-compliance questions. OllaBench is built on a foundation of 24 cognitive behavioral theories and empirical evidence from 38 peer-reviewed papers. OllaBench was used to evaluate 21 LLMs, including both open-weight and commercial models from OpenAI, Anthropic, Google, Microsoft, Meta and so on. The results reveal that while commercial LLMs have the highest overall accuracy scores, there is significant room for improvement. Smaller low-resolution open-weight LLMs are not far behind in performance, and there are significant differences in token efficiency and consistency among the evaluated models. OllaBench provides a user-friendly interface and supports a wide range of LLM platforms, making it a valuable tool for researchers and solution developers in the field of human-centric interdependent cybersecurity and beyond.",
    "authors": [
      "Tam n. Nguyen"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-06-11T00:35:39Z",
    "pdf_url": "https://arxiv.org/pdf/2406.06863v1"
  },
  {
    "arxiv_id": "2406.06852v5",
    "entry_id": "http://arxiv.org/abs/2406.06852v5",
    "title": "A Survey of Recent Backdoor Attacks and Defenses in Large Language Models",
    "summary": "Large Language Models (LLMs), which bridge the gap between human language understanding and complex problem-solving, achieve state-of-the-art performance on several NLP tasks, particularly in few-shot and zero-shot settings. Despite the demonstrable efficacy of LLMs, due to constraints on computational resources, users have to engage with open-source language models or outsource the entire training process to third-party platforms. However, research has demonstrated that language models are susceptible to potential security vulnerabilities, particularly in backdoor attacks. Backdoor attacks are designed to introduce targeted vulnerabilities into language models by poisoning training samples or model weights, allowing attackers to manipulate model responses through malicious triggers. While existing surveys on backdoor attacks provide a comprehensive overview, they lack an in-depth examination of backdoor attacks specifically targeting LLMs. To bridge this gap and grasp the latest trends in the field, this paper presents a novel perspective on backdoor attacks for LLMs by focusing on fine-tuning methods. Specifically, we systematically classify backdoor attacks into three categories: full-parameter fine-tuning, parameter-efficient fine-tuning, and no fine-tuning Based on insights from a substantial review, we also discuss crucial issues for future research on backdoor attacks, such as further exploring attack algorithms that do not require fine-tuning, or developing more covert attack algorithms.",
    "authors": [
      "Shuai Zhao",
      "Meihuizi Jia",
      "Zhongliang Guo",
      "Leilei Gan",
      "Xiaoyu Xu",
      "Xiaobao Wu",
      "Jie Fu",
      "Yichao Feng",
      "Fengjun Pan",
      "Luu Anh Tuan"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-06-10T23:54:21Z",
    "pdf_url": "https://arxiv.org/pdf/2406.06852v5"
  },
  {
    "arxiv_id": "2406.06451v1",
    "entry_id": "http://arxiv.org/abs/2406.06451v1",
    "title": "Insights from Social Shaping Theory: The Appropriation of Large Language Models in an Undergraduate Programming Course",
    "summary": "The capability of large language models (LLMs) to generate, debug, and explain code has sparked the interest of researchers and educators in undergraduate programming, with many anticipating their transformative potential in programming education. However, decisions about why and how to use LLMs in programming education may involve more than just the assessment of an LLM's technical capabilities. Using the social shaping of technology theory as a guiding framework, our study explores how students' social perceptions influence their own LLM usage. We then examine the correlation of self-reported LLM usage with students' self-efficacy and midterm performances in an undergraduate programming course. Triangulating data from an anonymous end-of-course student survey (n = 158), a mid-course self-efficacy survey (n=158), student interviews (n = 10), self-reported LLM usage on homework, and midterm performances, we discovered that students' use of LLMs was associated with their expectations for their future careers and their perceptions of peer usage. Additionally, early self-reported LLM usage in our context correlated with lower self-efficacy and lower midterm scores, while students' perceived over-reliance on LLMs, rather than their usage itself, correlated with decreased self-efficacy later in the course.",
    "authors": [
      "Aadarsh Padiyath",
      "Xinying Hou",
      "Amy Pang",
      "Diego Viramontes Vargas",
      "Xingjian Gu",
      "Tamara Nelson-Fromm",
      "Zihan Wu",
      "Mark Guzdial",
      "Barbara Ericson"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-06-10T16:40:14Z",
    "pdf_url": "https://arxiv.org/pdf/2406.06451v1"
  },
  {
    "arxiv_id": "2406.06391v1",
    "entry_id": "http://arxiv.org/abs/2406.06391v1",
    "title": "Towards Lifelong Learning of Large Language Models: A Survey",
    "summary": "As the applications of large language models (LLMs) expand across diverse fields, the ability of these models to adapt to ongoing changes in data, tasks, and user preferences becomes crucial. Traditional training methods, relying on static datasets, are increasingly inadequate for coping with the dynamic nature of real-world information. Lifelong learning, also known as continual or incremental learning, addresses this challenge by enabling LLMs to learn continuously and adaptively over their operational lifetime, integrating new knowledge while retaining previously learned information and preventing catastrophic forgetting. This survey delves into the sophisticated landscape of lifelong learning, categorizing strategies into two primary groups: Internal Knowledge and External Knowledge. Internal Knowledge includes continual pretraining and continual finetuning, each enhancing the adaptability of LLMs in various scenarios. External Knowledge encompasses retrieval-based and tool-based lifelong learning, leveraging external data sources and computational tools to extend the model's capabilities without modifying core parameters. The key contributions of our survey are: (1) Introducing a novel taxonomy categorizing the extensive literature of lifelong learning into 12 scenarios; (2) Identifying common techniques across all lifelong learning scenarios and classifying existing literature into various technique groups within each scenario; (3) Highlighting emerging techniques such as model expansion and data selection, which were less explored in the pre-LLM era. Through a detailed examination of these groups and their respective categories, this survey aims to enhance the adaptability, reliability, and overall performance of LLMs in real-world applications.",
    "authors": [
      "Junhao Zheng",
      "Shengjie Qiu",
      "Chengming Shi",
      "Qianli Ma"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-06-10T15:46:25Z",
    "pdf_url": "https://arxiv.org/pdf/2406.06391v1"
  },
  {
    "arxiv_id": "2406.06357v1",
    "entry_id": "http://arxiv.org/abs/2406.06357v1",
    "title": "MASSW: A New Dataset and Benchmark Tasks for AI-Assisted Scientific Workflows",
    "summary": "Scientific innovation relies on detailed workflows, which include critical steps such as analyzing literature, generating ideas, validating these ideas, interpreting results, and inspiring follow-up research. However, scientific publications that document these workflows are extensive and unstructured. This makes it difficult for both human researchers and AI systems to effectively navigate and explore the space of scientific innovation. To address this issue, we introduce MASSW, a comprehensive text dataset on Multi-Aspect Summarization of Scientific Workflows. MASSW includes more than 152,000 peer-reviewed publications from 17 leading computer science conferences spanning the past 50 years. Using Large Language Models (LLMs), we automatically extract five core aspects from these publications -- context, key idea, method, outcome, and projected impact -- which correspond to five key steps in the research workflow. These structured summaries facilitate a variety of downstream tasks and analyses. The quality of the LLM-extracted summaries is validated by comparing them with human annotations. We demonstrate the utility of MASSW through multiple novel machine-learning tasks that can be benchmarked using this new dataset, which make various types of predictions and recommendations along the scientific workflow. MASSW holds significant potential for researchers to create and benchmark new AI methods for optimizing scientific workflows and fostering scientific innovation in the field. Our dataset is openly available at \\url{https://github.com/xingjian-zhang/massw}.",
    "authors": [
      "Xingjian Zhang",
      "Yutong Xie",
      "Jin Huang",
      "Jinge Ma",
      "Zhaoying Pan",
      "Qijia Liu",
      "Ziyang Xiong",
      "Tolga Ergen",
      "Dongsub Shim",
      "Honglak Lee",
      "Qiaozhu Mei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-10T15:19:09Z",
    "pdf_url": "https://arxiv.org/pdf/2406.06357v1"
  },
  {
    "arxiv_id": "2406.10252v2",
    "entry_id": "http://arxiv.org/abs/2406.10252v2",
    "title": "AutoSurvey: Large Language Models Can Automatically Write Surveys",
    "summary": "This paper introduces AutoSurvey, a speedy and well-organized methodology for automating the creation of comprehensive literature surveys in rapidly evolving fields like artificial intelligence. Traditional survey paper creation faces challenges due to the vast volume and complexity of information, prompting the need for efficient survey methods. While large language models (LLMs) offer promise in automating this process, challenges such as context window limitations, parametric knowledge constraints, and the lack of evaluation benchmarks remain. AutoSurvey addresses these challenges through a systematic approach that involves initial retrieval and outline generation, subsection drafting by specialized LLMs, integration and refinement, and rigorous evaluation and iteration. Our contributions include a comprehensive solution to the survey problem, a reliable evaluation method, and experimental validation demonstrating AutoSurvey's effectiveness.We open our resources at \\url{https://github.com/AutoSurveys/AutoSurvey}.",
    "authors": [
      "Yidong Wang",
      "Qi Guo",
      "Wenjin Yao",
      "Hongbo Zhang",
      "Xin Zhang",
      "Zhen Wu",
      "Meishan Zhang",
      "Xinyu Dai",
      "Min Zhang",
      "Qingsong Wen",
      "Wei Ye",
      "Shikun Zhang",
      "Yue Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-06-10T12:56:06Z",
    "pdf_url": "https://arxiv.org/pdf/2406.10252v2"
  },
  {
    "arxiv_id": "2406.06041v1",
    "entry_id": "http://arxiv.org/abs/2406.06041v1",
    "title": "Risk Sensitivity in Markov Games and Multi-Agent Reinforcement Learning: A Systematic Review",
    "summary": "Markov games (MGs) and multi-agent reinforcement learning (MARL) are studied to model decision making in multi-agent systems. Traditionally, the objective in MG and MARL has been risk-neutral, i.e., agents are assumed to optimize a performance metric such as expected return, without taking into account subjective or cognitive preferences of themselves or of other agents. However, ignoring such preferences leads to inaccurate models of decision making in many real-world scenarios in finance, operations research, and behavioral economics. Therefore, when these preferences are present, it is necessary to incorporate a suitable measure of risk into the optimization objective of agents, which opens the door to risk-sensitive MG and MARL. In this paper, we systemically review the literature on risk sensitivity in MG and MARL that has been growing in recent years alongside other areas of reinforcement learning and game theory. We define and mathematically describe different risk measures used in MG and MARL and individually for each measure, discuss articles that incorporate it. Finally, we identify recent trends in theoretical and applied works in the field and discuss possible directions of future research.",
    "authors": [
      "Hafez Ghaemi",
      "Shirin Jamshidi",
      "Mohammad Mashreghi",
      "Majid Nili Ahmadabadi",
      "Hamed Kebriaei"
    ],
    "categories": [
      "cs.GT",
      "cs.LG",
      "cs.MA",
      "eess.SY"
    ],
    "published": "2024-06-10T06:19:33Z",
    "pdf_url": "https://arxiv.org/pdf/2406.06041v1"
  },
  {
    "arxiv_id": "2406.05804v6",
    "entry_id": "http://arxiv.org/abs/2406.05804v6",
    "title": "A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning",
    "summary": "Tool use, planning, and feedback learning are currently three prominent paradigms for developing Large Language Model (LLM)-based agents across various tasks. Although numerous frameworks have been devised for each paradigm, their intricate workflows and inconsistent taxonomy create challenges in understanding and reviewing the frameworks across different paradigms. This survey introduces a unified taxonomy to systematically review and discuss these frameworks. Specifically, 1) the taxonomy defines environments/tasks, common LLM-profiled roles or LMPRs (policy models, evaluators, and dynamic models), and universally applicable workflows found in prior work, and 2) it enables a comparison of key perspectives on the implementations of LMPRs and workflow designs across different agent paradigms and frameworks. 3) Finally, we identify three limitations in existing workflow designs and systematically discuss the future work. Resources have been made publicly available at in our GitHub repository https://github.com/xinzhel/LLM-Agent-Survey.",
    "authors": [
      "Xinzhe Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "published": "2024-06-09T14:42:55Z",
    "pdf_url": "https://arxiv.org/pdf/2406.05804v6"
  },
  {
    "arxiv_id": "2406.05688v1",
    "entry_id": "http://arxiv.org/abs/2406.05688v1",
    "title": "Peer Review as A Multi-Turn and Long-Context Dialogue with Role-Based Interactions",
    "summary": "Large Language Models (LLMs) have demonstrated wide-ranging applications across various fields and have shown significant potential in the academic peer-review process. However, existing applications are primarily limited to static review generation based on submitted papers, which fail to capture the dynamic and iterative nature of real-world peer reviews. In this paper, we reformulate the peer-review process as a multi-turn, long-context dialogue, incorporating distinct roles for authors, reviewers, and decision makers. We construct a comprehensive dataset containing over 26,841 papers with 92,017 reviews collected from multiple sources, including the top-tier conference and prestigious journal. This dataset is meticulously designed to facilitate the applications of LLMs for multi-turn dialogues, effectively simulating the complete peer-review process. Furthermore, we propose a series of metrics to evaluate the performance of LLMs for each role under this reformulated peer-review setting, ensuring fair and comprehensive evaluations. We believe this work provides a promising perspective on enhancing the LLM-driven peer-review process by incorporating dynamic, role-based interactions. It aligns closely with the iterative and interactive nature of real-world academic peer review, offering a robust foundation for future research and development in this area. We open-source the dataset at https://github.com/chengtan9907/ReviewMT.",
    "authors": [
      "Cheng Tan",
      "Dongxin Lyu",
      "Siyuan Li",
      "Zhangyang Gao",
      "Jingxuan Wei",
      "Siqi Ma",
      "Zicheng Liu",
      "Stan Z. Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-09T08:24:17Z",
    "pdf_url": "https://arxiv.org/pdf/2406.05688v1"
  },
  {
    "arxiv_id": "2406.05392v2",
    "entry_id": "http://arxiv.org/abs/2406.05392v2",
    "title": "Deconstructing The Ethics of Large Language Models from Long-standing Issues to New-emerging Dilemmas: A Survey",
    "summary": "Large Language Models (LLMs) have achieved unparalleled success across diverse language modeling tasks in recent years. However, this progress has also intensified ethical concerns, impacting the deployment of LLMs in everyday contexts. This paper provides a comprehensive survey of ethical challenges associated with LLMs, from longstanding issues such as copyright infringement, systematic bias, and data privacy, to emerging problems like truthfulness and social norms. We critically analyze existing research aimed at understanding, examining, and mitigating these ethical risks. Our survey underscores integrating ethical standards and societal values into the development of LLMs, thereby guiding the development of responsible and ethically aligned language models.",
    "authors": [
      "Chengyuan Deng",
      "Yiqun Duan",
      "Xin Jin",
      "Heng Chang",
      "Yijun Tian",
      "Han Liu",
      "Yichen Wang",
      "Kuofeng Gao",
      "Henry Peng Zou",
      "Yiqiao Jin",
      "Yijia Xiao",
      "Shenghao Wu",
      "Zongxing Xie",
      "Weimin Lyu",
      "Sihong He",
      "Lu Cheng",
      "Haohan Wang",
      "Jun Zhuang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-06-08T07:55:01Z",
    "pdf_url": "https://arxiv.org/pdf/2406.05392v2"
  },
  {
    "arxiv_id": "2406.04956v1",
    "entry_id": "http://arxiv.org/abs/2406.04956v1",
    "title": "Expansion of situations theory for exploring shared awareness in human-intelligent autonomous systems",
    "summary": "Intelligent autonomous systems are part of a system of systems that interact with other agents to accomplish tasks in complex environments. However, intelligent autonomous systems integrated system of systems add additional layers of complexity based on their limited cognitive processes, specifically shared situation awareness that allows a team to respond to novel tasks. Intelligent autonomous systems' lack of shared situation awareness adversely influences team effectiveness in complex task environments, such as military command-and-control. A complementary approach of shared situation awareness, called situations theory, is beneficial for understanding the relationship between system of systems shared situation awareness and effectiveness. The current study elucidates a conceptual discussion on situations theory to investigate the development of an system of systems shared situational awareness when humans team with intelligent autonomous system agents. To ground the discussion, the reviewed studies expanded situations theory within the context of a system of systems that result in three major conjectures that can be beneficial to the design and development of future systems of systems.",
    "authors": [
      "Scott A. Humr",
      "Mustafa Canan",
      "Mustafa Demir"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-06-07T14:21:01Z",
    "pdf_url": "https://arxiv.org/pdf/2406.04956v1"
  },
  {
    "arxiv_id": "2406.04482v1",
    "entry_id": "http://arxiv.org/abs/2406.04482v1",
    "title": "Automatic Bug Detection in LLM-Powered Text-Based Games Using LLMs",
    "summary": "Advancements in large language models (LLMs) are revolutionizing interactive game design, enabling dynamic plotlines and interactions between players and non-player characters (NPCs). However, LLMs may exhibit flaws such as hallucinations, forgetfulness, or misinterpretations of prompts, causing logical inconsistencies and unexpected deviations from intended designs. Automated techniques for detecting such game bugs are still lacking. To address this, we propose a systematic LLM-based method for automatically identifying such bugs from player game logs, eliminating the need for collecting additional data such as post-play surveys. Applied to a text-based game DejaBoom!, our approach effectively identifies bugs inherent in LLM-powered interactive games, surpassing unstructured LLM-powered bug-catching methods and filling the gap in automated detection of logical and design flaws.",
    "authors": [
      "Claire Jin",
      "Sudha Rao",
      "Xiangyu Peng",
      "Portia Botchway",
      "Jessica Quaye",
      "Chris Brockett",
      "Bill Dolan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.SE"
    ],
    "published": "2024-06-06T20:11:08Z",
    "pdf_url": "https://arxiv.org/pdf/2406.04482v1"
  },
  {
    "arxiv_id": "2406.06608v6",
    "entry_id": "http://arxiv.org/abs/2406.06608v6",
    "title": "The Prompt Report: A Systematic Survey of Prompt Engineering Techniques",
    "summary": "Generative Artificial Intelligence (GenAI) systems are increasingly being deployed across diverse industries and research domains. Developers and end-users interact with these systems through the use of prompting and prompt engineering. Although prompt engineering is a widely adopted and extensively researched area, it suffers from conflicting terminology and a fragmented ontological understanding of what constitutes an effective prompt due to its relatively recent emergence. We establish a structured understanding of prompt engineering by assembling a taxonomy of prompting techniques and analyzing their applications. We present a detailed vocabulary of 33 vocabulary terms, a taxonomy of 58 LLM prompting techniques, and 40 techniques for other modalities. Additionally, we provide best practices and guidelines for prompt engineering, including advice for prompting state-of-the-art (SOTA) LLMs such as ChatGPT. We further present a meta-analysis of the entire literature on natural language prefix-prompting. As a culmination of these efforts, this paper presents the most comprehensive survey on prompt engineering to date.",
    "authors": [
      "Sander Schulhoff",
      "Michael Ilie",
      "Nishant Balepur",
      "Konstantine Kahadze",
      "Amanda Liu",
      "Chenglei Si",
      "Yinheng Li",
      "Aayush Gupta",
      "HyoJung Han",
      "Sevien Schulhoff",
      "Pranav Sandeep Dulepet",
      "Saurav Vidyadhara",
      "Dayeon Ki",
      "Sweta Agrawal",
      "Chau Pham",
      "Gerson Kroiz",
      "Feileen Li",
      "Hudson Tao",
      "Ashay Srivastava",
      "Hevander Da Costa",
      "Saloni Gupta",
      "Megan L. Rogers",
      "Inna Goncearenco",
      "Giuseppe Sarli",
      "Igor Galynker",
      "Denis Peskoff",
      "Marine Carpuat",
      "Jules White",
      "Shyamal Anadkat",
      "Alexander Hoyle",
      "Philip Resnik"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-06T18:10:11Z",
    "pdf_url": "https://arxiv.org/pdf/2406.06608v6"
  },
  {
    "arxiv_id": "2406.04276v1",
    "entry_id": "http://arxiv.org/abs/2406.04276v1",
    "title": "Generative AI-in-the-loop: Integrating LLMs and GPTs into the Next Generation Networks",
    "summary": "In recent years, machine learning (ML) techniques have created numerous opportunities for intelligent mobile networks and have accelerated the automation of network operations. However, complex network tasks may involve variables and considerations even beyond the capacity of traditional ML algorithms. On the other hand, large language models (LLMs) have recently emerged, demonstrating near-human-level performance in cognitive tasks across various fields. However, they remain prone to hallucinations and often lack common sense in basic tasks. Therefore, they are regarded as assistive tools for humans. In this work, we propose the concept of \"generative AI-in-the-loop\" and utilize the semantic understanding, context awareness, and reasoning abilities of LLMs to assist humans in handling complex or unforeseen situations in mobile communication networks. We believe that combining LLMs and ML models allows both to leverage their respective capabilities and achieve better results than either model alone. To support this idea, we begin by analyzing the capabilities of LLMs and compare them with traditional ML algorithms. We then explore potential LLM-based applications in line with the requirements of next-generation networks. We further examine the integration of ML and LLMs, discussing how they can be used together in mobile networks. Unlike existing studies, our research emphasizes the fusion of LLMs with traditional ML-driven next-generation networks and serves as a comprehensive refinement of existing surveys. Finally, we provide a case study to enhance ML-based network intrusion detection with synthesized data generated by LLMs. Our case study further demonstrates the advantages of our proposed idea.",
    "authors": [
      "Han Zhang",
      "Akram Bin Sediq",
      "Ali Afana",
      "Melike Erol-Kantarci"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-06-06T17:25:07Z",
    "pdf_url": "https://arxiv.org/pdf/2406.04276v1"
  },
  {
    "arxiv_id": "2406.03712v2",
    "entry_id": "http://arxiv.org/abs/2406.03712v2",
    "title": "A Survey on Medical Large Language Models: Technology, Application, Trustworthiness, and Future Directions",
    "summary": "With the advent of Large Language Models (LLMs), medical artificial intelligence (AI) has experienced substantial technological progress and paradigm shifts, highlighting the potential of LLMs to streamline healthcare delivery and improve patient outcomes. Considering this rapid technical progress, in this survey, we trace the recent advances of Medical Large Language Models (Med-LLMs), including the background, key findings, and mainstream techniques, especially for the evolution from general-purpose models to medical-specialized applications. Firstly, we delve into the foundational technology of Med-LLMs, indicating how general models can be progressively adapted and refined for the complicated medical tasks. Secondly, the wide-ranging applications of Med-LLMs are investigated across various healthcare domains, as well as an up-to-date review of existing Med-LLMs. The transformative impact of these models on daily medical practice is evident through their ability to assist clinicians, educators, and patients. Recognizing the importance of responsible innovation, we discuss the challenges associated with ensuring fairness, accountability, privacy, and robustness. Ethical considerations, rigorous evaluation methodologies, and the establishment of regulatory frameworks are crucial for building trustworthiness in the real-world system. We emphasize the need for ongoing scrutiny and development to maintain high standards of safety and reliability. Finally, we anticipate possible future trajectories for Med-LLMs, identifying key avenues for prudent expansion. By consolidating these insights, our review aims to provide professionals and researchers with a thorough understanding of the strengths and limitations of Med-LLMs, fostering a balanced and ethical approach to their integration into the healthcare ecosystem.",
    "authors": [
      "Lei Liu",
      "Xiaoyan Yang",
      "Junchi Lei",
      "Yue Shen",
      "Jian Wang",
      "Peng Wei",
      "Zhixuan Chu",
      "Zhan Qin",
      "Kui Ren"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-06-06T03:15:13Z",
    "pdf_url": "https://arxiv.org/pdf/2406.03712v2"
  },
  {
    "arxiv_id": "2407.13105v1",
    "entry_id": "http://arxiv.org/abs/2407.13105v1",
    "title": "Survey on Plagiarism Detection in Large Language Models: The Impact of ChatGPT and Gemini on Academic Integrity",
    "summary": "The rise of Large Language Models (LLMs) such as ChatGPT and Gemini has posed new challenges for the academic community. With the help of these models, students can easily complete their assignments and exams, while educators struggle to detect AI-generated content. This has led to a surge in academic misconduct, as students present work generated by LLMs as their own, without putting in the effort required for learning. As AI tools become more advanced and produce increasingly human-like text, detecting such content becomes more challenging. This development has significantly impacted the academic world, where many educators are finding it difficult to adapt their assessment methods to this challenge.\n  This research first demonstrates how LLMs have increased academic dishonesty, and then reviews state-of-the-art solutions for academic plagiarism in detail. A survey of datasets, algorithms, tools, and evasion strategies for plagiarism detection has been conducted, focusing on how LLMs and AI-generated content (AIGC) detection have affected this area. The survey aims to identify the gaps in existing solutions. Lastly, potential long-term solutions are presented to address the issue of academic plagiarism using LLMs based on AI tools and educational approaches in an ever-changing world.",
    "authors": [
      "Shushanta Pudasaini",
      "Luis Miralles-Pechuán",
      "David Lillis",
      "Marisa Llorens Salvador"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-06-04T09:38:03Z",
    "pdf_url": "https://arxiv.org/pdf/2407.13105v1"
  },
  {
    "arxiv_id": "2406.02063v1",
    "entry_id": "http://arxiv.org/abs/2406.02063v1",
    "title": "An agent-based model of modal choice with perception biases and habits",
    "summary": "This paper presents an agent-based model of mobility choice, influenced by human factors such as habits and perception biases. It is implemented in a Netlogo simulator, calibrated from results of an online survey about perceptions of mobility. The simulator can be played online. It allows to modify urban infrastructure and observe modal report.",
    "authors": [
      "Carole Adam",
      "Benoit Gaudou"
    ],
    "categories": [
      "cs.CY",
      "cs.MA"
    ],
    "published": "2024-06-04T07:44:57Z",
    "pdf_url": "https://arxiv.org/pdf/2406.02063v1"
  },
  {
    "arxiv_id": "2406.01943v1",
    "entry_id": "http://arxiv.org/abs/2406.01943v1",
    "title": "Enhancing Trust in LLMs: Algorithms for Comparing and Interpreting LLMs",
    "summary": "This paper surveys evaluation techniques to enhance the trustworthiness and understanding of Large Language Models (LLMs). As reliance on LLMs grows, ensuring their reliability, fairness, and transparency is crucial. We explore algorithmic methods and metrics to assess LLM performance, identify weaknesses, and guide development towards more trustworthy applications. Key evaluation metrics include Perplexity Measurement, NLP metrics (BLEU, ROUGE, METEOR, BERTScore, GLEU, Word Error Rate, Character Error Rate), Zero-Shot and Few-Shot Learning Performance, Transfer Learning Evaluation, Adversarial Testing, and Fairness and Bias Evaluation. We introduce innovative approaches like LLMMaps for stratified evaluation, Benchmarking and Leaderboards for competitive assessment, Stratified Analysis for in-depth understanding, Visualization of Blooms Taxonomy for cognitive level accuracy distribution, Hallucination Score for quantifying inaccuracies, Knowledge Stratification Strategy for hierarchical analysis, and Machine Learning Models for Hierarchy Generation. Human Evaluation is highlighted for capturing nuances that automated metrics may miss. These techniques form a framework for evaluating LLMs, aiming to enhance transparency, guide development, and establish user trust. Future papers will describe metric visualization and demonstrate each approach on practical examples.",
    "authors": [
      "Nik Bear Brown"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-06-04T03:54:53Z",
    "pdf_url": "https://arxiv.org/pdf/2406.01943v1"
  },
  {
    "arxiv_id": "2406.02630v2",
    "entry_id": "http://arxiv.org/abs/2406.02630v2",
    "title": "AI Agents Under Threat: A Survey of Key Security Challenges and Future Pathways",
    "summary": "An Artificial Intelligence (AI) agent is a software entity that autonomously performs tasks or makes decisions based on pre-defined objectives and data inputs. AI agents, capable of perceiving user inputs, reasoning and planning tasks, and executing actions, have seen remarkable advancements in algorithm development and task performance. However, the security challenges they pose remain under-explored and unresolved. This survey delves into the emerging security threats faced by AI agents, categorizing them into four critical knowledge gaps: unpredictability of multi-step user inputs, complexity in internal executions, variability of operational environments, and interactions with untrusted external entities. By systematically reviewing these threats, this paper highlights both the progress made and the existing limitations in safeguarding AI agents. The insights provided aim to inspire further research into addressing the security threats associated with AI agents, thereby fostering the development of more robust and secure AI agent applications.",
    "authors": [
      "Zehang Deng",
      "Yongjian Guo",
      "Changzhou Han",
      "Wanlun Ma",
      "Junwu Xiong",
      "Sheng Wen",
      "Yang Xiang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-06-04T01:22:31Z",
    "pdf_url": "https://arxiv.org/pdf/2406.02630v2"
  },
  {
    "arxiv_id": "2406.02622v1",
    "entry_id": "http://arxiv.org/abs/2406.02622v1",
    "title": "Safeguarding Large Language Models: A Survey",
    "summary": "In the burgeoning field of Large Language Models (LLMs), developing a robust safety mechanism, colloquially known as \"safeguards\" or \"guardrails\", has become imperative to ensure the ethical use of LLMs within prescribed boundaries. This article provides a systematic literature review on the current status of this critical mechanism. It discusses its major challenges and how it can be enhanced into a comprehensive mechanism dealing with ethical issues in various contexts. First, the paper elucidates the current landscape of safeguarding mechanisms that major LLM service providers and the open-source community employ. This is followed by the techniques to evaluate, analyze, and enhance some (un)desirable properties that a guardrail might want to enforce, such as hallucinations, fairness, privacy, and so on. Based on them, we review techniques to circumvent these controls (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the techniques mentioned above represent the current status and the active research trends, we also discuss several challenges that cannot be easily dealt with by the methods and present our vision on how to implement a comprehensive guardrail through the full consideration of multi-disciplinary approach, neural-symbolic method, and systems development lifecycle.",
    "authors": [
      "Yi Dong",
      "Ronghui Mu",
      "Yanghao Zhang",
      "Siqi Sun",
      "Tianle Zhang",
      "Changshun Wu",
      "Gaojie Jin",
      "Yi Qi",
      "Jinwei Hu",
      "Jie Meng",
      "Saddek Bensalem",
      "Xiaowei Huang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-06-03T19:27:46Z",
    "pdf_url": "https://arxiv.org/pdf/2406.02622v1"
  },
  {
    "arxiv_id": "2406.01252v3",
    "entry_id": "http://arxiv.org/abs/2406.01252v3",
    "title": "Towards Scalable Automated Alignment of LLMs: A Survey",
    "summary": "Alignment is the most critical step in building large language models (LLMs) that meet human needs. With the rapid development of LLMs gradually surpassing human capabilities, traditional alignment methods based on human-annotation are increasingly unable to meet the scalability demands. Therefore, there is an urgent need to explore new sources of automated alignment signals and technical approaches. In this paper, we systematically review the recently emerging methods of automated alignment, attempting to explore how to achieve effective, scalable, automated alignment once the capabilities of LLMs exceed those of humans. Specifically, we categorize existing automated alignment methods into 4 major categories based on the sources of alignment signals and discuss the current status and potential development of each category. Additionally, we explore the underlying mechanisms that enable automated alignment and discuss the essential factors that make automated alignment technologies feasible and effective from the fundamental role of alignment.",
    "authors": [
      "Boxi Cao",
      "Keming Lu",
      "Xinyu Lu",
      "Jiawei Chen",
      "Mengjie Ren",
      "Hao Xiang",
      "Peilin Liu",
      "Yaojie Lu",
      "Ben He",
      "Xianpei Han",
      "Le Sun",
      "Hongyu Lin",
      "Bowen Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2024-06-03T12:10:26Z",
    "pdf_url": "https://arxiv.org/pdf/2406.01252v3"
  },
  {
    "arxiv_id": "2406.00515v2",
    "entry_id": "http://arxiv.org/abs/2406.00515v2",
    "title": "A Survey on Large Language Models for Code Generation",
    "summary": "Large Language Models (LLMs) have garnered remarkable advancements across diverse code-related tasks, known as Code LLMs, particularly in code generation that generates source code with LLM from natural language descriptions. This burgeoning field has captured significant interest from both academic researchers and industry professionals due to its practical significance in software development, e.g., GitHub Copilot. Despite the active exploration of LLMs for a variety of code tasks, either from the perspective of natural language processing (NLP) or software engineering (SE) or both, there is a noticeable absence of a comprehensive and up-to-date literature review dedicated to LLM for code generation. In this survey, we aim to bridge this gap by providing a systematic literature review that serves as a valuable reference for researchers investigating the cutting-edge progress in LLMs for code generation. We introduce a taxonomy to categorize and discuss the recent developments in LLMs for code generation, covering aspects such as data curation, latest advances, performance evaluation, ethical implications, environmental impact, and real-world applications. In addition, we present a historical overview of the evolution of LLMs for code generation and offer an empirical comparison using the HumanEval, MBPP, and BigCodeBench benchmarks across various levels of difficulty and types of programming tasks to highlight the progressive enhancements in LLM capabilities for code generation. We identify critical challenges and promising opportunities regarding the gap between academia and practical development. Furthermore, we have established a dedicated resource GitHub page (https://github.com/juyongjiang/CodeLLMSurvey) to continuously document and disseminate the most recent advances in the field.",
    "authors": [
      "Juyong Jiang",
      "Fan Wang",
      "Jiasi Shen",
      "Sungju Kim",
      "Sunghun Kim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-06-01T17:48:15Z",
    "pdf_url": "https://arxiv.org/pdf/2406.00515v2"
  },
  {
    "arxiv_id": "2407.13934v1",
    "entry_id": "http://arxiv.org/abs/2407.13934v1",
    "title": "Towards Trustworthy AI: A Review of Ethical and Robust Large Language Models",
    "summary": "The rapid progress in Large Language Models (LLMs) could transform many fields, but their fast development creates significant challenges for oversight, ethical creation, and building user trust. This comprehensive review looks at key trust issues in LLMs, such as unintended harms, lack of transparency, vulnerability to attacks, alignment with human values, and environmental impact. Many obstacles can undermine user trust, including societal biases, opaque decision-making, potential for misuse, and the challenges of rapidly evolving technology. Addressing these trust gaps is critical as LLMs become more common in sensitive areas like finance, healthcare, education, and policy. To tackle these issues, we suggest combining ethical oversight, industry accountability, regulation, and public involvement. AI development norms should be reshaped, incentives aligned, and ethics integrated throughout the machine learning process, which requires close collaboration across technology, ethics, law, policy, and other fields. Our review contributes a robust framework to assess trust in LLMs and analyzes the complex trust dynamics in depth. We provide contextualized guidelines and standards for responsibly developing and deploying these powerful AI systems. This review identifies key limitations and challenges in creating trustworthy AI. By addressing these issues, we aim to build a transparent, accountable AI ecosystem that benefits society while minimizing risks. Our findings provide valuable guidance for researchers, policymakers, and industry leaders striving to establish trust in LLMs and ensure they are used responsibly across various applications for the good of society.",
    "authors": [
      "Md Meftahul Ferdaus",
      "Mahdi Abdelguerfi",
      "Elias Ioup",
      "Kendall N. Niles",
      "Ken Pathak",
      "Steven Sloan"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-06-01T14:47:58Z",
    "pdf_url": "https://arxiv.org/pdf/2407.13934v1"
  },
  {
    "arxiv_id": "2406.00252v6",
    "entry_id": "http://arxiv.org/abs/2406.00252v6",
    "title": "Towards Rationality in Language and Multimodal Agents: A Survey",
    "summary": "This work discusses how to build more rational language and multimodal agents and what criteria define rationality in intelligent systems. Rationality is the quality of being guided by reason, characterized by decision-making that aligns with evidence and logical principles. It plays a crucial role in reliable problem-solving by ensuring well-grounded and consistent solutions. Despite their progress, large language models (LLMs) often fall short of rationality due to their bounded knowledge space and inconsistent outputs. In response, recent efforts have shifted toward developing multimodal and multi-agent systems, as well as integrating modules like external tools, programming codes, symbolic reasoners, utility function, and conformal risk controls rather than relying solely on a single LLM for decision-making. This paper surveys state-of-the-art advancements in language and multimodal agents, assesses their role in enhancing rationality, and outlines open challenges and future research directions. We maintain an open repository at https://github.com/bowen-upenn/Agent_Rationality.",
    "authors": [
      "Bowen Jiang",
      "Yangxinyu Xie",
      "Xiaomeng Wang",
      "Yuan Yuan",
      "Zhuoqun Hao",
      "Xinyi Bai",
      "Weijie J. Su",
      "Camillo J. Taylor",
      "Tanwi Mallick"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MA"
    ],
    "published": "2024-06-01T01:17:25Z",
    "pdf_url": "https://arxiv.org/pdf/2406.00252v6"
  },
  {
    "arxiv_id": "2406.00240v1",
    "entry_id": "http://arxiv.org/abs/2406.00240v1",
    "title": "Exploring Vulnerabilities and Protections in Large Language Models: A Survey",
    "summary": "As Large Language Models (LLMs) increasingly become key components in various AI applications, understanding their security vulnerabilities and the effectiveness of defense mechanisms is crucial. This survey examines the security challenges of LLMs, focusing on two main areas: Prompt Hacking and Adversarial Attacks, each with specific types of threats. Under Prompt Hacking, we explore Prompt Injection and Jailbreaking Attacks, discussing how they work, their potential impacts, and ways to mitigate them. Similarly, we analyze Adversarial Attacks, breaking them down into Data Poisoning Attacks and Backdoor Attacks. This structured examination helps us understand the relationships between these vulnerabilities and the defense strategies that can be implemented. The survey highlights these security challenges and discusses robust defensive frameworks to protect LLMs against these threats. By detailing these security issues, the survey contributes to the broader discussion on creating resilient AI systems that can resist sophisticated attacks.",
    "authors": [
      "Frank Weizhen Liu",
      "Chenhui Hu"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ],
    "published": "2024-06-01T00:11:09Z",
    "pdf_url": "https://arxiv.org/pdf/2406.00240v1"
  },
  {
    "arxiv_id": "2406.04369v1",
    "entry_id": "http://arxiv.org/abs/2406.04369v1",
    "title": "RAG Does Not Work for Enterprises",
    "summary": "Retrieval-Augmented Generation (RAG) improves the accuracy and relevance of large language model outputs by incorporating knowledge retrieval. However, implementing RAG in enterprises poses challenges around data security, accuracy, scalability, and integration. This paper explores the unique requirements for enterprise RAG, surveys current approaches and limitations, and discusses potential advances in semantic search, hybrid queries, and optimized retrieval. It proposes an evaluation framework to validate enterprise RAG solutions, including quantitative testing, qualitative analysis, ablation studies, and industry case studies. This framework aims to help demonstrate the ability of purpose-built RAG architectures to deliver accuracy and relevance improvements with enterprise-grade security, compliance and integration. The paper concludes with implications for enterprise deployments, limitations, and future research directions. Close collaboration between researchers and industry partners may accelerate progress in developing and deploying retrieval-augmented generation technology.",
    "authors": [
      "Tilmann Bruckhaus"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-05-31T23:30:52Z",
    "pdf_url": "https://arxiv.org/pdf/2406.04369v1"
  },
  {
    "arxiv_id": "2405.20624v1",
    "entry_id": "http://arxiv.org/abs/2405.20624v1",
    "title": "Leveraging Large Language Models for Entity Matching",
    "summary": "Entity matching (EM) is a critical task in data integration, aiming to identify records across different datasets that refer to the same real-world entities. Traditional methods often rely on manually engineered features and rule-based systems, which struggle with diverse and unstructured data. The emergence of Large Language Models (LLMs) such as GPT-4 offers transformative potential for EM, leveraging their advanced semantic understanding and contextual capabilities. This vision paper explores the application of LLMs to EM, discussing their advantages, challenges, and future research directions. Additionally, we review related work on applying weak supervision and unsupervised approaches to EM, highlighting how LLMs can enhance these methods.",
    "authors": [
      "Qianyu Huang",
      "Tongfang Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-31T05:22:07Z",
    "pdf_url": "https://arxiv.org/pdf/2405.20624v1"
  },
  {
    "arxiv_id": "2405.20551v1",
    "entry_id": "http://arxiv.org/abs/2405.20551v1",
    "title": "EM-Assist: Safe Automated ExtractMethod Refactoring with LLMs",
    "summary": "Excessively long methods, loaded with multiple responsibilities, are challenging to understand, debug, reuse, and maintain. The solution lies in the widely recognized Extract Method refactoring. While the application of this refactoring is supported in modern IDEs, recommending which code fragments to extract has been the topic of many research tools. However, they often struggle to replicate real-world developer practices, resulting in recommendations that do not align with what a human developer would do in real life. To address this issue, we introduce EM-Assist, an IntelliJ IDEA plugin that uses LLMs to generate refactoring suggestions and subsequently validates, enhances, and ranks them. Finally, EM-Assist uses the IntelliJ IDE to apply the user-selected recommendation. In our extensive evaluation of 1,752 real-world refactorings that actually took place in open-source projects, EM-Assist's recall rate was 53.4% among its top-5 recommendations, compared to 39.4% for the previous best-in-class tool that relies solely on static analysis. Moreover, we conducted a usability survey with 18 industrial developers and 94.4% gave a positive rating.",
    "authors": [
      "Dorin Pomian",
      "Abhiram Bellur",
      "Malinda Dilhara",
      "Zarina Kurbatova",
      "Egor Bogomolov",
      "Andrey Sokolov",
      "Timofey Bryksin",
      "Danny Dig"
    ],
    "categories": [
      "cs.SE",
      "cs.HC",
      "cs.LG",
      "cs.PL"
    ],
    "published": "2024-05-31T00:32:04Z",
    "pdf_url": "https://arxiv.org/pdf/2405.20551v1"
  },
  {
    "arxiv_id": "2405.20183v1",
    "entry_id": "http://arxiv.org/abs/2405.20183v1",
    "title": "A Survey Study on the State of the Art of Programming Exercise Generation using Large Language Models",
    "summary": "This paper analyzes Large Language Models (LLMs) with regard to their programming exercise generation capabilities. Through a survey study, we defined the state of the art, extracted their strengths and weaknesses and finally proposed an evaluation matrix, helping researchers and educators to decide which LLM is the best fitting for the programming exercise generation use case. We also found that multiple LLMs are capable of producing useful programming exercises. Nevertheless, there exist challenges like the ease with which LLMs might solve exercises generated by LLMs. This paper contributes to the ongoing discourse on the integration of LLMs in education.",
    "authors": [
      "Eduard Frankford",
      "Ingo Höhn",
      "Clemens Sauerwein",
      "Ruth Breu"
    ],
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-05-30T15:49:34Z",
    "pdf_url": "https://arxiv.org/pdf/2405.20183v1"
  },
  {
    "arxiv_id": "2406.15443v1",
    "entry_id": "http://arxiv.org/abs/2406.15443v1",
    "title": "ExU: AI Models for Examining Multilingual Disinformation Narratives and Understanding their Spread",
    "summary": "Addressing online disinformation requires analysing narratives across languages to help fact-checkers and journalists sift through large amounts of data. The ExU project focuses on developing AI-based models for multilingual disinformation analysis, addressing the tasks of rumour stance classification and claim retrieval. We describe the ExU project proposal and summarise the results of a user requirements survey regarding the design of tools to support fact-checking.",
    "authors": [
      "Jake Vasilakes",
      "Zhixue Zhao",
      "Ivan Vykopal",
      "Michal Gregor",
      "Martin Hyben",
      "Carolina Scarton"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-30T11:13:57Z",
    "pdf_url": "https://arxiv.org/pdf/2406.15443v1"
  },
  {
    "arxiv_id": "2405.19850v1",
    "entry_id": "http://arxiv.org/abs/2405.19850v1",
    "title": "Deciphering Human Mobility: Inferring Semantics of Trajectories with Large Language Models",
    "summary": "Understanding human mobility patterns is essential for various applications, from urban planning to public safety. The individual trajectory such as mobile phone location data, while rich in spatio-temporal information, often lacks semantic detail, limiting its utility for in-depth mobility analysis. Existing methods can infer basic routine activity sequences from this data, lacking depth in understanding complex human behaviors and users' characteristics. Additionally, they struggle with the dependency on hard-to-obtain auxiliary datasets like travel surveys. To address these limitations, this paper defines trajectory semantic inference through three key dimensions: user occupation category, activity sequence, and trajectory description, and proposes the Trajectory Semantic Inference with Large Language Models (TSI-LLM) framework to leverage LLMs infer trajectory semantics comprehensively and deeply. We adopt spatio-temporal attributes enhanced data formatting (STFormat) and design a context-inclusive prompt, enabling LLMs to more effectively interpret and infer the semantics of trajectory data. Experimental validation on real-world trajectory datasets demonstrates the efficacy of TSI-LLM in deciphering complex human mobility patterns. This study explores the potential of LLMs in enhancing the semantic analysis of trajectory data, paving the way for more sophisticated and accessible human mobility research.",
    "authors": [
      "Yuxiao Luo",
      "Zhongcai Cao",
      "Xin Jin",
      "Kang Liu",
      "Ling Yin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-05-30T08:55:48Z",
    "pdf_url": "https://arxiv.org/pdf/2405.19850v1"
  },
  {
    "arxiv_id": "2405.19694v1",
    "entry_id": "http://arxiv.org/abs/2405.19694v1",
    "title": "Grade Like a Human: Rethinking Automated Assessment with Large Language Models",
    "summary": "While large language models (LLMs) have been used for automated grading, they have not yet achieved the same level of performance as humans, especially when it comes to grading complex questions. Existing research on this topic focuses on a particular step in the grading procedure: grading using predefined rubrics. However, grading is a multifaceted procedure that encompasses other crucial steps, such as grading rubrics design and post-grading review. There has been a lack of systematic research exploring the potential of LLMs to enhance the entire grading~process.\n  In this paper, we propose an LLM-based grading system that addresses the entire grading procedure, including the following key components: 1) Developing grading rubrics that not only consider the questions but also the student answers, which can more accurately reflect students' performance. 2) Under the guidance of grading rubrics, providing accurate and consistent scores for each student, along with customized feedback. 3) Conducting post-grading review to better ensure accuracy and fairness. Additionally, we collected a new dataset named OS from a university operating system course and conducted extensive experiments on both our new dataset and the widely used Mohler dataset. Experiments demonstrate the effectiveness of our proposed approach, providing some new insights for developing automated grading systems based on LLMs.",
    "authors": [
      "Wenjing Xie",
      "Juxin Niu",
      "Chun Jason Xue",
      "Nan Guan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-05-30T05:08:15Z",
    "pdf_url": "https://arxiv.org/pdf/2405.19694v1"
  },
  {
    "arxiv_id": "2405.20354v2",
    "entry_id": "http://arxiv.org/abs/2405.20354v2",
    "title": "Efficient Systematic Reviews: Literature Filtering with Transformers & Transfer Learning",
    "summary": "Identifying critical research within the growing body of academic work is an intrinsic aspect of conducting quality research. Systematic review processes used in evidence-based medicine formalise this as a procedure that must be followed in a research program. However, it comes with an increasing burden in terms of the time required to identify the important articles of research for a given topic. In this work, we develop a method for building a general-purpose filtering system that matches a research question, posed as a natural language description of the required content, against a candidate set of articles obtained via the application of broad search terms. Our results demonstrate that transformer models, pre-trained on biomedical literature, and then fine tuned for the specific task, offer a promising solution to this problem. The model can remove large volumes of irrelevant articles for most research questions. Furthermore, analysis of the specific research questions in our training data suggest natural avenues for further improvement.",
    "authors": [
      "John Hawkins",
      "David Tivey"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-05-30T02:55:49Z",
    "pdf_url": "https://arxiv.org/pdf/2405.20354v2"
  },
  {
    "arxiv_id": "2405.19334v2",
    "entry_id": "http://arxiv.org/abs/2405.19334v2",
    "title": "LLMs Meet Multimodal Generation and Editing: A Survey",
    "summary": "With the recent advancement in large language models (LLMs), there is a growing interest in combining LLMs with multimodal learning. Previous surveys of multimodal large language models (MLLMs) mainly focus on multimodal understanding. This survey elaborates on multimodal generation and editing across various domains, comprising image, video, 3D, and audio. Specifically, we summarize the notable advancements with milestone works in these fields and categorize these studies into LLM-based and CLIP/T5-based methods. Then, we summarize the various roles of LLMs in multimodal generation and exhaustively investigate the critical technical components behind these methods and the multimodal datasets utilized in these studies. Additionally, we dig into tool-augmented multimodal agents that can leverage existing generative models for human-computer interaction. Lastly, we discuss the advancements in the generative AI safety field, investigate emerging applications, and discuss future prospects. Our work provides a systematic and insightful overview of multimodal generation and processing, which is expected to advance the development of Artificial Intelligence for Generative Content (AIGC) and world models. A curated list of all related papers can be found at https://github.com/YingqingHe/Awesome-LLMs-meet-Multimodal-Generation",
    "authors": [
      "Yingqing He",
      "Zhaoyang Liu",
      "Jingye Chen",
      "Zeyue Tian",
      "Hongyu Liu",
      "Xiaowei Chi",
      "Runtao Liu",
      "Ruibin Yuan",
      "Yazhou Xing",
      "Wenhai Wang",
      "Jifeng Dai",
      "Yong Zhang",
      "Wei Xue",
      "Qifeng Liu",
      "Yike Guo",
      "Qifeng Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM",
      "cs.SD"
    ],
    "published": "2024-05-29T17:59:20Z",
    "pdf_url": "https://arxiv.org/pdf/2405.19334v2"
  },
  {
    "arxiv_id": "2405.19323v2",
    "entry_id": "http://arxiv.org/abs/2405.19323v2",
    "title": "Are Large Language Models Chameleons? An Attempt to Simulate Social Surveys",
    "summary": "Can large language models (LLMs) simulate social surveys? To answer this question, we conducted millions of simulations in which LLMs were asked to answer subjective questions. A comparison of different LLM responses with the European Social Survey (ESS) data suggests that the effect of prompts on bias and variability is fundamental, highlighting major cultural, age, and gender biases. We further discussed statistical methods for measuring the difference between LLM answers and survey data and proposed a novel measure inspired by Jaccard similarity, as LLM-generated responses are likely to have a smaller variance. Our experiments also reveal that it is important to analyze the robustness and variability of prompts before using LLMs to simulate social surveys, as their imitation abilities are approximate at best.",
    "authors": [
      "Mingmeng Geng",
      "Sihong He",
      "Roberto Trotta"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-05-29T17:54:22Z",
    "pdf_url": "https://arxiv.org/pdf/2405.19323v2"
  },
  {
    "arxiv_id": "2405.19164v2",
    "entry_id": "http://arxiv.org/abs/2405.19164v2",
    "title": "Learning from Litigation: Graphs and LLMs for Retrieval and Reasoning in eDiscovery",
    "summary": "Electronic Discovery (eDiscovery) requires identifying relevant documents from vast collections for legal production requests. While artificial intelligence (AI) and natural language processing (NLP) have improved document review efficiency, current methods still struggle with legal entities, citations, and complex legal artifacts. To address these challenges, we introduce DISCOvery Graph (DISCOG), an emerging system that integrates knowledge graphs for enhanced document ranking and classification, augmented by LLM-driven reasoning. DISCOG outperforms strong baselines in F1-score, precision, and recall across both balanced and imbalanced datasets. In real-world deployments, it has reduced litigation-related document review costs by approximately 98\\%, demonstrating significant business impact.",
    "authors": [
      "Sounak Lahiri",
      "Sumit Pai",
      "Tim Weninger",
      "Sanmitra Bhattacharya"
    ],
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-05-29T15:08:55Z",
    "pdf_url": "https://arxiv.org/pdf/2405.19164v2"
  },
  {
    "arxiv_id": "2405.19032v1",
    "entry_id": "http://arxiv.org/abs/2405.19032v1",
    "title": "Large Language Models for Code Summarization",
    "summary": "Recently, there has been increasing activity in using deep learning for software engineering, including tasks like code generation and summarization. In particular, the most recent coding Large Language Models seem to perform well on these problems. In this technical report, we aim to review how these models perform in code explanation/summarization, while also investigating their code generation capabilities (based on natural language descriptions).",
    "authors": [
      "Balázs Szalontai",
      "Gergő Szalay",
      "Tamás Márton",
      "Anna Sike",
      "Balázs Pintér",
      "Tibor Gregorics"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.PL",
      "cs.SE"
    ],
    "published": "2024-05-29T12:18:51Z",
    "pdf_url": "https://arxiv.org/pdf/2405.19032v1"
  },
  {
    "arxiv_id": "2405.17974v1",
    "entry_id": "http://arxiv.org/abs/2405.17974v1",
    "title": "Recent Trends in Personalized Dialogue Generation: A Review of Datasets, Methodologies, and Evaluations",
    "summary": "Enhancing user engagement through personalization in conversational agents has gained significance, especially with the advent of large language models that generate fluent responses. Personalized dialogue generation, however, is multifaceted and varies in its definition -- ranging from instilling a persona in the agent to capturing users' explicit and implicit cues. This paper seeks to systemically survey the recent landscape of personalized dialogue generation, including the datasets employed, methodologies developed, and evaluation metrics applied. Covering 22 datasets, we highlight benchmark datasets and newer ones enriched with additional features. We further analyze 17 seminal works from top conferences between 2021-2023 and identify five distinct types of problems. We also shed light on recent progress by LLMs in personalized dialogue generation. Our evaluation section offers a comprehensive summary of assessment facets and metrics utilized in these works. In conclusion, we discuss prevailing challenges and envision prospect directions for future research in personalized dialogue generation.",
    "authors": [
      "Yi-Pei Chen",
      "Noriki Nishida",
      "Hideki Nakayama",
      "Yuji Matsumoto"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-28T09:04:13Z",
    "pdf_url": "https://arxiv.org/pdf/2405.17974v1"
  },
  {
    "arxiv_id": "2405.17935v3",
    "entry_id": "http://arxiv.org/abs/2405.17935v3",
    "title": "Tool Learning with Large Language Models: A Survey",
    "summary": "Recently, tool learning with large language models (LLMs) has emerged as a promising paradigm for augmenting the capabilities of LLMs to tackle highly complex problems. Despite growing attention and rapid advancements in this field, the existing literature remains fragmented and lacks systematic organization, posing barriers to entry for newcomers. This gap motivates us to conduct a comprehensive survey of existing works on tool learning with LLMs. In this survey, we focus on reviewing existing literature from the two primary aspects (1) why tool learning is beneficial and (2) how tool learning is implemented, enabling a comprehensive understanding of tool learning with LLMs. We first explore the \"why\" by reviewing both the benefits of tool integration and the inherent benefits of the tool learning paradigm from six specific aspects. In terms of \"how\", we systematically review the literature according to a taxonomy of four key stages in the tool learning workflow: task planning, tool selection, tool calling, and response generation. Additionally, we provide a detailed summary of existing benchmarks and evaluation methods, categorizing them according to their relevance to different stages. Finally, we discuss current challenges and outline potential future directions, aiming to inspire both researchers and industrial developers to further explore this emerging and promising area. We also maintain a GitHub repository to continually keep track of the relevant papers and resources in this rising area at https://github.com/quchangle1/LLM-Tool-Survey.",
    "authors": [
      "Changle Qu",
      "Sunhao Dai",
      "Xiaochi Wei",
      "Hengyi Cai",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Jun Xu",
      "Ji-Rong Wen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-28T08:01:26Z",
    "pdf_url": "https://arxiv.org/pdf/2405.17935v3"
  },
  {
    "arxiv_id": "2406.01607v2",
    "entry_id": "http://arxiv.org/abs/2406.01607v2",
    "title": "Recent advances in text embedding: A Comprehensive Review of Top-Performing Methods on the MTEB Benchmark",
    "summary": "Text embedding methods have become increasingly popular in both industrial and academic fields due to their critical role in a variety of natural language processing tasks. The significance of universal text embeddings has been further highlighted with the rise of Large Language Models (LLMs) applications such as Retrieval-Augmented Systems (RAGs). While previous models have attempted to be general-purpose, they often struggle to generalize across tasks and domains. However, recent advancements in training data quantity, quality and diversity; synthetic data generation from LLMs as well as using LLMs as backbones encourage great improvements in pursuing universal text embeddings. In this paper, we provide an overview of the recent advances in universal text embedding models with a focus on the top performing text embeddings on Massive Text Embedding Benchmark (MTEB). Through detailed comparison and analysis, we highlight the key contributions and limitations in this area, and propose potentially inspiring future research directions.",
    "authors": [
      "Hongliu Cao"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-05-27T09:52:54Z",
    "pdf_url": "https://arxiv.org/pdf/2406.01607v2"
  },
  {
    "arxiv_id": "2406.01606v1",
    "entry_id": "http://arxiv.org/abs/2406.01606v1",
    "title": "SymTax: Symbiotic Relationship and Taxonomy Fusion for Effective Citation Recommendation",
    "summary": "Citing pertinent literature is pivotal to writing and reviewing a scientific document. Existing techniques mainly focus on the local context or the global context for recommending citations but fail to consider the actual human citation behaviour. We propose SymTax, a three-stage recommendation architecture that considers both the local and the global context, and additionally the taxonomical representations of query-candidate tuples and the Symbiosis prevailing amongst them. SymTax learns to embed the infused taxonomies in the hyperbolic space and uses hyperbolic separation as a latent feature to compute query-candidate similarity. We build a novel and large dataset ArSyTa containing 8.27 million citation contexts and describe the creation process in detail. We conduct extensive experiments and ablation studies to demonstrate the effectiveness and design choice of each module in our framework. Also, combinatorial analysis from our experiments shed light on the choice of language models (LMs) and fusion embedding, and the inclusion of section heading as a signal. Our proposed module that captures the symbiotic relationship solely leads to performance gains of 26.66% and 39.25% in Recall@5 w.r.t. SOTA on ACL-200 and RefSeer datasets, respectively. The complete framework yields a gain of 22.56% in Recall@5 wrt SOTA on our proposed dataset. The code and dataset are available at https://github.com/goyalkaraniit/SymTax",
    "authors": [
      "Karan Goyal",
      "Mayank Goel",
      "Vikram Goyal",
      "Mukesh Mohania"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-05-26T21:51:58Z",
    "pdf_url": "https://arxiv.org/pdf/2406.01606v1"
  },
  {
    "arxiv_id": "2405.16640v2",
    "entry_id": "http://arxiv.org/abs/2405.16640v2",
    "title": "A Survey of Multimodal Large Language Model from A Data-centric Perspective",
    "summary": "Multimodal large language models (MLLMs) enhance the capabilities of standard large language models by integrating and processing data from multiple modalities, including text, vision, audio, video, and 3D environments. Data plays a pivotal role in the development and refinement of these models. In this survey, we comprehensively review the literature on MLLMs from a data-centric perspective. Specifically, we explore methods for preparing multimodal data during the pretraining and adaptation phases of MLLMs. Additionally, we analyze the evaluation methods for the datasets and review the benchmarks for evaluating MLLMs. Our survey also outlines potential future research directions. This work aims to provide researchers with a detailed understanding of the data-driven aspects of MLLMs, fostering further exploration and innovation in this field.",
    "authors": [
      "Tianyi Bai",
      "Hao Liang",
      "Binwang Wan",
      "Yanran Xu",
      "Xi Li",
      "Shiyu Li",
      "Ling Yang",
      "Bozhou Li",
      "Yifan Wang",
      "Bin Cui",
      "Ping Huang",
      "Jiulong Shan",
      "Conghui He",
      "Binhang Yuan",
      "Wentao Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.MM"
    ],
    "published": "2024-05-26T17:31:21Z",
    "pdf_url": "https://arxiv.org/pdf/2405.16640v2"
  },
  {
    "arxiv_id": "2406.18842v3",
    "entry_id": "http://arxiv.org/abs/2406.18842v3",
    "title": "The global landscape of academic guidelines for generative AI and Large Language Models",
    "summary": "The integration of Generative Artificial Intelligence (GAI) and Large Language Models (LLMs) in academia has spurred a global discourse on their potential pedagogical benefits and ethical considerations. Positive reactions highlight some potential, such as collaborative creativity, increased access to education, and empowerment of trainers and trainees. However, negative reactions raise concerns about ethical complexities, balancing innovation and academic integrity, unequal access, and misinformation risks. Through a systematic survey and text-mining-based analysis of global and national directives, insights from independent research, and eighty university-level guidelines, this study provides a nuanced understanding of the opportunities and challenges posed by GAI and LLMs in education. It emphasizes the importance of balanced approaches that harness the benefits of these technologies while addressing ethical considerations and ensuring equitable access and educational outcomes. The paper concludes with recommendations for fostering responsible innovation and ethical practices to guide the integration of GAI and LLMs in academia.",
    "authors": [
      "Junfeng Jiao",
      "Saleh Afroogh",
      "Kevin Chen",
      "David Atkinson",
      "Amit Dhurandhar"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-05-26T15:28:24Z",
    "pdf_url": "https://arxiv.org/pdf/2406.18842v3"
  },
  {
    "arxiv_id": "2405.16334v4",
    "entry_id": "http://arxiv.org/abs/2405.16334v4",
    "title": "Devil's Advocate: Anticipatory Reflection for LLM Agents",
    "summary": "In this work, we introduce a novel approach that equips LLM agents with introspection, enhancing consistency and adaptability in solving complex tasks. Our approach prompts LLM agents to decompose a given task into manageable subtasks (i.e., to make a plan), and to continuously introspect upon the suitability and results of their actions. %; and when necessary, to explore ``the road not taken.'' We implement a three-fold introspective intervention: 1) anticipatory reflection on potential failures and alternative remedy before action execution, 2) post-action alignment with subtask objectives and backtracking with remedy to ensure utmost effort in plan execution, and 3) comprehensive review upon plan completion for future strategy refinement. By deploying and experimenting with this methodology -- a zero-shot approach -- within WebArena for practical tasks in web environments, our agent demonstrates superior performance with a success rate of 23.5% over existing zero-shot methods by 3.5%. The experimental results suggest that our introspection-driven approach not only enhances the agent's ability to navigate unanticipated challenges through a robust mechanism of plan execution, but also improves efficiency by reducing the number of trials and plan revisions by 45% needed to achieve a task.",
    "authors": [
      "Haoyu Wang",
      "Tao Li",
      "Zhiwei Deng",
      "Dan Roth",
      "Yang Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-05-25T19:20:15Z",
    "pdf_url": "https://arxiv.org/pdf/2405.16334v4"
  },
  {
    "arxiv_id": "2406.00033v1",
    "entry_id": "http://arxiv.org/abs/2406.00033v1",
    "title": "Retrieval-Augmented Conversational Recommendation with Prompt-based Semi-Structured Natural Language State Tracking",
    "summary": "Conversational recommendation (ConvRec) systems must understand rich and diverse natural language (NL) expressions of user preferences and intents, often communicated in an indirect manner (e.g., \"I'm watching my weight\"). Such complex utterances make retrieving relevant items challenging, especially if only using often incomplete or out-of-date metadata. Fortunately, many domains feature rich item reviews that cover standard metadata categories and offer complex opinions that might match a user's interests (e.g., \"classy joint for a date\"). However, only recently have large language models (LLMs) let us unlock the commonsense connections between user preference utterances and complex language in user-generated reviews. Further, LLMs enable novel paradigms for semi-structured dialogue state tracking, complex intent and preference understanding, and generating recommendations, explanations, and question answers. We thus introduce a novel technology RA-Rec, a Retrieval-Augmented, LLM-driven dialogue state tracking system for ConvRec, showcased with a video, open source GitHub repository, and interactive Google Colab notebook.",
    "authors": [
      "Sara Kemper",
      "Justin Cui",
      "Kai Dicarlantonio",
      "Kathy Lin",
      "Danjie Tang",
      "Anton Korikov",
      "Scott Sanner"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-25T15:41:26Z",
    "pdf_url": "https://arxiv.org/pdf/2406.00033v1"
  },
  {
    "arxiv_id": "2405.16205v1",
    "entry_id": "http://arxiv.org/abs/2405.16205v1",
    "title": "GeneAgent: Self-verification Language Agent for Gene Set Knowledge Discovery using Domain Databases",
    "summary": "Gene set knowledge discovery is essential for advancing human functional genomics. Recent studies have shown promising performance by harnessing the power of Large Language Models (LLMs) on this task. Nonetheless, their results are subject to several limitations common in LLMs such as hallucinations. In response, we present GeneAgent, a first-of-its-kind language agent featuring self-verification capability. It autonomously interacts with various biological databases and leverages relevant domain knowledge to improve accuracy and reduce hallucination occurrences. Benchmarking on 1,106 gene sets from different sources, GeneAgent consistently outperforms standard GPT-4 by a significant margin. Moreover, a detailed manual review confirms the effectiveness of the self-verification module in minimizing hallucinations and generating more reliable analytical narratives. To demonstrate its practical utility, we apply GeneAgent to seven novel gene sets derived from mouse B2905 melanoma cell lines, with expert evaluations showing that GeneAgent offers novel insights into gene functions and subsequently expedites knowledge discovery.",
    "authors": [
      "Zhizheng Wang",
      "Qiao Jin",
      "Chih-Hsuan Wei",
      "Shubo Tian",
      "Po-Ting Lai",
      "Qingqing Zhu",
      "Chi-Ping Day",
      "Christina Ross",
      "Zhiyong Lu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-05-25T12:35:15Z",
    "pdf_url": "https://arxiv.org/pdf/2405.16205v1"
  },
  {
    "arxiv_id": "2406.00029v1",
    "entry_id": "http://arxiv.org/abs/2406.00029v1",
    "title": "Clustered Retrieved Augmented Generation (CRAG)",
    "summary": "Providing external knowledge to Large Language Models (LLMs) is a key point for using these models in real-world applications for several reasons, such as incorporating up-to-date content in a real-time manner, providing access to domain-specific knowledge, and contributing to hallucination prevention. The vector database-based Retrieval Augmented Generation (RAG) approach has been widely adopted to this end. Thus, any part of external knowledge can be retrieved and provided to some LLM as the input context. Despite RAG approach's success, it still might be unfeasible for some applications, because the context retrieved can demand a longer context window than the size supported by LLM. Even when the context retrieved fits into the context window size, the number of tokens might be expressive and, consequently, impact costs and processing time, becoming impractical for most applications. To address these, we propose CRAG, a novel approach able to effectively reduce the number of prompting tokens without degrading the quality of the response generated compared to a solution using RAG. Through our experiments, we show that CRAG can reduce the number of tokens by at least 46\\%, achieving more than 90\\% in some cases, compared to RAG. Moreover, the number of tokens with CRAG does not increase considerably when the number of reviews analyzed is higher, unlike RAG, where the number of tokens is almost 9x higher when there are 75 reviews compared to 4 reviews.",
    "authors": [
      "Simon Akesson",
      "Frances A. Santos"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-24T16:36:47Z",
    "pdf_url": "https://arxiv.org/pdf/2406.00029v1"
  },
  {
    "arxiv_id": "2405.15436v1",
    "entry_id": "http://arxiv.org/abs/2405.15436v1",
    "title": "Hybrid Context Retrieval Augmented Generation Pipeline: LLM-Augmented Knowledge Graphs and Vector Database for Accreditation Reporting Assistance",
    "summary": "In higher education, accreditation is a quality assurance process, where an institution demonstrates a commitment to delivering high quality programs and services to their students. For business schools nationally and internationally the Association to Advance Collegiate Schools of Business (AACSB) accreditation is the gold standard. For a business school to receive and subsequently maintain accreditation, the school must undertake a rigorous, time consuming reporting and peer review process, to demonstrate alignment with the AACSB Standards. For this project we create a hybrid context retrieval augmented generation pipeline that can assist in the documentation alignment and reporting process necessary for accreditation. We implement both a vector database and knowledge graph, as knowledge stores containing both institutional data and AACSB Standard data. The output of the pipeline can be used by institution stakeholders to build their accreditation report, dually grounded by the context from the knowledge stores. To develop our knowledge graphs we utilized both a manual construction process as well as an LLM Augmented Knowledge Graph approach. We evaluated the pipeline using the RAGAs framework and observed optimal performance on answer relevancy and answer correctness metrics.",
    "authors": [
      "Candace Edwards"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-05-24T11:05:45Z",
    "pdf_url": "https://arxiv.org/pdf/2405.15436v1"
  },
  {
    "arxiv_id": "2406.00024v2",
    "entry_id": "http://arxiv.org/abs/2406.00024v2",
    "title": "Embedding-Aligned Language Models",
    "summary": "We propose a novel approach for training large language models (LLMs) to adhere to objectives defined within a latent embedding space. Our method leverages reinforcement learning (RL), treating a pre-trained LLM as an environment. Our embedding-aligned guided language (EAGLE) agent is trained to iteratively steer the LLM's generation towards optimal regions of the latent embedding space, w.r.t. some predefined criterion. We demonstrate the effectiveness of the EAGLE agent using the MovieLens 25M and Amazon Review datasets to surface content gaps that satisfy latent user demand. We also demonstrate the benefit of using an optimal design of a state-dependent action set to improve EAGLE's efficiency. Our work paves the way for controlled and grounded text generation using LLMs, ensuring consistency with domain-specific knowledge and data representations.",
    "authors": [
      "Guy Tennenholtz",
      "Yinlam Chow",
      "Chih-Wei Hsu",
      "Lior Shani",
      "Ethan Liang",
      "Craig Boutilier"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "published": "2024-05-24T06:11:17Z",
    "pdf_url": "https://arxiv.org/pdf/2406.00024v2"
  },
  {
    "arxiv_id": "2405.15164v2",
    "entry_id": "http://arxiv.org/abs/2405.15164v2",
    "title": "From Frege to chatGPT: Compositionality in language, cognition, and deep neural networks",
    "summary": "Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs), which share the same fundamental design principles as their predecessors, have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs -- all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.",
    "authors": [
      "Jacob Russin",
      "Sam Whitman McGrath",
      "Danielle J. Williams"
    ],
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-05-24T02:36:07Z",
    "pdf_url": "https://arxiv.org/pdf/2405.15164v2"
  },
  {
    "arxiv_id": "2405.15079v1",
    "entry_id": "http://arxiv.org/abs/2405.15079v1",
    "title": "A Survey of Distributed Learning in Cloud, Mobile, and Edge Settings",
    "summary": "In the era of deep learning (DL), convolutional neural networks (CNNs), and large language models (LLMs), machine learning (ML) models are becoming increasingly complex, demanding significant computational resources for both inference and training stages. To address this challenge, distributed learning has emerged as a crucial approach, employing parallelization across various devices and environments. This survey explores the landscape of distributed learning, encompassing cloud and edge settings. We delve into the core concepts of data and model parallelism, examining how models are partitioned across different dimensions and layers to optimize resource utilization and performance. We analyze various partitioning schemes for different layer types, including fully connected, convolutional, and recurrent layers, highlighting the trade-offs between computational efficiency, communication overhead, and memory constraints. This survey provides valuable insights for future research and development in this rapidly evolving field by comparing and contrasting distributed learning approaches across diverse contexts.",
    "authors": [
      "Madison Threadgill",
      "Andreas Gerstlauer"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-05-23T22:00:38Z",
    "pdf_url": "https://arxiv.org/pdf/2405.15079v1"
  },
  {
    "arxiv_id": "2405.15077v4",
    "entry_id": "http://arxiv.org/abs/2405.15077v4",
    "title": "Eliciting Informative Text Evaluations with Large Language Models",
    "summary": "Peer prediction mechanisms motivate high-quality feedback with provable guarantees. However, current methods only apply to rather simple reports, like multiple-choice or scalar numbers. We aim to broaden these techniques to the larger domain of text-based reports, drawing on the recent developments in large language models. This vastly increases the applicability of peer prediction mechanisms as textual feedback is the norm in a large variety of feedback channels: peer reviews, e-commerce customer reviews, and comments on social media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM) and the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms utilize LLMs as predictors, mapping from one agent's report to a prediction of her peer's report. Theoretically, we show that when the LLM prediction is sufficiently accurate, our mechanisms can incentivize high effort and truth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we confirm the efficacy of our mechanisms through experiments conducted on two real datasets: the Yelp review dataset and the ICLR OpenReview dataset. We highlight the results that on the ICLR dataset, our mechanisms can differentiate three quality levels -- human-written reviews, GPT-4-generated reviews, and GPT-3.5-generated reviews in terms of expected scores. Additionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.",
    "authors": [
      "Yuxuan Lu",
      "Shengwei Xu",
      "Yichi Zhang",
      "Yuqing Kong",
      "Grant Schoenebeck"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.GT"
    ],
    "published": "2024-05-23T21:56:12Z",
    "pdf_url": "https://arxiv.org/pdf/2405.15077v4"
  },
  {
    "arxiv_id": "2405.14930v1",
    "entry_id": "http://arxiv.org/abs/2405.14930v1",
    "title": "AstroPT: Scaling Large Observation Models for Astronomy",
    "summary": "This work presents AstroPT, an autoregressive pretrained transformer developed with astronomical use-cases in mind. The AstroPT models presented here have been pretrained on 8.6 million $512 \\times 512$ pixel $grz$-band galaxy postage stamp observations from the DESI Legacy Survey DR8. We train a selection of foundation models of increasing size from 1 million to 2.1 billion parameters, and find that AstroPT follows a similar saturating log-log scaling law to textual models. We also find that the models' performances on downstream tasks as measured by linear probing improves with model size up to the model parameter saturation point. We believe that collaborative community development paves the best route towards realising an open source `Large Observation Model' -- a model trained on data taken from the observational sciences at the scale seen in natural language processing. To this end, we release the source code, weights, and dataset for AstroPT under the MIT license, and invite potential collaborators to join us in collectively building and researching these models.",
    "authors": [
      "Michael J. Smith",
      "Ryan J. Roberts",
      "Eirini Angeloudi",
      "Marc Huertas-Company"
    ],
    "categories": [
      "astro-ph.IM",
      "astro-ph.GA",
      "cs.LG"
    ],
    "published": "2024-05-23T18:00:00Z",
    "pdf_url": "https://arxiv.org/pdf/2405.14930v1"
  },
  {
    "arxiv_id": "2405.14445v2",
    "entry_id": "http://arxiv.org/abs/2405.14445v2",
    "title": "Exploring the use of a Large Language Model for data extraction in systematic reviews: a rapid feasibility study",
    "summary": "This paper describes a rapid feasibility study of using GPT-4, a large language model (LLM), to (semi)automate data extraction in systematic reviews. Despite the recent surge of interest in LLMs there is still a lack of understanding of how to design LLM-based automation tools and how to robustly evaluate their performance. During the 2023 Evidence Synthesis Hackathon we conducted two feasibility studies. Firstly, to automatically extract study characteristics from human clinical, animal, and social science domain studies. We used two studies from each category for prompt-development; and ten for evaluation. Secondly, we used the LLM to predict Participants, Interventions, Controls and Outcomes (PICOs) labelled within 100 abstracts in the EBM-NLP dataset. Overall, results indicated an accuracy of around 80%, with some variability between domains (82% for human clinical, 80% for animal, and 72% for studies of human social sciences). Causal inference methods and study design were the data extraction items with the most errors. In the PICO study, participants and intervention/control showed high accuracy (>80%), outcomes were more challenging. Evaluation was done manually; scoring methods such as BLEU and ROUGE showed limited value. We observed variability in the LLMs predictions and changes in response quality. This paper presents a template for future evaluations of LLMs in the context of data extraction for systematic review automation. Our results show that there might be value in using LLMs, for example as second or third reviewers. However, caution is advised when integrating models such as GPT-4 into tools. Further research on stability and reliability in practical settings is warranted for each type of data that is processed by the LLM.",
    "authors": [
      "Lena Schmidt",
      "Kaitlyn Hair",
      "Sergio Graziosi",
      "Fiona Campbell",
      "Claudia Kapp",
      "Alireza Khanteymoori",
      "Dawn Craig",
      "Mark Engelbert",
      "James Thomas"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-23T11:24:23Z",
    "pdf_url": "https://arxiv.org/pdf/2405.14445v2"
  },
  {
    "arxiv_id": "2405.13606v1",
    "entry_id": "http://arxiv.org/abs/2405.13606v1",
    "title": "From the evolution of public data ecosystems to the evolving horizons of the forward-looking intelligent public data ecosystem empowered by emerging technologies",
    "summary": "Public data ecosystems (PDEs) represent complex socio-technical systems crucial for optimizing data use in the public sector and outside it. Recognizing their multifaceted nature, previous research pro-posed a six-generation Evolutionary Model of Public Data Ecosystems (EMPDE). Designed as a result of a systematic literature review on the topic spanning three decade, this model, while theoretically robust, necessitates empirical validation to enhance its practical applicability. This study addresses this gap by validating the theoretical model through a real-life examination in five European countries - Latvia, Serbia, Czech Republic, Spain, and Poland. This empirical validation provides insights into PDEs dynamics and variations of implementations across contexts, particularly focusing on the 6th generation of forward-looking PDE generation named \"Intelligent Public Data Generation\" that represents a paradigm shift driven by emerging technologies such as cloud computing, Artificial Intelligence, Natural Language Processing tools, Generative AI, and Large Language Models (LLM) with potential to contribute to both automation and augmentation of business processes within these ecosystems. By transcending their traditional status as a mere component, evolving into both an actor and a stakeholder simultaneously, these technologies catalyze innovation and progress, enhancing PDE management strategies to align with societal, regulatory, and technical imperatives in the digital era.",
    "authors": [
      "Anastasija Nikiforova",
      "Martin Lnenicka",
      "Petar Milić",
      "Mariusz Luterek",
      "Manuel Pedro Rodríguez Bolívar"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.IR"
    ],
    "published": "2024-05-22T12:58:02Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13606v1"
  },
  {
    "arxiv_id": "2405.13581v1",
    "entry_id": "http://arxiv.org/abs/2405.13581v1",
    "title": "Safety Alignment for Vision Language Models",
    "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs), pre-trained visual encoder models connected to an LLMs can realize Vision Language Models (VLMs). However, existing research shows that the visual modality of VLMs is vulnerable, with attackers easily bypassing LLMs' safety alignment through visual modality features to launch attacks. To address this issue, we enhance the existing VLMs' visual modality safety alignment by adding safety modules, including a safety projector, safety tokens, and a safety head, through a two-stage training process, effectively improving the model's defense against risky images. For example, building upon the LLaVA-v1.5 model, we achieve a safety score of 8.26, surpassing the GPT-4V on the Red Teaming Visual Language Models (RTVLM) benchmark. Our method boasts ease of use, high flexibility, and strong controllability, and it enhances safety while having minimal impact on the model's general performance. Moreover, our alignment strategy also uncovers some possible risky content within commonly used open-source multimodal datasets. Our code will be open sourced after the anonymous review.",
    "authors": [
      "Zhendong Liu",
      "Yuanbi Nie",
      "Yingshui Tan",
      "Xiangyu Yue",
      "Qiushi Cui",
      "Chongjun Wang",
      "Xiaoyong Zhu",
      "Bo Zheng"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-05-22T12:21:27Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13581v1"
  },
  {
    "arxiv_id": "2405.13565v1",
    "entry_id": "http://arxiv.org/abs/2405.13565v1",
    "title": "AI-Assisted Assessment of Coding Practices in Modern Code Review",
    "summary": "Modern code review is a process in which an incremental code contribution made by a code author is reviewed by one or more peers before it is committed to the version control system. An important element of modern code review is verifying that code contributions adhere to best practices. While some of these best practices can be automatically verified, verifying others is commonly left to human reviewers. This paper reports on the development, deployment, and evaluation of AutoCommenter, a system backed by a large language model that automatically learns and enforces coding best practices. We implemented AutoCommenter for four programming languages (C++, Java, Python, and Go) and evaluated its performance and adoption in a large industrial setting. Our evaluation shows that an end-to-end system for learning and enforcing coding best practices is feasible and has a positive impact on the developer workflow. Additionally, this paper reports on the challenges associated with deploying such a system to tens of thousands of developers and the corresponding lessons learned.",
    "authors": [
      "Manushree Vijayvergiya",
      "Małgorzata Salawa",
      "Ivan Budiselić",
      "Dan Zheng",
      "Pascal Lamblin",
      "Marko Ivanković",
      "Juanjo Carin",
      "Mateusz Lewko",
      "Jovan Andonov",
      "Goran Petrović",
      "Daniel Tarlow",
      "Petros Maniatis",
      "René Just"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-05-22T11:57:18Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13565v1"
  },
  {
    "arxiv_id": "2405.13245v2",
    "entry_id": "http://arxiv.org/abs/2405.13245v2",
    "title": "A Survey of Robotic Language Grounding: Tradeoffs between Symbols and Embeddings",
    "summary": "With large language models, robots can understand language more flexibly and more capable than ever before. This survey reviews and situates recent literature into a spectrum with two poles: 1) mapping between language and some manually defined formal representation of meaning, and 2) mapping between language and high-dimensional vector spaces that translate directly to low-level robot policy. Using a formal representation allows the meaning of the language to be precisely represented, limits the size of the learning problem, and leads to a framework for interpretability and formal safety guarantees. Methods that embed language and perceptual data into high-dimensional spaces avoid this manually specified symbolic structure and thus have the potential to be more general when fed enough data but require more data and computing to train. We discuss the benefits and tradeoffs of each approach and finish by providing directions for future work that achieves the best of both worlds.",
    "authors": [
      "Vanya Cohen",
      "Jason Xinyu Liu",
      "Raymond Mooney",
      "Stefanie Tellex",
      "David Watkins"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-05-21T23:12:03Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13245v2"
  },
  {
    "arxiv_id": "2405.12923v1",
    "entry_id": "http://arxiv.org/abs/2405.12923v1",
    "title": "Panmodal Information Interaction",
    "summary": "The emergence of generative artificial intelligence (GenAI) is transforming information interaction. For decades, search engines such as Google and Bing have been the primary means of locating relevant information for the general population. They have provided search results in the same standard format (the so-called \"10 blue links\"). The recent ability to chat via natural language with AI-based agents and have GenAI automatically synthesize answers in real-time (grounded in top-ranked results) is changing how people interact with and consume information at massive scale. These two information interaction modalities (traditional search and AI-powered chat) coexist in current search engines, either loosely coupled (e.g., as separate options/tabs) or tightly coupled (e.g., integrated as a chat answer embedded directly within a traditional search result page). We believe that the existence of these two different modalities, and potentially many others, is creating an opportunity to re-imagine the search experience, capitalize on the strengths of many modalities, and develop systems and strategies to support seamless flow between them. We refer to these as panmodal experiences. Unlike monomodal experiences, where only one modality is available and/or used for the task at hand, panmodal experiences make multiple modalities available to users (multimodal), directly support transitions between modalities (crossmodal), and seamlessly combine modalities to tailor task assistance (transmodal). While our focus is search and chat, with learnings from insights from a survey of over 100 individuals who have recently performed common tasks on these two modalities, we also present a more general vision for the future of information interaction using multiple modalities and the emergent capabilities of GenAI.",
    "authors": [
      "Chirag Shah",
      "Ryen W. White"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-05-21T16:49:14Z",
    "pdf_url": "https://arxiv.org/pdf/2405.12923v1"
  },
  {
    "arxiv_id": "2405.12819v2",
    "entry_id": "http://arxiv.org/abs/2405.12819v2",
    "title": "Large Language Models Meet NLP: A Survey",
    "summary": "While large language models (LLMs) like ChatGPT have shown impressive capabilities in Natural Language Processing (NLP) tasks, a systematic investigation of their potential in this field remains largely unexplored. This study aims to address this gap by exploring the following questions: (1) How are LLMs currently applied to NLP tasks in the literature? (2) Have traditional NLP tasks already been solved with LLMs? (3) What is the future of the LLMs for NLP? To answer these questions, we take the first step to provide a comprehensive overview of LLMs in NLP. Specifically, we first introduce a unified taxonomy including (1) parameter-frozen paradigm and (2) parameter-tuning paradigm to offer a unified perspective for understanding the current progress of LLMs in NLP. Furthermore, we summarize the new frontiers and the corresponding challenges, aiming to inspire further groundbreaking advancements. We hope this work offers valuable insights into the potential and limitations of LLMs, while also serving as a practical guide for building effective LLMs in NLP.",
    "authors": [
      "Libo Qin",
      "Qiguang Chen",
      "Xiachong Feng",
      "Yang Wu",
      "Yongheng Zhang",
      "Yinghui Li",
      "Min Li",
      "Wanxiang Che",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-21T14:24:01Z",
    "pdf_url": "https://arxiv.org/pdf/2405.12819v2"
  },
  {
    "arxiv_id": "2405.12779v1",
    "entry_id": "http://arxiv.org/abs/2405.12779v1",
    "title": "Transformer in Touch: A Survey",
    "summary": "The Transformer model, initially achieving significant success in the field of natural language processing, has recently shown great potential in the application of tactile perception. This review aims to comprehensively outline the application and development of Transformers in tactile technology. We first introduce the two fundamental concepts behind the success of the Transformer: the self-attention mechanism and large-scale pre-training. Then, we delve into the application of Transformers in various tactile tasks, including but not limited to object recognition, cross-modal generation, and object manipulation, offering a concise summary of the core methodologies, performance benchmarks, and design highlights. Finally, we suggest potential areas for further research and future work, aiming to generate more interest within the community, tackle existing challenges, and encourage the use of Transformer models in the tactile field.",
    "authors": [
      "Jing Gao",
      "Ning Cheng",
      "Bin Fang",
      "Wenjuan Han"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-05-21T13:26:27Z",
    "pdf_url": "https://arxiv.org/pdf/2405.12779v1"
  },
  {
    "arxiv_id": "2405.12750v2",
    "entry_id": "http://arxiv.org/abs/2405.12750v2",
    "title": "Generative AI in Cybersecurity: A Comprehensive Review of LLM Applications and Vulnerabilities",
    "summary": "This paper provides a comprehensive review of the future of cybersecurity through Generative AI and Large Language Models (LLMs). We explore LLM applications across various domains, including hardware design security, intrusion detection, software engineering, design verification, cyber threat intelligence, malware detection, and phishing detection. We present an overview of LLM evolution and its current state, focusing on advancements in models such as GPT-4, GPT-3.5, Mixtral-8x7B, BERT, Falcon2, and LLaMA. Our analysis extends to LLM vulnerabilities, such as prompt injection, insecure output handling, data poisoning, DDoS attacks, and adversarial instructions. We delve into mitigation strategies to protect these models, providing a comprehensive look at potential attack scenarios and prevention techniques. Furthermore, we evaluate the performance of 42 LLM models in cybersecurity knowledge and hardware security, highlighting their strengths and weaknesses. We thoroughly evaluate cybersecurity datasets for LLM training and testing, covering the lifecycle from data creation to usage and identifying gaps for future research. In addition, we review new strategies for leveraging LLMs, including techniques like Half-Quadratic Quantization (HQQ), Reinforcement Learning with Human Feedback (RLHF), Direct Preference Optimization (DPO), Quantized Low-Rank Adapters (QLoRA), and Retrieval-Augmented Generation (RAG). These insights aim to enhance real-time cybersecurity defenses and improve the sophistication of LLM applications in threat detection and response. Our paper provides a foundational understanding and strategic direction for integrating LLMs into future cybersecurity frameworks, emphasizing innovation and robust model deployment to safeguard against evolving cyber threats.",
    "authors": [
      "Mohamed Amine Ferrag",
      "Fatima Alwahedi",
      "Ammar Battah",
      "Bilel Cherif",
      "Abdechakour Mechri",
      "Norbert Tihanyi",
      "Tamas Bisztray",
      "Merouane Debbah"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-05-21T13:02:27Z",
    "pdf_url": "https://arxiv.org/pdf/2405.12750v2"
  },
  {
    "arxiv_id": "2405.12732v1",
    "entry_id": "http://arxiv.org/abs/2405.12732v1",
    "title": "Review on modeling the societal impact of infrastructure disruptions due to disasters",
    "summary": "Infrastructure systems play a critical role in providing essential products and services for the functioning of modern society; however, they are vulnerable to disasters and their service disruptions can cause severe societal impacts. To protect infrastructure from disasters and reduce potential impacts, great achievements have been made in modeling interdependent infrastructure systems in past decades. In recent years, scholars have gradually shifted their research focus to understanding and modeling societal impacts of disruptions considering the fact that infrastructure systems are critical because of their role in societal functioning, especially under situations of modern societies. Exploring how infrastructure disruptions impair society to enhance resilient city has become a key field of study. By comprehensively reviewing relevant studies, this paper demonstrated the definition and types of societal impact of infrastructure disruptions, and summarized the modeling approaches into four types: extended infrastructure modeling approaches, empirical approaches, agent-based approaches, and big data-driven approaches. For each approach, this paper organized relevant literature in terms of modeling ideas, advantages, and disadvantages. Furthermore, the four approaches were compared according to several criteria, including the input data, types of societal impact, and application scope. Finally, this paper illustrated the challenges and future research directions in the field.",
    "authors": [
      "Yongsheng Yang",
      "Huan Liu",
      "Ali Mostafavi",
      "Hirokazu Tatano"
    ],
    "categories": [
      "cs.MA",
      "physics.data-an"
    ],
    "published": "2024-05-21T12:37:45Z",
    "pdf_url": "https://arxiv.org/pdf/2405.12732v1"
  },
  {
    "arxiv_id": "2405.13081v1",
    "entry_id": "http://arxiv.org/abs/2405.13081v1",
    "title": "Children's Mental Models of Generative Visual and Text Based AI Models",
    "summary": "In this work we investigate how children ages 5-12 perceive, understand, and use generative AI models such as a text-based LLMs ChatGPT and a visual-based model DALL-E. Generative AI is newly being used widely since chatGPT. Children are also building mental models of generative AI. Those haven't been studied before and it is also the case that the children's models are dynamic as they use the tools, even with just very short usage. Upon surveying and experimentally observing over 40 children ages 5-12, we found that children generally have a very positive outlook towards AI and are excited about the ways AI may benefit and aid them in their everyday lives. In a forced choice, children robustly associated AI with positive adjectives versus negative ones. We also categorize what children are querying AI models for and find that children search for more imaginative things that don't exist when using a visual-based AI and not when using a text-based one. Our follow-up study monitored children's responses and feelings towards AI before and after interacting with GenAI models. We even find that children find AI to be less scary after interacting with it. We hope that these findings will shine a light on children's mental models of AI and provide insight for how to design the best possible tools for children who will inevitably be using AI in their lifetimes. The motivation of this work is to bridge the gap between Human-Computer Interaction (HCI) and Psychology in an effort to study the effects of AI on society. We aim to identify the gaps in humans' mental models of what AI is and how it works. Previous work has investigated how both adults and children perceive various kinds of robots, computers, and other technological concepts. However, there is very little work investigating these concepts for generative AI models and not simply embodied robots or physical technology.",
    "authors": [
      "Eliza Kosoy",
      "Soojin Jeong",
      "Anoop Sinha",
      "Alison Gopnik",
      "Tanya Kraljic"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-05-21T06:18:00Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13081v1"
  },
  {
    "arxiv_id": "2405.11983v2",
    "entry_id": "http://arxiv.org/abs/2405.11983v2",
    "title": "A review on the use of large language models as virtual tutors",
    "summary": "Transformer architectures contribute to managing long-term dependencies for Natural Language Processing, representing one of the most recent changes in the field. These architectures are the basis of the innovative, cutting-edge Large Language Models (LLMs) that have produced a huge buzz in several fields and industrial sectors, among the ones education stands out. Accordingly, these generative Artificial Intelligence-based solutions have directed the change in techniques and the evolution in educational methods and contents, along with network infrastructure, towards high-quality learning. Given the popularity of LLMs, this review seeks to provide a comprehensive overview of those solutions designed specifically to generate and evaluate educational materials and which involve students and teachers in their design or experimental plan. To the best of our knowledge, this is the first review of educational applications (e.g., student assessment) of LLMs. As expected, the most common role of these systems is as virtual tutors for automatic question generation. Moreover, the most popular models are GTP-3 and BERT. However, due to the continuous launch of new generative models, new works are expected to be published shortly.",
    "authors": [
      "Silvia García-Méndez",
      "Francisco de Arriba-Pérez",
      "María del Carmen Somoza-López"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-20T12:33:42Z",
    "pdf_url": "https://arxiv.org/pdf/2405.11983v2"
  },
  {
    "arxiv_id": "2405.13055v1",
    "entry_id": "http://arxiv.org/abs/2405.13055v1",
    "title": "Large Language Models for Medicine: A Survey",
    "summary": "To address challenges in the digital economy's landscape of digital intelligence, large language models (LLMs) have been developed. Improvements in computational power and available resources have significantly advanced LLMs, allowing their integration into diverse domains for human life. Medical LLMs are essential application tools with potential across various medical scenarios. In this paper, we review LLM developments, focusing on the requirements and applications of medical LLMs. We provide a concise overview of existing models, aiming to explore advanced research directions and benefit researchers for future medical applications. We emphasize the advantages of medical LLMs in applications, as well as the challenges encountered during their development. Finally, we suggest directions for technical integration to mitigate challenges and potential research directions for the future of medical LLMs, aiming to meet the demands of the medical field better.",
    "authors": [
      "Yanxin Zheng",
      "Wensheng Gan",
      "Zefeng Chen",
      "Zhenlian Qi",
      "Qian Liang",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-05-20T02:32:26Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13055v1"
  },
  {
    "arxiv_id": "2405.11704v1",
    "entry_id": "http://arxiv.org/abs/2405.11704v1",
    "title": "Efficiency optimization of large-scale language models based on deep learning in natural language processing tasks",
    "summary": "The internal structure and operation mechanism of large-scale language models are analyzed theoretically, especially how Transformer and its derivative architectures can restrict computing efficiency while capturing long-term dependencies. Further, we dig deep into the efficiency bottleneck of the training phase, and evaluate in detail the contribution of adaptive optimization algorithms (such as AdamW), massively parallel computing techniques, and mixed precision training strategies to accelerate convergence and reduce memory footprint. By analyzing the mathematical principles and implementation details of these algorithms, we reveal how they effectively improve training efficiency in practice. In terms of model deployment and inference optimization, this paper systematically reviews the latest advances in model compression techniques, focusing on strategies such as quantification, pruning, and knowledge distillation. By comparing the theoretical frameworks of these techniques and their effects in different application scenarios, we demonstrate their ability to significantly reduce model size and inference delay while maintaining model prediction accuracy. In addition, this paper critically examines the limitations of current efficiency optimization methods, such as the increased risk of overfitting, the control of performance loss after compression, and the problem of algorithm generality, and proposes some prospects for future research. In conclusion, this study provides a comprehensive theoretical framework for understanding the efficiency optimization of large-scale language models.",
    "authors": [
      "Taiyuan Mei",
      "Yun Zi",
      "Xiaohan Cheng",
      "Zijun Gao",
      "Qi Wang",
      "Haowei Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-05-20T00:10:00Z",
    "pdf_url": "https://arxiv.org/pdf/2405.11704v1"
  },
  {
    "arxiv_id": "2405.11299v2",
    "entry_id": "http://arxiv.org/abs/2405.11299v2",
    "title": "The CAP Principle for LLM Serving: A Survey of Long-Context Large Language Model Serving",
    "summary": "We survey the large language model (LLM) serving area to understand the intricate dynamics between cost-efficiency and accuracy, which is magnified by the growing need for longer contextual understanding when deploying models at a massive scale. Our findings reveal that works in this space optimize along three distinct but conflicting goals: improving serving context length (C), improving serving accuracy (A), and improving serving performance (P). Drawing inspiration from the CAP theorem in databases, we propose a CAP principle for LLM serving, which suggests that any optimization can improve at most two of these three goals simultaneously. Our survey categorizes existing works within this framework. We find the definition and continuity of user-perceived measurement metrics are crucial in determining whether a goal has been met, akin to prior CAP databases in the wild. We recognize the CAP principle for LLM serving as a guiding principle, rather than a formal theorem, to inform designers of the inherent and dynamic trade-offs in serving models. As serving accuracy and performance have been extensively studied, this survey focuses on works that extend serving context length and address the resulting challenges.",
    "authors": [
      "Pai Zeng",
      "Zhenyu Ning",
      "Jieru Zhao",
      "Weihao Cui",
      "Mengwei Xu",
      "Liwei Guo",
      "Xusheng Chen",
      "Yizhou Shan"
    ],
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "published": "2024-05-18T14:00:04Z",
    "pdf_url": "https://arxiv.org/pdf/2405.11299v2"
  },
  {
    "arxiv_id": "2405.11106v1",
    "entry_id": "http://arxiv.org/abs/2405.11106v1",
    "title": "LLM-based Multi-Agent Reinforcement Learning: Current and Future Directions",
    "summary": "In recent years, Large Language Models (LLMs) have shown great abilities in various tasks, including question answering, arithmetic problem solving, and poem writing, among others. Although research on LLM-as-an-agent has shown that LLM can be applied to Reinforcement Learning (RL) and achieve decent results, the extension of LLM-based RL to Multi-Agent System (MAS) is not trivial, as many aspects, such as coordination and communication between agents, are not considered in the RL frameworks of a single agent. To inspire more research on LLM-based MARL, in this letter, we survey the existing LLM-based single-agent and multi-agent RL frameworks and provide potential research directions for future research. In particular, we focus on the cooperative tasks of multiple agents with a common goal and communication among them. We also consider human-in/on-the-loop scenarios enabled by the language component in the framework.",
    "authors": [
      "Chuanneng Sun",
      "Songjun Huang",
      "Dario Pompili"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2024-05-17T22:10:23Z",
    "pdf_url": "https://arxiv.org/pdf/2405.11106v1"
  },
  {
    "arxiv_id": "2405.11029v1",
    "entry_id": "http://arxiv.org/abs/2405.11029v1",
    "title": "Generative Artificial Intelligence: A Systematic Review and Applications",
    "summary": "In recent years, the study of artificial intelligence (AI) has undergone a paradigm shift. This has been propelled by the groundbreaking capabilities of generative models both in supervised and unsupervised learning scenarios. Generative AI has shown state-of-the-art performance in solving perplexing real-world conundrums in fields such as image translation, medical diagnostics, textual imagery fusion, natural language processing, and beyond. This paper documents the systematic review and analysis of recent advancements and techniques in Generative AI with a detailed discussion of their applications including application-specific models. Indeed, the major impact that generative AI has made to date, has been in language generation with the development of large language models, in the field of image translation and several other interdisciplinary applications of generative AI. Moreover, the primary contribution of this paper lies in its coherent synthesis of the latest advancements in these areas, seamlessly weaving together contemporary breakthroughs in the field. Particularly, how it shares an exploration of the future trajectory for generative AI. In conclusion, the paper ends with a discussion of Responsible AI principles, and the necessary ethical considerations for the sustainability and growth of these generative models.",
    "authors": [
      "Sandeep Singh Sengar",
      "Affan Bin Hasan",
      "Sanjay Kumar",
      "Fiona Carroll"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-05-17T18:03:59Z",
    "pdf_url": "https://arxiv.org/pdf/2405.11029v1"
  },
  {
    "arxiv_id": "2405.10936v2",
    "entry_id": "http://arxiv.org/abs/2405.10936v2",
    "title": "A Survey on Large Language Models with Multilingualism: Recent Advances and New Frontiers",
    "summary": "The rapid development of Large Language Models (LLMs) demonstrates remarkable multilingual capabilities in natural language processing, attracting global attention in both academia and industry. To mitigate potential discrimination and enhance the overall usability and accessibility for diverse language user groups, it is important for the development of language-fair technology. Despite the breakthroughs of LLMs, the investigation into the multilingual scenario remains insufficient, where a comprehensive survey to summarize recent approaches, developments, limitations, and potential solutions is desirable. To this end, we provide a survey with multiple perspectives on the utilization of LLMs in the multilingual scenario. We first rethink the transitions between previous and current research on pre-trained language models. Then we introduce several perspectives on the multilingualism of LLMs, including training and inference methods, information retrieval, model security, multi-domain with language culture, and usage of datasets. We also discuss the major challenges that arise in these aspects, along with possible solutions. Besides, we highlight future research directions that aim at further enhancing LLMs with multilingualism. The survey aims to help the research community address multilingual problems and provide a comprehensive understanding of the core concepts, key techniques, and latest developments in multilingual natural language processing based on LLMs.",
    "authors": [
      "Kaiyu Huang",
      "Fengran Mo",
      "Xinyu Zhang",
      "Hongliang Li",
      "You Li",
      "Yuanchi Zhang",
      "Weijian Yi",
      "Yulong Mao",
      "Jinchen Liu",
      "Yuzhuang Xu",
      "Jinan Xu",
      "Jian-Yun Nie",
      "Yang Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-17T17:47:39Z",
    "pdf_url": "https://arxiv.org/pdf/2405.10936v2"
  },
  {
    "arxiv_id": "2405.11013v1",
    "entry_id": "http://arxiv.org/abs/2405.11013v1",
    "title": "ARDDQN: Attention Recurrent Double Deep Q-Network for UAV Coverage Path Planning and Data Harvesting",
    "summary": "Unmanned Aerial Vehicles (UAVs) have gained popularity in data harvesting (DH) and coverage path planning (CPP) to survey a given area efficiently and collect data from aerial perspectives, while data harvesting aims to gather information from various Internet of Things (IoT) sensor devices, coverage path planning guarantees that every location within the designated area is visited with minimal redundancy and maximum efficiency. We propose the ARDDQN (Attention-based Recurrent Double Deep Q Network), which integrates double deep Q-networks (DDQN) with recurrent neural networks (RNNs) and an attention mechanism to generate path coverage choices that maximize data collection from IoT devices and to learn a control scheme for the UAV that generalizes energy restrictions. We employ a structured environment map comprising a compressed global environment map and a local map showing the UAV agent's locate efficiently scaling to large environments. We have compared Long short-term memory (LSTM), Bi-directional long short-term memory (Bi-LSTM), Gated recurrent unit (GRU) and Bidirectional gated recurrent unit (Bi-GRU) as recurrent neural networks (RNN) to the result without RNN We propose integrating the LSTM with the Attention mechanism to the existing DDQN model, which works best on evolution parameters, i.e., data collection, landing, and coverage ratios for the CPP and data harvesting scenarios.",
    "authors": [
      "Praveen Kumar",
      "Priyadarshni",
      "Rajiv Misra"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-05-17T16:53:19Z",
    "pdf_url": "https://arxiv.org/pdf/2405.11013v1"
  },
  {
    "arxiv_id": "2405.10825v2",
    "entry_id": "http://arxiv.org/abs/2405.10825v2",
    "title": "Large Language Model (LLM) for Telecommunications: A Comprehensive Survey on Principles, Key Techniques, and Opportunities",
    "summary": "Large language models (LLMs) have received considerable attention recently due to their outstanding comprehension and reasoning capabilities, leading to great progress in many fields. The advancement of LLM techniques also offers promising opportunities to automate many tasks in the telecommunication (telecom) field. After pre-training and fine-tuning, LLMs can perform diverse downstream tasks based on human instructions, paving the way to artificial general intelligence (AGI)-enabled 6G. Given the great potential of LLM technologies, this work aims to provide a comprehensive overview of LLM-enabled telecom networks. In particular, we first present LLM fundamentals, including model architecture, pre-training, fine-tuning, inference and utilization, model evaluation, and telecom deployment. Then, we introduce LLM-enabled key techniques and telecom applications in terms of generation, classification, optimization, and prediction problems. Specifically, the LLM-enabled generation applications include telecom domain knowledge, code, and network configuration generation. After that, the LLM-based classification applications involve network security, text, image, and traffic classification problems. Moreover, multiple LLM-enabled optimization techniques are introduced, such as automated reward function design for reinforcement learning and verbal reinforcement learning. Furthermore, for LLM-aided prediction problems, we discussed time-series prediction models and multi-modality prediction problems for telecom. Finally, we highlight the challenges and identify the future directions of LLM-enabled telecom networks.",
    "authors": [
      "Hao Zhou",
      "Chengming Hu",
      "Ye Yuan",
      "Yufei Cui",
      "Yili Jin",
      "Can Chen",
      "Haolun Wu",
      "Dun Yuan",
      "Li Jiang",
      "Di Wu",
      "Xue Liu",
      "Charlie Zhang",
      "Xianbin Wang",
      "Jiangchuan Liu"
    ],
    "categories": [
      "eess.SY",
      "cs.LG"
    ],
    "published": "2024-05-17T14:46:13Z",
    "pdf_url": "https://arxiv.org/pdf/2405.10825v2"
  },
  {
    "arxiv_id": "2405.10739v2",
    "entry_id": "http://arxiv.org/abs/2405.10739v2",
    "title": "Efficient Multimodal Large Language Models: A Survey",
    "summary": "In the past year, Multimodal Large Language Models (MLLMs) have demonstrated remarkable performance in tasks such as visual question answering, visual understanding and reasoning. However, the extensive model size and high training and inference costs have hindered the widespread application of MLLMs in academia and industry. Thus, studying efficient and lightweight MLLMs has enormous potential, especially in edge computing scenarios. In this survey, we provide a comprehensive and systematic review of the current state of efficient MLLMs. Specifically, we summarize the timeline of representative efficient MLLMs, research state of efficient structures and strategies, and the applications. Finally, we discuss the limitations of current efficient MLLM research and promising future directions. Please refer to our GitHub repository for more details: https://github.com/lijiannuist/Efficient-Multimodal-LLMs-Survey.",
    "authors": [
      "Yizhang Jin",
      "Jian Li",
      "Yexin Liu",
      "Tianjun Gu",
      "Kai Wu",
      "Zhengkai Jiang",
      "Muyang He",
      "Bo Zhao",
      "Xin Tan",
      "Zhenye Gan",
      "Yabiao Wang",
      "Chengjie Wang",
      "Lizhuang Ma"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-05-17T12:37:10Z",
    "pdf_url": "https://arxiv.org/pdf/2405.10739v2"
  },
  {
    "arxiv_id": "2405.10630v1",
    "entry_id": "http://arxiv.org/abs/2405.10630v1",
    "title": "Medical Dialogue: A Survey of Categories, Methods, Evaluation and Challenges",
    "summary": "This paper surveys and organizes research works on medical dialog systems, which is an important yet challenging task. Although these systems have been surveyed in the medical community from an application perspective, a systematic review from a rigorous technical perspective has to date remained noticeably absent. As a result, an overview of the categories, methods, and evaluation of medical dialogue systems remain limited and underspecified, hindering the further improvement of this area. To fill this gap, we investigate an initial pool of 325 papers from well-known computer science, and natural language processing conferences and journals, and make an overview. Recently, large language models have shown strong model capacity on downstream tasks, which also reshaped medical dialog systems' foundation. Despite the alluring practical application value, current medical dialogue systems still suffer from problems. To this end, this paper lists the grand challenges of medical dialog systems, especially of large language models.",
    "authors": [
      "Xiaoming Shi",
      "Zeming Liu",
      "Li Du",
      "Yuxuan Wang",
      "Hongru Wang",
      "Yuhang Guo",
      "Tong Ruan",
      "Jie Xu",
      "Shaoting Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-17T08:46:15Z",
    "pdf_url": "https://arxiv.org/pdf/2405.10630v1"
  },
  {
    "arxiv_id": "2405.10467v4",
    "entry_id": "http://arxiv.org/abs/2405.10467v4",
    "title": "Agent Design Pattern Catalogue: A Collection of Architectural Patterns for Foundation Model based Agents",
    "summary": "Foundation model-enabled generative artificial intelligence facilitates the development and implementation of agents, which can leverage distinguished reasoning and language processing capabilities to takes a proactive, autonomous role to pursue users' goals. Nevertheless, there is a lack of systematic knowledge to guide practitioners in designing the agents considering challenges of goal-seeking (including generating instrumental goals and plans), such as hallucinations inherent in foundation models, explainability of reasoning process, complex accountability, etc. To address this issue, we have performed a systematic literature review to understand the state-of-the-art foundation model-based agents and the broader ecosystem. In this paper, we present a pattern catalogue consisting of 18 architectural patterns with analyses of the context, forces, and trade-offs as the outcomes from the previous literature review. We propose a decision model for selecting the patterns. The proposed catalogue can provide holistic guidance for the effective use of patterns, and support the architecture design of foundation model-based agents by facilitating goal-seeking and plan generation.",
    "authors": [
      "Yue Liu",
      "Sin Kit Lo",
      "Qinghua Lu",
      "Liming Zhu",
      "Dehai Zhao",
      "Xiwei Xu",
      "Stefan Harrer",
      "Jon Whittle"
    ],
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-05-16T23:24:48Z",
    "pdf_url": "https://arxiv.org/pdf/2405.10467v4"
  },
  {
    "arxiv_id": "2405.10313v2",
    "entry_id": "http://arxiv.org/abs/2405.10313v2",
    "title": "How Far Are We From AGI: Are LLMs All We Need?",
    "summary": "The evolution of artificial intelligence (AI) has profoundly impacted human society, driving significant advancements in multiple sectors. AGI, distinguished by its ability to execute diverse real-world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution. While existing studies have reviewed specific advancements in AI and proposed potential paths to AGI, such as large language models (LLMs), they fall short of providing a thorough exploration of AGI's definitions, objectives, and developmental trajectories. Unlike previous survey papers, this work goes beyond summarizing LLMs by addressing key questions about our progress toward AGI and outlining the strategies essential for its realization through comprehensive analysis, in-depth discussions, and novel insights. We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions. As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmonize these factors. Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status quo, and finally giving our roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains. In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI.",
    "authors": [
      "Tao Feng",
      "Chuanyang Jin",
      "Jingyu Liu",
      "Kunlun Zhu",
      "Haoqin Tu",
      "Zirui Cheng",
      "Guanyu Lin",
      "Jiaxuan You"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-05-16T17:59:02Z",
    "pdf_url": "https://arxiv.org/pdf/2405.10313v2"
  },
  {
    "arxiv_id": "2405.13025v2",
    "entry_id": "http://arxiv.org/abs/2405.13025v2",
    "title": "A survey on fairness of large language models in e-commerce: progress, application, and challenge",
    "summary": "This survey explores the fairness of large language models (LLMs) in e-commerce, examining their progress, applications, and the challenges they face. LLMs have become pivotal in the e-commerce domain, offering innovative solutions and enhancing customer experiences. This work presents a comprehensive survey on the applications and challenges of LLMs in e-commerce. The paper begins by introducing the key principles underlying the use of LLMs in e-commerce, detailing the processes of pretraining, fine-tuning, and prompting that tailor these models to specific needs. It then explores the varied applications of LLMs in e-commerce, including product reviews, where they synthesize and analyze customer feedback; product recommendations, where they leverage consumer data to suggest relevant items; product information translation, enhancing global accessibility; and product question and answer sections, where they automate customer support. The paper critically addresses the fairness challenges in e-commerce, highlighting how biases in training data and algorithms can lead to unfair outcomes, such as reinforcing stereotypes or discriminating against certain groups. These issues not only undermine consumer trust, but also raise ethical and legal concerns. Finally, the work outlines future research directions, emphasizing the need for more equitable and transparent LLMs in e-commerce. It advocates for ongoing efforts to mitigate biases and improve the fairness of these systems, ensuring they serve diverse global markets effectively and ethically. Through this comprehensive analysis, the survey provides a holistic view of the current landscape of LLMs in e-commerce, offering insights into their potential and limitations, and guiding future endeavors in creating fairer and more inclusive e-commerce environments.",
    "authors": [
      "Qingyang Ren",
      "Zilin Jiang",
      "Jinghan Cao",
      "Sijia Li",
      "Chiqu Li",
      "Yiyang Liu",
      "Shuning Huo",
      "Tiange He",
      "Yuan Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-05-15T23:25:19Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13025v2"
  },
  {
    "arxiv_id": "2405.09713v2",
    "entry_id": "http://arxiv.org/abs/2405.09713v2",
    "title": "SOK-Bench: A Situated Video Reasoning Benchmark with Aligned Open-World Knowledge",
    "summary": "Learning commonsense reasoning from visual contexts and scenes in real-world is a crucial step toward advanced artificial intelligence. However, existing video reasoning benchmarks are still inadequate since they were mainly designed for factual or situated reasoning and rarely involve broader knowledge in the real world. Our work aims to delve deeper into reasoning evaluations, specifically within dynamic, open-world, and structured context knowledge. We propose a new benchmark (SOK-Bench), consisting of 44K questions and 10K situations with instance-level annotations depicted in the videos. The reasoning process is required to understand and apply situated knowledge and general knowledge for problem-solving. To create such a dataset, we propose an automatic and scalable generation method to generate question-answer pairs, knowledge graphs, and rationales by instructing the combinations of LLMs and MLLMs. Concretely, we first extract observable situated entities, relations, and processes from videos for situated knowledge and then extend to open-world knowledge beyond the visible content. The task generation is facilitated through multiple dialogues as iterations and subsequently corrected and refined by our designed self-promptings and demonstrations. With a corpus of both explicit situated facts and implicit commonsense, we generate associated question-answer pairs and reasoning processes, finally followed by manual reviews for quality assurance. We evaluated recent mainstream large vision-language models on the benchmark and found several insightful conclusions. For more information, please refer to our benchmark at www.bobbywu.com/SOKBench.",
    "authors": [
      "Andong Wang",
      "Bo Wu",
      "Sunli Chen",
      "Zhenfang Chen",
      "Haotian Guan",
      "Wei-Ning Lee",
      "Li Erran Li",
      "Chuang Gan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-05-15T21:55:31Z",
    "pdf_url": "https://arxiv.org/pdf/2405.09713v2"
  },
  {
    "arxiv_id": "2405.09592v1",
    "entry_id": "http://arxiv.org/abs/2405.09592v1",
    "title": "A Survey of Generative Techniques for Spatial-Temporal Data Mining",
    "summary": "This paper focuses on the integration of generative techniques into spatial-temporal data mining, considering the significant growth and diverse nature of spatial-temporal data. With the advancements in RNNs, CNNs, and other non-generative techniques, researchers have explored their application in capturing temporal and spatial dependencies within spatial-temporal data. However, the emergence of generative techniques such as LLMs, SSL, Seq2Seq and diffusion models has opened up new possibilities for enhancing spatial-temporal data mining further. The paper provides a comprehensive analysis of generative technique-based spatial-temporal methods and introduces a standardized framework specifically designed for the spatial-temporal data mining pipeline. By offering a detailed review and a novel taxonomy of spatial-temporal methodology utilizing generative techniques, the paper enables a deeper understanding of the various techniques employed in this field. Furthermore, the paper highlights promising future research directions, urging researchers to delve deeper into spatial-temporal data mining. It emphasizes the need to explore untapped opportunities and push the boundaries of knowledge to unlock new insights and improve the effectiveness and efficiency of spatial-temporal data mining. By integrating generative techniques and providing a standardized framework, the paper contributes to advancing the field and encourages researchers to explore the vast potential of generative techniques in spatial-temporal data mining.",
    "authors": [
      "Qianru Zhang",
      "Haixin Wang",
      "Cheng Long",
      "Liangcai Su",
      "Xingwei He",
      "Jianlong Chang",
      "Tailin Wu",
      "Hongzhi Yin",
      "Siu-Ming Yiu",
      "Qi Tian",
      "Christian S. Jensen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "published": "2024-05-15T12:07:43Z",
    "pdf_url": "https://arxiv.org/pdf/2405.09592v1"
  },
  {
    "arxiv_id": "2405.09589v4",
    "entry_id": "http://arxiv.org/abs/2405.09589v4",
    "title": "A Comprehensive Survey of Hallucination in Large Language, Image, Video and Audio Foundation Models",
    "summary": "The rapid advancement of foundation models (FMs) across language, image, audio, and video domains has shown remarkable capabilities in diverse tasks. However, the proliferation of FMs brings forth a critical challenge: the potential to generate hallucinated outputs, particularly in high-stakes applications. The tendency of foundation models to produce hallucinated content arguably represents the biggest hindrance to their widespread adoption in real-world scenarios, especially in domains where reliability and accuracy are paramount. This survey paper presents a comprehensive overview of recent developments that aim to identify and mitigate the problem of hallucination in FMs, spanning text, image, video, and audio modalities. By synthesizing recent advancements in detecting and mitigating hallucination across various modalities, the paper aims to provide valuable insights for researchers, developers, and practitioners. Essentially, it establishes a clear framework encompassing definition, taxonomy, and detection strategies for addressing hallucination in multimodal foundation models, laying the foundation for future research in this pivotal area.",
    "authors": [
      "Pranab Sahoo",
      "Prabhash Meharia",
      "Akash Ghosh",
      "Sriparna Saha",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2024-05-15T10:16:25Z",
    "pdf_url": "https://arxiv.org/pdf/2405.09589v4"
  },
  {
    "arxiv_id": "2405.13019v2",
    "entry_id": "http://arxiv.org/abs/2405.13019v2",
    "title": "A Comprehensive Survey of Accelerated Generation Techniques in Large Language Models",
    "summary": "Despite the crucial importance of accelerating text generation in large language models (LLMs) for efficiently producing content, the sequential nature of this process often leads to high inference latency, posing challenges for real-time applications. Various techniques have been proposed and developed to address these challenges and improve efficiency. This paper presents a comprehensive survey of accelerated generation techniques in autoregressive language models, aiming to understand the state-of-the-art methods and their applications. We categorize these techniques into several key areas: speculative decoding, early exiting mechanisms, and non-autoregressive methods. We discuss each category's underlying principles, advantages, limitations, and recent advancements. Through this survey, we aim to offer insights into the current landscape of techniques in LLMs and provide guidance for future research directions in this critical area of natural language processing.",
    "authors": [
      "Mahsa Khoshnoodi",
      "Vinija Jain",
      "Mingye Gao",
      "Malavika Srikanth",
      "Aman Chadha"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-15T07:36:56Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13019v2"
  },
  {
    "arxiv_id": "2405.08120v1",
    "entry_id": "http://arxiv.org/abs/2405.08120v1",
    "title": "From Questions to Insightful Answers: Building an Informed Chatbot for University Resources",
    "summary": "This paper presents BARKPLUG V.2, a Large Language Model (LLM)-based chatbot system built using Retrieval Augmented Generation (RAG) pipelines to enhance the user experience and access to information within academic settings.The objective of BARKPLUG V.2 is to provide information to users about various campus resources, including academic departments, programs, campus facilities, and student resources at a university setting in an interactive fashion. Our system leverages university data as an external data corpus and ingests it into our RAG pipelines for domain-specific question-answering tasks. We evaluate the effectiveness of our system in generating accurate and pertinent responses for Mississippi State University, as a case study, using quantitative measures, employing frameworks such as Retrieval Augmented Generation Assessment(RAGAS). Furthermore, we evaluate the usability of this system via subjective satisfaction surveys using the System Usability Scale (SUS). Our system demonstrates impressive quantitative performance, with a mean RAGAS score of 0.96, and experience, as validated by usability assessments.",
    "authors": [
      "Subash Neupane",
      "Elias Hossain",
      "Jason Keith",
      "Himanshu Tripathi",
      "Farbod Ghiasi",
      "Noorbakhsh Amiri Golilarz",
      "Amin Amirlatifi",
      "Sudip Mittal",
      "Shahram Rahimi"
    ],
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "published": "2024-05-13T19:05:42Z",
    "pdf_url": "https://arxiv.org/pdf/2405.08120v1"
  },
  {
    "arxiv_id": "2405.07468v1",
    "entry_id": "http://arxiv.org/abs/2405.07468v1",
    "title": "Evaluating large language models in medical applications: a survey",
    "summary": "Large language models (LLMs) have emerged as powerful tools with transformative potential across numerous domains, including healthcare and medicine. In the medical domain, LLMs hold promise for tasks ranging from clinical decision support to patient education. However, evaluating the performance of LLMs in medical contexts presents unique challenges due to the complex and critical nature of medical information. This paper provides a comprehensive overview of the landscape of medical LLM evaluation, synthesizing insights from existing studies and highlighting evaluation data sources, task scenarios, and evaluation methods. Additionally, it identifies key challenges and opportunities in medical LLM evaluation, emphasizing the need for continued research and innovation to ensure the responsible integration of LLMs into clinical practice.",
    "authors": [
      "Xiaolan Chen",
      "Jiayang Xiang",
      "Shanfu Lu",
      "Yexin Liu",
      "Mingguang He",
      "Danli Shi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-13T05:08:33Z",
    "pdf_url": "https://arxiv.org/pdf/2405.07468v1"
  },
  {
    "arxiv_id": "2405.07278v2",
    "entry_id": "http://arxiv.org/abs/2405.07278v2",
    "title": "Human-interpretable clustering of short-text using large language models",
    "summary": "Clustering short text is a difficult problem, due to the low word co-occurrence between short text documents. This work shows that large language models (LLMs) can overcome the limitations of traditional clustering approaches by generating embeddings that capture the semantic nuances of short text. In this study clusters are found in the embedding space using Gaussian Mixture Modelling (GMM). The resulting clusters are found to be more distinctive and more human-interpretable than clusters produced using the popular methods of doc2vec and Latent Dirichlet Allocation (LDA). The success of the clustering approach is quantified using human reviewers and through the use of a generative LLM. The generative LLM shows good agreement with the human reviewers, and is suggested as a means to bridge the `validation gap' which often exists between cluster production and cluster interpretation. The comparison between LLM-coding and human-coding reveals intrinsic biases in each, challenging the conventional reliance on human coding as the definitive standard for cluster validation.",
    "authors": [
      "Justin K. Miller",
      "Tristram J. Alexander"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-05-12T12:55:40Z",
    "pdf_url": "https://arxiv.org/pdf/2405.07278v2"
  },
  {
    "arxiv_id": "2405.07248v1",
    "entry_id": "http://arxiv.org/abs/2405.07248v1",
    "title": "Limited Ability of LLMs to Simulate Human Psychological Behaviours: a Psychometric Analysis",
    "summary": "The humanlike responses of large language models (LLMs) have prompted social scientists to investigate whether LLMs can be used to simulate human participants in experiments, opinion polls and surveys. Of central interest in this line of research has been mapping out the psychological profiles of LLMs by prompting them to respond to standardized questionnaires. The conflicting findings of this research are unsurprising given that mapping out underlying, or latent, traits from LLMs' text responses to questionnaires is no easy task. To address this, we use psychometrics, the science of psychological measurement. In this study, we prompt OpenAI's flagship models, GPT-3.5 and GPT-4, to assume different personas and respond to a range of standardized measures of personality constructs. We used two kinds of persona descriptions: either generic (four or five random person descriptions) or specific (mostly demographics of actual humans from a large-scale human dataset). We found that the responses from GPT-4, but not GPT-3.5, using generic persona descriptions show promising, albeit not perfect, psychometric properties, similar to human norms, but the data from both LLMs when using specific demographic profiles, show poor psychometrics properties. We conclude that, currently, when LLMs are asked to simulate silicon personas, their responses are poor signals of potentially underlying latent traits. Thus, our work casts doubt on LLMs' ability to simulate individual-level human behaviour across multiple-choice question answering tasks.",
    "authors": [
      "Nikolay B Petrov",
      "Gregory Serapio-García",
      "Jason Rentfrow"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2024-05-12T10:52:15Z",
    "pdf_url": "https://arxiv.org/pdf/2405.07248v1"
  },
  {
    "arxiv_id": "2405.07195v1",
    "entry_id": "http://arxiv.org/abs/2405.07195v1",
    "title": "InsightNet: Structured Insight Mining from Customer Feedback",
    "summary": "We propose InsightNet, a novel approach for the automated extraction of structured insights from customer reviews. Our end-to-end machine learning framework is designed to overcome the limitations of current solutions, including the absence of structure for identified topics, non-standard aspect names, and lack of abundant training data. The proposed solution builds a semi-supervised multi-level taxonomy from raw reviews, a semantic similarity heuristic approach to generate labelled data and employs a multi-task insight extraction architecture by fine-tuning an LLM. InsightNet identifies granular actionable topics with customer sentiments and verbatim for each topic. Evaluations on real-world customer review data show that InsightNet performs better than existing solutions in terms of structure, hierarchy and completeness. We empirically demonstrate that InsightNet outperforms the current state-of-the-art methods in multi-label topic classification, achieving an F1 score of 0.85, which is an improvement of 11% F1-score over the previous best results. Additionally, InsightNet generalises well for unseen aspects and suggests new topics to be added to the taxonomy.",
    "authors": [
      "Sandeep Sricharan Mukku",
      "Manan Soni",
      "Jitenkumar Rana",
      "Chetan Aggarwal",
      "Promod Yenigalla",
      "Rashmi Patange",
      "Shyam Mohan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-12T07:40:12Z",
    "pdf_url": "https://arxiv.org/pdf/2405.07195v1"
  },
  {
    "arxiv_id": "2405.13001v1",
    "entry_id": "http://arxiv.org/abs/2405.13001v1",
    "title": "Large Language Models for Education: A Survey",
    "summary": "Artificial intelligence (AI) has a profound impact on traditional education. In recent years, large language models (LLMs) have been increasingly used in various applications such as natural language processing, computer vision, speech recognition, and autonomous driving. LLMs have also been applied in many fields, including recommendation, finance, government, education, legal affairs, and finance. As powerful auxiliary tools, LLMs incorporate various technologies such as deep learning, pre-training, fine-tuning, and reinforcement learning. The use of LLMs for smart education (LLMEdu) has been a significant strategic direction for countries worldwide. While LLMs have shown great promise in improving teaching quality, changing education models, and modifying teacher roles, the technologies are still facing several challenges. In this paper, we conduct a systematic review of LLMEdu, focusing on current technologies, challenges, and future developments. We first summarize the current state of LLMEdu and then introduce the characteristics of LLMs and education, as well as the benefits of integrating LLMs into education. We also review the process of integrating LLMs into the education industry, as well as the introduction of related technologies. Finally, we discuss the challenges and problems faced by LLMEdu, as well as prospects for future optimization of LLMEdu.",
    "authors": [
      "Hanyi Xu",
      "Wensheng Gan",
      "Zhenlian Qi",
      "Jiayang Wu",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-05-12T01:50:01Z",
    "pdf_url": "https://arxiv.org/pdf/2405.13001v1"
  },
  {
    "arxiv_id": "2406.16891v1",
    "entry_id": "http://arxiv.org/abs/2406.16891v1",
    "title": "Survey on Reasoning Capabilities and Accessibility of Large Language Models Using Biology-related Questions",
    "summary": "This research paper discusses the advances made in the past decade in biomedicine and Large Language Models. To understand how the advances have been made hand-in-hand with one another, the paper also discusses the integration of Natural Language Processing techniques and tools into biomedicine. Finally, the goal of this paper is to expand on a survey conducted last year (2023) by introducing a new list of questions and prompts for the top two language models. Through this survey, this paper seeks to quantify the improvement made in the reasoning abilities in LLMs and to what extent those improvements are felt by the average user. Additionally, this paper seeks to extend research on retrieval of biological literature by prompting the LLM to answer open-ended questions in great depth.",
    "authors": [
      "Michael Ackerman"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-11T20:25:40Z",
    "pdf_url": "https://arxiv.org/pdf/2406.16891v1"
  },
  {
    "arxiv_id": "2405.07076v2",
    "entry_id": "http://arxiv.org/abs/2405.07076v2",
    "title": "Integrating Emotional and Linguistic Models for Ethical Compliance in Large Language Models",
    "summary": "This research develops advanced methodologies for Large Language Models (LLMs) to better manage linguistic behaviors related to emotions and ethics. We introduce DIKE, an adversarial framework that enhances the LLMs' ability to internalize and reflect global human values, adapting to varied cultural contexts to promote transparency and trust among users. The methodology involves detailed modeling of emotions, classification of linguistic behaviors, and implementation of ethical guardrails. Our innovative approaches include mapping emotions and behaviors using self-supervised learning techniques, refining these guardrails through adversarial reviews, and systematically adjusting outputs to ensure ethical alignment. This framework establishes a robust foundation for AI systems to operate with ethical integrity and cultural sensitivity, paving the way for more responsible and context-aware AI interactions.",
    "authors": [
      "Edward Y. Chang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-11T19:26:00Z",
    "pdf_url": "https://arxiv.org/pdf/2405.07076v2"
  },
  {
    "arxiv_id": "2405.07061v1",
    "entry_id": "http://arxiv.org/abs/2405.07061v1",
    "title": "LLMs and the Future of Chip Design: Unveiling Security Risks and Building Trust",
    "summary": "Chip design is about to be revolutionized by the integration of large language, multimodal, and circuit models (collectively LxMs). While exploring this exciting frontier with tremendous potential, the community must also carefully consider the related security risks and the need for building trust into using LxMs for chip design. First, we review the recent surge of using LxMs for chip design in general. We cover state-of-the-art works for the automation of hardware description language code generation and for scripting and guidance of essential but cumbersome tasks for electronic design automation tools, e.g., design-space exploration, tuning, or designer training. Second, we raise and provide initial answers to novel research questions on critical issues for security and trustworthiness of LxM-powered chip design from both the attack and defense perspectives.",
    "authors": [
      "Zeng Wang",
      "Lilas Alrahis",
      "Likhitha Mankali",
      "Johann Knechtel",
      "Ozgur Sinanoglu"
    ],
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.CR"
    ],
    "published": "2024-05-11T17:27:41Z",
    "pdf_url": "https://arxiv.org/pdf/2405.07061v1"
  },
  {
    "arxiv_id": "2405.08015v1",
    "entry_id": "http://arxiv.org/abs/2405.08015v1",
    "title": "A Methodology-Oriented Study of Catastrophic Forgetting in Incremental Deep Neural Networks",
    "summary": "Human being and different species of animals having the skills to gather, transferring knowledge, processing, fine-tune and generating information throughout their lifetime. The ability of learning throughout their lifespan is referred as continuous learning which is using neurocognition mechanism. Consequently, in real world computational system of incremental learning autonomous agents also needs such continuous learning mechanism which provide retrieval of information and long-term memory consolidation. However, the main challenge in artificial intelligence is that the incremental learning of the autonomous agent when new data confronted. In such scenarios, the main concern is catastrophic forgetting(CF), i.e., while learning the sequentially, neural network underfits the old data when it confronted with new data. To tackle this CF problem many numerous studied have been proposed, however it is very difficult to compare their performance due to dissimilarity in their evaluation mechanism. Here we focus on the comparison of all algorithms which are having similar type of evaluation mechanism. Here we are comparing three types of incremental learning methods: (1) Exemplar based methods, (2) Memory based methods, and (3) Network based method. In this survey paper, methodology oriented study for catastrophic forgetting in incremental deep neural network is addressed. Furthermore, it contains the mathematical overview of impact-full methods which can be help researchers to deal with CF.",
    "authors": [
      "Ashutosh Kumar",
      "Sonali Agarwal",
      "D Jude Hemanth"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-05-11T05:10:07Z",
    "pdf_url": "https://arxiv.org/pdf/2405.08015v1"
  },
  {
    "arxiv_id": "2405.06909v1",
    "entry_id": "http://arxiv.org/abs/2405.06909v1",
    "title": "Fairness in Reinforcement Learning: A Survey",
    "summary": "While our understanding of fairness in machine learning has significantly progressed, our understanding of fairness in reinforcement learning (RL) remains nascent. Most of the attention has been on fairness in one-shot classification tasks; however, real-world, RL-enabled systems (e.g., autonomous vehicles) are much more complicated in that agents operate in dynamic environments over a long period of time. To ensure the responsible development and deployment of these systems, we must better understand fairness in RL. In this paper, we survey the literature to provide the most up-to-date snapshot of the frontiers of fairness in RL. We start by reviewing where fairness considerations can arise in RL, then discuss the various definitions of fairness in RL that have been put forth thus far. We continue to highlight the methodologies researchers used to implement fairness in single- and multi-agent RL systems before showcasing the distinct application domains that fair RL has been investigated in. Finally, we critically examine gaps in the literature, such as understanding fairness in the context of RLHF, that still need to be addressed in future work to truly operationalize fair RL in real-world systems.",
    "authors": [
      "Anka Reuel",
      "Devin Ma"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-05-11T04:36:46Z",
    "pdf_url": "https://arxiv.org/pdf/2405.06909v1"
  },
  {
    "arxiv_id": "2405.08011v3",
    "entry_id": "http://arxiv.org/abs/2405.08011v3",
    "title": "A Survey of Large Language Models for Graphs",
    "summary": "Graphs are an essential data structure utilized to represent relationships in real-world scenarios. Prior research has established that Graph Neural Networks (GNNs) deliver impressive outcomes in graph-centric tasks, such as link prediction and node classification. Despite these advancements, challenges like data sparsity and limited generalization capabilities continue to persist. Recently, Large Language Models (LLMs) have gained attention in natural language processing. They excel in language comprehension and summarization. Integrating LLMs with graph learning techniques has attracted interest as a way to enhance performance in graph learning tasks. In this survey, we conduct an in-depth review of the latest state-of-the-art LLMs applied in graph learning and introduce a novel taxonomy to categorize existing methods based on their framework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key methodologies within each category. We explore the strengths and limitations of each framework, and emphasize potential avenues for future research, including overcoming current integration challenges between LLMs and graph learning techniques, and venturing into new application areas. This survey aims to serve as a valuable resource for researchers and practitioners eager to leverage large language models in graph learning, and to inspire continued progress in this dynamic field. We consistently maintain the related open-source materials at \\url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.",
    "authors": [
      "Xubin Ren",
      "Jiabin Tang",
      "Dawei Yin",
      "Nitesh Chawla",
      "Chao Huang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-05-10T18:05:37Z",
    "pdf_url": "https://arxiv.org/pdf/2405.08011v3"
  },
  {
    "arxiv_id": "2405.06211v3",
    "entry_id": "http://arxiv.org/abs/2405.06211v3",
    "title": "A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models",
    "summary": "As one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations, such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the generation quality of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: architectures, training strategies, and applications. As the preliminary knowledge, we briefly introduce the foundations and recent advances of LLMs. Then, to illustrate the practical significance of RAG for LLMs, we systematically review mainstream relevant work by their architectures, training strategies, and application areas, detailing specifically the challenges of each and the corresponding capabilities of RA-LLMs. Finally, to deliver deeper insights, we discuss current limitations and several promising directions for future research. Updated information about this survey can be found at https://advanced-recommender-systems.github.io/RAG-Meets-LLMs/",
    "authors": [
      "Wenqi Fan",
      "Yujuan Ding",
      "Liangbo Ning",
      "Shijie Wang",
      "Hengyun Li",
      "Dawei Yin",
      "Tat-Seng Chua",
      "Qing Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-05-10T02:48:45Z",
    "pdf_url": "https://arxiv.org/pdf/2405.06211v3"
  },
  {
    "arxiv_id": "2405.06058v2",
    "entry_id": "http://arxiv.org/abs/2405.06058v2",
    "title": "Large Language Models Show Human-like Social Desirability Biases in Survey Responses",
    "summary": "As Large Language Models (LLMs) become widely used to model and simulate human behavior, understanding their biases becomes critical. We developed an experimental framework using Big Five personality surveys and uncovered a previously undetected social desirability bias in a wide range of LLMs. By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated. When personality evaluation is inferred, LLMs skew their scores towards the desirable ends of trait dimensions (i.e., increased extraversion, decreased neuroticism, etc). This bias exists in all tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent models, with GPT-4's survey responses changing by 1.20 (human) standard deviations and Llama 3's by 0.98 standard deviations-very large effects. This bias is robust to randomization of question order and paraphrasing. Reverse-coding all the questions decreases bias levels but does not eliminate them, suggesting that this effect cannot be attributed to acquiescence bias. Our findings reveal an emergent social desirability bias and suggest constraints on profiling LLMs with psychometric tests and on using LLMs as proxies for human participants.",
    "authors": [
      "Aadesh Salecha",
      "Molly E. Ireland",
      "Shashanka Subrahmanya",
      "João Sedoc",
      "Lyle H. Ungar",
      "Johannes C. Eichstaedt"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2024-05-09T19:02:53Z",
    "pdf_url": "https://arxiv.org/pdf/2405.06058v2"
  },
  {
    "arxiv_id": "2405.05930v2",
    "entry_id": "http://arxiv.org/abs/2405.05930v2",
    "title": "Trustworthy AI-Generative Content for Intelligent Network Service: Robustness, Security, and Fairness",
    "summary": "AI-generated content (AIGC) models, represented by large language models (LLM), have revolutionized content creation. High-speed next-generation communication technology is an ideal platform for providing powerful AIGC network services. At the same time, advanced AIGC techniques can also make future network services more intelligent, especially various online content generation services. However, the significant untrustworthiness concerns of current AIGC models, such as robustness, security, and fairness, greatly affect the credibility of intelligent network services, especially in ensuring secure AIGC services. This paper proposes TrustGAIN, a trustworthy AIGC framework that incorporates robust, secure, and fair network services. We first discuss the robustness to adversarial attacks faced by AIGC models in network systems and the corresponding protection issues. Subsequently, we emphasize the importance of avoiding unsafe and illegal services and ensuring the fairness of the AIGC network services. Then as a case study, we propose a novel sentiment analysis-based detection method to guide the robust detection of unsafe content in network services. We conduct our experiments on fake news, malicious code, and unsafe review datasets to represent LLM application scenarios. Our results indicate that TrustGAIN is an exploration of future networks that can support trustworthy AIGC network services.",
    "authors": [
      "Siyuan Li",
      "Xi Lin",
      "Yaju Liu",
      "Xiang Chen",
      "Jianhua Li"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.NI"
    ],
    "published": "2024-05-09T17:16:20Z",
    "pdf_url": "https://arxiv.org/pdf/2405.05930v2"
  },
  {
    "arxiv_id": "2405.04760v5",
    "entry_id": "http://arxiv.org/abs/2405.04760v5",
    "title": "Large Language Models for Cyber Security: A Systematic Literature Review",
    "summary": "The rapid advancement of Large Language Models (LLMs) has opened up new opportunities for leveraging artificial intelligence in a variety of application domains, including cybersecurity. As the volume and sophistication of cyber threats continue to grow, there is an increasing need for intelligent systems that can automatically detect vulnerabilities, analyze malware, and respond to attacks. In this survey, we conduct a comprehensive review of the literature on the application of LLMs in cybersecurity~(LLM4Security). By comprehensively collecting over 40K relevant papers and systematically analyzing 185 papers from top security and software engineering venues, we aim to provide a holistic view of how LLMs are being used to solve diverse problems across the cybersecurity domain. Through our analysis, we identify several key findings. First, we observe that LLMs are being applied to an expanding range of cybersecurity tasks, including vulnerability detection, malware analysis, and network intrusion detection. Second, we analyze application trends of different LLM architectures (such as encoder-only, encoder-decoder, and decoder-only) across security domains. Third, we identify increasingly sophisticated techniques for adapting LLMs to cybersecurity, such as advanced fine-tuning, prompt engineering, and external augmentation strategies. A significant emerging trend is the use of LLM-based autonomous agents, which represent a paradigm shift from single-task execution to orchestrating complex, multi-step security workflows.",
    "authors": [
      "Hanxiang Xu",
      "Shenao Wang",
      "Ningke Li",
      "Kailong Wang",
      "Yanjie Zhao",
      "Kai Chen",
      "Ting Yu",
      "Yang Liu",
      "Haoyu Wang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-05-08T02:09:17Z",
    "pdf_url": "https://arxiv.org/pdf/2405.04760v5"
  },
  {
    "arxiv_id": "2405.04294v1",
    "entry_id": "http://arxiv.org/abs/2405.04294v1",
    "title": "Enhancing the Efficiency and Accuracy of Underlying Asset Reviews in Structured Finance: The Application of Multi-agent Framework",
    "summary": "Structured finance, which involves restructuring diverse assets into securities like MBS, ABS, and CDOs, enhances capital market efficiency but presents significant due diligence challenges. This study explores the integration of artificial intelligence (AI) with traditional asset review processes to improve efficiency and accuracy in structured finance. Using both open-sourced and close-sourced large language models (LLMs), we demonstrate that AI can automate the verification of information between loan applications and bank statements effectively. While close-sourced models such as GPT-4 show superior performance, open-sourced models like LLAMA3 offer a cost-effective alternative. Dual-agent systems further increase accuracy, though this comes with higher operational costs. This research highlights AI's potential to minimize manual errors and streamline due diligence, suggesting a broader application of AI in financial document analysis and risk management.",
    "authors": [
      "Xiangpeng Wan",
      "Haicheng Deng",
      "Kai Zou",
      "Shiqi Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-05-07T13:09:49Z",
    "pdf_url": "https://arxiv.org/pdf/2405.04294v1"
  },
  {
    "arxiv_id": "2405.03988v3",
    "entry_id": "http://arxiv.org/abs/2405.03988v3",
    "title": "LEARN: Knowledge Adaptation from Large Language Model to Recommendation for Practical Industrial Application",
    "summary": "Contemporary recommendation systems predominantly rely on ID embedding to capture latent associations among users and items. However, this approach overlooks the wealth of semantic information embedded within textual descriptions of items, leading to suboptimal performance and poor generalizations. Leveraging the capability of large language models to comprehend and reason about textual content presents a promising avenue for advancing recommendation systems. To achieve this, we propose an Llm-driven knowlEdge Adaptive RecommeNdation (LEARN) framework that synergizes open-world knowledge with collaborative knowledge. We address computational complexity concerns by utilizing pretrained LLMs as item encoders and freezing LLM parameters to avoid catastrophic forgetting and preserve open-world knowledge. To bridge the gap between the open-world and collaborative domains, we design a twin-tower structure supervised by the recommendation task and tailored for practical industrial application. Through experiments on the real large-scale industrial dataset and online A/B tests, we demonstrate the efficacy of our approach in industry application. We also achieve state-of-the-art performance on six Amazon Review datasets to verify the superiority of our method.",
    "authors": [
      "Jian Jia",
      "Yipei Wang",
      "Yan Li",
      "Honggang Chen",
      "Xuehan Bai",
      "Zhaocheng Liu",
      "Jian Liang",
      "Quan Chen",
      "Han Li",
      "Peng Jiang",
      "Kun Gai"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-05-07T04:00:30Z",
    "pdf_url": "https://arxiv.org/pdf/2405.03988v3"
  },
  {
    "arxiv_id": "2405.03845v1",
    "entry_id": "http://arxiv.org/abs/2405.03845v1",
    "title": "Self-Improving Customer Review Response Generation Based on LLMs",
    "summary": "Previous studies have demonstrated that proactive interaction with user reviews has a positive impact on the perception of app users and encourages them to submit revised ratings. Nevertheless, developers encounter challenges in managing a high volume of reviews, particularly in the case of popular apps with a substantial influx of daily reviews. Consequently, there is a demand for automated solutions aimed at streamlining the process of responding to user reviews. To address this, we have developed a new system for generating automatic responses by leveraging user-contributed documents with the help of retrieval-augmented generation (RAG) and advanced Large Language Models (LLMs). Our solution, named SCRABLE, represents an adaptive customer review response automation that enhances itself with self-optimizing prompts and a judging mechanism based on LLMs. Additionally, we introduce an automatic scoring mechanism that mimics the role of a human evaluator to assess the quality of responses generated in customer review domains. Extensive experiments and analyses conducted on real-world datasets reveal that our method is effective in producing high-quality responses, yielding improvement of more than 8.5% compared to the baseline. Further validation through manual examination of the generated responses underscores the efficacy our proposed system.",
    "authors": [
      "Guy Azov",
      "Tatiana Pelc",
      "Adi Fledel Alon",
      "Gila Kamhi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-06T20:50:17Z",
    "pdf_url": "https://arxiv.org/pdf/2405.03845v1"
  },
  {
    "arxiv_id": "2405.03644v2",
    "entry_id": "http://arxiv.org/abs/2405.03644v2",
    "title": "When LLMs Meet Cybersecurity: A Systematic Literature Review",
    "summary": "The rapid development of large language models (LLMs) has opened new avenues across various fields, including cybersecurity, which faces an evolving threat landscape and demand for innovative technologies. Despite initial explorations into the application of LLMs in cybersecurity, there is a lack of a comprehensive overview of this research area. This paper addresses this gap by providing a systematic literature review, covering the analysis of over 300 works, encompassing 25 LLMs and more than 10 downstream scenarios. Our comprehensive overview addresses three key research questions: the construction of cybersecurity-oriented LLMs, the application of LLMs to various cybersecurity tasks, the challenges and further research in this area. This study aims to shed light on the extensive potential of LLMs in enhancing cybersecurity practices and serve as a valuable resource for applying LLMs in this field. We also maintain and regularly update a list of practical guides on LLMs for cybersecurity at https://github.com/tmylla/Awesome-LLM4Cybersecurity.",
    "authors": [
      "Jie Zhang",
      "Haoyu Bu",
      "Hui Wen",
      "Yongji Liu",
      "Haiqiang Fei",
      "Rongrong Xi",
      "Lun Li",
      "Yun Yang",
      "Hongsong Zhu",
      "Dan Meng"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-05-06T17:07:28Z",
    "pdf_url": "https://arxiv.org/pdf/2405.03644v2"
  },
  {
    "arxiv_id": "2405.02828v1",
    "entry_id": "http://arxiv.org/abs/2405.02828v1",
    "title": "Trojans in Large Language Models of Code: A Critical Review through a Trigger-Based Taxonomy",
    "summary": "Large language models (LLMs) have provided a lot of exciting new capabilities in software development. However, the opaque nature of these models makes them difficult to reason about and inspect. Their opacity gives rise to potential security risks, as adversaries can train and deploy compromised models to disrupt the software development process in the victims' organization.\n  This work presents an overview of the current state-of-the-art trojan attacks on large language models of code, with a focus on triggers -- the main design point of trojans -- with the aid of a novel unifying trigger taxonomy framework. We also aim to provide a uniform definition of the fundamental concepts in the area of trojans in Code LLMs. Finally, we draw implications of findings on how code models learn on trigger design.",
    "authors": [
      "Aftab Hussain",
      "Md Rafiqul Islam Rabin",
      "Toufique Ahmed",
      "Bowen Xu",
      "Premkumar Devanbu",
      "Mohammad Amin Alipour"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2024-05-05T06:43:52Z",
    "pdf_url": "https://arxiv.org/pdf/2405.02828v1"
  },
  {
    "arxiv_id": "2405.02559v2",
    "entry_id": "http://arxiv.org/abs/2405.02559v2",
    "title": "A Framework for Human Evaluation of Large Language Models in Healthcare Derived from Literature Review",
    "summary": "With generative artificial intelligence (AI), particularly large language models (LLMs), continuing to make inroads in healthcare, it is critical to supplement traditional automated evaluations with human evaluations. Understanding and evaluating the output of LLMs is essential to assuring safety, reliability, and effectiveness. However, human evaluation's cumbersome, time-consuming, and non-standardized nature presents significant obstacles to comprehensive evaluation and widespread adoption of LLMs in practice. This study reviews existing literature on human evaluation methodologies for LLMs in healthcare. We highlight a notable need for a standardized and consistent human evaluation approach. Our extensive literature search, adhering to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines, includes publications from January 2018 to February 2024. The review examines the human evaluation of LLMs across various medical specialties, addressing factors such as evaluation dimensions, sample types and sizes, selection, and recruitment of evaluators, frameworks and metrics, evaluation process, and statistical analysis type. Drawing on the diverse evaluation strategies employed in these studies, we propose a comprehensive and practical framework for human evaluation of LLMs: QUEST: Quality of Information, Understanding and Reasoning, Expression Style and Persona, Safety and Harm, and Trust and Confidence. This framework aims to improve the reliability, generalizability, and applicability of human evaluation of LLMs in different healthcare applications by defining clear evaluation dimensions and offering detailed guidelines.",
    "authors": [
      "Thomas Yu Chow Tam",
      "Sonish Sivarajkumar",
      "Sumit Kapoor",
      "Alisa V Stolyar",
      "Katelyn Polanska",
      "Karleigh R McCarthy",
      "Hunter Osterhoudt",
      "Xizhi Wu",
      "Shyam Visweswaran",
      "Sunyang Fu",
      "Piyush Mathur",
      "Giovanni E. Cacciamani",
      "Cong Sun",
      "Yifan Peng",
      "Yanshan Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-05-04T04:16:07Z",
    "pdf_url": "https://arxiv.org/pdf/2405.02559v2"
  },
  {
    "arxiv_id": "2405.02198v2",
    "entry_id": "http://arxiv.org/abs/2405.02198v2",
    "title": "The Cambridge RoboMaster: An Agile Multi-Robot Research Platform",
    "summary": "Compact robotic platforms with powerful compute and actuation capabilities are key enablers for practical, real-world deployments of multi-agent research. This article introduces a tightly integrated hardware, control, and simulation software stack on a fleet of holonomic ground robot platforms designed with this motivation. Our robots, a fleet of customised DJI Robomaster S1 vehicles, offer a balance between small robots that do not possess sufficient compute or actuation capabilities and larger robots that are unsuitable for indoor multi-robot tests. They run a modular ROS2-based optimal estimation and control stack for full onboard autonomy, contain ad-hoc peer-to-peer communication infrastructure, and can zero-shot run multi-agent reinforcement learning (MARL) policies trained in our vectorized multi-agent simulation framework. We present an in-depth review of other platforms currently available, showcase new experimental validation of our system's capabilities, and introduce case studies that highlight the versatility and reliability of our system as a testbed for a wide range of research demonstrations. Our system as well as supplementary material is available online. https://proroklab.github.io/cambridge-robomaster",
    "authors": [
      "Jan Blumenkamp",
      "Ajay Shankar",
      "Matteo Bettini",
      "Joshua Bird",
      "Amanda Prorok"
    ],
    "categories": [
      "cs.RO",
      "cs.MA",
      "eess.SY"
    ],
    "published": "2024-05-03T15:54:20Z",
    "pdf_url": "https://arxiv.org/pdf/2405.02198v2"
  },
  {
    "arxiv_id": "2405.02105v1",
    "entry_id": "http://arxiv.org/abs/2405.02105v1",
    "title": "Evaluating Large Language Models for Structured Science Summarization in the Open Research Knowledge Graph",
    "summary": "Structured science summaries or research contributions using properties or dimensions beyond traditional keywords enhances science findability. Current methods, such as those used by the Open Research Knowledge Graph (ORKG), involve manually curating properties to describe research papers' contributions in a structured manner, but this is labor-intensive and inconsistent between the domain expert human curators. We propose using Large Language Models (LLMs) to automatically suggest these properties. However, it's essential to assess the readiness of LLMs like GPT-3.5, Llama 2, and Mistral for this task before application. Our study performs a comprehensive comparative analysis between ORKG's manually curated properties and those generated by the aforementioned state-of-the-art LLMs. We evaluate LLM performance through four unique perspectives: semantic alignment and deviation with ORKG properties, fine-grained properties mapping accuracy, SciNCL embeddings-based cosine similarity, and expert surveys comparing manual annotations with LLM outputs. These evaluations occur within a multidisciplinary science setting. Overall, LLMs show potential as recommendation systems for structuring science, but further finetuning is recommended to improve their alignment with scientific tasks and mimicry of human expertise.",
    "authors": [
      "Vladyslav Nechakhin",
      "Jennifer D'Souza",
      "Steffen Eger"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IT"
    ],
    "published": "2024-05-03T14:03:04Z",
    "pdf_url": "https://arxiv.org/pdf/2405.02105v1"
  },
  {
    "arxiv_id": "2405.01976v1",
    "entry_id": "http://arxiv.org/abs/2405.01976v1",
    "title": "Conformal Prediction for Natural Language Processing: A Survey",
    "summary": "The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its model-agnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.",
    "authors": [
      "Margarida M. Campos",
      "António Farinhas",
      "Chrysoula Zerva",
      "Mário A. T. Figueiredo",
      "André F. T. Martins"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-05-03T10:00:45Z",
    "pdf_url": "https://arxiv.org/pdf/2405.01976v1"
  },
  {
    "arxiv_id": "2405.01964v3",
    "entry_id": "http://arxiv.org/abs/2405.01964v3",
    "title": "Position: Understanding LLMs Requires More Than Statistical Generalization",
    "summary": "The last decade has seen blossoming research in deep learning theory attempting to answer, \"Why does deep learning generalize?\" A powerful shift in perspective precipitated this progress: the study of overparametrized models in the interpolation regime. In this paper, we argue that another perspective shift is due, since some of the desirable qualities of LLMs are not a consequence of good statistical generalization and require a separate theoretical explanation. Our core argument relies on the observation that AR probabilistic models are inherently non-identifiable: models zero or near-zero KL divergence apart -- thus, equivalent test loss -- can exhibit markedly different behaviors. We support our position with mathematical examples and empirical observations, illustrating why non-identifiability has practical relevance through three case studies: (1) the non-identifiability of zero-shot rule extrapolation; (2) the approximate non-identifiability of in-context learning; and (3) the non-identifiability of fine-tunability. We review promising research directions focusing on LLM-relevant generalization measures, transferability, and inductive biases.",
    "authors": [
      "Patrik Reizinger",
      "Szilvia Ujváry",
      "Anna Mészáros",
      "Anna Kerekes",
      "Wieland Brendel",
      "Ferenc Huszár"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2024-05-03T09:41:39Z",
    "pdf_url": "https://arxiv.org/pdf/2405.01964v3"
  },
  {
    "arxiv_id": "2405.01883v1",
    "entry_id": "http://arxiv.org/abs/2405.01883v1",
    "title": "DALLMi: Domain Adaption for LLM-based Multi-label Classifier",
    "summary": "Large language models (LLMs) increasingly serve as the backbone for classifying text associated with distinct domains and simultaneously several labels (classes). When encountering domain shifts, e.g., classifier of movie reviews from IMDb to Rotten Tomatoes, adapting such an LLM-based multi-label classifier is challenging due to incomplete label sets at the target domain and daunting training overhead. The existing domain adaptation methods address either image multi-label classifiers or text binary classifiers. In this paper, we design DALLMi, Domain Adaptation Large Language Model interpolator, a first-of-its-kind semi-supervised domain adaptation method for text data models based on LLMs, specifically BERT. The core of DALLMi is the novel variation loss and MixUp regularization, which jointly leverage the limited positively labeled and large quantity of unlabeled text and, importantly, their interpolation from the BERT word embeddings. DALLMi also introduces a label-balanced sampling strategy to overcome the imbalance between labeled and unlabeled data. We evaluate DALLMi against the partial-supervised and unsupervised approach on three datasets under different scenarios of label availability for the target domain. Our results show that DALLMi achieves higher mAP than unsupervised and partially-supervised approaches by 19.9% and 52.2%, respectively.",
    "authors": [
      "Miruna Beţianu",
      "Abele Mălan",
      "Marco Aldinucci",
      "Robert Birke",
      "Lydia Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-05-03T07:04:26Z",
    "pdf_url": "https://arxiv.org/pdf/2405.01883v1"
  },
  {
    "arxiv_id": "2405.02358v4",
    "entry_id": "http://arxiv.org/abs/2405.02358v4",
    "title": "Empowering Time Series Analysis with Foundation Models: A Comprehensive Survey",
    "summary": "Time series data are ubiquitous across diverse real-world applications, making time series analysis critically important. Traditional approaches are largely task-specific, offering limited functionality and poor transferability. In recent years, foundation models have revolutionized NLP and CV with their remarkable cross-task transferability, zero-/few-shot learning capabilities, and multimodal integration capacity. This success has motivated increasing efforts to explore foundation models for addressing time series modeling challenges. Although some tutorials and surveys were published in the early stages of this field, the rapid pace of recent developments necessitates a more comprehensive and in-depth synthesis to cover the latest advances. Our survey aims to fill this gap by introducing a modality-aware, challenge-oriented perspective, which reveals how foundation models pre-trained on different modalities face distinct hurdles when adapted to time series tasks. Building on this perspective, we propose a taxonomy of existing works organized by pre-training modality (time series, language, and vision), analyze modality-specific challenges and categorize corresponding solutions, discussing their advantages and limitations. Beyond this, we review real-world applications to illustrate domain-specific advancements, provide open-source codes, and conclude with potential future research directions in this rapidly evolving field.",
    "authors": [
      "Jiexia Ye",
      "Yongzi Yu",
      "Weiqi Zhang",
      "Le Wang",
      "Jia Li",
      "Fugee Tsung"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-05-03T03:12:55Z",
    "pdf_url": "https://arxiv.org/pdf/2405.02358v4"
  },
  {
    "arxiv_id": "2405.02357v2",
    "entry_id": "http://arxiv.org/abs/2405.02357v2",
    "title": "Large Language Models for Mobility Analysis in Transportation Systems: A Survey on Forecasting Tasks",
    "summary": "Mobility analysis is a crucial element in the research area of transportation systems. Forecasting traffic information offers a viable solution to address the conflict between increasing transportation demands and the limitations of transportation infrastructure. Predicting human travel is significant in aiding various transportation and urban management tasks, such as taxi dispatch and urban planning. Machine learning and deep learning methods are favored for their flexibility and accuracy. Nowadays, with the advent of large language models (LLMs), many researchers have combined these models with previous techniques or applied LLMs to directly predict future traffic information and human travel behaviors. However, there is a lack of comprehensive studies on how LLMs can contribute to this field. This survey explores existing approaches using LLMs for time series forecasting problems for mobility in transportation systems. We provide a literature review concerning the forecasting applications within transportation systems, elucidating how researchers utilize LLMs, showcasing recent state-of-the-art advancements, and identifying the challenges that must be overcome to fully leverage LLMs in this domain.",
    "authors": [
      "Zijian Zhang",
      "Yujie Sun",
      "Zepu Wang",
      "Yuqi Nie",
      "Xiaobo Ma",
      "Ruolin Li",
      "Peng Sun",
      "Xuegang Ban"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-05-03T02:54:43Z",
    "pdf_url": "https://arxiv.org/pdf/2405.02357v2"
  },
  {
    "arxiv_id": "2405.01745v1",
    "entry_id": "http://arxiv.org/abs/2405.01745v1",
    "title": "Large Language Models for UAVs: Current State and Pathways to the Future",
    "summary": "Unmanned Aerial Vehicles (UAVs) have emerged as a transformative technology across diverse sectors, offering adaptable solutions to complex challenges in both military and civilian domains. Their expanding capabilities present a platform for further advancement by integrating cutting-edge computational tools like Artificial Intelligence (AI) and Machine Learning (ML) algorithms. These advancements have significantly impacted various facets of human life, fostering an era of unparalleled efficiency and convenience. Large Language Models (LLMs), a key component of AI, exhibit remarkable learning and adaptation capabilities within deployed environments, demonstrating an evolving form of intelligence with the potential to approach human-level proficiency. This work explores the significant potential of integrating UAVs and LLMs to propel the development of autonomous systems. We comprehensively review LLM architectures, evaluating their suitability for UAV integration. Additionally, we summarize the state-of-the-art LLM-based UAV architectures and identify novel opportunities for LLM embedding within UAV frameworks. Notably, we focus on leveraging LLMs to refine data analysis and decision-making processes, specifically for enhanced spectral sensing and sharing in UAV applications. Furthermore, we investigate how LLM integration expands the scope of existing UAV applications, enabling autonomous data processing, improved decision-making, and faster response times in emergency scenarios like disaster response and network restoration. Finally, we highlight crucial areas for future research that are critical for facilitating the effective integration of LLMs and UAVs.",
    "authors": [
      "Shumaila Javaid",
      "Nasir Saeed",
      "Bin He"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2024-05-02T21:30:10Z",
    "pdf_url": "https://arxiv.org/pdf/2405.01745v1"
  },
  {
    "arxiv_id": "2405.01249v1",
    "entry_id": "http://arxiv.org/abs/2405.01249v1",
    "title": "Prompt engineering paradigms for medical applications: scoping review and recommendations for better practices",
    "summary": "Prompt engineering is crucial for harnessing the potential of large language models (LLMs), especially in the medical domain where specialized terminology and phrasing is used. However, the efficacy of prompt engineering in the medical domain remains to be explored. In this work, 114 recent studies (2022-2024) applying prompt engineering in medicine, covering prompt learning (PL), prompt tuning (PT), and prompt design (PD) are reviewed. PD is the most prevalent (78 articles). In 12 papers, PD, PL, and PT terms were used interchangeably. ChatGPT is the most commonly used LLM, with seven papers using it for processing sensitive clinical data. Chain-of-Thought emerges as the most common prompt engineering technique. While PL and PT articles typically provide a baseline for evaluating prompt-based approaches, 64% of PD studies lack non-prompt-related baselines. We provide tables and figures summarizing existing work, and reporting recommendations to guide future research contributions.",
    "authors": [
      "Jamil Zaghir",
      "Marco Naguib",
      "Mina Bjelogrlic",
      "Aurélie Névéol",
      "Xavier Tannier",
      "Christian Lovis"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-05-02T12:52:23Z",
    "pdf_url": "https://arxiv.org/pdf/2405.01249v1"
  },
  {
    "arxiv_id": "2405.00748v2",
    "entry_id": "http://arxiv.org/abs/2405.00748v2",
    "title": "ChatGPT in Data Visualization Education: A Student Perspective",
    "summary": "Unlike traditional educational chatbots that rely on pre-programmed responses, large-language model-driven chatbots, such as ChatGPT, demonstrate remarkable versatility to serve as a dynamic resource for addressing student needs from understanding advanced concepts to solving complex problems. This work explores the impact of such technology on student learning in an interdisciplinary, project-oriented data visualization course. Throughout the semester, students engaged with ChatGPT across four distinct projects, designing and implementing data visualizations using a variety of tools such as Tableau, D3, and Vega-lite. We collected conversation logs and reflection surveys after each assignment and conducted interviews with selected students to gain deeper insights into their experiences with ChatGPT. Our analysis examined the advantages and barriers of using ChatGPT, students' querying behavior, the types of assistance sought, and its impact on assignment outcomes and engagement. We discuss design considerations for an educational solution tailored for data visualization education, extending beyond ChatGPT's basic interface.",
    "authors": [
      "Nam Wook Kim",
      "Hyung-Kwon Ko",
      "Grace Myers",
      "Benjamin Bach"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-05-01T02:40:20Z",
    "pdf_url": "https://arxiv.org/pdf/2405.00748v2"
  },
  {
    "arxiv_id": "2404.19664v5",
    "entry_id": "http://arxiv.org/abs/2404.19664v5",
    "title": "Towards Generalist Robot Learning from Internet Video: A Survey",
    "summary": "Scaling deep learning to massive and diverse internet data has driven remarkable breakthroughs in domains such as video generation and natural language processing. Robot learning, however, has thus far failed to replicate this success and remains constrained by a scarcity of available data. Learning from videos (LfV) methods aim to address this data bottleneck by augmenting traditional robot data with large-scale internet video. This video data provides foundational information regarding physical dynamics, behaviours, and tasks, and can be highly informative for general-purpose robots.\n  This survey systematically examines the emerging field of LfV. We first outline essential concepts, including detailing fundamental LfV challenges such as distribution shift and missing action labels in video data. Next, we comprehensively review current methods for extracting knowledge from large-scale internet video, overcoming LfV challenges, and improving robot learning through video-informed training. The survey concludes with a critical discussion of future opportunities. Here, we emphasize the need for scalable foundation model approaches that can leverage the full range of available internet video and enhance the learning of robot policies and dynamics models. Overall, the survey aims to inform and catalyse future LfV research, driving progress towards general-purpose robots.",
    "authors": [
      "Robert McCarthy",
      "Daniel C. H. Tan",
      "Dominik Schmidt",
      "Fernando Acero",
      "Nathan Herr",
      "Yilun Du",
      "Thomas G. Thuruthel",
      "Zhibin Li"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2024-04-30T15:57:41Z",
    "pdf_url": "https://arxiv.org/pdf/2404.19664v5"
  },
  {
    "arxiv_id": "2404.19631v1",
    "entry_id": "http://arxiv.org/abs/2404.19631v1",
    "title": "On Training a Neural Network to Explain Binaries",
    "summary": "In this work, we begin to investigate the possibility of training a deep neural network on the task of binary code understanding. Specifically, the network would take, as input, features derived directly from binaries and output English descriptions of functionality to aid a reverse engineer in investigating the capabilities of a piece of closed-source software, be it malicious or benign. Given recent success in applying large language models (generative AI) to the task of source code summarization, this seems a promising direction. However, in our initial survey of the available datasets, we found nothing of sufficiently high quality and volume to train these complex models. Instead, we build our own dataset derived from a capture of Stack Overflow containing 1.1M entries. A major result of our work is a novel dataset evaluation method using the correlation between two distances on sample pairs: one distance in the embedding space of inputs and the other in the embedding space of outputs. Intuitively, if two samples have inputs close in the input embedding space, their outputs should also be close in the output embedding space. We found this Embedding Distance Correlation (EDC) test to be highly diagnostic, indicating that our collected dataset and several existing open-source datasets are of low quality as the distances are not well correlated. We proceed to explore the general applicability of EDC, applying it to a number of qualitatively known good datasets and a number of synthetically known bad ones and found it to be a reliable indicator of dataset value.",
    "authors": [
      "Alexander Interrante-Grant",
      "Andy Davis",
      "Heather Preslier",
      "Tim Leek"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.SE"
    ],
    "published": "2024-04-30T15:34:51Z",
    "pdf_url": "https://arxiv.org/pdf/2404.19631v1"
  },
  {
    "arxiv_id": "2404.19543v2",
    "entry_id": "http://arxiv.org/abs/2404.19543v2",
    "title": "RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing",
    "summary": "Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.",
    "authors": [
      "Yucheng Hu",
      "Yuxing Lu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-30T13:14:51Z",
    "pdf_url": "https://arxiv.org/pdf/2404.19543v2"
  },
  {
    "arxiv_id": "2404.19456v2",
    "entry_id": "http://arxiv.org/abs/2404.19456v2",
    "title": "A Survey of Imitation Learning Methods, Environments and Metrics",
    "summary": "Imitation learning is an approach in which an agent learns how to execute a task by trying to mimic how one or more teachers perform it. This learning approach offers a compromise between the time it takes to learn a new task and the effort needed to collect teacher samples for the agent. It achieves this by balancing learning from the teacher, who has some information on how to perform the task, and deviating from their examples when necessary, such as states not present in the teacher samples. Consequently, the field of imitation learning has received much attention from researchers in recent years, resulting in many new methods and applications. However, with this increase in published work and past surveys focusing mainly on methodology, a lack of standardisation became more prominent in the field. This non-standardisation is evident in the use of environments, which appear in no more than two works, and evaluation processes, such as qualitative analysis, that have become rare in current literature. In this survey, we systematically review current imitation learning literature and present our findings by (i) classifying imitation learning techniques, environments and metrics by introducing novel taxonomies; (ii) reflecting on main problems from the literature; and (iii) presenting challenges and future directions for researchers.",
    "authors": [
      "Nathan Gavenski",
      "Felipe Meneguzzi",
      "Michael Luck",
      "Odinaldo Rodrigues"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-04-30T11:13:23Z",
    "pdf_url": "https://arxiv.org/pdf/2404.19456v2"
  },
  {
    "arxiv_id": "2407.01553v2",
    "entry_id": "http://arxiv.org/abs/2407.01553v2",
    "title": "Fish-bone diagram of research issue: Gain a bird's-eye view on a specific research topic",
    "summary": "Novice researchers often face difficulties in understanding a multitude of academic papers and grasping the fundamentals of a new research field. To solve such problems, the knowledge graph supporting research survey is gradually being developed. Existing keyword-based knowledge graphs make it difficult for researchers to deeply understand abstract concepts. Meanwhile, novice researchers may find it difficult to use ChatGPT effectively for research surveys due to their limited understanding of the research field. Without the ability to ask proficient questions that align with key concepts, obtaining desired and accurate answers from this large language model (LLM) could be inefficient. This study aims to help novice researchers by providing a fish-bone diagram that includes causal relationships, offering an overview of the research topic. The diagram is constructed using the issue ontology from academic papers, and it offers a broad, highly generalized perspective of the research field, based on relevance and logical factors. Furthermore, we evaluate the strengths and improvable points of the fish-bone diagram derived from this study's development pattern, emphasizing its potential as a viable tool for supporting research survey.",
    "authors": [
      "JingHong Li",
      "Huy Phan",
      "Wen Gu",
      "Koichi Ota",
      "Shinobu Hasegawa"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-30T05:43:41Z",
    "pdf_url": "https://arxiv.org/pdf/2407.01553v2"
  },
  {
    "arxiv_id": "2404.19093v1",
    "entry_id": "http://arxiv.org/abs/2404.19093v1",
    "title": "Large Language Models as Conversational Movie Recommenders: A User Study",
    "summary": "This paper explores the effectiveness of using large language models (LLMs) for personalized movie recommendations from users' perspectives in an online field experiment. Our study involves a combination of between-subject prompt and historic consumption assessments, along with within-subject recommendation scenario evaluations. By examining conversation and survey response data from 160 active users, we find that LLMs offer strong recommendation explainability but lack overall personalization, diversity, and user trust. Our results also indicate that different personalized prompting techniques do not significantly affect user-perceived recommendation quality, but the number of movies a user has watched plays a more significant role. Furthermore, LLMs show a greater ability to recommend lesser-known or niche movies. Through qualitative analysis, we identify key conversational patterns linked to positive and negative user interaction experiences and conclude that providing personal context and examples is crucial for obtaining high-quality recommendations from LLMs.",
    "authors": [
      "Ruixuan Sun",
      "Xinyi Li",
      "Avinash Akella",
      "Joseph A. Konstan"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-04-29T20:17:06Z",
    "pdf_url": "https://arxiv.org/pdf/2404.19093v1"
  },
  {
    "arxiv_id": "2404.18311v5",
    "entry_id": "http://arxiv.org/abs/2404.18311v5",
    "title": "Towards Incremental Learning in Large Language Models: A Critical Review",
    "summary": "Incremental learning is the ability of systems to acquire knowledge over time, enabling their adaptation and generalization to novel tasks. It is a critical ability for intelligent, real-world systems, especially when data changes frequently or is limited. This review provides a comprehensive analysis of incremental learning in Large Language Models. It synthesizes the state-of-the-art incremental learning paradigms, including continual learning, meta-learning, parameter-efficient learning, and mixture-of-experts learning. We demonstrate their utility for incremental learning by describing specific achievements from these related topics and their critical factors. An important finding is that many of these approaches do not update the core model, and none of them update incrementally in real-time. The paper highlights current problems and challenges for future research in the field. By consolidating the latest relevant research developments, this review offers a comprehensive understanding of incremental learning and its implications for designing and developing LLM-based learning systems.",
    "authors": [
      "Mladjan Jovanovic",
      "Peter Voss"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-28T20:44:53Z",
    "pdf_url": "https://arxiv.org/pdf/2404.18311v5"
  },
  {
    "arxiv_id": "2404.18253v5",
    "entry_id": "http://arxiv.org/abs/2404.18253v5",
    "title": "Efficient Remote Sensing with Harmonized Transfer Learning and Modality Alignment",
    "summary": "With the rise of Visual and Language Pretraining (VLP), an increasing number of downstream tasks are adopting the paradigm of pretraining followed by fine-tuning. Although this paradigm has demonstrated potential in various multimodal downstream tasks, its implementation in the remote sensing domain encounters some obstacles. Specifically, the tendency for same-modality embeddings to cluster together impedes efficient transfer learning. To tackle this issue, we review the aim of multimodal transfer learning for downstream tasks from a unified perspective, and rethink the optimization process based on three distinct objectives. We propose \"Harmonized Transfer Learning and Modality Alignment (HarMA)\", a method that simultaneously satisfies task constraints, modality alignment, and single-modality uniform alignment, while minimizing training overhead through parameter-efficient fine-tuning. Remarkably, without the need for external data for training, HarMA achieves state-of-the-art performance in two popular multimodal retrieval tasks in the field of remote sensing. Our experiments reveal that HarMA achieves competitive and even superior performance to fully fine-tuned models with only minimal adjustable parameters. Due to its simplicity, HarMA can be integrated into almost all existing multimodal pretraining models. We hope this method can facilitate the efficient application of large models to a wide range of downstream tasks while significantly reducing the resource consumption. Code is available at https://github.com/seekerhuang/HarMA.",
    "authors": [
      "Tengjun Huang"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-04-28T17:20:08Z",
    "pdf_url": "https://arxiv.org/pdf/2404.18253v5"
  },
  {
    "arxiv_id": "2404.18231v2",
    "entry_id": "http://arxiv.org/abs/2404.18231v2",
    "title": "From Persona to Personalization: A Survey on Role-Playing Language Agents",
    "summary": "Recent advancements in large language models (LLMs) have significantly boosted the rise of Role-Playing Language Agents (RPLAs), i.e., specialized AI systems designed to simulate assigned personas. By harnessing multiple advanced abilities of LLMs, including in-context learning, instruction following, and social intelligence, RPLAs achieve a remarkable sense of human likeness and vivid role-playing performance. RPLAs can mimic a wide range of personas, ranging from historical figures and fictional characters to real-life individuals. Consequently, they have catalyzed numerous AI applications, such as emotional companions, interactive video games, personalized assistants and copilots, and digital clones. In this paper, we conduct a comprehensive survey of this field, illustrating the evolution and recent progress in RPLAs integrating with cutting-edge LLM technologies. We categorize personas into three types: 1) Demographic Persona, which leverages statistical stereotypes; 2) Character Persona, focused on well-established figures; and 3) Individualized Persona, customized through ongoing user interactions for personalized services. We begin by presenting a comprehensive overview of current methodologies for RPLAs, followed by the details for each persona type, covering corresponding data sourcing, agent construction, and evaluation. Afterward, we discuss the fundamental risks, existing limitations, and future prospects of RPLAs. Additionally, we provide a brief review of RPLAs in AI applications, which reflects practical user demands that shape and drive RPLA research. Through this work, we aim to establish a clear taxonomy of RPLA research and applications, and facilitate future research in this critical and ever-evolving field, and pave the way for a future where humans and RPLAs coexist in harmony.",
    "authors": [
      "Jiangjie Chen",
      "Xintao Wang",
      "Rui Xu",
      "Siyu Yuan",
      "Yikai Zhang",
      "Wei Shi",
      "Jian Xie",
      "Shuang Li",
      "Ruihan Yang",
      "Tinghui Zhu",
      "Aili Chen",
      "Nianqi Li",
      "Lida Chen",
      "Caiyu Hu",
      "Siye Wu",
      "Scott Ren",
      "Ziquan Fu",
      "Yanghua Xiao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-28T15:56:41Z",
    "pdf_url": "https://arxiv.org/pdf/2404.18231v2"
  },
  {
    "arxiv_id": "2404.18144v1",
    "entry_id": "http://arxiv.org/abs/2404.18144v1",
    "title": "Generative AI for Visualization: State of the Art and Future Directions",
    "summary": "Generative AI (GenAI) has witnessed remarkable progress in recent years and demonstrated impressive performance in various generation tasks in different domains such as computer vision and computational design. Many researchers have attempted to integrate GenAI into visualization framework, leveraging the superior generative capacity for different operations. Concurrently, recent major breakthroughs in GenAI like diffusion model and large language model have also drastically increase the potential of GenAI4VIS. From a technical perspective, this paper looks back on previous visualization studies leveraging GenAI and discusses the challenges and opportunities for future research. Specifically, we cover the applications of different types of GenAI methods including sequence, tabular, spatial and graph generation techniques for different tasks of visualization which we summarize into four major stages: data enhancement, visual mapping generation, stylization and interaction. For each specific visualization sub-task, we illustrate the typical data and concrete GenAI algorithms, aiming to provide in-depth understanding of the state-of-the-art GenAI4VIS techniques and their limitations. Furthermore, based on the survey, we discuss three major aspects of challenges and research opportunities including evaluation, dataset, and the gap between end-to-end GenAI and generative algorithms. By summarizing different generation algorithms, their current applications and limitations, this paper endeavors to provide useful insights for future GenAI4VIS research.",
    "authors": [
      "Yilin Ye",
      "Jianing Hao",
      "Yihan Hou",
      "Zhan Wang",
      "Shishi Xiao",
      "Yuyu Luo",
      "Wei Zeng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-04-28T11:27:30Z",
    "pdf_url": "https://arxiv.org/pdf/2404.18144v1"
  },
  {
    "arxiv_id": "2404.17687v2",
    "entry_id": "http://arxiv.org/abs/2404.17687v2",
    "title": "Knowledge Transfer for Cross-Domain Reinforcement Learning: A Systematic Review",
    "summary": "Reinforcement Learning (RL) provides a framework in which agents can be trained, via trial and error, to solve complex decision-making problems. Learning with little supervision causes RL methods to require large amounts of data, rendering them too expensive for many applications (e.g., robotics). By reusing knowledge from a different task, knowledge transfer methods present an alternative to reduce the training time in RL. Given the severe data scarcity, due to their flexibility, there has been a growing interest in methods capable of transferring knowledge across different domains (i.e., problems with different representations). However, identifying similarities and adapting knowledge across tasks from different domains requires matching their representations or finding domain-invariant features. These processes can be data-demanding, which poses the main challenge in cross-domain knowledge transfer: to select and transform knowledge in a data-efficient way, such that it accelerates learning in the target task, despite the presence of significant differences across problems (e.g., robots with distinct morphologies). Thus, this review presents a unifying analysis of methods focused on transferring knowledge across different domains. Through a taxonomy based on a transfer-approach categorization and a characterization of works based on their data-assumption requirements, the contributions of this article are 1) a comprehensive and systematic revision of knowledge transfer methods for the cross-domain RL setting, 2) a categorization and characterization of such methods to provide an analysis based on relevant features such as their transfer approach and data requirements, and 3) a discussion on the main challenges regarding cross-domain knowledge transfer, as well as on ideas of future directions worth exploring to address these problems.",
    "authors": [
      "Sergio A. Serrano",
      "Jose Martinez-Carranza",
      "L. Enrique Sucar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2024-04-26T20:36:58Z",
    "pdf_url": "https://arxiv.org/pdf/2404.17687v2"
  },
  {
    "arxiv_id": "2404.17546v1",
    "entry_id": "http://arxiv.org/abs/2404.17546v1",
    "title": "Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo",
    "summary": "Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.",
    "authors": [
      "Stephen Zhao",
      "Rob Brekelmans",
      "Alireza Makhzani",
      "Roger Grosse"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "stat.ML"
    ],
    "published": "2024-04-26T17:18:32Z",
    "pdf_url": "https://arxiv.org/pdf/2404.17546v1"
  },
  {
    "arxiv_id": "2404.17443v1",
    "entry_id": "http://arxiv.org/abs/2404.17443v1",
    "title": "\"ChatGPT Is Here to Help, Not to Replace Anybody\" -- An Evaluation of Students' Opinions On Integrating ChatGPT In CS Courses",
    "summary": "Large Language Models (LLMs) like GPT and Bard are capable of producing code based on textual descriptions, with remarkable efficacy. Such technology will have profound implications for computing education, raising concerns about cheating, excessive dependence, and a decline in computational thinking skills, among others. There has been extensive research on how teachers should handle this challenge but it is also important to understand how students feel about this paradigm shift. In this research, 52 first-year CS students were surveyed in order to assess their views on technologies with code-generation capabilities, both from academic and professional perspectives. Our findings indicate that while students generally favor the academic use of GPT, they don't over rely on it, only mildly asking for its help. Although most students benefit from GPT, some struggle to use it effectively, urging the need for specific GPT training. Opinions on GPT's impact on their professional lives vary, but there is a consensus on its importance in academic practice.",
    "authors": [
      "Bruno Pereira Cipriano",
      "Pedro Alves"
    ],
    "categories": [
      "cs.ET",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-04-26T14:29:16Z",
    "pdf_url": "https://arxiv.org/pdf/2404.17443v1"
  },
  {
    "arxiv_id": "2404.16921v1",
    "entry_id": "http://arxiv.org/abs/2404.16921v1",
    "title": "A Short Survey of Human Mobility Prediction in Epidemic Modeling from Transformers to LLMs",
    "summary": "This paper provides a comprehensive survey of recent advancements in leveraging machine learning techniques, particularly Transformer models, for predicting human mobility patterns during epidemics. Understanding how people move during epidemics is essential for modeling the spread of diseases and devising effective response strategies. Forecasting population movement is crucial for informing epidemiological models and facilitating effective response planning in public health emergencies. Predicting mobility patterns can enable authorities to better anticipate the geographical and temporal spread of diseases, allocate resources more efficiently, and implement targeted interventions. We review a range of approaches utilizing both pretrained language models like BERT and Large Language Models (LLMs) tailored specifically for mobility prediction tasks. These models have demonstrated significant potential in capturing complex spatio-temporal dependencies and contextual patterns in textual data.",
    "authors": [
      "Christian N. Mayemba",
      "D'Jeff K. Nkashama",
      "Jean Marie Tshimula",
      "Maximilien V. Dialufuma",
      "Jean Tshibangu Muabila",
      "Mbuyi Mukendi Didier",
      "Hugues Kanda",
      "René Manassé Galekwa",
      "Heber Dibwe Fita",
      "Serge Mundele",
      "Kalonji Kalala",
      "Aristarque Ilunga",
      "Lambert Mukendi Ntobo",
      "Dominique Muteba",
      "Aaron Aruna Abedi"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-04-25T17:52:19Z",
    "pdf_url": "https://arxiv.org/pdf/2404.16921v1"
  },
  {
    "arxiv_id": "2404.16789v3",
    "entry_id": "http://arxiv.org/abs/2404.16789v3",
    "title": "Continual Learning of Large Language Models: A Comprehensive Survey",
    "summary": "The recent success of large language models (LLMs) trained on static, pre-collected, general datasets has sparked numerous research directions and applications. One such direction addresses the non-trivial challenge of integrating pre-trained LLMs into dynamic data distributions, task structures, and user preferences. Pre-trained LLMs, when tailored for specific needs, often experience significant performance degradation in previous knowledge domains -- a phenomenon known as \"catastrophic forgetting\". While extensively studied in the continual learning (CL) community, it presents new manifestations in the realm of LLMs. In this survey, we provide a comprehensive overview of the current research progress on LLMs within the context of CL. This survey is structured into four main sections: we first describe an overview of continually learning LLMs, consisting of two directions of continuity: vertical continuity (or vertical continual learning), i.e., continual adaptation from general to specific capabilities, and horizontal continuity (or horizontal continual learning), i.e., continual adaptation across time and domains (Section 3). We then summarize three stages of learning LLMs in the context of modern CL: Continual Pre-Training (CPT), Domain-Adaptive Pre-training (DAP), and Continual Fine-Tuning (CFT) (Section 4). Then we provide an overview of evaluation protocols for continual learning with LLMs, along with the current available data sources (Section 5). Finally, we discuss intriguing questions pertaining to continual learning for LLMs (Section 6). The full list of papers examined in this survey is available at https://github.com/Wang-ML-Lab/llm-continual-learning-survey.",
    "authors": [
      "Haizhou Shi",
      "Zihao Xu",
      "Hengyi Wang",
      "Weiyi Qin",
      "Wenyuan Wang",
      "Yibin Wang",
      "Zifeng Wang",
      "Sayna Ebrahimi",
      "Hao Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-25T17:38:57Z",
    "pdf_url": "https://arxiv.org/pdf/2404.16789v3"
  },
  {
    "arxiv_id": "2405.00711v2",
    "entry_id": "http://arxiv.org/abs/2405.00711v2",
    "title": "Fake Artificial Intelligence Generated Contents (FAIGC): A Survey of Theories, Detection Methods, and Opportunities",
    "summary": "In recent years, generative artificial intelligence models, represented by Large Language Models (LLMs) and Diffusion Models (DMs), have revolutionized content production methods. These artificial intelligence-generated content (AIGC) have become deeply embedded in various aspects of daily life and work. However, these technologies have also led to the emergence of Fake Artificial Intelligence Generated Content (FAIGC), posing new challenges in distinguishing genuine information. It is crucial to recognize that AIGC technology is akin to a double-edged sword; its potent generative capabilities, while beneficial, also pose risks for the creation and dissemination of FAIGC. In this survey, We propose a new taxonomy that provides a more comprehensive breakdown of the space of FAIGC methods today. Next, we explore the modalities and generative technologies of FAIGC. We introduce FAIGC detection methods and summarize the related benchmark from various perspectives. Finally, we discuss outstanding challenges and promising areas for future research.",
    "authors": [
      "Xiaomin Yu",
      "Yezhaohui Wang",
      "Yanfang Chen",
      "Zhen Tao",
      "Dinghao Xi",
      "Shichao Song",
      "Simin Niu",
      "Zhiyu Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-04-25T04:44:09Z",
    "pdf_url": "https://arxiv.org/pdf/2405.00711v2"
  },
  {
    "arxiv_id": "2404.17605v1",
    "entry_id": "http://arxiv.org/abs/2404.17605v1",
    "title": "Autonomous LLM-driven research from data to human-verifiable research papers",
    "summary": "As AI promises to accelerate scientific discovery, it remains unclear whether fully AI-driven research is possible and whether it can adhere to key scientific values, such as transparency, traceability and verifiability. Mimicking human scientific practices, we built data-to-paper, an automation platform that guides interacting LLM agents through a complete stepwise research process, while programmatically back-tracing information flow and allowing human oversight and interactions. In autopilot mode, provided with annotated data alone, data-to-paper raised hypotheses, designed research plans, wrote and debugged analysis codes, generated and interpreted results, and created complete and information-traceable research papers. Even though research novelty was relatively limited, the process demonstrated autonomous generation of de novo quantitative insights from data. For simple research goals, a fully-autonomous cycle can create manuscripts which recapitulate peer-reviewed publications without major errors in about 80-90%, yet as goal complexity increases, human co-piloting becomes critical for assuring accuracy. Beyond the process itself, created manuscripts too are inherently verifiable, as information-tracing allows to programmatically chain results, methods and data. Our work thereby demonstrates a potential for AI-driven acceleration of scientific discovery while enhancing, rather than jeopardizing, traceability, transparency and verifiability.",
    "authors": [
      "Tal Ifargan",
      "Lukas Hafner",
      "Maor Kern",
      "Ori Alcalay",
      "Roy Kishony"
    ],
    "categories": [
      "q-bio.OT",
      "cs.AI"
    ],
    "published": "2024-04-24T23:15:49Z",
    "pdf_url": "https://arxiv.org/pdf/2404.17605v1"
  },
  {
    "arxiv_id": "2404.15676v3",
    "entry_id": "http://arxiv.org/abs/2404.15676v3",
    "title": "Beyond Chain-of-Thought: A Survey of Chain-of-X Paradigms for LLMs",
    "summary": "Chain-of-Thought (CoT) has been a widely adopted prompting method, eliciting impressive reasoning abilities of Large Language Models (LLMs). Inspired by the sequential thought structure of CoT, a number of Chain-of-X (CoX) methods have been developed to address various challenges across diverse domains and tasks involving LLMs. In this paper, we provide a comprehensive survey of Chain-of-X methods for LLMs in different contexts. Specifically, we categorize them by taxonomies of nodes, i.e., the X in CoX, and application tasks. We also discuss the findings and implications of existing CoX methods, as well as potential future directions. Our survey aims to serve as a detailed and up-to-date resource for researchers seeking to apply the idea of CoT to broader scenarios.",
    "authors": [
      "Yu Xia",
      "Rui Wang",
      "Xu Liu",
      "Mingyan Li",
      "Tong Yu",
      "Xiang Chen",
      "Julian McAuley",
      "Shuai Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-24T06:12:00Z",
    "pdf_url": "https://arxiv.org/pdf/2404.15676v3"
  },
  {
    "arxiv_id": "2404.15667v4",
    "entry_id": "http://arxiv.org/abs/2404.15667v4",
    "title": "The Promise and Challenges of Using LLMs to Accelerate the Screening Process of Systematic Reviews",
    "summary": "Systematic review (SR) is a popular research method in software engineering (SE). However, conducting an SR takes an average of 67 weeks. Thus, automating any step of the SR process could reduce the effort associated with SRs. Our objective is to investigate if Large Language Models (LLMs) can accelerate title-abstract screening by simplifying abstracts for human screeners, and automating title-abstract screening. We performed an experiment where humans screened titles and abstracts for 20 papers with both original and simplified abstracts from a prior SR. The experiment with human screeners was reproduced with GPT-3.5 and GPT-4 LLMs to perform the same screening tasks. We also studied if different prompting techniques (Zero-shot (ZS), One-shot (OS), Few-shot (FS), and Few-shot with Chain-of-Thought (FS-CoT)) improve the screening performance of LLMs. Lastly, we studied if redesigning the prompt used in the LLM reproduction of screening leads to improved performance. Text simplification did not increase the screeners' screening performance, but reduced the time used in screening. Screeners' scientific literacy skills and researcher status predict screening performance. Some LLM and prompt combinations perform as well as human screeners in the screening tasks. Our results indicate that the GPT-4 LLM is better than its predecessor, GPT-3.5. Additionally, Few-shot and One-shot prompting outperforms Zero-shot prompting. Using LLMs for text simplification in the screening process does not significantly improve human performance. Using LLMs to automate title-abstract screening seems promising, but current LLMs are not significantly more accurate than human screeners. To recommend the use of LLMs in the screening process of SRs, more research is needed. We recommend future SR studies publish replication packages with screening data to enable more conclusive experimenting with LLM screening.",
    "authors": [
      "Aleksi Huotala",
      "Miikka Kuutila",
      "Paul Ralph",
      "Mika Mäntylä"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-24T05:53:20Z",
    "pdf_url": "https://arxiv.org/pdf/2404.15667v4"
  },
  {
    "arxiv_id": "2404.15583v3",
    "entry_id": "http://arxiv.org/abs/2404.15583v3",
    "title": "Multi-Agent Reinforcement Learning for Energy Networks: Computational Challenges, Progress and Open Problems",
    "summary": "The rapidly changing architecture and functionality of electrical networks and the increasing penetration of renewable and distributed energy resources have resulted in various technological and managerial challenges. These have rendered traditional centralized energy-market paradigms insufficient due to their inability to support the dynamic and evolving nature of the network. This survey explores how multi-agent reinforcement learning (MARL) can support the decentralization and decarbonization of energy networks and mitigate the associated challenges. This is achieved by specifying key computational challenges in managing energy networks, reviewing recent research progress on addressing them, and highlighting open challenges that may be addressed using MARL.",
    "authors": [
      "Sarah Keren",
      "Chaimaa Essayeh",
      "Stefano V. Albrecht",
      "Thomas Morstyn"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-24T01:35:27Z",
    "pdf_url": "https://arxiv.org/pdf/2404.15583v3"
  },
  {
    "arxiv_id": "2404.16886v1",
    "entry_id": "http://arxiv.org/abs/2404.16886v1",
    "title": "Review of Data-centric Time Series Analysis from Sample, Feature, and Period",
    "summary": "Data is essential to performing time series analysis utilizing machine learning approaches, whether for classic models or today's large language models. A good time-series dataset is advantageous for the model's accuracy, robustness, and convergence, as well as task outcomes and costs. The emergence of data-centric AI represents a shift in the landscape from model refinement to prioritizing data quality. Even though time-series data processing methods frequently come up in a wide range of research fields, it hasn't been well investigated as a specific topic. To fill the gap, in this paper, we systematically review different data-centric methods in time series analysis, covering a wide range of research topics. Based on the time-series data characteristics at sample, feature, and period, we propose a taxonomy for the reviewed data selection methods. In addition to discussing and summarizing their characteristics, benefits, and drawbacks targeting time-series data, we also introduce the challenges and opportunities by proposing recommendations, open problems, and possible research topics.",
    "authors": [
      "Chenxi Sun",
      "Hongyan Li",
      "Yaliang Li",
      "Shenda Hong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-04-24T00:34:44Z",
    "pdf_url": "https://arxiv.org/pdf/2404.16886v1"
  },
  {
    "arxiv_id": "2404.14928v2",
    "entry_id": "http://arxiv.org/abs/2404.14928v2",
    "title": "Graph Machine Learning in the Era of Large Language Models (LLMs)",
    "summary": "Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks (GNNs) have emerged as a cornerstone in Graph Machine Learning (Graph ML), facilitating the representation and processing of graph structures. Recently, LLMs have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying LLMs to the graph domain. Increasing efforts have been made to explore the potential of LLMs in advancing Graph ML's generalization, transferability, and few-shot learning ability. Meanwhile, graphs, especially knowledge graphs, are rich in reliable factual knowledge, which can be utilized to enhance the reasoning capabilities of LLMs and potentially alleviate their limitations such as hallucinations and the lack of explainability. Given the rapid progress of this research direction, a systematic review summarizing the latest advancements for Graph ML in the era of LLMs is necessary to provide an in-depth understanding to researchers and practitioners. Therefore, in this survey, we first review the recent developments in Graph ML. We then explore how LLMs can be utilized to enhance the quality of graph features, alleviate the reliance on labeled data, and address challenges such as graph heterogeneity and out-of-distribution (OOD) generalization. Afterward, we delve into how graphs can enhance LLMs, highlighting their abilities to enhance LLM pre-training and inference. Furthermore, we investigate various applications and discuss the potential future directions in this promising field.",
    "authors": [
      "Wenqi Fan",
      "Shijie Wang",
      "Jiani Huang",
      "Zhikai Chen",
      "Yu Song",
      "Wenzhuo Tang",
      "Haitao Mao",
      "Hui Liu",
      "Xiaorui Liu",
      "Dawei Yin",
      "Qing Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "published": "2024-04-23T11:13:39Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14928v2"
  },
  {
    "arxiv_id": "2404.14901v2",
    "entry_id": "http://arxiv.org/abs/2404.14901v2",
    "title": "Beyond Code Generation: An Observational Study of ChatGPT Usage in Software Engineering Practice",
    "summary": "Large Language Models (LLMs) are frequently discussed in academia and the general public as support tools for virtually any use case that relies on the production of text, including software engineering. Currently there is much debate, but little empirical evidence, regarding the practical usefulness of LLM-based tools such as ChatGPT for engineers in industry. We conduct an observational study of 24 professional software engineers who have been using ChatGPT over a period of one week in their jobs, and qualitatively analyse their dialogues with the chatbot as well as their overall experience (as captured by an exit survey). We find that, rather than expecting ChatGPT to generate ready-to-use software artifacts (e.g., code), practitioners more often use ChatGPT to receive guidance on how to solve their tasks or learn about a topic in more abstract terms. We also propose a theoretical framework for how (i) purpose of the interaction, (ii) internal factors (e.g., the user's personality), and (iii) external factors (e.g., company policy) together shape the experience (in terms of perceived usefulness and trust). We envision that our framework can be used by future research to further the academic discussion on LLM usage by software engineering practitioners, and to serve as a reference point for the design of future empirical LLM research in this domain.",
    "authors": [
      "Ranim Khojah",
      "Mazen Mohamad",
      "Philipp Leitner",
      "Francisco Gomes de Oliveira Neto"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-04-23T10:34:16Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14901v2"
  },
  {
    "arxiv_id": "2404.14897v1",
    "entry_id": "http://arxiv.org/abs/2404.14897v1",
    "title": "Beyond the Speculative Game: A Survey of Speculative Execution in Large Language Models",
    "summary": "With the increasingly giant scales of (causal) large language models (LLMs), the inference efficiency comes as one of the core concerns along the improved performance. In contrast to the memory footprint, the latency bottleneck seems to be of greater importance as there can be billions of requests to a LLM (e.g., GPT-4) per day. The bottleneck is mainly due to the autoregressive innateness of LLMs, where tokens can only be generated sequentially during decoding. To alleviate the bottleneck, the idea of speculative execution, which originates from the field of computer architecture, is introduced to LLM decoding in a \\textit{draft-then-verify} style. Under this regime, a sequence of tokens will be drafted in a fast pace by utilizing some heuristics, and then the tokens shall be verified in parallel by the LLM. As the costly sequential inference is parallelized, LLM decoding speed can be significantly boosted. Driven by the success of LLMs in recent couple of years, a growing literature in this direction has emerged. Yet, there lacks a position survey to summarize the current landscape and draw a roadmap for future development of this promising area. To meet this demand, we present the very first survey paper that reviews and unifies literature of speculative execution in LLMs (e.g., blockwise parallel decoding, speculative decoding, etc.) in a comprehensive framework and a systematic taxonomy. Based on the taxonomy, we present a critical review and comparative analysis of the current arts. Finally we highlight various key challenges and future directions to further develop the area.",
    "authors": [
      "Chen Zhang",
      "Zhuorui Liu",
      "Dawei Song"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-23T10:25:45Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14897v1"
  },
  {
    "arxiv_id": "2404.14809v2",
    "entry_id": "http://arxiv.org/abs/2404.14809v2",
    "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
    "summary": "A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, and financial networks. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various natural language processing tasks to answer users' arbitrary questions and generate specific-domain content. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. However, LLMs are sequential models for textual data, but graphs are non-sequential topological data. It is challenging to adapt LLMs to tackle graph analytics tasks. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) in terms of three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graphs and LLMs, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning, and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of the discussed LLM-GGA models. We also explore open problems and future directions in the research area of LLMs and graph analytics.",
    "authors": [
      "Wenbo Shang",
      "Xin Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "published": "2024-04-23T07:39:24Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14809v2"
  },
  {
    "arxiv_id": "2404.14387v2",
    "entry_id": "http://arxiv.org/abs/2404.14387v2",
    "title": "A Survey on Self-Evolution of Large Language Models",
    "summary": "Large language models (LLMs) have significantly advanced in various fields and intelligent agent applications. However, current LLMs that learn from human or external model supervision are costly and may face performance ceilings as task complexity and diversity increase. To address this issue, self-evolution approaches that enable LLM to autonomously acquire, refine, and learn from experiences generated by the model itself are rapidly growing. This new training paradigm inspired by the human experiential learning process offers the potential to scale LLMs towards superintelligence. In this work, we present a comprehensive survey of self-evolution approaches in LLMs. We first propose a conceptual framework for self-evolution and outline the evolving process as iterative cycles composed of four phases: experience acquisition, experience refinement, updating, and evaluation. Second, we categorize the evolution objectives of LLMs and LLM-based agents; then, we summarize the literature and provide taxonomy and insights for each module. Lastly, we pinpoint existing challenges and propose future directions to improve self-evolution frameworks, equipping researchers with critical insights to fast-track the development of self-evolving LLMs. Our corresponding GitHub repository is available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/Awesome-Self-Evolution-of-LLM",
    "authors": [
      "Zhengwei Tao",
      "Ting-En Lin",
      "Xiancai Chen",
      "Hangyu Li",
      "Yuchuan Wu",
      "Yongbin Li",
      "Zhi Jin",
      "Fei Huang",
      "Dacheng Tao",
      "Jingren Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-22T17:43:23Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14387v2"
  },
  {
    "arxiv_id": "2404.14294v3",
    "entry_id": "http://arxiv.org/abs/2404.14294v3",
    "title": "A Survey on Efficient Inference for Large Language Models",
    "summary": "Large Language Models (LLMs) have attracted extensive attention due to their remarkable performance across various tasks. However, the substantial computational and memory requirements of LLM inference pose challenges for deployment in resource-constrained scenarios. Efforts within the field have been directed towards developing techniques aimed at enhancing the efficiency of LLM inference. This paper presents a comprehensive survey of the existing literature on efficient LLM inference. We start by analyzing the primary causes of the inefficient LLM inference, i.e., the large model size, the quadratic-complexity attention operation, and the auto-regressive decoding approach. Then, we introduce a comprehensive taxonomy that organizes the current literature into data-level, model-level, and system-level optimization. Moreover, the paper includes comparative experiments on representative methods within critical sub-fields to provide quantitative insights. Last but not least, we provide some knowledge summary and discuss future research directions.",
    "authors": [
      "Zixuan Zhou",
      "Xuefei Ning",
      "Ke Hong",
      "Tianyu Fu",
      "Jiaming Xu",
      "Shiyao Li",
      "Yuming Lou",
      "Luning Wang",
      "Zhihang Yuan",
      "Xiuhong Li",
      "Shengen Yan",
      "Guohao Dai",
      "Xiao-Ping Zhang",
      "Yuhan Dong",
      "Yu Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-22T15:53:08Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14294v3"
  },
  {
    "arxiv_id": "2404.14464v1",
    "entry_id": "http://arxiv.org/abs/2404.14464v1",
    "title": "Tree of Reviews: A Tree-based Dynamic Iterative Retrieval Framework for Multi-hop Question Answering",
    "summary": "Multi-hop question answering is a knowledge-intensive complex problem. Large Language Models (LLMs) use their Chain of Thoughts (CoT) capability to reason complex problems step by step, and retrieval-augmentation can effectively alleviate factual errors caused by outdated and unknown knowledge in LLMs. Recent works have introduced retrieval-augmentation in the CoT reasoning to solve multi-hop question answering. However, these chain methods have the following problems: 1) Retrieved irrelevant paragraphs may mislead the reasoning; 2) An error in the chain structure may lead to a cascade of errors.\n  In this paper, we propose a dynamic retrieval framework called Tree of Reviews (ToR), where the root node is the question, and the other nodes are paragraphs from retrieval, extending different reasoning paths from the root node to other nodes. Our framework dynamically decides to initiate a new search, reject, or accept based on the paragraphs on the reasoning paths. Compared to related work, we introduce a tree structure to handle each retrieved paragraph separately, alleviating the misleading effect of irrelevant paragraphs on the reasoning path; the diversity of reasoning path extension reduces the impact of a single reasoning error on the whole. We conducted experiments on three different multi-hop question answering datasets. The results show that compared to the baseline methods, ToR achieves state-of-the-art performance in both retrieval and response generation. In addition, we propose two tree-based search optimization strategies, pruning and effective expansion, to reduce time overhead and increase the diversity of path extension. We will release our code.",
    "authors": [
      "Li Jiapeng",
      "Liu Runze",
      "Li Yabo",
      "Zhou Tong",
      "Li Mingling",
      "Chen Xiang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-04-22T09:25:05Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14464v1"
  },
  {
    "arxiv_id": "2404.13954v1",
    "entry_id": "http://arxiv.org/abs/2404.13954v1",
    "title": "A survey of air combat behavior modeling using machine learning",
    "summary": "With the recent advances in machine learning, creating agents that behave realistically in simulated air combat has become a growing field of interest. This survey explores the application of machine learning techniques for modeling air combat behavior, motivated by the potential to enhance simulation-based pilot training. Current simulated entities tend to lack realistic behavior, and traditional behavior modeling is labor-intensive and prone to loss of essential domain knowledge between development steps. Advancements in reinforcement learning and imitation learning algorithms have demonstrated that agents may learn complex behavior from data, which could be faster and more scalable than manual methods. Yet, making adaptive agents capable of performing tactical maneuvers and operating weapons and sensors still poses a significant challenge. The survey examines applications, behavior model types, prevalent machine learning methods, and the technical and human challenges in developing adaptive and realistically behaving agents. Another challenge is the transfer of agents from learning environments to military simulation systems and the consequent demand for standardization. Four primary recommendations are presented regarding increased emphasis on beyond-visual-range scenarios, multi-agent machine learning and cooperation, utilization of hierarchical behavior models, and initiatives for standardization and research collaboration. These recommendations aim to address current issues and guide the development of more comprehensive, adaptable, and realistic machine learning-based behavior models for air combat applications.",
    "authors": [
      "Patrick Ribu Gorton",
      "Andreas Strand",
      "Karsten Brathen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2024-04-22T07:54:56Z",
    "pdf_url": "https://arxiv.org/pdf/2404.13954v1"
  },
  {
    "arxiv_id": "2404.13906v2",
    "entry_id": "http://arxiv.org/abs/2404.13906v2",
    "title": "Generating Attractive and Authentic Copywriting from Customer Reviews",
    "summary": "The goal of product copywriting is to capture the interest of potential buyers by emphasizing the features of products through text descriptions. As e-commerce platforms offer a wide range of services, it's becoming essential to dynamically adjust the styles of these auto-generated descriptions. Typical approaches to copywriting generation often rely solely on specified product attributes, which may result in dull and repetitive content. To tackle this issue, we propose to generate copywriting based on customer reviews, as they provide firsthand practical experiences with products, offering a richer source of information than just product attributes. We have developed a sequence-to-sequence framework, enhanced with reinforcement learning, to produce copywriting that is attractive, authentic, and rich in information. Our framework outperforms all existing baseline and zero-shot large language models, including LLaMA-2-chat-7B and GPT-3.5, in terms of both attractiveness and faithfulness. Furthermore, this work features the use of LLMs for aspect-based summaries collection and argument allure assessment. Experiments demonstrate the effectiveness of using LLMs for marketing domain corpus construction. The code and the dataset is publicly available at: https://github.com/YuXiangLin1234/Copywriting-Generation.",
    "authors": [
      "Yu-Xiang Lin",
      "Wei-Yun Ma"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-22T06:33:28Z",
    "pdf_url": "https://arxiv.org/pdf/2404.13906v2"
  },
  {
    "arxiv_id": "2404.15155v3",
    "entry_id": "http://arxiv.org/abs/2404.15155v3",
    "title": "MDAgents: An Adaptive Collaboration of LLMs for Medical Decision-Making",
    "summary": "Foundation models are becoming valuable tools in medicine. Yet despite their promise, the best way to leverage Large Language Models (LLMs) in complex medical tasks remains an open question. We introduce a novel multi-agent framework, named Medical Decision-making Agents (MDAgents) that helps address this gap by automatically assigning a collaboration structure to a team of LLMs. The assigned solo or group collaboration structure is tailored to the medical task at hand, emulating real-world medical decision-making processes adapted to tasks of varying complexities. We evaluate our framework and baseline methods using state-of-the-art LLMs across a suite of real-world medical knowledge and medical diagnosis benchmarks, including a comparison of LLMs' medical complexity classification against human physicians. MDAgents achieved the best performance in seven out of ten benchmarks on tasks requiring an understanding of medical knowledge and multi-modal reasoning, showing a significant improvement of up to 4.2% (p < 0.05) compared to previous methods' best performances. Ablation studies reveal that MDAgents effectively determines medical complexity to optimize for efficiency and accuracy across diverse medical tasks. Notably, the combination of moderator review and external medical knowledge in group collaboration resulted in an average accuracy improvement of 11.8%. Our code can be found at https://github.com/mitmedialab/MDAgents.",
    "authors": [
      "Yubin Kim",
      "Chanwoo Park",
      "Hyewon Jeong",
      "Yik Siu Chan",
      "Xuhai Xu",
      "Daniel McDuff",
      "Hyeonhoon Lee",
      "Marzyeh Ghassemi",
      "Cynthia Breazeal",
      "Hae Won Park"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-04-22T06:30:05Z",
    "pdf_url": "https://arxiv.org/pdf/2404.15155v3"
  },
  {
    "arxiv_id": "2404.13885v2",
    "entry_id": "http://arxiv.org/abs/2404.13885v2",
    "title": "Surveying Attitudinal Alignment Between Large Language Models Vs. Humans Towards 17 Sustainable Development Goals",
    "summary": "Large Language Models (LLMs) have emerged as potent tools for advancing the United Nations' Sustainable Development Goals (SDGs). However, the attitudinal disparities between LLMs and humans towards these goals can pose significant challenges. This study conducts a comprehensive review and analysis of the existing literature on the attitudes of LLMs towards the 17 SDGs, emphasizing the comparison between their attitudes and support for each goal and those of humans. We examine the potential disparities, primarily focusing on aspects such as understanding and emotions, cultural and regional differences, task objective variations, and factors considered in the decision-making process. These disparities arise from the underrepresentation and imbalance in LLM training data, historical biases, quality issues, lack of contextual understanding, and skewed ethical values reflected. The study also investigates the risks and harms that may arise from neglecting the attitudes of LLMs towards the SDGs, including the exacerbation of social inequalities, racial discrimination, environmental destruction, and resource wastage. To address these challenges, we propose strategies and recommendations to guide and regulate the application of LLMs, ensuring their alignment with the principles and goals of the SDGs, and therefore creating a more just, inclusive, and sustainable future.",
    "authors": [
      "Qingyang Wu",
      "Ying Xu",
      "Tingsong Xiao",
      "Yunze Xiao",
      "Yitong Li",
      "Tianyang Wang",
      "Yichi Zhang",
      "Shanghai Zhong",
      "Yuwei Zhang",
      "Wei Lu",
      "Yifan Yang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-04-22T05:12:52Z",
    "pdf_url": "https://arxiv.org/pdf/2404.13885v2"
  },
  {
    "arxiv_id": "2404.14459v2",
    "entry_id": "http://arxiv.org/abs/2404.14459v2",
    "title": "LLMs in Web Development: Evaluating LLM-Generated PHP Code Unveiling Vulnerabilities and Limitations",
    "summary": "This study evaluates the security of web application code generated by Large Language Models, analyzing 2,500 GPT-4 generated PHP websites. These were deployed in Docker containers and tested for vulnerabilities using a hybrid approach of Burp Suite active scanning, static analysis, and manual review. Our investigation focuses on identifying Insecure File Upload, SQL Injection, Stored XSS, and Reflected XSS in GPT-4 generated PHP code. This analysis highlights potential security risks and the implications of deploying such code in real-world scenarios. Overall, our analysis found 2,440 vulnerable parameters. According to Burp's Scan, 11.56% of the sites can be straight out compromised. Adding static scan results, 26% had at least one vulnerability that can be exploited through web interaction. Certain coding scenarios, like file upload functionality, are insecure 78% of the time, underscoring significant risks to software safety and security. To support further research, we have made the source codes and a detailed vulnerability record for each sample publicly available. This study emphasizes the crucial need for thorough testing and evaluation if generative AI technologies are used in software development.",
    "authors": [
      "Rebeka Tóth",
      "Tamas Bisztray",
      "László Erdodi"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-04-21T20:56:02Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14459v2"
  },
  {
    "arxiv_id": "2404.13501v1",
    "entry_id": "http://arxiv.org/abs/2404.13501v1",
    "title": "A Survey on the Memory Mechanism of Large Language Model based Agents",
    "summary": "Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \\url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.",
    "authors": [
      "Zeyu Zhang",
      "Xiaohe Bo",
      "Chen Ma",
      "Rui Li",
      "Xu Chen",
      "Quanyu Dai",
      "Jieming Zhu",
      "Zhenhua Dong",
      "Ji-Rong Wen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-21T01:49:46Z",
    "pdf_url": "https://arxiv.org/pdf/2404.13501v1"
  },
  {
    "arxiv_id": "2404.13348v4",
    "entry_id": "http://arxiv.org/abs/2404.13348v4",
    "title": "Socialized Learning: A Survey of the Paradigm Shift for Edge Intelligence in Networked Systems",
    "summary": "Amidst the robust impetus from artificial intelligence (AI) and big data, edge intelligence (EI) has emerged as a nascent computing paradigm, synthesizing AI with edge computing (EC) to become an exemplary solution for unleashing the full potential of AI services. Nonetheless, challenges in communication costs, resource allocation, privacy, and security continue to constrain its proficiency in supporting services with diverse requirements. In response to these issues, this paper introduces socialized learning (SL) as a promising solution, further propelling the advancement of EI. SL is a learning paradigm predicated on social principles and behaviors, aimed at amplifying the collaborative capacity and collective intelligence of agents within the EI system. SL not only enhances the system's adaptability but also optimizes communication, and networking processes, essential for distributed intelligence across diverse devices and platforms. Therefore, a combination of SL and EI may greatly facilitate the development of collaborative intelligence in the future network. This paper presents the findings of a literature review on the integration of EI and SL, summarizing the latest achievements in existing research on EI and SL. Subsequently, we delve comprehensively into the limitations of EI and how it could benefit from SL. Special emphasis is placed on the communication challenges and networking strategies and other aspects within these systems, underlining the role of optimized network solutions in improving system efficiency. Based on these discussions, we elaborate in detail on three integrated components: socialized architecture, socialized training, and socialized inference, analyzing their strengths and weaknesses. Finally, we identify some possible future applications of combining SL and EI, discuss open problems and suggest some future research.",
    "authors": [
      "Xiaofei Wang",
      "Yunfeng Zhao",
      "Chao Qiu",
      "Qinghua Hu",
      "Victor C. M. Leung"
    ],
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "published": "2024-04-20T11:07:29Z",
    "pdf_url": "https://arxiv.org/pdf/2404.13348v4"
  },
  {
    "arxiv_id": "2404.14445v2",
    "entry_id": "http://arxiv.org/abs/2404.14445v2",
    "title": "A Multi-Faceted Evaluation Framework for Assessing Synthetic Data Generated by Large Language Models",
    "summary": "The rapid advancements in generative AI and large language models (LLMs) have opened up new avenues for producing synthetic data, particularly in the realm of structured tabular formats, such as product reviews. Despite the potential benefits, concerns regarding privacy leakage have surfaced, especially when personal information is utilized in the training datasets. In addition, there is an absence of a comprehensive evaluation framework capable of quantitatively measuring the quality of the generated synthetic data and their utility for downstream tasks. In response to this gap, we introduce SynEval, an open-source evaluation framework designed to assess the fidelity, utility, and privacy preservation of synthetically generated tabular data via a suite of diverse evaluation metrics. We validate the efficacy of our proposed framework - SynEval - by applying it to synthetic product review data generated by three state-of-the-art LLMs: ChatGPT, Claude, and Llama. Our experimental findings illuminate the trade-offs between various evaluation metrics in the context of synthetic data generation. Furthermore, SynEval stands as a critical instrument for researchers and practitioners engaged with synthetic tabular data,, empowering them to judiciously determine the suitability of the generated data for their specific applications, with an emphasis on upholding user privacy.",
    "authors": [
      "Yefeng Yuan",
      "Yuhong Liu",
      "Liang Cheng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-20T08:08:28Z",
    "pdf_url": "https://arxiv.org/pdf/2404.14445v2"
  },
  {
    "arxiv_id": "2404.13244v1",
    "entry_id": "http://arxiv.org/abs/2404.13244v1",
    "title": "Intelligent Agents for Auction-based Federated Learning: A Survey",
    "summary": "Auction-based federated learning (AFL) is an important emerging category of FL incentive mechanism design, due to its ability to fairly and efficiently motivate high-quality data owners to join data consumers' (i.e., servers') FL training tasks. To enhance the efficiency in AFL decision support for stakeholders (i.e., data consumers, data owners, and the auctioneer), intelligent agent-based techniques have emerged. However, due to the highly interdisciplinary nature of this field and the lack of a comprehensive survey providing an accessible perspective, it is a challenge for researchers to enter and contribute to this field. This paper bridges this important gap by providing a first-of-its-kind survey on the Intelligent Agents for AFL (IA-AFL) literature. We propose a unique multi-tiered taxonomy that organises existing IA-AFL works according to 1) the stakeholders served, 2) the auction mechanism adopted, and 3) the goals of the agents, to provide readers with a multi-perspective view into this field. In addition, we analyse the limitations of existing approaches, summarise the commonly adopted performance evaluation metrics, and discuss promising future directions leading towards effective and efficient stakeholder-oriented decision support in IA-AFL ecosystems.",
    "authors": [
      "Xiaoli Tang",
      "Han Yu",
      "Xiaoxiao Li",
      "Sarit Kraus"
    ],
    "categories": [
      "cs.LG",
      "cs.GT"
    ],
    "published": "2024-04-20T03:05:25Z",
    "pdf_url": "https://arxiv.org/pdf/2404.13244v1"
  },
  {
    "arxiv_id": "2404.12975v1",
    "entry_id": "http://arxiv.org/abs/2404.12975v1",
    "title": "FineRec:Exploring Fine-grained Sequential Recommendation",
    "summary": "Sequential recommendation is dedicated to offering items of interest for users based on their history behaviors. The attribute-opinion pairs, expressed by users in their reviews for items, provide the potentials to capture user preferences and item characteristics at a fine-grained level. To this end, we propose a novel framework FineRec that explores the attribute-opinion pairs of reviews to finely handle sequential recommendation. Specifically, we utilize a large language model to extract attribute-opinion pairs from reviews. For each attribute, a unique attribute-specific user-opinion-item graph is created, where corresponding opinions serve as the edges linking heterogeneous user and item nodes. To tackle the diversity of opinions, we devise a diversity-aware convolution operation to aggregate information within the graphs, enabling attribute-specific user and item representation learning. Ultimately, we present an interaction-driven fusion mechanism to integrate attribute-specific user/item representations across all attributes for generating recommendations. Extensive experiments conducted on several realworld datasets demonstrate the superiority of our FineRec over existing state-of-the-art methods. Further analysis also verifies the effectiveness of our fine-grained manner in handling the task.",
    "authors": [
      "Xiaokun Zhang",
      "Bo Xu",
      "Youlin Wu",
      "Yuan Zhong",
      "Hongfei Lin",
      "Fenglong Ma"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-04-19T16:04:26Z",
    "pdf_url": "https://arxiv.org/pdf/2404.12975v1"
  },
  {
    "arxiv_id": "2404.12938v2",
    "entry_id": "http://arxiv.org/abs/2404.12938v2",
    "title": "MAiDE-up: Multilingual Deception Detection of GPT-generated Hotel Reviews",
    "summary": "Deceptive reviews are becoming increasingly common, especially given the increase in performance and the prevalence of LLMs. While work to date has addressed the development of models to differentiate between truthful and deceptive human reviews, much less is known about the distinction between real reviews and AI-authored fake reviews. Moreover, most of the research so far has focused primarily on English, with very little work dedicated to other languages. In this paper, we compile and make publicly available the MAiDE-up dataset, consisting of 10,000 real and 10,000 AI-generated fake hotel reviews, balanced across ten languages. Using this dataset, we conduct extensive linguistic analyses to (1) compare the AI fake hotel reviews to real hotel reviews, and (2) identify the factors that influence the deception detection model performance. We explore the effectiveness of several models for deception detection in hotel reviews across three main dimensions: sentiment, location, and language. We find that these dimensions influence how well we can detect AI-generated fake reviews.",
    "authors": [
      "Oana Ignat",
      "Xiaomeng Xu",
      "Rada Mihalcea"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-19T15:08:06Z",
    "pdf_url": "https://arxiv.org/pdf/2404.12938v2"
  },
  {
    "arxiv_id": "2404.12901v2",
    "entry_id": "http://arxiv.org/abs/2404.12901v2",
    "title": "Large Language Models for Networking: Workflow, Advances and Challenges",
    "summary": "The networking field is characterized by its high complexity and rapid iteration, requiring extensive expertise to accomplish network tasks, ranging from network design, configuration, diagnosis and security. The inherent complexity of these tasks, coupled with the ever-changing landscape of networking technologies and protocols, poses significant hurdles for traditional machine learning-based methods. These methods often struggle to generalize and automate complex tasks in networking, as they require extensive labeled data, domain-specific feature engineering, and frequent retraining to adapt to new scenarios. However, the recent emergence of large language models (LLMs) has sparked a new wave of possibilities in addressing these challenges. LLMs have demonstrated remarkable capabilities in natural language understanding, generation, and reasoning. These models, trained on extensive data, can benefit the networking domain. Some efforts have already explored the application of LLMs in the networking domain and revealed promising results. By reviewing recent advances, we present an abstract workflow to describe the fundamental process involved in applying LLM for Networking. We introduce the highlights of existing works by category and explain in detail how they operate at different stages of the workflow. Furthermore, we delve into the challenges encountered, discuss potential solutions, and outline future research prospects. We hope that this survey will provide insight for researchers and practitioners, promoting the development of this interdisciplinary research field.",
    "authors": [
      "Chang Liu",
      "Xiaohui Xie",
      "Xinggong Zhang",
      "Yong Cui"
    ],
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "published": "2024-04-19T14:17:02Z",
    "pdf_url": "https://arxiv.org/pdf/2404.12901v2"
  },
  {
    "arxiv_id": "2404.12259v1",
    "entry_id": "http://arxiv.org/abs/2404.12259v1",
    "title": "Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM",
    "summary": "Data analysts have long sought to turn unstructured text data into meaningful concepts. Though common, topic modeling and clustering focus on lower-level keywords and require significant interpretative work. We introduce concept induction, a computational process that instead produces high-level concepts, defined by explicit inclusion criteria, from unstructured text. For a dataset of toxic online comments, where a state-of-the-art BERTopic model outputs \"women, power, female,\" concept induction produces high-level concepts such as \"Criticism of traditional gender roles\" and \"Dismissal of women's concerns.\" We present LLooM, a concept induction algorithm that leverages large language models to iteratively synthesize sampled text and propose human-interpretable concepts of increasing generality. We then instantiate LLooM in a mixed-initiative text analysis tool, enabling analysts to shift their attention from interpreting topics to engaging in theory-driven analysis. Through technical evaluations and four analysis scenarios ranging from literature review to content moderation, we find that LLooM's concepts improve upon the prior art of topic models in terms of quality and data coverage. In expert case studies, LLooM helped researchers to uncover new insights even from familiar datasets, for example by suggesting a previously unnoticed concept of attacks on out-party stances in a political social media dataset.",
    "authors": [
      "Michelle S. Lam",
      "Janice Teoh",
      "James Landay",
      "Jeffrey Heer",
      "Michael S. Bernstein"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-04-18T15:26:02Z",
    "pdf_url": "https://arxiv.org/pdf/2404.12259v1"
  },
  {
    "arxiv_id": "2404.12056v1",
    "entry_id": "http://arxiv.org/abs/2404.12056v1",
    "title": "Deconstructing Human-AI Collaboration: Agency, Interaction, and Adaptation",
    "summary": "As full AI-based automation remains out of reach in most real-world applications, the focus has instead shifted to leveraging the strengths of both human and AI agents, creating effective collaborative systems. The rapid advances in this area have yielded increasingly more complex systems and frameworks, while the nuance of their characterization has gotten more vague. Similarly, the existing conceptual models no longer capture the elaborate processes of these systems nor describe the entire scope of their collaboration paradigms. In this paper, we propose a new unified set of dimensions through which to analyze and describe human-AI systems. Our conceptual model is centered around three high-level aspects - agency, interaction, and adaptation - and is developed through a multi-step process. Firstly, an initial design space is proposed by surveying the literature and consolidating existing definitions and conceptual frameworks. Secondly, this model is iteratively refined and validated by conducting semi-structured interviews with nine researchers in this field. Lastly, to illustrate the applicability of our design space, we utilize it to provide a structured description of selected human-AI systems.",
    "authors": [
      "Steffen Holter",
      "Mennatallah El-Assady"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-04-18T10:12:18Z",
    "pdf_url": "https://arxiv.org/pdf/2404.12056v1"
  },
  {
    "arxiv_id": "2404.12041v4",
    "entry_id": "http://arxiv.org/abs/2404.12041v4",
    "title": "A Survey of Automatic Hallucination Evaluation on Natural Language Generation",
    "summary": "The rapid advancement of Large Language Models (LLMs) has brought a pressing challenge: how to reliably assess hallucinations to guarantee model trustworthiness. Although Automatic Hallucination Evaluation (AHE) has become an indispensable component of this effort, the field remains fragmented in its methodologies, limiting both conceptual clarity and practical progress. This survey addresses this critical gap through a systematic analysis of 105 evaluation methods, revealing that 77.1% specifically target LLMs, a paradigm shift that demands new evaluation frameworks. We formulate a structured framework to organize the field, based on a survey of foundational datasets and benchmarks and a taxonomy of evaluation methodologies, which together systematically document the evolution from pre-LLM to post-LLM approaches. Beyond taxonomical organization, we identify fundamental limitations in current approaches and their implications for real-world deployment. To guide future research, we delineate key challenges and propose strategic directions, including enhanced interpretability mechanisms and integration of application-specific evaluation criteria, ultimately providing a roadmap for developing more robust and practical hallucination evaluation systems.",
    "authors": [
      "Siya Qi",
      "Lin Gui",
      "Yulan He",
      "Zheng Yuan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-18T09:52:18Z",
    "pdf_url": "https://arxiv.org/pdf/2404.12041v4"
  },
  {
    "arxiv_id": "2404.11973v2",
    "entry_id": "http://arxiv.org/abs/2404.11973v2",
    "title": "A critical review of methods and challenges in large language models",
    "summary": "This critical review provides an in-depth analysis of Large Language Models (LLMs), encompassing their foundational principles, diverse applications, and advanced training methodologies. We critically examine the evolution from Recurrent Neural Networks (RNNs) to Transformer models, highlighting the significant advancements and innovations in LLM architectures. The review explores state-of-the-art techniques such as in-context learning and various fine-tuning approaches, with an emphasis on optimizing parameter efficiency. We also discuss methods for aligning LLMs with human preferences, including reinforcement learning frameworks and human feedback mechanisms. The emerging technique of retrieval-augmented generation, which integrates external knowledge into LLMs, is also evaluated. Additionally, we address the ethical considerations of deploying LLMs, stressing the importance of responsible and mindful application. By identifying current gaps and suggesting future research directions, this review provides a comprehensive and critical overview of the present state and potential advancements in LLMs. This work serves as an insightful guide for researchers and practitioners in artificial intelligence, offering a unified perspective on the strengths, limitations, and future prospects of LLMs.",
    "authors": [
      "Milad Moradi",
      "Ke Yan",
      "David Colwell",
      "Matthias Samwald",
      "Rhona Asgari"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-18T08:01:20Z",
    "pdf_url": "https://arxiv.org/pdf/2404.11973v2"
  },
  {
    "arxiv_id": "2404.11584v1",
    "entry_id": "http://arxiv.org/abs/2404.11584v1",
    "title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
    "summary": "This survey paper examines the recent advancements in AI agent implementations, with a focus on their ability to achieve complex goals that require enhanced reasoning, planning, and tool execution capabilities. The primary objectives of this work are to a) communicate the current capabilities and limitations of existing AI agent implementations, b) share insights gained from our observations of these systems in action, and c) suggest important considerations for future developments in AI agent design. We achieve this by providing overviews of single-agent and multi-agent architectures, identifying key patterns and divergences in design choices, and evaluating their overall impact on accomplishing a provided goal. Our contribution outlines key themes when selecting an agentic architecture, the impact of leadership on agent systems, agent communication styles, and key phases for planning, execution, and reflection that enable robust AI agent systems.",
    "authors": [
      "Tula Masterman",
      "Sandi Besen",
      "Mason Sawtell",
      "Alex Chao"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-17T17:32:41Z",
    "pdf_url": "https://arxiv.org/pdf/2404.11584v1"
  },
  {
    "arxiv_id": "2404.11457v2",
    "entry_id": "http://arxiv.org/abs/2404.11457v2",
    "title": "Bias and Unfairness in Information Retrieval Systems: New Challenges in the LLM Era",
    "summary": "With the rapid advancements of large language models (LLMs), information retrieval (IR) systems, such as search engines and recommender systems, have undergone a significant paradigm shift. This evolution, while heralding new opportunities, introduces emerging challenges, particularly in terms of biases and unfairness, which may threaten the information ecosystem. In this paper, we present a comprehensive survey of existing works on emerging and pressing bias and unfairness issues in IR systems when the integration of LLMs. We first unify bias and unfairness issues as distribution mismatch problems, providing a groundwork for categorizing various mitigation strategies through distribution alignment. Subsequently, we systematically delve into the specific bias and unfairness issues arising from three critical stages of LLMs integration into IR systems: data collection, model development, and result evaluation. In doing so, we meticulously review and analyze recent literature, focusing on the definitions, characteristics, and corresponding mitigation strategies associated with these issues. Finally, we identify and highlight some open problems and challenges for future work, aiming to inspire researchers and stakeholders in the IR field and beyond to better understand and mitigate bias and unfairness issues of IR in this LLM era. We also consistently maintain a GitHub repository for the relevant papers and resources in this rising direction at https://github.com/KID-22/LLM-IR-Bias-Fairness-Survey.",
    "authors": [
      "Sunhao Dai",
      "Chen Xu",
      "Shicheng Xu",
      "Liang Pang",
      "Zhenhua Dong",
      "Jun Xu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-17T15:05:03Z",
    "pdf_url": "https://arxiv.org/pdf/2404.11457v2"
  },
  {
    "arxiv_id": "2404.11160v2",
    "entry_id": "http://arxiv.org/abs/2404.11160v2",
    "title": "Low-Cost Language Models: Survey and Performance Evaluation on Python Code Generation",
    "summary": "Large Language Models (LLMs) have become a popular choice for many Natural Language Processing (NLP) tasks due to their versatility and ability to produce high-quality results. Specifically, they are increasingly used for automatic code generation to help developers tackle repetitive coding tasks. However, LLMs' substantial computational and memory requirements often make them inaccessible to users with limited resources. This paper focuses on very low-cost models which offer a more accessible alternative to resource-intensive LLMs. We notably: (1) propose a thorough semi-manual evaluation of their performance in generating Python code, (2) introduce a Chain-of-Thought (CoT) prompting strategy to improve model reasoning and code quality, and (3) propose a new dataset of 60 programming problems, with varied difficulty levels, designed to extend existing benchmarks like HumanEval and EvalPlus. Our findings show that some low-cost compatible models achieve competitive results compared to larger models like ChatGPT despite using significantly fewer resources. We will make our dataset and prompts publicly available to support further research.",
    "authors": [
      "Jessica López Espejel",
      "Mahaman Sanoussi Yahaya Alassan",
      "Merieme Bouhandi",
      "Walid Dahhane",
      "El Hassane Ettifouri"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-17T08:16:48Z",
    "pdf_url": "https://arxiv.org/pdf/2404.11160v2"
  },
  {
    "arxiv_id": "2404.10981v2",
    "entry_id": "http://arxiv.org/abs/2404.10981v2",
    "title": "A Survey on Retrieval-Augmented Text Generation for Large Language Models",
    "summary": "Retrieval-Augmented Generation (RAG) merges retrieval methods with deep learning advancements to address the static limitations of large language models (LLMs) by enabling the dynamic integration of up-to-date external information. This methodology, focusing primarily on the text domain, provides a cost-effective solution to the generation of plausible but possibly incorrect responses by LLMs, thereby enhancing the accuracy and reliability of their outputs through the use of real-world data. As RAG grows in complexity and incorporates multiple concepts that can influence its performance, this paper organizes the RAG paradigm into four categories: pre-retrieval, retrieval, post-retrieval, and generation, offering a detailed perspective from the retrieval viewpoint. It outlines RAG's evolution and discusses the field's progression through the analysis of significant studies. Additionally, the paper introduces evaluation methods for RAG, addressing the challenges faced and proposing future research directions. By offering an organized framework and categorization, the study aims to consolidate existing research on RAG, clarify its technological underpinnings, and highlight its potential to broaden the adaptability and applications of LLMs.",
    "authors": [
      "Yizheng Huang",
      "Jimmy Huang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-17T01:27:42Z",
    "pdf_url": "https://arxiv.org/pdf/2404.10981v2"
  },
  {
    "arxiv_id": "2404.10876v2",
    "entry_id": "http://arxiv.org/abs/2404.10876v2",
    "title": "Course Recommender Systems Need to Consider the Job Market",
    "summary": "Current course recommender systems primarily leverage learner-course interactions, course content, learner preferences, and supplementary course details like instructor, institution, ratings, and reviews, to make their recommendation. However, these systems often overlook a critical aspect: the evolving skill demand of the job market. This paper focuses on the perspective of academic researchers, working in collaboration with the industry, aiming to develop a course recommender system that incorporates job market skill demands. In light of the job market's rapid changes and the current state of research in course recommender systems, we outline essential properties for course recommender systems to address these demands effectively, including explainable, sequential, unsupervised, and aligned with the job market and user's goals. Our discussion extends to the challenges and research questions this objective entails, including unsupervised skill extraction from job listings, course descriptions, and resumes, as well as predicting recommendations that align with learner objectives and the job market and designing metrics to evaluate this alignment. Furthermore, we introduce an initial system that addresses some existing limitations of course recommender systems using large Language Models (LLMs) for skill extraction and Reinforcement Learning (RL) for alignment with the job market. We provide empirical results using open-source data to demonstrate its effectiveness.",
    "authors": [
      "Jibril Frej",
      "Anna Dai",
      "Syrielle Montariol",
      "Antoine Bosselut",
      "Tanja Käser"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-04-16T19:52:57Z",
    "pdf_url": "https://arxiv.org/pdf/2404.10876v2"
  },
  {
    "arxiv_id": "2404.10508v5",
    "entry_id": "http://arxiv.org/abs/2404.10508v5",
    "title": "White Men Lead, Black Women Help? Benchmarking and Mitigating Language Agency Social Biases in LLMs",
    "summary": "Social biases can manifest in language agency. However, very limited research has investigated such biases in Large Language Model (LLM)-generated content. In addition, previous works often rely on string-matching techniques to identify agentic and communal words within texts, falling short of accurately classifying language agency. We introduce the Language Agency Bias Evaluation (LABE) benchmark, which comprehensively evaluates biases in LLMs by analyzing agency levels attributed to different demographic groups in model generations. LABE tests for gender, racial, and intersectional language agency biases in LLMs on 3 text generation tasks: biographies, professor reviews, and reference letters. Using LABE, we unveil language agency social biases in 3 recent LLMs: ChatGPT, Llama3, and Mistral. We observe that: (1) LLM generations tend to demonstrate greater gender bias than human-written texts; (2) Models demonstrate remarkably higher levels of intersectional bias than the other bias aspects. (3) Prompt-based mitigation is unstable and frequently leads to bias exacerbation. Based on our observations, we propose Mitigation via Selective Rewrite (MSR), a novel bias mitigation strategy that leverages an agency classifier to identify and selectively revise parts of generated texts that demonstrate communal traits. Empirical results prove MSR to be more effective and reliable than prompt-based mitigation method, showing a promising research direction.",
    "authors": [
      "Yixin Wan",
      "Kai-Wei Chang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-04-16T12:27:54Z",
    "pdf_url": "https://arxiv.org/pdf/2404.10508v5"
  },
  {
    "arxiv_id": "2404.10097v1",
    "entry_id": "http://arxiv.org/abs/2404.10097v1",
    "title": "LegalPro-BERT: Classification of Legal Provisions by fine-tuning BERT Large Language Model",
    "summary": "A contract is a type of legal document commonly used in organizations. Contract review is an integral and repetitive process to avoid business risk and liability. Contract analysis requires the identification and classification of key provisions and paragraphs within an agreement. Identification and validation of contract clauses can be a time-consuming and challenging task demanding the services of trained and expensive lawyers, paralegals or other legal assistants. Classification of legal provisions in contracts using artificial intelligence and natural language processing is complex due to the requirement of domain-specialized legal language for model training and the scarcity of sufficient labeled data in the legal domain. Using general-purpose models is not effective in this context due to the use of specialized legal vocabulary in contracts which may not be recognized by a general model. To address this problem, we propose the use of a pre-trained large language model which is subsequently calibrated on legal taxonomy. We propose LegalPro-BERT, a BERT transformer architecture model that we fine-tune to efficiently handle classification task for legal provisions. We conducted experiments to measure and compare metrics with current benchmark results. We found that LegalPro-BERT outperforms the previous benchmark used for comparison in this research.",
    "authors": [
      "Amit Tewari"
    ],
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-04-15T19:08:48Z",
    "pdf_url": "https://arxiv.org/pdf/2404.10097v1"
  },
  {
    "arxiv_id": "2404.09939v3",
    "entry_id": "http://arxiv.org/abs/2404.09939v3",
    "title": "A Survey on Deep Learning for Theorem Proving",
    "summary": "Theorem proving is a fundamental aspect of mathematics, spanning from informal reasoning in natural language to rigorous derivations in formal systems. In recent years, the advancement of deep learning, especially the emergence of large language models, has sparked a notable surge of research exploring these techniques to enhance the process of theorem proving. This paper presents a comprehensive survey of deep learning for theorem proving by offering (i) a thorough review of existing approaches across various tasks such as autoformalization, premise selection, proofstep generation, and proof search; (ii) an extensive summary of curated datasets and strategies for synthetic data generation; (iii) a detailed analysis of evaluation metrics and the performance of state-of-the-art methods; and (iv) a critical discussion on the persistent challenges and the promising avenues for future exploration. Our survey aims to serve as a foundational reference for deep learning approaches in theorem proving, inspiring and catalyzing further research endeavors in this rapidly growing field. A curated list of papers is available at https://github.com/zhaoyu-li/DL4TP.",
    "authors": [
      "Zhaoyu Li",
      "Jialiang Sun",
      "Logan Murphy",
      "Qidong Su",
      "Zenan Li",
      "Xian Zhang",
      "Kaiyu Yang",
      "Xujie Si"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-15T17:07:55Z",
    "pdf_url": "https://arxiv.org/pdf/2404.09939v3"
  },
  {
    "arxiv_id": "2404.13074v1",
    "entry_id": "http://arxiv.org/abs/2404.13074v1",
    "title": "Towards Compositionally Generalizable Semantic Parsing in Large Language Models: A Survey",
    "summary": "Compositional generalization is the ability of a model to generalize to complex, previously unseen types of combinations of entities from just having seen the primitives. This type of generalization is particularly relevant to the semantic parsing community for applications such as task-oriented dialogue, text-to-SQL parsing, and information retrieval, as they can harbor infinite complexity. Despite the success of large language models (LLMs) in a wide range of NLP tasks, unlocking perfect compositional generalization still remains one of the few last unsolved frontiers. The past few years has seen a surge of interest in works that explore the limitations of, methods to improve, and evaluation metrics for compositional generalization capabilities of LLMs for semantic parsing tasks. In this work, we present a literature survey geared at synthesizing recent advances in analysis, methods, and evaluation schemes to offer a starting point for both practitioners and researchers in this area.",
    "authors": [
      "Amogh Mannekote"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-15T10:44:58Z",
    "pdf_url": "https://arxiv.org/pdf/2404.13074v1"
  },
  {
    "arxiv_id": "2404.15192v3",
    "entry_id": "http://arxiv.org/abs/2404.15192v3",
    "title": "Measuring Diversity of Game Scenarios",
    "summary": "This survey comprehensively reviews the multi-dimensionality of game scenario diversity, spotlighting the innovative use of procedural content generation and other fields as cornerstones for enriching player experiences through diverse game scenarios. By traversing a wide array of disciplines, from affective modeling and multi-agent systems to psychological studies, our research underscores the importance of diverse game scenarios in gameplay and education. Through a taxonomy of diversity metrics and evaluation methods, we aim to bridge the current gaps in literature and practice, offering insights into effective strategies for measuring and integrating diversity in game scenarios. Our analysis highlights the necessity for a unified taxonomy to aid developers and researchers in crafting more engaging and varied game worlds. This survey not only charts a path for future research in diverse game scenarios but also serves as a handbook for industry practitioners seeking to leverage diversity as a key component of game design and development.",
    "authors": [
      "Yuchen Li",
      "Ziqi Wang",
      "Qingquan Zhang",
      "Bo Yuan",
      "Jialin Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-15T07:59:52Z",
    "pdf_url": "https://arxiv.org/pdf/2404.15192v3"
  },
  {
    "arxiv_id": "2404.09024v2",
    "entry_id": "http://arxiv.org/abs/2404.09024v2",
    "title": "An Agent-Based Model of Elephant Crop Raid Dynamics in the Periyar-Agasthyamalai Complex, India",
    "summary": "Human-wildlife conflict challenges conservation worldwide, which requires innovative management solutions. We developed a prototype Agent-Based Model (ABM) to simulate interactions between humans and solitary bull Asian elephants in the Periyar-Agasthyamalai complex of the Western Ghats in Kerala, India. The main challenges were the complex behavior of elephants and insufficient movement data from the region. Using literature, expert insights, and field surveys, we created a prototype behavior model that incorporates crop habituation, thermoregulation, and aggression. We designed a four-step calibration method to adapt relocation data from radio-tagged elephants in Indonesia to model elephant movements in the model domain. The ABM's structure, including the assumptions, submodels, and data usage are detailed following the Overview, Design concepts, Details protocol. The ABM simulates various food availability scenarios to study elephant behavior and environmental impact on space use and conflict patterns. The results indicate that the wet months increase conflict and thermoregulation significantly influences elephant movements and crop raiding. Starvation and crop habituation intensify these patterns. This prototype ABM is an initial model that offers information on the development of a decision support system in wildlife management and will be further enhanced with layers of complexity and subtlety across various dimensions. Access the ABM at https://github.com/quest-lab-iisc/abm-elephant-project.",
    "authors": [
      "Anjali Purathekandy",
      "Meera Anna Oommen",
      "Martin Wikelski",
      "Deepak N Subramani"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2024-04-13T15:14:53Z",
    "pdf_url": "https://arxiv.org/pdf/2404.09024v2"
  },
  {
    "arxiv_id": "2404.09022v1",
    "entry_id": "http://arxiv.org/abs/2404.09022v1",
    "title": "Navigating the Landscape of Large Language Models: A Comprehensive Review and Analysis of Paradigms and Fine-Tuning Strategies",
    "summary": "With the surge of ChatGPT,the use of large models has significantly increased,rapidly rising to prominence across the industry and sweeping across the internet. This article is a comprehensive review of fine-tuning methods for large models. This paper investigates the latest technological advancements and the application of advanced methods in aspects such as task-adaptive fine-tuning,domain-adaptive fine-tuning,few-shot learning,knowledge distillation,multi-task learning,parameter-efficient fine-tuning,and dynamic fine-tuning.",
    "authors": [
      "Benjue Weng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-13T15:03:03Z",
    "pdf_url": "https://arxiv.org/pdf/2404.09022v1"
  },
  {
    "arxiv_id": "2404.08760v4",
    "entry_id": "http://arxiv.org/abs/2404.08760v4",
    "title": "The Generation Gap: Exploring Age Bias in the Value Systems of Large Language Models",
    "summary": "We explore the alignment of values in Large Language Models (LLMs) with specific age groups, leveraging data from the World Value Survey across thirteen categories. Through a diverse set of prompts tailored to ensure response robustness, we find a general inclination of LLM values towards younger demographics, especially when compared to the US population. Although a general inclination can be observed, we also found that this inclination toward younger groups can be different across different value categories. Additionally, we explore the impact of incorporating age identity information in prompts and observe challenges in mitigating value discrepancies with different age cohorts. Our findings highlight the age bias in LLMs and provide insights for future work. Materials for our analysis are available at \\url{ https://github.com/MichiganNLP/Age-Bias-In-LLMs}",
    "authors": [
      "Siyang Liu",
      "Trish Maturi",
      "Bowen Yi",
      "Siqi Shen",
      "Rada Mihalcea"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-12T18:36:20Z",
    "pdf_url": "https://arxiv.org/pdf/2404.08760v4"
  },
  {
    "arxiv_id": "2404.08555v2",
    "entry_id": "http://arxiv.org/abs/2404.08555v2",
    "title": "RLHF Deciphered: A Critical Analysis of Reinforcement Learning from Human Feedback for LLMs",
    "summary": "State-of-the-art large language models (LLMs) have become indispensable tools for various tasks. However, training LLMs to serve as effective assistants for humans requires careful consideration. A promising approach is reinforcement learning from human feedback (RLHF), which leverages human feedback to update the model in accordance with human preferences and mitigate issues like toxicity and hallucinations. Yet, an understanding of RLHF for LLMs is largely entangled with initial design choices that popularized the method and current research focuses on augmenting those choices rather than fundamentally improving the framework. In this paper, we analyze RLHF through the lens of reinforcement learning principles to develop an understanding of its fundamentals, dedicating substantial focus to the core component of RLHF -- the reward model. Our study investigates modeling choices, caveats of function approximation, and their implications on RLHF training algorithms, highlighting the underlying assumptions made about the expressivity of reward. Our analysis improves the understanding of the role of reward models and methods for their training, concurrently revealing limitations of the current methodology. We characterize these limitations, including incorrect generalization, model misspecification, and the sparsity of feedback, along with their impact on the performance of a language model. The discussion and analysis are substantiated by a categorical review of current literature, serving as a reference for researchers and practitioners to understand the challenges of RLHF and build upon existing efforts.",
    "authors": [
      "Shreyas Chaudhari",
      "Pranjal Aggarwal",
      "Vishvak Murahari",
      "Tanmay Rajpurohit",
      "Ashwin Kalyan",
      "Karthik Narasimhan",
      "Ameet Deshpande",
      "Bruno Castro da Silva"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-12T15:54:15Z",
    "pdf_url": "https://arxiv.org/pdf/2404.08555v2"
  },
  {
    "arxiv_id": "2405.01440v2",
    "entry_id": "http://arxiv.org/abs/2405.01440v2",
    "title": "A Review of Reward Functions for Reinforcement Learning in the context of Autonomous Driving",
    "summary": "Reinforcement learning has emerged as an important approach for autonomous driving. A reward function is used in reinforcement learning to establish the learned skill objectives and guide the agent toward the optimal policy. Since autonomous driving is a complex domain with partly conflicting objectives with varying degrees of priority, developing a suitable reward function represents a fundamental challenge. This paper aims to highlight the gap in such function design by assessing different proposed formulations in the literature and dividing individual objectives into Safety, Comfort, Progress, and Traffic Rules compliance categories. Additionally, the limitations of the reviewed reward functions are discussed, such as objectives aggregation and indifference to driving context. Furthermore, the reward categories are frequently inadequately formulated and lack standardization. This paper concludes by proposing future research that potentially addresses the observed shortcomings in rewards, including a reward validation framework and structured rewards that are context-aware and able to resolve conflicts.",
    "authors": [
      "Ahmed Abouelazm",
      "Jonas Michel",
      "J. Marius Zoellner"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-04-12T08:32:54Z",
    "pdf_url": "https://arxiv.org/pdf/2405.01440v2"
  },
  {
    "arxiv_id": "2404.07738v2",
    "entry_id": "http://arxiv.org/abs/2404.07738v2",
    "title": "ResearchAgent: Iterative Research Idea Generation over Scientific Literature with Large Language Models",
    "summary": "The pace of scientific research, vital for improving human life, is complex, slow, and needs specialized expertise. Meanwhile, novel, impactful research often stems from both a deep understanding of prior work, and a cross-pollination of ideas across domains and fields. To enhance the productivity of researchers, we propose ResearchAgent, which leverages the encyclopedic knowledge and linguistic reasoning capabilities of Large Language Models (LLMs) to assist them in their work. This system automatically defines novel problems, proposes methods and designs experiments, while iteratively refining them based on the feedback from collaborative LLM-powered reviewing agents. Specifically, starting with a core scientific paper, ResearchAgent is augmented not only with relevant publications by connecting information over an academic graph but also entities retrieved from a knowledge store derived from shared underlying concepts mined across numerous papers. Then, mimicking a scientific approach to improving ideas with peer discussions, we leverage multiple LLM-based ReviewingAgents that provide reviews and feedback via iterative revision processes. These reviewing agents are instantiated with human preference-aligned LLMs whose criteria for evaluation are elicited from actual human judgments via LLM prompting. We experimentally validate our ResearchAgent on scientific publications across multiple disciplines, showing its effectiveness in generating novel, clear, and valid ideas based on both human and model-based evaluation results. Our initial foray into AI-mediated scientific research has important implications for the development of future systems aimed at supporting researchers in their ideation and operationalization of novel work.",
    "authors": [
      "Jinheon Baek",
      "Sujay Kumar Jauhar",
      "Silviu Cucerzan",
      "Sung Ju Hwang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-04-11T13:36:29Z",
    "pdf_url": "https://arxiv.org/pdf/2404.07738v2"
  },
  {
    "arxiv_id": "2404.07461v2",
    "entry_id": "http://arxiv.org/abs/2404.07461v2",
    "title": "An Audit on the Perspectives and Challenges of Hallucinations in NLP",
    "summary": "We audit how hallucination in large language models (LLMs) is characterized in peer-reviewed literature, using a critical examination of 103 publications across NLP research. Through the examination of the literature, we identify a lack of agreement with the term `hallucination' in the field of NLP. Additionally, to compliment our audit, we conduct a survey with 171 practitioners from the field of NLP and AI to capture varying perspectives on hallucination. Our analysis calls for the necessity of explicit definitions and frameworks outlining hallucination within NLP, highlighting potential challenges, and our survey inputs provide a thematic understanding of the influence and ramifications of hallucination in society.",
    "authors": [
      "Pranav Narayanan Venkit",
      "Tatiana Chakravorti",
      "Vipul Gupta",
      "Heidi Biggs",
      "Mukund Srinath",
      "Koustava Goswami",
      "Sarah Rajtmajer",
      "Shomir Wilson"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-11T03:51:29Z",
    "pdf_url": "https://arxiv.org/pdf/2404.07461v2"
  },
  {
    "arxiv_id": "2406.04346v1",
    "entry_id": "http://arxiv.org/abs/2406.04346v1",
    "title": "Automating Patch Set Generation from Code Review Comments Using Large Language Models",
    "summary": "The advent of Large Language Models (LLMs) has revolutionized various domains of artificial intelligence, including the realm of software engineering. In this research, we evaluate the efficacy of pre-trained LLMs in replicating the tasks traditionally performed by developers in response to code review comments. We provide code contexts to five popular LLMs and obtain the suggested code-changes (patch sets) derived from real-world code-review comments. The performance of each model is meticulously assessed by comparing their generated patch sets against the historical data of human-generated patch-sets from the same repositories. This comparative analysis aims to determine the accuracy, relevance, and depth of the LLMs' feedback, thereby evaluating their readiness to support developers in responding to code-review comments.\n  Novelty: This particular research area is still immature requiring a substantial amount of studies yet to be done. No prior research has compared the performance of existing Large Language Models (LLMs) in code-review comments. This in-progress study assesses current LLMs in code review and paves the way for future advancements in automated code quality assurance, reducing context-switching overhead due to interruptions from code change requests.",
    "authors": [
      "Tajmilur Rahman",
      "Rahul Singh",
      "Mir Yousuf Sultan"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-04-10T02:46:08Z",
    "pdf_url": "https://arxiv.org/pdf/2406.04346v1"
  },
  {
    "arxiv_id": "2404.06404v1",
    "entry_id": "http://arxiv.org/abs/2404.06404v1",
    "title": "Apprentices to Research Assistants: Advancing Research with Large Language Models",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools in various research domains. This article examines their potential through a literature review and firsthand experimentation. While LLMs offer benefits like cost-effectiveness and efficiency, challenges such as prompt tuning, biases, and subjectivity must be addressed. The study presents insights from experiments utilizing LLMs for qualitative analysis, highlighting successes and limitations. Additionally, it discusses strategies for mitigating challenges, such as prompt optimization techniques and leveraging human expertise. This study aligns with the 'LLMs as Research Tools' workshop's focus on integrating LLMs into HCI data work critically and ethically. By addressing both opportunities and challenges, our work contributes to the ongoing dialogue on their responsible application in research.",
    "authors": [
      "M. Namvarpour",
      "A. Razi"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-04-09T15:53:06Z",
    "pdf_url": "https://arxiv.org/pdf/2404.06404v1"
  },
  {
    "arxiv_id": "2404.06114v1",
    "entry_id": "http://arxiv.org/abs/2404.06114v1",
    "title": "Communication-Efficient Large-Scale Distributed Deep Learning: A Comprehensive Survey",
    "summary": "With the rapid growth in the volume of data sets, models, and devices in the domain of deep learning, there is increasing attention on large-scale distributed deep learning. In contrast to traditional distributed deep learning, the large-scale scenario poses new challenges that include fault tolerance, scalability of algorithms and infrastructures, and heterogeneity in data sets, models, and resources. Due to intensive synchronization of models and sharing of data across GPUs and computing nodes during distributed training and inference processes, communication efficiency becomes the bottleneck for achieving high performance at a large scale. This article surveys the literature over the period of 2018-2023 on algorithms and technologies aimed at achieving efficient communication in large-scale distributed deep learning at various levels, including algorithms, frameworks, and infrastructures. Specifically, we first introduce efficient algorithms for model synchronization and communication data compression in the context of large-scale distributed training. Next, we introduce efficient strategies related to resource allocation and task scheduling for use in distributed training and inference. After that, we present the latest technologies pertaining to modern communication infrastructures used in distributed deep learning with a focus on examining the impact of the communication overhead in a large-scale and heterogeneous setting. Finally, we conduct a case study on the distributed training of large language models at a large scale to illustrate how to apply these technologies in real cases. This article aims to offer researchers a comprehensive understanding of the current landscape of large-scale distributed deep learning and to reveal promising future research directions toward communication-efficient solutions in this scope.",
    "authors": [
      "Feng Liang",
      "Zhen Zhang",
      "Haifeng Lu",
      "Victor C. M. Leung",
      "Yanyi Guo",
      "Xiping Hu"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2024-04-09T08:35:04Z",
    "pdf_url": "https://arxiv.org/pdf/2404.06114v1"
  },
  {
    "arxiv_id": "2404.05893v5",
    "entry_id": "http://arxiv.org/abs/2404.05893v5",
    "title": "Use of a Structured Knowledge Base Enhances Metadata Curation by Large Language Models",
    "summary": "Metadata play a crucial role in ensuring the findability, accessibility, interoperability, and reusability of datasets. This paper investigates the potential of large language models (LLMs), specifically GPT-4, to improve adherence to metadata standards. We conducted experiments on 200 random data records describing human samples relating to lung cancer from the NCBI BioSample repository, evaluating GPT-4's ability to suggest edits for adherence to metadata standards. We computed the adherence accuracy of field name-field value pairs through a peer review process, and we observed a marginal average improvement in adherence to the standard data dictionary from 79% to 80% (p<0.5). We then prompted GPT-4 with domain information in the form of the textual descriptions of CEDAR templates and recorded a significant improvement to 97% from 79% (p<0.01). These results indicate that, while LLMs may not be able to correct legacy metadata to ensure satisfactory adherence to standards when unaided, they do show promise for use in automated metadata curation when integrated with a structured knowledge base",
    "authors": [
      "Sowmya S. Sundaram",
      "Benjamin Solomon",
      "Avani Khatri",
      "Anisha Laumas",
      "Purvesh Khatri",
      "Mark A. Musen"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2024-04-08T22:29:53Z",
    "pdf_url": "https://arxiv.org/pdf/2404.05893v5"
  },
  {
    "arxiv_id": "2404.05783v2",
    "entry_id": "http://arxiv.org/abs/2404.05783v2",
    "title": "A Survey on Responsible Generative AI: What to Generate and What Not",
    "summary": "In recent years, generative AI (GenAI), like large language models and text-to-image models, has received significant attention across various domains. However, ensuring the responsible generation of content by these models is crucial for their real-world applicability. This raises an interesting question: What should responsible GenAI generate, and what should it not? To answer the question, this paper investigates the practical responsible requirements of both textual and visual generative models, outlining five key considerations: generating truthful content, avoiding toxic content, refusing harmful instruction, leaking no training data-related content, and ensuring generated content identifiable. Specifically, we review recent advancements and challenges in addressing these requirements. Besides, we discuss and emphasize the importance of responsible GenAI across healthcare, education, finance, and artificial general intelligence domains. Through a unified perspective on both textual and visual generative models, this paper aims to provide insights into practical safety-related issues and further benefit the community in building responsible GenAI.",
    "authors": [
      "Jindong Gu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-04-08T17:53:21Z",
    "pdf_url": "https://arxiv.org/pdf/2404.05783v2"
  },
  {
    "arxiv_id": "2404.05399v2",
    "entry_id": "http://arxiv.org/abs/2404.05399v2",
    "title": "SafetyPrompts: a Systematic Review of Open Datasets for Evaluating and Improving Large Language Model Safety",
    "summary": "The last two years have seen a rapid growth in concerns around the safety of large language models (LLMs). Researchers and practitioners have met these concerns by creating an abundance of datasets for evaluating and improving LLM safety. However, much of this work has happened in parallel, and with very different goals in mind, ranging from the mitigation of near-term risks around bias and toxic content generation to the assessment of longer-term catastrophic risk potential. This makes it difficult for researchers and practitioners to find the most relevant datasets for their use case, and to identify gaps in dataset coverage that future work may fill. To remedy these issues, we conduct a first systematic review of open datasets for evaluating and improving LLM safety. We review 144 datasets, which we identified through an iterative and community-driven process over the course of several months. We highlight patterns and trends, such as a trend towards fully synthetic datasets, as well as gaps in dataset coverage, such as a clear lack of non-English and naturalistic datasets. We also examine how LLM safety datasets are used in practice -- in LLM release publications and popular LLM benchmarks -- finding that current evaluation practices are highly idiosyncratic and make use of only a small fraction of available datasets. Our contributions are based on SafetyPrompts.com, a living catalogue of open datasets for LLM safety, which we plan to update continuously as the field of LLM safety develops.",
    "authors": [
      "Paul Röttger",
      "Fabio Pernisi",
      "Bertie Vidgen",
      "Dirk Hovy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-08T10:57:25Z",
    "pdf_url": "https://arxiv.org/pdf/2404.05399v2"
  },
  {
    "arxiv_id": "2404.07236v2",
    "entry_id": "http://arxiv.org/abs/2404.07236v2",
    "title": "Lightweight Deep Learning for Resource-Constrained Environments: A Survey",
    "summary": "Over the past decade, the dominance of deep learning has prevailed across various domains of artificial intelligence, including natural language processing, computer vision, and biomedical signal processing. While there have been remarkable improvements in model accuracy, deploying these models on lightweight devices, such as mobile phones and microcontrollers, is constrained by limited resources. In this survey, we provide comprehensive design guidance tailored for these devices, detailing the meticulous design of lightweight models, compression methods, and hardware acceleration strategies. The principal goal of this work is to explore methods and concepts for getting around hardware constraints without compromising the model's accuracy. Additionally, we explore two notable paths for lightweight deep learning in the future: deployment techniques for TinyML and Large Language Models. Although these paths undoubtedly have potential, they also present significant challenges, encouraging research into unexplored areas.",
    "authors": [
      "Hou-I Liu",
      "Marco Galindo",
      "Hongxia Xie",
      "Lai-Kuan Wong",
      "Hong-Han Shuai",
      "Yung-Hui Li",
      "Wen-Huang Cheng"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-04-08T08:50:09Z",
    "pdf_url": "https://arxiv.org/pdf/2404.07236v2"
  },
  {
    "arxiv_id": "2404.04442v1",
    "entry_id": "http://arxiv.org/abs/2404.04442v1",
    "title": "Exploring Autonomous Agents through the Lens of Large Language Models: A Review",
    "summary": "Large Language Models (LLMs) are transforming artificial intelligence, enabling autonomous agents to perform diverse tasks across various domains. These agents, proficient in human-like text comprehension and generation, have the potential to revolutionize sectors from customer service to healthcare. However, they face challenges such as multimodality, human value alignment, hallucinations, and evaluation. Techniques like prompting, reasoning, tool utilization, and in-context learning are being explored to enhance their capabilities. Evaluation platforms like AgentBench, WebArena, and ToolLLM provide robust methods for assessing these agents in complex scenarios. These advancements are leading to the development of more resilient and capable autonomous agents, anticipated to become integral in our digital lives, assisting in tasks from email responses to disease diagnosis. The future of AI, with LLMs at the forefront, is promising.",
    "authors": [
      "Saikat Barua"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-05T22:59:02Z",
    "pdf_url": "https://arxiv.org/pdf/2404.04442v1"
  },
  {
    "arxiv_id": "2404.04351v2",
    "entry_id": "http://arxiv.org/abs/2404.04351v2",
    "title": "Assisting humans in complex comparisons: automated information comparison at scale",
    "summary": "Generative Large Language Models enable efficient analytics across knowledge domains, rivalling human experts in information comparisons. However, the applications of LLMs for information comparisons face scalability challenges due to the difficulties in maintaining information across large contexts and overcoming model token limitations. To address these challenges, we developed the novel Abstractive Summarization & Criteria-driven Comparison Endpoint (ASC$^2$End) system to automate information comparison at scale. Our system employs Semantic Text Similarity comparisons for generating evidence-supported analyses. We utilize proven data-handling strategies such as abstractive summarization and retrieval augmented generation to overcome token limitations and retain relevant information during model inference. Prompts were designed using zero-shot strategies to contextualize information for improved model reasoning. We evaluated abstractive summarization using ROUGE scoring and assessed the generated comparison quality using survey responses. Models evaluated on the ASC$^2$End system show desirable results providing insights on the expected performance of the system. ASC$^2$End is a novel system and tool that enables accurate, automated information comparison at scale across knowledge domains, overcoming limitations in context length and retrieval.",
    "authors": [
      "Truman Yuen",
      "Graham A. Watt",
      "Yuri Lawryshyn"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-04-05T18:44:54Z",
    "pdf_url": "https://arxiv.org/pdf/2404.04351v2"
  },
  {
    "arxiv_id": "2404.03745v3",
    "entry_id": "http://arxiv.org/abs/2404.03745v3",
    "title": "Fakes of Varying Shades: How Warning Affects Human Perception and Engagement Regarding LLM Hallucinations",
    "summary": "The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N=419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Participants ranked content as truthful in the order of genuine, minor hallucination, and major hallucination, and user engagement behaviors mirrored this pattern. More importantly, we observed that warning improved the detection of hallucination without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations. All survey materials, demographic questions, and post-session questions are available at: https://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials",
    "authors": [
      "Mahjabin Nahar",
      "Haeseung Seo",
      "Eun-Ju Lee",
      "Aiping Xiong",
      "Dongwon Lee"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-04-04T18:34:32Z",
    "pdf_url": "https://arxiv.org/pdf/2404.03745v3"
  },
  {
    "arxiv_id": "2404.02817v5",
    "entry_id": "http://arxiv.org/abs/2404.02817v5",
    "title": "A Survey of Optimization-based Task and Motion Planning: From Classical To Learning Approaches",
    "summary": "Task and Motion Planning (TAMP) integrates high-level task planning and low-level motion planning to equip robots with the autonomy to effectively reason over long-horizon, dynamic tasks. Optimization-based TAMP focuses on hybrid optimization approaches that define goal conditions via objective functions and are capable of handling open-ended goals, robotic dynamics, and physical interaction between the robot and the environment. Therefore, optimization-based TAMP is particularly suited to solve highly complex, contact-rich locomotion and manipulation problems. This survey provides a comprehensive review on optimization-based TAMP, covering (i) planning domain representations, including action description languages and temporal logic, (ii) individual solution strategies for components of TAMP, including AI planning and trajectory optimization (TO), and (iii) the dynamic interplay between logic-based task planning and model-based TO. A particular focus of this survey is to highlight the algorithm structures to efficiently solve TAMP, especially hierarchical and distributed approaches. Additionally, the survey emphasizes the synergy between the classical methods and contemporary learning-based innovations such as large language models. Furthermore, the future research directions for TAMP is discussed in this survey, highlighting both algorithmic and application-specific challenges.",
    "authors": [
      "Zhigen Zhao",
      "Shuo Cheng",
      "Yan Ding",
      "Ziyi Zhou",
      "Shiqi Zhang",
      "Danfei Xu",
      "Ye Zhao"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2024-04-03T15:38:36Z",
    "pdf_url": "https://arxiv.org/pdf/2404.02817v5"
  },
  {
    "arxiv_id": "2404.02548v2",
    "entry_id": "http://arxiv.org/abs/2404.02548v2",
    "title": "AI-Tutoring in Software Engineering Education",
    "summary": "With the rapid advancement of artificial intelligence (AI) in various domains, the education sector is set for transformation. The potential of AI-driven tools in enhancing the learning experience, especially in programming, is immense. However, the scientific evaluation of Large Language Models (LLMs) used in Automated Programming Assessment Systems (APASs) as an AI-Tutor remains largely unexplored. Therefore, there is a need to understand how students interact with such AI-Tutors and to analyze their experiences. In this paper, we conducted an exploratory case study by integrating the GPT-3.5-Turbo model as an AI-Tutor within the APAS Artemis. Through a combination of empirical data collection and an exploratory survey, we identified different user types based on their interaction patterns with the AI-Tutor. Additionally, the findings highlight advantages, such as timely feedback and scalability. However, challenges like generic responses and students' concerns about a learning progress inhibition when using the AI-Tutor were also evident. This research adds to the discourse on AI's role in education.",
    "authors": [
      "Eduard Frankford",
      "Clemens Sauerwein",
      "Patrick Bassner",
      "Stephan Krusche",
      "Ruth Breu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-04-03T08:15:08Z",
    "pdf_url": "https://arxiv.org/pdf/2404.02548v2"
  },
  {
    "arxiv_id": "2404.02330v1",
    "entry_id": "http://arxiv.org/abs/2404.02330v1",
    "title": "Comparative Study of Domain Driven Terms Extraction Using Large Language Models",
    "summary": "Keywords play a crucial role in bridging the gap between human understanding and machine processing of textual data. They are essential to data enrichment because they form the basis for detailed annotations that provide a more insightful and in-depth view of the underlying data. Keyword/domain driven term extraction is a pivotal task in natural language processing, facilitating information retrieval, document summarization, and content categorization. This review focuses on keyword extraction methods, emphasizing the use of three major Large Language Models(LLMs): Llama2-7B, GPT-3.5, and Falcon-7B. We employed a custom Python package to interface with these LLMs, simplifying keyword extraction. Our study, utilizing the Inspec and PubMed datasets, evaluates the performance of these models. The Jaccard similarity index was used for assessment, yielding scores of 0.64 (Inspec) and 0.21 (PubMed) for GPT-3.5, 0.40 and 0.17 for Llama2-7B, and 0.23 and 0.12 for Falcon-7B. This paper underlines the role of prompt engineering in LLMs for better keyword extraction and discusses the impact of hallucination in LLMs on result evaluation. It also sheds light on the challenges in using LLMs for keyword extraction, including model complexity, resource demands, and optimization techniques.",
    "authors": [
      "Sandeep Chataut",
      "Tuyen Do",
      "Bichar Dip Shrestha Gurung",
      "Shiva Aryal",
      "Anup Khanal",
      "Carol Lushbough",
      "Etienne Gnimpieba"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-02T22:04:51Z",
    "pdf_url": "https://arxiv.org/pdf/2404.02330v1"
  },
  {
    "arxiv_id": "2404.08668v3",
    "entry_id": "http://arxiv.org/abs/2404.08668v3",
    "title": "A Survey on Patent Analysis: From NLP to Multimodal AI",
    "summary": "Recent advances in Pretrained Language Models (PLMs) and Large Language Models (LLMs) have demonstrated transformative capabilities across diverse domains. The field of patent analysis and innovation is not an exception, where natural language processing (NLP) techniques presents opportunities to streamline and enhance important tasks -- such as patent classification and patent retrieval -- in the patent cycle. This not only accelerates the efficiency of patent researchers and applicants, but also opens new avenues for technological innovation and discovery. Our survey provides a comprehensive summary of recent NLP-based methods -- including multimodal ones -- in patent analysis. We also introduce a novel taxonomy for categorization based on tasks in the patent life cycle, as well as the specifics of the methods. This interdisciplinary survey aims to serve as a comprehensive resource for researchers and practitioners who work at the intersection of NLP, Multimodal AI, and patent analysis, as well as patent offices to build efficient patent systems.",
    "authors": [
      "Homaira Huda Shomee",
      "Zhu Wang",
      "Sathya N. Ravi",
      "Sourav Medya"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-04-02T20:44:06Z",
    "pdf_url": "https://arxiv.org/pdf/2404.08668v3"
  },
  {
    "arxiv_id": "2404.02062v1",
    "entry_id": "http://arxiv.org/abs/2404.02062v1",
    "title": "Digital Forgetting in Large Language Models: A Survey of Unlearning Methods",
    "summary": "The objective of digital forgetting is, given a model with undesirable knowledge or behavior, obtain a new model where the detected issues are no longer present. The motivations for forgetting include privacy protection, copyright protection, elimination of biases and discrimination, and prevention of harmful content generation. Effective digital forgetting has to be effective (meaning how well the new model has forgotten the undesired knowledge/behavior), retain the performance of the original model on the desirable tasks, and be scalable (in particular forgetting has to be more efficient than retraining from scratch on just the tasks/data to be retained). This survey focuses on forgetting in large language models (LLMs). We first provide background on LLMs, including their components, the types of LLMs, and their usual training pipeline. Second, we describe the motivations, types, and desired properties of digital forgetting. Third, we introduce the approaches to digital forgetting in LLMs, among which unlearning methodologies stand out as the state of the art. Fourth, we provide a detailed taxonomy of machine unlearning methods for LLMs, and we survey and compare current approaches. Fifth, we detail datasets, models and metrics used for the evaluation of forgetting, retaining and runtime. Sixth, we discuss challenges in the area. Finally, we provide some concluding remarks.",
    "authors": [
      "Alberto Blanco-Justicia",
      "Najeeb Jebreel",
      "Benet Manzanares",
      "David Sánchez",
      "Josep Domingo-Ferrer",
      "Guillem Collell",
      "Kuan Eeik Tan"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-04-02T16:01:18Z",
    "pdf_url": "https://arxiv.org/pdf/2404.02062v1"
  },
  {
    "arxiv_id": "2404.02039v4",
    "entry_id": "http://arxiv.org/abs/2404.02039v4",
    "title": "A Survey on Large Language Model-Based Game Agents",
    "summary": "Game environments provide rich, controllable settings that stimulate many aspects of real-world complexity. As such, game agents offer a valuable testbed for exploring capabilities relevant to Artificial General Intelligence. Recently, the emergence of Large Language Models (LLMs) provides new opportunities to endow these agents with generalizable reasoning, memory, and adaptability in complex game environments. This survey offers an up-to-date review of LLM-based game agents (LLMGAs) through a unified reference architecture. At the single-agent level, we synthesize existing studies around three core components: memory, reasoning, and perception-action interfaces, which jointly characterize how language enables agents to perceive, think, and act. At the multi-agent level, we outline how communication protocols and organizational models support coordination, role differentiation, and large-scale social behaviors. To contextualize these designs, we introduce a challenge-centered taxonomy linking six major game genres to their dominant agent requirements, from low-latency control in action games to open-ended goal formation in sandbox worlds. A curated list of related papers is available at https://github.com/git-disl/awesome-LLM-game-agent-papers",
    "authors": [
      "Sihao Hu",
      "Tiansheng Huang",
      "Gaowen Liu",
      "Ramana Rao Kompella",
      "Fatih Ilhan",
      "Selim Furkan Tekin",
      "Yichang Xu",
      "Zachary Yahn",
      "Ling Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-04-02T15:34:18Z",
    "pdf_url": "https://arxiv.org/pdf/2404.02039v4"
  },
  {
    "arxiv_id": "2404.01869v2",
    "entry_id": "http://arxiv.org/abs/2404.01869v2",
    "title": "Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language Models -- A Survey",
    "summary": "Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on sophisticated reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs.",
    "authors": [
      "Philipp Mondorf",
      "Barbara Plank"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-02T11:46:31Z",
    "pdf_url": "https://arxiv.org/pdf/2404.01869v2"
  },
  {
    "arxiv_id": "2404.01206v1",
    "entry_id": "http://arxiv.org/abs/2404.01206v1",
    "title": "Machine Unlearning for Traditional Models and Large Language Models: A Short Survey",
    "summary": "With the implementation of personal data privacy regulations, the field of machine learning (ML) faces the challenge of the \"right to be forgotten\". Machine unlearning has emerged to address this issue, aiming to delete data and reduce its impact on models according to user requests. Despite the widespread interest in machine unlearning, comprehensive surveys on its latest advancements, especially in the field of Large Language Models (LLMs) is lacking. This survey aims to fill this gap by providing an in-depth exploration of machine unlearning, including the definition, classification and evaluation criteria, as well as challenges in different environments and their solutions. Specifically, this paper categorizes and investigates unlearning on both traditional models and LLMs, and proposes methods for evaluating the effectiveness and efficiency of unlearning, and standards for performance measurement. This paper reveals the limitations of current unlearning techniques and emphasizes the importance of a comprehensive unlearning evaluation to avoid arbitrary forgetting. This survey not only summarizes the key concepts of unlearning technology but also points out its prominent issues and feasible directions for future research, providing valuable guidance for scholars in the field.",
    "authors": [
      "Yi Xu"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2024-04-01T16:08:18Z",
    "pdf_url": "https://arxiv.org/pdf/2404.01206v1"
  },
  {
    "arxiv_id": "2404.00929v3",
    "entry_id": "http://arxiv.org/abs/2404.00929v3",
    "title": "A Survey on Multilingual Large Language Models: Corpora, Alignment, and Bias",
    "summary": "Based on the foundation of Large Language Models (LLMs), Multilingual LLMs (MLLMs) have been developed to address the challenges faced in multilingual natural language processing, hoping to achieve knowledge transfer from high-resource languages to low-resource languages. However, significant limitations and challenges still exist, such as language imbalance, multilingual alignment, and inherent bias. In this paper, we aim to provide a comprehensive analysis of MLLMs, delving deeply into discussions surrounding these critical issues. First of all, we start by presenting an overview of MLLMs, covering their evolutions, key techniques, and multilingual capacities. Secondly, we explore the multilingual training corpora of MLLMs and the multilingual datasets oriented for downstream tasks that are crucial to enhance the cross-lingual capability of MLLMs. Thirdly, we survey the state-of-the-art studies of multilingual representations and investigate whether the current MLLMs can learn a universal language representation. Fourthly, we discuss bias on MLLMs, including its categories, evaluation metrics, and debiasing techniques. Finally, we discuss existing challenges and point out promising research directions of MLLMs.",
    "authors": [
      "Yuemei Xu",
      "Ling Hu",
      "Jiayi Zhao",
      "Zihan Qiu",
      "Kexin XU",
      "Yuqi Ye",
      "Hanwen Gu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-04-01T05:13:56Z",
    "pdf_url": "https://arxiv.org/pdf/2404.00929v3"
  },
  {
    "arxiv_id": "2404.01349v2",
    "entry_id": "http://arxiv.org/abs/2404.01349v2",
    "title": "Fairness in Large Language Models: A Taxonomic Survey",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable success across various domains. However, despite their promising performance in numerous real-world applications, most of these algorithms lack fairness considerations. Consequently, they may lead to discriminatory outcomes against certain communities, particularly marginalized populations, prompting extensive study in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in traditional machine learning, entails exclusive backgrounds, taxonomies, and fulfillment techniques. To this end, this survey presents a comprehensive overview of recent advances in the existing literature concerning fair LLMs. Specifically, a brief introduction to LLMs is provided, followed by an analysis of factors contributing to bias in LLMs. Additionally, the concept of fairness in LLMs is discussed categorically, summarizing metrics for evaluating bias in LLMs and existing algorithms for promoting fairness. Furthermore, resources for evaluating bias in LLMs, including toolkits and datasets, are summarized. Finally, existing research challenges and open questions are discussed.",
    "authors": [
      "Zhibo Chu",
      "Zichong Wang",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-31T22:22:53Z",
    "pdf_url": "https://arxiv.org/pdf/2404.01349v2"
  },
  {
    "arxiv_id": "2404.00579v2",
    "entry_id": "http://arxiv.org/abs/2404.00579v2",
    "title": "A Review of Modern Recommender Systems Using Generative Models (Gen-RecSys)",
    "summary": "Traditional recommender systems (RS) typically use user-item rating histories as their main data source. However, deep generative models now have the capability to model and sample from complex data distributions, including user-item interactions, text, images, and videos, enabling novel recommendation tasks. This comprehensive, multidisciplinary survey connects key advancements in RS using Generative Models (Gen-RecSys), covering: interaction-driven generative models; the use of large language models (LLM) and textual data for natural language recommendation; and the integration of multimodal models for generating and processing images/videos in RS. Our work highlights necessary paradigms for evaluating the impact and harm of Gen-RecSys and identifies open challenges. This survey accompanies a tutorial presented at ACM KDD'24, with supporting materials provided at: https://encr.pw/vDhLq.",
    "authors": [
      "Yashar Deldjoo",
      "Zhankui He",
      "Julian McAuley",
      "Anton Korikov",
      "Scott Sanner",
      "Arnau Ramisa",
      "René Vidal",
      "Maheswaran Sathiamoorthy",
      "Atoosa Kasirzadeh",
      "Silvia Milano"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-03-31T06:57:57Z",
    "pdf_url": "https://arxiv.org/pdf/2404.00579v2"
  },
  {
    "arxiv_id": "2404.00399v3",
    "entry_id": "http://arxiv.org/abs/2404.00399v3",
    "title": "Aurora-M: Open Source Continual Pre-training for Multilingual Language and Code",
    "summary": "Pretrained language models are an integral part of AI applications, but their high computational cost for training limits accessibility. Initiatives such as Bloom and StarCoder aim to democratize access to pretrained models for collaborative community development. Despite these efforts, such models encounter challenges such as limited multilingual capabilities, risks of catastrophic forgetting during continual pretraining, and the high costs of training models from scratch, alongside the need to align with AI safety standards and regulatory frameworks.\n  This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435B additional tokens, Aurora-M surpasses 2T tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventional red-teaming considerations, but also with the specific concerns articulated in the Biden-Harris Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence.\n  We evaluate Aurora-M across a wide range of tasks and languages, showcasing its robustness against catastrophic forgetting and its superior performance in multilingual settings, particularly in safety evaluations. We open-source Aurora-M and its variants to encourage responsible open-source development of large language models at https://huggingface.co/aurora-m.",
    "authors": [
      "Taishi Nakamura",
      "Mayank Mishra",
      "Simone Tedeschi",
      "Yekun Chai",
      "Jason T Stillerman",
      "Felix Friedrich",
      "Prateek Yadav",
      "Tanmay Laud",
      "Vu Minh Chien",
      "Terry Yue Zhuo",
      "Diganta Misra",
      "Ben Bogin",
      "Xuan-Son Vu",
      "Marzena Karpinska",
      "Arnav Varma Dantuluri",
      "Wojciech Kusa",
      "Tommaso Furlanello",
      "Rio Yokota",
      "Niklas Muennighoff",
      "Suhas Pai",
      "Tosin Adewumi",
      "Veronika Laippala",
      "Xiaozhe Yao",
      "Adalberto Junior",
      "Alpay Ariyak",
      "Aleksandr Drozd",
      "Jordan Clive",
      "Kshitij Gupta",
      "Liangyu Chen",
      "Qi Sun",
      "Ken Tsui",
      "Noah Persaud",
      "Nour Fahmy",
      "Tianlong Chen",
      "Mohit Bansal",
      "Nicolo Monti",
      "Tai Dang",
      "Ziyang Luo",
      "Tien-Tung Bui",
      "Roberto Navigli",
      "Virendra Mehta",
      "Matthew Blumberg",
      "Victor May",
      "Huu Nguyen",
      "Sampo Pyysalo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-03-30T15:38:54Z",
    "pdf_url": "https://arxiv.org/pdf/2404.00399v3"
  },
  {
    "arxiv_id": "2404.00282v3",
    "entry_id": "http://arxiv.org/abs/2404.00282v3",
    "title": "Survey on Large Language Model-Enhanced Reinforcement Learning: Concept, Taxonomy, and Methods",
    "summary": "With extensive pre-trained knowledge and high-level general capabilities, large language models (LLMs) emerge as a promising avenue to augment reinforcement learning (RL) in aspects such as multi-task learning, sample efficiency, and high-level task planning. In this survey, we provide a comprehensive review of the existing literature in LLM-enhanced RL and summarize its characteristics compared to conventional RL methods, aiming to clarify the research scope and directions for future studies. Utilizing the classical agent-environment interaction paradigm, we propose a structured taxonomy to systematically categorize LLMs' functionalities in RL, including four roles: information processor, reward designer, decision-maker, and generator. For each role, we summarize the methodologies, analyze the specific RL challenges that are mitigated, and provide insights into future directions. Lastly, a comparative analysis of each role, potential applications, prospective opportunities, and challenges of the LLM-enhanced RL are discussed. By proposing this taxonomy, we aim to provide a framework for researchers to effectively leverage LLMs in the RL field, potentially accelerating RL applications in complex applications such as robotics, autonomous driving, and energy systems.",
    "authors": [
      "Yuji Cao",
      "Huan Zhao",
      "Yuheng Cheng",
      "Ting Shu",
      "Yue Chen",
      "Guolong Liu",
      "Gaoqi Liang",
      "Junhua Zhao",
      "Jinyue Yan",
      "Yun Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "published": "2024-03-30T08:28:08Z",
    "pdf_url": "https://arxiv.org/pdf/2404.00282v3"
  },
  {
    "arxiv_id": "2404.00225v1",
    "entry_id": "http://arxiv.org/abs/2404.00225v1",
    "title": "Heterogeneous Contrastive Learning for Foundation Models and Beyond",
    "summary": "In the era of big data and Artificial Intelligence, an emerging paradigm is to utilize contrastive self-supervised learning to model large-scale heterogeneous data. Many existing foundation models benefit from the generalization capability of contrastive self-supervised learning by learning compact and high-quality representations without relying on any label information. Amidst the explosive advancements in foundation models across multiple domains, including natural language processing and computer vision, a thorough survey on heterogeneous contrastive learning for the foundation model is urgently needed. In response, this survey critically evaluates the current landscape of heterogeneous contrastive learning for foundation models, highlighting the open challenges and future trends of contrastive learning. In particular, we first present how the recent advanced contrastive learning-based methods deal with view heterogeneity and how contrastive learning is applied to train and fine-tune the multi-view foundation models. Then, we move to contrastive learning methods for task heterogeneity, including pretraining tasks and downstream tasks, and show how different tasks are combined with contrastive learning loss for different purposes. Finally, we conclude this survey by discussing the open challenges and shedding light on the future directions of contrastive learning.",
    "authors": [
      "Lecheng Zheng",
      "Baoyu Jing",
      "Zihao Li",
      "Hanghang Tong",
      "Jingrui He"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-03-30T02:55:49Z",
    "pdf_url": "https://arxiv.org/pdf/2404.00225v1"
  },
  {
    "arxiv_id": "2404.01332v3",
    "entry_id": "http://arxiv.org/abs/2404.01332v3",
    "title": "Explaining Large Language Models Decisions Using Shapley Values",
    "summary": "The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications - a discrete choice experiment and an investigation of cognitive biases - we demonstrate how the Shapley value method can uncover what we term \"token noise\" effects, a phenomenon where LLM decisions are disproportionately influenced by tokens providing minimal informative content. This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation. Our model-agnostic approach extends its utility to proprietary LLMs, providing a valuable tool for practitioners and researchers to strategically optimize prompts and mitigate apparent cognitive biases. Our findings underscore the need for a more nuanced understanding of the factors driving LLM responses before relying on them as substitutes for human subjects in survey settings. We emphasize the importance of researchers reporting results conditioned on specific prompt templates and exercising caution when drawing parallels between human behavior and LLMs.",
    "authors": [
      "Behnam Mohammadi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-03-29T22:49:43Z",
    "pdf_url": "https://arxiv.org/pdf/2404.01332v3"
  },
  {
    "arxiv_id": "2406.15360v1",
    "entry_id": "http://arxiv.org/abs/2406.15360v1",
    "title": "Generative AI Adoption in Classroom in Context of Technology Acceptance Model (TAM) and the Innovation Diffusion Theory (IDT)",
    "summary": "The burgeoning development of generative artificial intelligence (GenAI) and the widespread adoption of large language models (LLMs) in educational settings have sparked considerable debate regarding their efficacy and acceptability.Despite the potential benefits, the assimilation of these cutting-edge technologies among educators exhibits a broad spectrum of attitudes, from enthusiastic advocacy to profound skepticism.This study aims to dissect the underlying factors influencing educators' perceptions and acceptance of GenAI and LLMs.We conducted a survey among educators and analyzed the data through the frameworks of the Technology Acceptance Model (TAM) and Innovation Diffusion Theory (IDT). Our investigation reveals a strong positive correlation between the perceived usefulness of GenAI tools and their acceptance, underscoring the importance of demonstrating tangible benefits to educators. Additionally, the perceived ease of use emerged as a significant factor, though to a lesser extent, influencing acceptance. Our findings also show that the knowledge and acceptance of these tools is not uniform, suggesting that targeted strategies are required to address the specific needs and concerns of each adopter category to facilitate broader integration of AI tools.in education.",
    "authors": [
      "Aashish Ghimire",
      "John Edwards"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-03-29T22:41:51Z",
    "pdf_url": "https://arxiv.org/pdf/2406.15360v1"
  },
  {
    "arxiv_id": "2403.20252v1",
    "entry_id": "http://arxiv.org/abs/2403.20252v1",
    "title": "Using LLMs to Model the Beliefs and Preferences of Targeted Populations",
    "summary": "We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.",
    "authors": [
      "Keiichi Namikoshi",
      "Alex Filipowicz",
      "David A. Shamma",
      "Rumen Iliev",
      "Candice L. Hogan",
      "Nikos Arechiga"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-03-29T15:58:46Z",
    "pdf_url": "https://arxiv.org/pdf/2403.20252v1"
  },
  {
    "arxiv_id": "2404.01322v1",
    "entry_id": "http://arxiv.org/abs/2404.01322v1",
    "title": "A Review of Multi-Modal Large Language and Vision Models",
    "summary": "Large Language Models (LLMs) have recently emerged as a focal point of research and application, driven by their unprecedented ability to understand and generate text with human-like quality. Even more recently, LLMs have been extended into multi-modal large language models (MM-LLMs) which extends their capabilities to deal with image, video and audio information, in addition to text. This opens up applications like text-to-video generation, image captioning, text-to-speech, and more and is achieved either by retro-fitting an LLM with multi-modal capabilities, or building a MM-LLM from scratch. This paper provides an extensive review of the current state of those LLMs with multi-modal capabilities as well as the very recent MM-LLMs. It covers the historical development of LLMs especially the advances enabled by transformer-based architectures like OpenAI's GPT series and Google's BERT, as well as the role of attention mechanisms in enhancing model performance. The paper includes coverage of the major and most important of the LLMs and MM-LLMs and also covers the techniques of model tuning, including fine-tuning and prompt engineering, which tailor pre-trained models to specific tasks or domains. Ethical considerations and challenges, such as data bias and model misuse, are also analysed to underscore the importance of responsible AI development and deployment. Finally, we discuss the implications of open-source versus proprietary models in AI research. Through this review, we provide insights into the transformative potential of MM-LLMs in various applications.",
    "authors": [
      "Kilian Carolan",
      "Laura Fennelly",
      "Alan F. Smeaton"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-28T15:53:45Z",
    "pdf_url": "https://arxiv.org/pdf/2404.01322v1"
  },
  {
    "arxiv_id": "2403.18969v2",
    "entry_id": "http://arxiv.org/abs/2403.18969v2",
    "title": "A Survey on Large Language Models from Concept to Implementation",
    "summary": "Recent advancements in Large Language Models (LLMs), particularly those built on Transformer architectures, have significantly broadened the scope of natural language processing (NLP) applications, transcending their initial use in chatbot technology. This paper investigates the multifaceted applications of these models, with an emphasis on the GPT series. This exploration focuses on the transformative impact of artificial intelligence (AI) driven tools in revolutionizing traditional tasks like coding and problem-solving, while also paving new paths in research and development across diverse industries. From code interpretation and image captioning to facilitating the construction of interactive systems and advancing computational domains, Transformer models exemplify a synergy of deep learning, data analysis, and neural network design. This survey provides an in-depth look at the latest research in Transformer models, highlighting their versatility and the potential they hold for transforming diverse application sectors, thereby offering readers a comprehensive understanding of the current and future landscape of Transformer-based LLMs in practical applications.",
    "authors": [
      "Chen Wang",
      "Jin Zhao",
      "Jiaqi Gong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IT",
      "cs.LG"
    ],
    "published": "2024-03-27T19:35:41Z",
    "pdf_url": "https://arxiv.org/pdf/2403.18969v2"
  },
  {
    "arxiv_id": "2403.18958v1",
    "entry_id": "http://arxiv.org/abs/2403.18958v1",
    "title": "A State-of-the-practice Release-readiness Checklist for Generative AI-based Software Products",
    "summary": "This paper investigates the complexities of integrating Large Language Models (LLMs) into software products, with a focus on the challenges encountered for determining their readiness for release. Our systematic review of grey literature identifies common challenges in deploying LLMs, ranging from pre-training and fine-tuning to user experience considerations. The study introduces a comprehensive checklist designed to guide practitioners in evaluating key release readiness aspects such as performance, monitoring, and deployment strategies, aiming to enhance the reliability and effectiveness of LLM-based applications in real-world settings.",
    "authors": [
      "Harsh Patel",
      "Dominique Boucher",
      "Emad Fallahzadeh",
      "Ahmed E. Hassan",
      "Bram Adams"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-03-27T19:02:56Z",
    "pdf_url": "https://arxiv.org/pdf/2403.18958v1"
  },
  {
    "arxiv_id": "2403.18539v2",
    "entry_id": "http://arxiv.org/abs/2403.18539v2",
    "title": "Safe and Robust Reinforcement Learning: Principles and Practice",
    "summary": "Reinforcement Learning (RL) has shown remarkable success in solving relatively complex tasks, yet the deployment of RL systems in real-world scenarios poses significant challenges related to safety and robustness. This paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust RL landscape, encompassing algorithmic, ethical, and practical considerations. We conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with RL applications.\n  After discussing and proposing definitions for both safe and robust RL, the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of RL agents. We examine techniques such as uncertainty estimation, optimisation methodologies, exploration-exploitation trade-offs, and adversarial training. Environmental factors, including sim-to-real transfer and domain adaptation, are also scrutinized to understand how RL systems can adapt to diverse and dynamic surroundings. Moreover, human involvement is an integral ingredient of the analysis, acknowledging the broad set of roles that humans can take in this context.\n  Importantly, to aid practitioners in navigating the complexities of safe and robust RL implementation, this paper introduces a practical checklist derived from the synthesized literature. The checklist encompasses critical aspects of algorithm design, training environment considerations, and ethical guidelines. It will serve as a resource for developers and policymakers alike to ensure the responsible deployment of RL systems in many application domains.",
    "authors": [
      "Taku Yamagata",
      "Raul Santos-Rodriguez"
    ],
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "published": "2024-03-27T13:14:29Z",
    "pdf_url": "https://arxiv.org/pdf/2403.18539v2"
  },
  {
    "arxiv_id": "2403.18105v2",
    "entry_id": "http://arxiv.org/abs/2403.18105v2",
    "title": "Large Language Models for Education: A Survey and Outlook",
    "summary": "The advent of Large Language Models (LLMs) has brought in a new era of possibilities in the realm of education. This survey paper summarizes the various technologies of LLMs in educational settings from multifaceted perspectives, encompassing student and teacher assistance, adaptive learning, and commercial tools. We systematically review the technological advancements in each perspective, organize related datasets and benchmarks, and identify the risks and challenges associated with deploying LLMs in education. Furthermore, we outline future research opportunities, highlighting the potential promising directions. Our survey aims to provide a comprehensive technological picture for educators, researchers, and policymakers to harness the power of LLMs to revolutionize educational practices and foster a more effective personalized learning environment.",
    "authors": [
      "Shen Wang",
      "Tianlong Xu",
      "Hang Li",
      "Chaoli Zhang",
      "Joleen Liang",
      "Jiliang Tang",
      "Philip S. Yu",
      "Qingsong Wen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-26T21:04:29Z",
    "pdf_url": "https://arxiv.org/pdf/2403.18105v2"
  },
  {
    "arxiv_id": "2403.16812v2",
    "entry_id": "http://arxiv.org/abs/2403.16812v2",
    "title": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making",
    "summary": "In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans' appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.",
    "authors": [
      "Shuai Ma",
      "Qiaoyi Chen",
      "Xinru Wang",
      "Chengbo Zheng",
      "Zhenhui Peng",
      "Ming Yin",
      "Xiaojuan Ma"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-03-25T14:34:06Z",
    "pdf_url": "https://arxiv.org/pdf/2403.16812v2"
  },
  {
    "arxiv_id": "2403.16303v4",
    "entry_id": "http://arxiv.org/abs/2403.16303v4",
    "title": "Large Language Models in Biomedical and Health Informatics: A Review with Bibliometric Analysis",
    "summary": "Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This study aims to provide a comprehensive overview of LLM applications in BHI, highlighting their transformative potential and addressing the associated ethical and practical challenges. We reviewed 1,698 research articles from January 2022 to December 2023, categorizing them by research themes and diagnostic categories. Additionally, we conducted network analysis to map scholarly collaborations and research dynamics. Our findings reveal a substantial increase in the potential applications of LLMs to a variety of BHI tasks, including clinical decision support, patient interaction, and medical document analysis. Notably, LLMs are expected to be instrumental in enhancing the accuracy of diagnostic tools and patient care protocols. The network analysis highlights dense and dynamically evolving collaborations across institutions, underscoring the interdisciplinary nature of LLM research in BHI. A significant trend was the application of LLMs in managing specific disease categories such as mental health and neurological disorders, demonstrating their potential to influence personalized medicine and public health strategies. LLMs hold promising potential to further transform biomedical research and healthcare delivery. While promising, the ethical implications and challenges of model validation call for rigorous scrutiny to optimize their benefits in clinical settings. This survey serves as a resource for stakeholders in healthcare, including researchers, clinicians, and policymakers, to understand the current state and future potential of LLMs in BHI.",
    "authors": [
      "Huizi Yu",
      "Lizhou Fan",
      "Lingyao Li",
      "Jiayan Zhou",
      "Zihui Ma",
      "Lu Xian",
      "Wenyue Hua",
      "Sijia He",
      "Mingyu Jin",
      "Yongfeng Zhang",
      "Ashvin Gandhi",
      "Xin Ma"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CL",
      "cs.SI"
    ],
    "published": "2024-03-24T21:29:39Z",
    "pdf_url": "https://arxiv.org/pdf/2403.16303v4"
  },
  {
    "arxiv_id": "2403.16289v1",
    "entry_id": "http://arxiv.org/abs/2403.16289v1",
    "title": "Engineering Safety Requirements for Autonomous Driving with Large Language Models",
    "summary": "Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented at a case company and the responsible team evaluated its efficiency.",
    "authors": [
      "Ali Nouri",
      "Beatriz Cabrero-Daniel",
      "Fredrik Törner",
      "Hȧkan Sivencrona",
      "Christian Berger"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-24T20:40:51Z",
    "pdf_url": "https://arxiv.org/pdf/2403.16289v1"
  },
  {
    "arxiv_id": "2403.15938v1",
    "entry_id": "http://arxiv.org/abs/2403.15938v1",
    "title": "LlamBERT: Large-scale low-cost data annotation in NLP",
    "summary": "Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.",
    "authors": [
      "Bálint Csanády",
      "Lajos Muzsai",
      "Péter Vedres",
      "Zoltán Nádasdy",
      "András Lukács"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-03-23T21:54:34Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15938v1"
  },
  {
    "arxiv_id": "2403.15852v2",
    "entry_id": "http://arxiv.org/abs/2403.15852v2",
    "title": "SOEN-101: Code Generation by Emulating Software Process Models Using Large Language Model Agents",
    "summary": "Software process models are essential to facilitate collaboration and communication among software teams to solve complex development tasks. Inspired by these software engineering practices, we present FlowGen - a code generation framework that emulates software process models based on multiple Large Language Model (LLM) agents. We emulate three process models, FlowGenWaterfall, FlowGenTDD, and FlowGenScrum, by assigning LLM agents to embody roles (i.e., requirement engineer, architect, developer, tester, and scrum master) that correspond to everyday development activities and organize their communication patterns. The agents work collaboratively using chain-of-thought and prompt composition with continuous self-refinement to improve the code quality. We use GPT3.5 as our underlying LLM and several baselines (RawGPT, CodeT, Reflexion) to evaluate code generation on four benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Our findings show that FlowGenScrum excels compared to other process models, achieving a Pass@1 of 75.2, 65.5, 82.5, and 56.7 in HumanEval, HumanEval-ET, MBPP, and MBPP-ET, respectively (an average of 15% improvement over RawGPT). Compared with other state-of-the-art techniques, FlowGenScrum achieves a higher Pass@1 in MBPP compared to CodeT, with both outperforming Reflexion. Notably, integrating CodeT into FlowGenScrum resulted in statistically significant improvements, achieving the highest Pass@1 scores. Our analysis also reveals that the development activities impacted code smell and exception handling differently, with design and code review adding more exception handling and reducing code smells. Finally, FlowGen models maintain stable Pass@1 scores across GPT3.5 versions and temperature values, highlighting the effectiveness of software process models in enhancing the quality and stability of LLM-generated code.",
    "authors": [
      "Feng Lin",
      "Dong Jae Kim",
      "Tse-Husn",
      "Chen"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-03-23T14:04:48Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15852v2"
  },
  {
    "arxiv_id": "2403.15779v1",
    "entry_id": "http://arxiv.org/abs/2403.15779v1",
    "title": "The Frontier of Data Erasure: Machine Unlearning for Large Language Models",
    "summary": "Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdles in preserving model integrity, avoiding excessive or insufficient data removal, and ensuring consistent outputs, underlining the role of machine unlearning in advancing responsible, ethical AI.",
    "authors": [
      "Youyang Qu",
      "Ming Ding",
      "Nan Sun",
      "Kanchana Thilakarathna",
      "Tianqing Zhu",
      "Dusit Niyato"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-23T09:26:15Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15779v1"
  },
  {
    "arxiv_id": "2403.15587v2",
    "entry_id": "http://arxiv.org/abs/2403.15587v2",
    "title": "Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges",
    "summary": "Social Media and Internet have the potential to be exploited as a source of opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a methodology able to infer opinions and decisions from plain texts, such as reviews published in social media platforms, by means of Sentiment Analysis. Currently, the emergence and potential of Large Language Models (LLMs) lead us to explore new scenarios of automatically understand written texts, also known as natural language processing. This paper analyzes the use of ChatGPT based on prompt design strategies to assist in CDM processes to extract opinions and make decisions. We integrate ChatGPT in CDM processes as a flexible tool that infer the opinions expressed in texts, providing numerical or linguistic evaluations where the decision making models are based on the prompt design strategies. We include a multi-criteria decision making scenario with a category ontology for criteria. We also consider ChatGPT as an end-to-end CDM model able to provide a general opinion and score on the alternatives. We conduct empirical experiments on real data extracted from TripAdvisor, the TripR-2020Large dataset. The analysis of results show a promising branch for developing quality decision making models using ChatGPT. Finally, we discuss the challenges of consistency, sensitivity and explainability associated to the use of LLMs in CDM processes, raising open questions for future studies.",
    "authors": [
      "David Herrera-Poyatos",
      "Cristina Zuheros",
      "Rosana Montes",
      "Francisco Herrera"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-22T19:21:44Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15587v2"
  },
  {
    "arxiv_id": "2403.15586v1",
    "entry_id": "http://arxiv.org/abs/2403.15586v1",
    "title": "Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors",
    "summary": "The rapid advancement of artificial intelligence (AI) and the expanding integration of large language models (LLMs) have ignited a debate about their application in education. This study delves into university instructors' experiences and attitudes toward AI language models, filling a gap in the literature by analyzing educators' perspectives on AI's role in the classroom and its potential impacts on teaching and learning. The objective of this research is to investigate the level of awareness, overall sentiment towardsadoption, and the factors influencing these attitudes for LLMs and generative AI-based tools in higher education. Data was collected through a survey using a Likert scale, which was complemented by follow-up interviews to gain a more nuanced understanding of the instructors' viewpoints. The collected data was processed using statistical and thematic analysis techniques. Our findings reveal that educators are increasingly aware of and generally positive towards these tools. We find no correlation between teaching style and attitude toward generative AI. Finally, while CS educators show far more confidence in their technical understanding of generative AI tools and more positivity towards them than educators in other fields, they show no more confidence in their ability to detect AI-generated work.",
    "authors": [
      "Aashish Ghimire",
      "James Prather",
      "John Edwards"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-22T19:21:29Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15586v1"
  },
  {
    "arxiv_id": "2403.15529v2",
    "entry_id": "http://arxiv.org/abs/2403.15529v2",
    "title": "LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers",
    "summary": "Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called \\textbf{\\textit{LimGen}}, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at \\url{https://github.com/arbmf/LimGen}.",
    "authors": [
      "Abdur Rahman Bin Md Faizullah",
      "Ashok Urlana",
      "Rahul Mishra"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-03-22T17:31:43Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15529v2"
  },
  {
    "arxiv_id": "2403.15274v2",
    "entry_id": "http://arxiv.org/abs/2403.15274v2",
    "title": "Bioinformatics and Biomedical Informatics with ChatGPT: Year One Review",
    "summary": "The year 2023 marked a significant surge in the exploration of applying large language model (LLM) chatbots, notably ChatGPT, across various disciplines. We surveyed the applications of ChatGPT in bioinformatics and biomedical informatics throughout the year, covering omics, genetics, biomedical text mining, drug discovery, biomedical image understanding, bioinformatics programming, and bioinformatics education. Our survey delineates the current strengths and limitations of this chatbot in bioinformatics and offers insights into potential avenues for future developments.",
    "authors": [
      "Jinge Wang",
      "Zien Cheng",
      "Qiuming Yao",
      "Li Liu",
      "Dong Xu",
      "Gangqing Hu"
    ],
    "categories": [
      "q-bio.OT",
      "cs.AI"
    ],
    "published": "2024-03-22T15:16:23Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15274v2"
  },
  {
    "arxiv_id": "2405.01398v1",
    "entry_id": "http://arxiv.org/abs/2405.01398v1",
    "title": "Advancing Frontiers in SLAM: A Survey of Symbolic Representation and Human-Machine Teaming in Environmental Mapping",
    "summary": "This survey paper presents a comprehensive overview of the latest advancements in the field of Simultaneous Localization and Mapping (SLAM) with a focus on the integration of symbolic representation of environment features. The paper synthesizes research trends in multi-agent systems (MAS) and human-machine teaming, highlighting their applications in both symbolic and sub-symbolic SLAM tasks. The survey emphasizes the evolution and significance of ontological designs and symbolic reasoning in creating sophisticated 2D and 3D maps of various environments. Central to this review is the exploration of different architectural approaches in SLAM, with a particular interest in the functionalities and applications of edge and control agent architectures in MAS settings. This study acknowledges the growing demand for enhanced human-machine collaboration in mapping tasks and examines how these collaborative efforts improve the accuracy and efficiency of environmental mapping",
    "authors": [
      "Brandon Curtis Colelough"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-22T00:48:48Z",
    "pdf_url": "https://arxiv.org/pdf/2405.01398v1"
  },
  {
    "arxiv_id": "2403.14608v7",
    "entry_id": "http://arxiv.org/abs/2403.14608v7",
    "title": "Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey",
    "summary": "Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large model to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as a valuable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed ......",
    "authors": [
      "Zeyu Han",
      "Chao Gao",
      "Jinyang Liu",
      "Jeff Zhang",
      "Sai Qian Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-03-21T17:55:50Z",
    "pdf_url": "https://arxiv.org/pdf/2403.14608v7"
  },
  {
    "arxiv_id": "2403.14469v1",
    "entry_id": "http://arxiv.org/abs/2403.14469v1",
    "title": "ChatGPT Alternative Solutions: Large Language Models Survey",
    "summary": "In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights. A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift in the way we create and employ AI algorithms. Given this swift-paced technical evolution, our survey embarks on a journey to encapsulate the recent strides made in the world of LLMs. Through an exploration of the background, key discoveries, and prevailing methodologies, we offer an up-to-the-minute review of the literature. By examining multiple LLM models, our paper not only presents a comprehensive overview but also charts a course that identifies existing challenges and points toward potential future research trajectories. This survey furnishes a well-rounded perspective on the current state of generative AI, shedding light on opportunities for further exploration, enhancement, and innovation.",
    "authors": [
      "Hanieh Alipour",
      "Nick Pendar",
      "Kohinoor Roy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-21T15:16:50Z",
    "pdf_url": "https://arxiv.org/pdf/2403.14469v1"
  },
  {
    "arxiv_id": "2403.14298v1",
    "entry_id": "http://arxiv.org/abs/2403.14298v1",
    "title": "From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora",
    "summary": "Social media platforms are online fora where users engage in discussions, share content, and build connections. This review explores the dynamics of social interactions, user-generated contents, and biases within the context of social media analysis (analyzing works that use the tools offered by complex network analysis and natural language processing) through the lens of three key points of view: online debates, online support, and human-AI interactions. On the one hand, we delineate the phenomenon of online debates, where polarization, misinformation, and echo chamber formation often proliferate, driven by algorithmic biases and extreme mechanisms of homophily. On the other hand, we explore the emergence of online support groups through users' self-disclosure and social support mechanisms. Online debates and support mechanisms present a duality of both perils and possibilities within social media; perils of segregated communities and polarized debates, and possibilities of empathy narratives and self-help groups. This dichotomy also extends to a third perspective: users' reliance on AI-generated content, such as the ones produced by Large Language Models, which can manifest both human biases hidden in training sets and non-human biases that emerge from their artificial neural architectures. Analyzing interdisciplinary approaches, we aim to deepen the understanding of the complex interplay between social interactions, user-generated content, and biases within the realm of social media ecosystems.",
    "authors": [
      "Virginia Morini",
      "Valentina Pansanella",
      "Katherine Abramski",
      "Erica Cau",
      "Andrea Failla",
      "Salvatore Citraro",
      "Giulio Rossetti"
    ],
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-03-21T11:04:41Z",
    "pdf_url": "https://arxiv.org/pdf/2403.14298v1"
  },
  {
    "arxiv_id": "2403.14274v4",
    "entry_id": "http://arxiv.org/abs/2403.14274v4",
    "title": "Multi-role Consensus through LLMs Discussions for Vulnerability Detection",
    "summary": "Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. To this end, this paper introduces a multi-role approach to employ LLMs to act as different roles simulating a real-life code review process and engaging in discussions toward a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of this approach indicates a 13.48% increase in the precision rate, an 18.25% increase in the recall rate, and a 16.13% increase in the F1 score.",
    "authors": [
      "Zhenyu Mao",
      "Jialong Li",
      "Dongming Jin",
      "Munan Li",
      "Kenji Tei"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-03-21T10:28:18Z",
    "pdf_url": "https://arxiv.org/pdf/2403.14274v4"
  },
  {
    "arxiv_id": "2403.14734v5",
    "entry_id": "http://arxiv.org/abs/2403.14734v5",
    "title": "A Survey of Neural Code Intelligence: Paradigms, Advances and Beyond",
    "summary": "Neural Code Intelligence -- leveraging deep learning to understand, generate, and optimize code -- holds immense potential for transformative impacts on the whole society. Bridging the gap between Natural Language and Programming Language, this domain has drawn significant attention from researchers in both research communities over the past few years. This survey presents a systematic and chronological review of the advancements in code intelligence, encompassing over 50 representative models and their variants, more than 20 categories of tasks, and an extensive coverage of over 680 related works. We follow the historical progression to trace the paradigm shifts across different research phases (e.g., from modeling code with recurrent neural networks to the era of Large Language Models). Concurrently, we highlight the major technical transitions in models, tasks, and evaluations spanning through different stages. For applications, we also observe a co-evolving shift. It spans from initial endeavors to tackling specific scenarios, through exploring a diverse array of tasks during its rapid expansion, to currently focusing on tackling increasingly complex and varied real-world challenges. Building on our examination of the developmental trajectories, we further investigate the emerging synergies between code intelligence and broader machine intelligence, uncovering new cross-domain opportunities and illustrating the substantial influence of code intelligence across various domains. Finally, we delve into both the opportunities and challenges associated with this field, alongside elucidating our insights on the most promising research directions. An ongoing, dynamically updated project and resources associated with this survey have been released at https://github.com/QiushiSun/Awesome-Code-Intelligence.",
    "authors": [
      "Qiushi Sun",
      "Zhirui Chen",
      "Fangzhi Xu",
      "Kanzhi Cheng",
      "Chang Ma",
      "Zhangyue Yin",
      "Jianing Wang",
      "Chengcheng Han",
      "Renyu Zhu",
      "Shuai Yuan",
      "Qipeng Guo",
      "Xipeng Qiu",
      "Pengcheng Yin",
      "Xiaoli Li",
      "Fei Yuan",
      "Lingpeng Kong",
      "Xiang Li",
      "Zhiyong Wu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "published": "2024-03-21T08:54:56Z",
    "pdf_url": "https://arxiv.org/pdf/2403.14734v5"
  },
  {
    "arxiv_id": "2403.14151v1",
    "entry_id": "http://arxiv.org/abs/2403.14151v1",
    "title": "Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond",
    "summary": "Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related recommendation, trajectory classification, travel time estimation, anomaly detection, and mobility generation). Notably, we encapsulate recent advancements in Large Language Models (LLMs) that hold the potential to augment trajectory computing. Additionally, we summarize application scenarios, public datasets, and toolkits. Finally, we outline current challenges in DL4Traj research and propose future directions. Relevant papers and open-source resources have been collated and are continuously updated at: \\href{https://github.com/yoshall/Awesome-Trajectory-Computing}{DL4Traj Repo}.",
    "authors": [
      "Wei Chen",
      "Yuxuan Liang",
      "Yuanshao Zhu",
      "Yanchuan Chang",
      "Kang Luo",
      "Haomin Wen",
      "Lei Li",
      "Yanwei Yu",
      "Qingsong Wen",
      "Chao Chen",
      "Kai Zheng",
      "Yunjun Gao",
      "Xiaofang Zhou",
      "Yu Zheng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY",
      "cs.DB"
    ],
    "published": "2024-03-21T05:57:27Z",
    "pdf_url": "https://arxiv.org/pdf/2403.14151v1"
  },
  {
    "arxiv_id": "2404.00027v5",
    "entry_id": "http://arxiv.org/abs/2404.00027v5",
    "title": "LLMs as Writing Assistants: Exploring Perspectives on Sense of Ownership and Reasoning",
    "summary": "Sense of ownership in writing confines our investment of thoughts, time, and contribution, leading to attachment to the output. However, using writing assistants introduces a mental dilemma, as some content isn't directly our creation. For instance, we tend to credit Large Language Models (LLMs) more in creative tasks, even though all tasks are equal for them. Additionally, while we may not claim complete ownership of LLM-generated content, we freely claim authorship. We conduct a short survey to examine these issues and understand underlying cognitive processes in order to gain a better knowledge of human-computer interaction in writing and improve writing aid systems.",
    "authors": [
      "Azmine Toushik Wasi",
      "Mst Rafia Islam",
      "Raima Islam"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-03-20T21:06:42Z",
    "pdf_url": "https://arxiv.org/pdf/2404.00027v5"
  },
  {
    "arxiv_id": "2404.00026v5",
    "entry_id": "http://arxiv.org/abs/2404.00026v5",
    "title": "Ink and Individuality: Crafting a Personalised Narrative in the Age of LLMs",
    "summary": "Individuality and personalization comprise the distinctive characteristics that make each writer unique and influence their words in order to effectively engage readers while conveying authenticity. However, our growing reliance on LLM-based writing assistants risks compromising our creativity and individuality over time. We often overlook the negative impacts of this trend on our creativity and uniqueness, despite the possible consequences. This study investigates these concerns by performing a brief survey to explore different perspectives and concepts, as well as trying to understand people's viewpoints, in conjunction with past studies in the area. Addressing these issues is essential for improving human-computer interaction systems and enhancing writing assistants for personalization and individuality.",
    "authors": [
      "Azmine Toushik Wasi",
      "Raima Islam",
      "Mst Rafia Islam"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-03-20T21:02:16Z",
    "pdf_url": "https://arxiv.org/pdf/2404.00026v5"
  },
  {
    "arxiv_id": "2403.15458v1",
    "entry_id": "http://arxiv.org/abs/2403.15458v1",
    "title": "Fine-Tuning Pre-trained Language Models to Detect In-Game Trash Talks",
    "summary": "Common problems in playing online mobile and computer games were related to toxic behavior and abusive communication among players. Based on different reports and studies, the study also discusses the impact of online hate speech and toxicity on players' in-game performance and overall well-being. This study investigates the capability of pre-trained language models to classify or detect trash talk or toxic in-game messages The study employs and evaluates the performance of pre-trained BERT and GPT language models in detecting toxicity within in-game chats. Using publicly available APIs, in-game chat data from DOTA 2 game matches were collected, processed, reviewed, and labeled as non-toxic, mild (toxicity), and toxic. The study was able to collect around two thousand in-game chats to train and test BERT (Base-uncased), BERT (Large-uncased), and GPT-3 models. Based on the three models' state-of-the-art performance, this study concludes pre-trained language models' promising potential for addressing online hate speech and in-game insulting trash talk.",
    "authors": [
      "Daniel Fesalbon",
      "Arvin De La Cruz",
      "Marvin Mallari",
      "Nelson Rodelas"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-03-19T11:36:53Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15458v1"
  },
  {
    "arxiv_id": "2403.12196v4",
    "entry_id": "http://arxiv.org/abs/2403.12196v4",
    "title": "Leveraging Large Language Models to Detect npm Malicious Packages",
    "summary": "Existing malicious code detection techniques demand the integration of multiple tools to detect different malware patterns, often suffering from high misclassification rates. Therefore, malicious code detection techniques could be enhanced by adopting advanced, more automated approaches to achieve high accuracy and a low misclassification rate. The goal of this study is to aid security analysts in detecting malicious packages by empirically studying the effectiveness of Large Language Models (LLMs) in detecting malicious code. We present SocketAI, a malicious code review workflow to detect malicious code. To evaluate the effectiveness of SocketAI, we leverage a benchmark dataset of 5,115 npm packages, of which 2,180 packages have malicious code. We conducted a baseline comparison of GPT-3 and GPT-4 models with the state-of-the-art CodeQL static analysis tool, using 39 custom CodeQL rules developed in prior research to detect malicious Javascript code. We also compare the effectiveness of static analysis as a pre-screener with SocketAI workflow, measuring the number of files that need to be analyzed. and the associated costs. Additionally, we performed a qualitative study to understand the types of malicious activities detected or missed by our workflow. Our baseline comparison demonstrates a 16% and 9% improvement over static analysis in precision and F1 scores, respectively. GPT-4 achieves higher accuracy with 99% precision and 97% F1 scores, while GPT-3 offers a more cost-effective balance at 91% precision and 94% F1 scores. Pre-screening files with a static analyzer reduces the number of files requiring LLM analysis by 77.9% and decreases costs by 60.9% for GPT-3 and 76.1% for GPT-4. Our qualitative analysis identified data theft, execution of arbitrary code, and suspicious domain categories as the top detected malicious packages.",
    "authors": [
      "Nusrat Zahan",
      "Philipp Burckhardt",
      "Mikola Lysenko",
      "Feross Aboukhadijeh",
      "Laurie Williams"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-03-18T19:10:12Z",
    "pdf_url": "https://arxiv.org/pdf/2403.12196v4"
  },
  {
    "arxiv_id": "2403.12027v4",
    "entry_id": "http://arxiv.org/abs/2403.12027v4",
    "title": "From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models",
    "summary": "Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models, have revolutionized various natural language processing tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. We review fundamental building blocks crucial for studying chart understanding tasks. Additionally, we explore various tasks and their evaluation metrics and sources of both charts and textual inputs. Various modeling strategies are then examined, encompassing both classification-based and generation-based approaches, along with tool augmentation techniques that enhance chart understanding performance. Furthermore, we discuss the state-of-the-art performance of each task and discuss how we can improve the performance. Challenges and future directions are addressed, highlighting the importance of several topics, such as domain-specific charts, lack of efforts in developing evaluation metrics, and agent-oriented settings. This survey paper serves as a comprehensive resource for researchers and practitioners in the fields of natural language processing, computer vision, and data analysis, providing valuable insights and directions for future research in chart understanding leveraging large foundation models. The studies mentioned in this paper, along with emerging new research, will be continually updated at: https://github.com/khuangaf/Awesome-Chart-Understanding.",
    "authors": [
      "Kung-Hsiang Huang",
      "Hou Pong Chan",
      "Yi R. Fung",
      "Haoyi Qiu",
      "Mingyang Zhou",
      "Shafiq Joty",
      "Shih-Fu Chang",
      "Heng Ji"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-03-18T17:57:09Z",
    "pdf_url": "https://arxiv.org/pdf/2403.12027v4"
  },
  {
    "arxiv_id": "2403.12025v2",
    "entry_id": "http://arxiv.org/abs/2403.12025v2",
    "title": "A Toolbox for Surfacing Health Equity Harms and Biases in Large Language Models",
    "summary": "Large language models (LLMs) hold promise to serve complex health information needs but also have the potential to introduce harm and exacerbate health disparities. Reliably evaluating equity-related model failures is a critical step toward developing systems that promote health equity. We present resources and methodologies for surfacing biases with potential to precipitate equity-related harms in long-form, LLM-generated answers to medical questions and conduct a large-scale empirical case study with the Med-PaLM 2 LLM. Our contributions include a multifactorial framework for human assessment of LLM-generated answers for biases, and EquityMedQA, a collection of seven datasets enriched for adversarial queries. Both our human assessment framework and dataset design process are grounded in an iterative participatory approach and review of Med-PaLM 2 answers. Through our empirical study, we find that our approach surfaces biases that may be missed via narrower evaluation approaches. Our experience underscores the importance of using diverse assessment methodologies and involving raters of varying backgrounds and expertise. While our approach is not sufficient to holistically assess whether the deployment of an AI system promotes equitable health outcomes, we hope that it can be leveraged and built upon towards a shared goal of LLMs that promote accessible and equitable healthcare.",
    "authors": [
      "Stephen R. Pfohl",
      "Heather Cole-Lewis",
      "Rory Sayres",
      "Darlene Neal",
      "Mercy Asiedu",
      "Awa Dieng",
      "Nenad Tomasev",
      "Qazi Mamunur Rashid",
      "Shekoofeh Azizi",
      "Negar Rostamzadeh",
      "Liam G. McCoy",
      "Leo Anthony Celi",
      "Yun Liu",
      "Mike Schaekermann",
      "Alanna Walton",
      "Alicia Parrish",
      "Chirag Nagpal",
      "Preeti Singh",
      "Akeiylah Dewitt",
      "Philip Mansfield",
      "Sushant Prakash",
      "Katherine Heller",
      "Alan Karthikesalingam",
      "Christopher Semturs",
      "Joelle Barral",
      "Greg Corrado",
      "Yossi Matias",
      "Jamila Smith-Loud",
      "Ivor Horn",
      "Karan Singhal"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-03-18T17:56:37Z",
    "pdf_url": "https://arxiv.org/pdf/2403.12025v2"
  },
  {
    "arxiv_id": "2403.11894v4",
    "entry_id": "http://arxiv.org/abs/2403.11894v4",
    "title": "From Explainable to Interpretable Deep Learning for Natural Language Processing in Healthcare: How Far from Reality?",
    "summary": "Deep learning (DL) has substantially enhanced natural language processing (NLP) in healthcare research. However, the increasing complexity of DL-based NLP necessitates transparent model interpretability, or at least explainability, for reliable decision-making. This work presents a thorough scoping review of explainable and interpretable DL in healthcare NLP. The term \"eXplainable and Interpretable Artificial Intelligence\" (XIAI) is introduced to distinguish XAI from IAI. Different models are further categorized based on their functionality (model-, input-, output-based) and scope (local, global). Our analysis shows that attention mechanisms are the most prevalent emerging IAI technique. The use of IAI is growing, distinguishing it from XAI. The major challenges identified are that most XIAI does not explore \"global\" modelling processes, the lack of best practices, and the lack of systematic evaluation and benchmarks. One important opportunity is to use attention mechanisms to enhance multi-modal XIAI for personalized medicine. Additionally, combining DL with causal logic holds promise. Our discussion encourages the integration of XIAI in Large Language Models (LLMs) and domain-specific smaller models. In conclusion, XIAI adoption in healthcare requires dedicated in-house expertise. Collaboration with domain experts, end-users, and policymakers can lead to ready-to-use XIAI methods across NLP and medical tasks. While challenges exist, XIAI techniques offer a valuable foundation for interpretable NLP algorithms in healthcare.",
    "authors": [
      "Guangming Huang",
      "Yingya Li",
      "Shoaib Jameel",
      "Yunfei Long",
      "Giorgos Papanastasiou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-03-18T15:53:33Z",
    "pdf_url": "https://arxiv.org/pdf/2403.11894v4"
  },
  {
    "arxiv_id": "2403.14709v1",
    "entry_id": "http://arxiv.org/abs/2403.14709v1",
    "title": "ClimateQ&A: Bridging the gap between climate scientists and the general public",
    "summary": "This research paper investigates public views on climate change and biodiversity loss by analyzing questions asked to the ClimateQ&A platform. ClimateQ&A is a conversational agent that uses LLMs to respond to queries based on over 14,000 pages of scientific literature from the IPCC and IPBES reports. Launched online in March 2023, the tool has gathered over 30,000 questions, mainly from a French audience. Its chatbot interface allows for the free formulation of questions related to nature*. While its main goal is to make nature science more accessible, it also allows for the collection and analysis of questions and their themes. Unlike traditional surveys involving closed questions, this novel method offers a fresh perspective on individual interrogations about nature. Running NLP clustering algorithms on a sample of 3,425 questions, we find that a significant 25.8% inquire about how climate change and biodiversity loss will affect them personally (e.g., where they live or vacation, their consumption habits) and the specific impacts of their actions on nature (e.g., transportation or food choices). This suggests that traditional methods of surveying may not identify all existing knowledge gaps, and that relying solely on IPCC and IPBES reports may not address all individual inquiries about climate and biodiversity, potentially affecting public understanding and action on these issues. *we use 'nature' as an umbrella term for 'climate change' and 'biodiversity loss'",
    "authors": [
      "Natalia De La Calzada",
      "Théo Alves Da Costa",
      "Annabelle Blangero",
      "Nicolas Chesneau"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-03-18T08:16:02Z",
    "pdf_url": "https://arxiv.org/pdf/2403.14709v1"
  },
  {
    "arxiv_id": "2403.13843v2",
    "entry_id": "http://arxiv.org/abs/2403.13843v2",
    "title": "Machine Learning and Transformers for Thyroid Carcinoma Diagnosis: A Review",
    "summary": "The growing interest in developing smart diagnostic systems to help medical experts process extensive data for treating incurable diseases has been notable. In particular, the challenge of identifying thyroid cancer (TC) has seen progress with the use of machine learning (ML) and big data analysis, incorporating Transformers to evaluate TC prognosis and determine the risk of malignancy in individuals. This review article presents a summary of various studies on AI-based approaches, especially those employing Transformers, for diagnosing TC. It introduces a new categorization system for these methods based on artificial intelligence (AI) algorithms, the goals of the framework, and the computing environments used. Additionally, it scrutinizes and contrasts the available TC datasets by their features. The paper highlights the importance of AI instruments in aiding the diagnosis and treatment of TC through supervised, unsupervised, or mixed approaches, with a special focus on the ongoing importance of Transformers and large language models (LLMs) in medical diagnostics and disease management. It further discusses the progress made and the continuing obstacles in this area. Lastly, it explores future directions and focuses within this research field.",
    "authors": [
      "Yassine Habchi",
      "Hamza Kheddar",
      "Yassine Himeur",
      "Mohamed Chahine Ghanem"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.IV"
    ],
    "published": "2024-03-17T17:45:04Z",
    "pdf_url": "https://arxiv.org/pdf/2403.13843v2"
  },
  {
    "arxiv_id": "2403.11114v1",
    "entry_id": "http://arxiv.org/abs/2403.11114v1",
    "title": "Phasic Diversity Optimization for Population-Based Reinforcement Learning",
    "summary": "Reviewing the previous work of diversity Rein-forcement Learning,diversity is often obtained via an augmented loss function,which requires a balance between reward and diversity.Generally,diversity optimization algorithms use Multi-armed Bandits algorithms to select the coefficient in the pre-defined space. However, the dynamic distribution of reward signals for MABs or the conflict between quality and diversity limits the performance of these methods. We introduce the Phasic Diversity Optimization (PDO) algorithm, a Population-Based Training framework that separates reward and diversity training into distinct phases instead of optimizing a multi-objective function. In the auxiliary phase, agents with poor performance diversified via determinants will not replace the better agents in the archive. The decoupling of reward and diversity allows us to use an aggressive diversity optimization in the auxiliary phase without performance degradation. Furthermore, we construct a dogfight scenario for aerial agents to demonstrate the practicality of the PDO algorithm. We introduce two implementations of PDO archive and conduct tests in the newly proposed adversarial dogfight and MuJoCo simulations. The results show that our proposed algorithm achieves better performance than baselines.",
    "authors": [
      "Jingcheng Jiang",
      "Haiyin Piao",
      "Yu Fu",
      "Yihang Hao",
      "Chuanlu Jiang",
      "Ziqi Wei",
      "Xin Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-03-17T06:41:09Z",
    "pdf_url": "https://arxiv.org/pdf/2403.11114v1"
  },
  {
    "arxiv_id": "2403.11103v2",
    "entry_id": "http://arxiv.org/abs/2403.11103v2",
    "title": "ProgGen: Generating Named Entity Recognition Datasets Step-by-step with Self-Reflexive Large Language Models",
    "summary": "Although Large Language Models (LLMs) exhibit remarkable adaptability across domains, these models often fall short in structured knowledge extraction tasks such as named entity recognition (NER). This paper explores an innovative, cost-efficient strategy to harness LLMs with modest NER capabilities for producing superior NER datasets. Our approach diverges from the basic class-conditional prompts by instructing LLMs to self-reflect on the specific domain, thereby generating domain-relevant attributes (such as category and emotions for movie reviews), which are utilized for creating attribute-rich training data. Furthermore, we preemptively generate entity terms and then develop NER context data around these entities, effectively bypassing the LLMs' challenges with complex structures. Our experiments across both general and niche domains reveal significant performance enhancements over conventional data generation methods while being more cost-effective than existing alternatives.",
    "authors": [
      "Yuzhao Heng",
      "Chunyuan Deng",
      "Yitong Li",
      "Yue Yu",
      "Yinghao Li",
      "Rongzhi Zhang",
      "Chao Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-03-17T06:12:43Z",
    "pdf_url": "https://arxiv.org/pdf/2403.11103v2"
  },
  {
    "arxiv_id": "2403.10433v4",
    "entry_id": "http://arxiv.org/abs/2403.10433v4",
    "title": "AI-enhanced Collective Intelligence",
    "summary": "Current societal challenges exceed the capacity of humans operating either alone or collectively. As AI evolves, its role within human collectives will vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, together, can surpass the collective intelligence of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from complex network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising cognition, physical, and information layers. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. We explore how agents' diversity and interactions influence the system's collective intelligence and analyze real-world instances of AI-enhanced collective intelligence. We conclude by considering potential challenges and future developments in this field.",
    "authors": [
      "Hao Cui",
      "Taha Yasseri"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-03-15T16:11:15Z",
    "pdf_url": "https://arxiv.org/pdf/2403.10433v4"
  },
  {
    "arxiv_id": "2403.10249v1",
    "entry_id": "http://arxiv.org/abs/2403.10249v1",
    "title": "A Survey on Game Playing Agents and Large Models: Methods, Applications, and Challenges",
    "summary": "The swift evolution of Large-scale Models (LMs), either language-focused or multi-modal, has garnered extensive attention in both academy and industry. But despite the surge in interest in this rapidly evolving area, there are scarce systematic reviews on their capabilities and potential in distinct impactful scenarios. This paper endeavours to help bridge this gap, offering a thorough examination of the current landscape of LM usage in regards to complex game playing scenarios and the challenges still open. Here, we seek to systematically review the existing architectures of LM-based Agents (LMAs) for games and summarize their commonalities, challenges, and any other insights. Furthermore, we present our perspective on promising future research avenues for the advancement of LMs in games. We hope to assist researchers in gaining a clear understanding of the field and to generate more interest in this highly impactful research direction. A corresponding resource, continuously updated, can be found in our GitHub repository.",
    "authors": [
      "Xinrun Xu",
      "Yuxin Wang",
      "Chaoyi Xu",
      "Ziluo Ding",
      "Jiechuan Jiang",
      "Zhiming Ding",
      "Börje F. Karlsson"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-15T12:37:12Z",
    "pdf_url": "https://arxiv.org/pdf/2403.10249v1"
  },
  {
    "arxiv_id": "2403.09606v3",
    "entry_id": "http://arxiv.org/abs/2403.09606v3",
    "title": "Large Language Models and Causal Inference in Collaboration: A Survey",
    "summary": "Causal inference has shown potential in enhancing the predictive accuracy, fairness, robustness, and explainability of Natural Language Processing (NLP) models by capturing causal relationships among variables. The emergence of generative Large Language Models (LLMs) has significantly impacted various NLP domains, particularly through their advanced reasoning capabilities. This survey focuses on evaluating and improving LLMs from a causal view in the following areas: understanding and improving the LLMs' reasoning capacity, addressing fairness and safety issues in LLMs, complementing LLMs with explanations, and handling multimodality. Meanwhile, LLMs' strong reasoning capacities can in turn contribute to the field of causal inference by aiding causal relationship discovery and causal effect estimations. This review explores the interplay between causal inference frameworks and LLMs from both perspectives, emphasizing their collective potential to further the development of more advanced and equitable artificial intelligence systems.",
    "authors": [
      "Xiaoyu Liu",
      "Paiheng Xu",
      "Junda Wu",
      "Jiaxin Yuan",
      "Yifan Yang",
      "Yuhang Zhou",
      "Fuxiao Liu",
      "Tianrui Guan",
      "Haoliang Wang",
      "Tong Yu",
      "Julian McAuley",
      "Wei Ai",
      "Furong Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-14T17:47:20Z",
    "pdf_url": "https://arxiv.org/pdf/2403.09606v3"
  },
  {
    "arxiv_id": "2403.09565v1",
    "entry_id": "http://arxiv.org/abs/2403.09565v1",
    "title": "Welcome Your New AI Teammate: On Safety Analysis by Leashing Large Language Models",
    "summary": "DevOps is a necessity in many industries, including the development of Autonomous Vehicles. In those settings, there are iterative activities that reduce the speed of SafetyOps cycles. One of these activities is \"Hazard Analysis & Risk Assessment\" (HARA), which is an essential step to start the safety requirements specification. As a potential approach to increase the speed of this step in SafetyOps, we have delved into the capabilities of Large Language Models (LLMs).\n  Our objective is to systematically assess their potential for application in the field of safety engineering. To that end, we propose a framework to support a higher degree of automation of HARA with LLMs. Despite our endeavors to automate as much of the process as possible, expert review remains crucial to ensure the validity and correctness of the analysis results, with necessary modifications made accordingly.",
    "authors": [
      "Ali Nouri",
      "Beatriz Cabrero-Daniel",
      "Fredrik Törner",
      "Hȧkan Sivencrona",
      "Christian Berger"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-03-14T16:56:52Z",
    "pdf_url": "https://arxiv.org/pdf/2403.09565v1"
  },
  {
    "arxiv_id": "2403.09743v1",
    "entry_id": "http://arxiv.org/abs/2403.09743v1",
    "title": "The Human Factor in Detecting Errors of Large Language Models: A Systematic Literature Review and Future Research Directions",
    "summary": "The launch of ChatGPT by OpenAI in November 2022 marked a pivotal moment for Artificial Intelligence, introducing Large Language Models (LLMs) to the mainstream and setting new records in user adoption. LLMs, particularly ChatGPT, trained on extensive internet data, demonstrate remarkable conversational capabilities across various domains, suggesting a significant impact on the workforce. However, these models are susceptible to errors - \"hallucinations\" and omissions, generating incorrect or incomplete information. This poses risks especially in contexts where accuracy is crucial, such as legal compliance, medicine or fine-grained process frameworks.\n  There are both technical and human solutions to cope with this isse. This paper explores the human factors that enable users to detect errors in LLM outputs, a critical component in mitigating risks associated with their use in professional settings. Understanding these factors is essential for organizations aiming to leverage LLM technology efficiently, guiding targeted training and deployment strategies to enhance error detection by users. This approach not only aims to optimize the use of LLMs but also to prevent potential downstream issues stemming from reliance on inaccurate model responses. The research emphasizes the balance between technological advancement and human insight in maximizing the benefits of LLMs while minimizing the risks, particularly in areas where precision is paramount.\n  This paper performs a systematic literature research on this research topic, analyses and synthesizes the findings, and outlines future research directions. Literature selection cut-off date is January 11th 2024.",
    "authors": [
      "Christian A. Schiller"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-03-13T21:39:39Z",
    "pdf_url": "https://arxiv.org/pdf/2403.09743v1"
  },
  {
    "arxiv_id": "2403.12090v1",
    "entry_id": "http://arxiv.org/abs/2403.12090v1",
    "title": "Foundation Models and Information Retrieval in Digital Pathology",
    "summary": "The paper reviews the state-of-the-art of foundation models, LLMs, generative AI, information retrieval and CBIR in digital pathology",
    "authors": [
      "H. R. Tizhoosh"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "eess.IV"
    ],
    "published": "2024-03-13T20:28:08Z",
    "pdf_url": "https://arxiv.org/pdf/2403.12090v1"
  },
  {
    "arxiv_id": "2403.08937v2",
    "entry_id": "http://arxiv.org/abs/2403.08937v2",
    "title": "Bugs in Large Language Models Generated Code: An Empirical Study",
    "summary": "Large Language Models (LLMs) for code have gained significant attention recently. They can generate code in different programming languages based on provided prompts, fulfilling a long-lasting dream in Software Engineering (SE), i.e., automatic code generation. Similar to human-written code, LLM-generated code is prone to bugs, and these bugs have not yet been thoroughly examined by the community. Given the increasing adoption of LLM-based code generation tools (e.g., GitHub Copilot) in SE activities, it is critical to understand the characteristics of bugs contained in code generated by LLMs. This paper examines a sample of 333 bugs collected from code generated using three leading LLMs (i.e., CodeGen, PanGu-Coder, and Codex) and identifies the following 10 distinctive bug patterns: Misinterpretations, Syntax Error, Silly Mistake, Prompt-biased code, Missing Corner Case, Wrong Input Type, Hallucinated Object, Wrong Attribute, Incomplete Generation, and Non-Prompted Consideration. The bug patterns are presented in the form of a taxonomy. The identified bug patterns are validated using an online survey with 34 LLM practitioners and researchers. The surveyed participants generally asserted the significance and prevalence of the bug patterns. Researchers and practitioners can leverage these findings to develop effective quality assurance techniques for LLM-generated code. This study sheds light on the distinctive characteristics of LLM-generated code.",
    "authors": [
      "Florian Tambon",
      "Arghavan Moradi Dakhel",
      "Amin Nikanjam",
      "Foutse Khomh",
      "Michel C. Desmarais",
      "Giuliano Antoniol"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-03-13T20:12:01Z",
    "pdf_url": "https://arxiv.org/pdf/2403.08937v2"
  },
  {
    "arxiv_id": "2403.08502v1",
    "entry_id": "http://arxiv.org/abs/2403.08502v1",
    "title": "Masked Generative Story Transformer with Character Guidance and Caption Augmentation",
    "summary": "Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our approach. The combination of these methods culminates into state-of-the-art (SOTA) results over various metrics in the most prominent SV benchmark (Pororo-SV), attained with constraint resources while achieving superior computational complexity compared to previous arts. The validity of our quantitative results is supported by a human survey.",
    "authors": [
      "Christos Papadimitriou",
      "Giorgos Filandrianos",
      "Maria Lymperaiou",
      "Giorgos Stamou"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2024-03-13T13:10:20Z",
    "pdf_url": "https://arxiv.org/pdf/2403.08502v1"
  },
  {
    "arxiv_id": "2403.08481v1",
    "entry_id": "http://arxiv.org/abs/2403.08481v1",
    "title": "SoK: Reducing the Vulnerability of Fine-tuned Language Models to Membership Inference Attacks",
    "summary": "Natural language processing models have experienced a significant upsurge in recent years, with numerous applications being built upon them. Many of these applications require fine-tuning generic base models on customized, proprietary datasets. This fine-tuning data is especially likely to contain personal or sensitive information about individuals, resulting in increased privacy risk. Membership inference attacks are the most commonly employed attack to assess the privacy leakage of a machine learning model. However, limited research is available on the factors that affect the vulnerability of language models to this kind of attack, or on the applicability of different defense strategies in the language domain. We provide the first systematic review of the vulnerability of fine-tuned large language models to membership inference attacks, the various factors that come into play, and the effectiveness of different defense strategies. We find that some training methods provide significantly reduced privacy risk, with the combination of differential privacy and low-rank adaptors achieving the best privacy protection against these attacks.",
    "authors": [
      "Guy Amit",
      "Abigail Goldsteen",
      "Ariel Farkash"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2024-03-13T12:46:51Z",
    "pdf_url": "https://arxiv.org/pdf/2403.08481v1"
  },
  {
    "arxiv_id": "2403.08429v1",
    "entry_id": "http://arxiv.org/abs/2403.08429v1",
    "title": "Software Vulnerability and Functionality Assessment using LLMs",
    "summary": "While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large margin. Motivated by promising results, we finally ask our models to provide detailed descriptions of security vulnerabilities. Results show that 36.7% of LLM-generated descriptions can be associated with true CWE vulnerabilities.",
    "authors": [
      "Rasmus Ingemann Tuffveson Jensen",
      "Vali Tawosi",
      "Salwa Alamir"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-03-13T11:29:13Z",
    "pdf_url": "https://arxiv.org/pdf/2403.08429v1"
  },
  {
    "arxiv_id": "2403.08319v2",
    "entry_id": "http://arxiv.org/abs/2403.08319v2",
    "title": "Knowledge Conflicts for LLMs: A Survey",
    "summary": "This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.",
    "authors": [
      "Rongwu Xu",
      "Zehan Qi",
      "Zhijiang Guo",
      "Cunxiang Wang",
      "Hongru Wang",
      "Yue Zhang",
      "Wei Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-03-13T08:02:23Z",
    "pdf_url": "https://arxiv.org/pdf/2403.08319v2"
  },
  {
    "arxiv_id": "2403.14680v3",
    "entry_id": "http://arxiv.org/abs/2403.14680v3",
    "title": "Trust in AI: Progress, Challenges, and Future Directions",
    "summary": "The increasing use of artificial intelligence (AI) systems in our daily life through various applications, services, and products explains the significance of trust/distrust in AI from a user perspective. AI-driven systems (as opposed to other technologies) have ubiquitously diffused in our life not only as some beneficial tools to be used by human agents but also are going to be substitutive agents on our behalf, or manipulative minds that would influence human thought, decision, and agency. Trust/distrust in AI plays the role of a regulator and could significantly control the level of this diffusion, as trust can increase, and distrust may reduce the rate of adoption of AI. Recently, varieties of studies have paid attention to the variant dimension of trust/distrust in AI, and its relevant considerations. In this systematic literature review, after conceptualization of trust in the current AI literature review, we will investigate trust in different types of human-Machine interaction, and its impact on technology acceptance in different domains. In addition to that, we propose a taxonomy of technical (i.e., safety, accuracy, robustness) and non-technical axiological (i.e., ethical, legal, and mixed) trustworthiness metrics, and some trustworthy measurements. Moreover, we examine some major trust-breakers in AI (e.g., autonomy and dignity threat), and trust makers; and propose some future directions and probable solutions for the transition to a trustworthy AI.",
    "authors": [
      "Saleh Afroogh",
      "Ali Akbari",
      "Evan Malone",
      "Mohammadali Kargar",
      "Hananeh Alambeigi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-03-12T20:26:49Z",
    "pdf_url": "https://arxiv.org/pdf/2403.14680v3"
  },
  {
    "arxiv_id": "2403.08035v1",
    "entry_id": "http://arxiv.org/abs/2403.08035v1",
    "title": "Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection",
    "summary": "Large language models (LLMs) excel in many diverse applications beyond language generation, e.g., translation, summarization, and sentiment analysis. One intriguing application is in text classification. This becomes pertinent in the realm of identifying hateful or toxic speech -- a domain fraught with challenges and ethical dilemmas. In our study, we have two objectives: firstly, to offer a literature review revolving around LLMs as classifiers, emphasizing their role in detecting and classifying hateful or toxic content. Subsequently, we explore the efficacy of several LLMs in classifying hate speech: identifying which LLMs excel in this task as well as their underlying attributes and training. Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content. By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabilities and constraints of LLMs in the crucial domain of hate speech detection.",
    "authors": [
      "Tharindu Kumarage",
      "Amrita Bhattacharjee",
      "Joshua Garland"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-12T19:12:28Z",
    "pdf_url": "https://arxiv.org/pdf/2403.08035v1"
  },
  {
    "arxiv_id": "2403.07693v2",
    "entry_id": "http://arxiv.org/abs/2403.07693v2",
    "title": "Large, Small or Both: A Novel Data Augmentation Framework Based on Language Models for Debiasing Opinion Summarization",
    "summary": "As more than 70$\\%$ of reviews in the existing opinion summary data set are positive, current opinion summarization approaches are reluctant to generate negative summaries given the input of negative texts. To address such sentiment bias, a direct approach without the over-reliance on a specific framework is to generate additional data based on large language models to balance the emotional distribution of the dataset. However, data augmentation based on large language models faces two disadvantages: 1) the potential issues or toxicity in the augmented data; 2) the expensive costs. Therefore, in this paper, we propose a novel data augmentation framework based on both large and small language models for debiasing opinion summarization. In specific, a small size of synthesized negative reviews is obtained by rewriting the positive text via a large language model. Then, a disentangle reconstruction model is trained based on the generated data. After training, a large amount of synthetic data can be obtained by decoding the new representation obtained from the combination of different sample representations and filtering based on confusion degree and sentiment classification. Experiments have proved that our framework can effectively alleviate emotional bias same as using only large models, but more economically.",
    "authors": [
      "Yanyue Zhang",
      "Pengfei Li",
      "Yilong Lai",
      "Deyu Zhou",
      "Yulan He"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-12T14:37:03Z",
    "pdf_url": "https://arxiv.org/pdf/2403.07693v2"
  },
  {
    "arxiv_id": "2403.07183v2",
    "entry_id": "http://arxiv.org/abs/2403.07183v2",
    "title": "Monitoring AI-Modified Content at Scale: A Case Study on the Impact of ChatGPT on AI Conference Peer Reviews",
    "summary": "We present an approach for estimating the fraction of text in a large corpus which is likely to be substantially modified or produced by a large language model (LLM). Our maximum likelihood model leverages expert-written and AI-generated reference texts to accurately and efficiently examine real-world LLM-use at the corpus level. We apply this approach to a case study of scientific peer review in AI conferences that took place after the release of ChatGPT: ICLR 2024, NeurIPS 2023, CoRL 2023 and EMNLP 2023. Our results suggest that between 6.5% and 16.9% of text submitted as peer reviews to these conferences could have been substantially modified by LLMs, i.e. beyond spell-checking or minor writing updates. The circumstances in which generated text occurs offer insight into user behavior: the estimated fraction of LLM-generated text is higher in reviews which report lower confidence, were submitted close to the deadline, and from reviewers who are less likely to respond to author rebuttals. We also observe corpus-level trends in generated text which may be too subtle to detect at the individual level, and discuss the implications of such trends on peer review. We call for future interdisciplinary work to examine how LLM use is changing our information and knowledge practices.",
    "authors": [
      "Weixin Liang",
      "Zachary Izzo",
      "Yaohui Zhang",
      "Haley Lepp",
      "Hancheng Cao",
      "Xuandong Zhao",
      "Lingjiao Chen",
      "Haotian Ye",
      "Sheng Liu",
      "Zhi Huang",
      "Daniel A. McFarland",
      "James Y. Zou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "published": "2024-03-11T21:51:39Z",
    "pdf_url": "https://arxiv.org/pdf/2403.07183v2"
  },
  {
    "arxiv_id": "2403.07078v1",
    "entry_id": "http://arxiv.org/abs/2403.07078v1",
    "title": "Improving deep learning with prior knowledge and cognitive models: A survey on enhancing explainability, adversarial robustness and zero-shot learning",
    "summary": "We review current and emerging knowledge-informed and brain-inspired cognitive systems for realizing adversarial defenses, eXplainable Artificial Intelligence (XAI), and zero-shot or few-short learning. Data-driven deep learning models have achieved remarkable performance and demonstrated capabilities surpassing human experts in many applications. Yet, their inability to exploit domain knowledge leads to serious performance limitations in practical applications. In particular, deep learning systems are exposed to adversarial attacks, which can trick them into making glaringly incorrect decisions. Moreover, complex data-driven models typically lack interpretability or explainability, i.e., their decisions cannot be understood by human subjects. Furthermore, models are usually trained on standard datasets with a closed-world assumption. Hence, they struggle to generalize to unseen cases during inference in practical open-world environments, thus, raising the zero- or few-shot generalization problem. Although many conventional solutions exist, explicit domain knowledge, brain-inspired neural network and cognitive architectures offer powerful new dimensions towards alleviating these problems. Prior knowledge is represented in appropriate forms and incorporated in deep learning frameworks to improve performance. Brain-inspired cognition methods use computational models that mimic the human mind to enhance intelligent behavior in artificial agents and autonomous robots. Ultimately, these models achieve better explainability, higher adversarial robustness and data-efficient learning, and can, in turn, provide insights for cognitive science and neuroscience-that is, to deepen human understanding on how the brain works in general, and how it handles these problems.",
    "authors": [
      "Fuseinin Mumuni",
      "Alhassan Mumuni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-03-11T18:11:00Z",
    "pdf_url": "https://arxiv.org/pdf/2403.07078v1"
  },
  {
    "arxiv_id": "2403.06139v1",
    "entry_id": "http://arxiv.org/abs/2403.06139v1",
    "title": "Fine-grainedly Synthesize Streaming Data Based On Large Language Models With Graph Structure Understanding For Data Sparsity",
    "summary": "Due to the sparsity of user data, sentiment analysis on user reviews in e-commerce platforms often suffers from poor performance, especially when faced with extremely sparse user data or long-tail labels. Recently, the emergence of LLMs has introduced new solutions to such problems by leveraging graph structures to generate supplementary user profiles. However, previous approaches have not fully utilized the graph understanding capabilities of LLMs and have struggled to adapt to complex streaming data environments. In this work, we propose a fine-grained streaming data synthesis framework that categorizes sparse users into three categories: Mid-tail, Long-tail, and Extreme. Specifically, we design LLMs to comprehensively understand three key graph elements in streaming data, including Local-global Graph Understanding, Second-Order Relationship Extraction, and Product Attribute Understanding, which enables the generation of high-quality synthetic data to effectively address sparsity across different categories. Experimental results on three real datasets demonstrate significant performance improvements, with synthesized data contributing to MSE reductions of 45.85%, 3.16%, and 62.21%, respectively.",
    "authors": [
      "Xin Zhang",
      "Linhai Zhang",
      "Deyu Zhou",
      "Guoqiang Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-10T08:59:04Z",
    "pdf_url": "https://arxiv.org/pdf/2403.06139v1"
  },
  {
    "arxiv_id": "2403.06108v2",
    "entry_id": "http://arxiv.org/abs/2403.06108v2",
    "title": "Large Language Models on Fine-grained Emotion Detection Dataset with Data Augmentation and Transfer Learning",
    "summary": "This paper delves into enhancing the classification performance on the GoEmotions dataset, a large, manually annotated dataset for emotion detection in text. The primary goal of this paper is to address the challenges of detecting subtle emotions in text, a complex issue in Natural Language Processing (NLP) with significant practical applications. The findings offer valuable insights into addressing the challenges of emotion detection in text and suggest directions for future research, including the potential for a survey paper that synthesizes methods and performances across various datasets in this domain.",
    "authors": [
      "Kaipeng Wang",
      "Zhi Jing",
      "Yongye Su",
      "Yikun Han"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-10T06:30:54Z",
    "pdf_url": "https://arxiv.org/pdf/2403.06108v2"
  },
  {
    "arxiv_id": "2403.04454v1",
    "entry_id": "http://arxiv.org/abs/2403.04454v1",
    "title": "Low-Resource Court Judgment Summarization for Common Law Systems",
    "summary": "Common law courts need to refer to similar precedents' judgments to inform their current decisions. Generating high-quality summaries of court judgment documents can facilitate legal practitioners to efficiently review previous cases and assist the general public in accessing how the courts operate and how the law is applied. Previous court judgment summarization research focuses on civil law or a particular jurisdiction's judgments. However, judges can refer to the judgments from all common law jurisdictions. Current summarization datasets are insufficient to satisfy the demands of summarizing precedents across multiple jurisdictions, especially when labeled data are scarce for many jurisdictions. To address the lack of datasets, we present CLSum, the first dataset for summarizing multi-jurisdictional common law court judgment documents. Besides, this is the first court judgment summarization work adopting large language models (LLMs) in data augmentation, summary generation, and evaluation. Specifically, we design an LLM-based data augmentation method incorporating legal knowledge. We also propose a legal knowledge enhanced evaluation metric based on LLM to assess the quality of generated judgment summaries. Our experimental results verify that the LLM-based summarization methods can perform well in the few-shot and zero-shot settings. Our LLM-based data augmentation method can mitigate the impact of low data resources. Furthermore, we carry out comprehensive comparative experiments to find essential model components and settings that are capable of enhancing summarization performance.",
    "authors": [
      "Shuaiqi Liu",
      "Jiannong Cao",
      "Yicong Li",
      "Ruosong Yang",
      "Zhiyuan Wen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-07T12:47:42Z",
    "pdf_url": "https://arxiv.org/pdf/2403.04454v1"
  },
  {
    "arxiv_id": "2403.04382v1",
    "entry_id": "http://arxiv.org/abs/2403.04382v1",
    "title": "Acceleron: A Tool to Accelerate Research Ideation",
    "summary": "Several tools have recently been proposed for assisting researchers during various stages of the research life-cycle. However, these primarily concentrate on tasks such as retrieving and recommending relevant literature, reviewing and critiquing the draft, and writing of research manuscripts. Our investigation reveals a significant gap in availability of tools specifically designed to assist researchers during the challenging ideation phase of the research life-cycle. To aid with research ideation, we propose `Acceleron', a research accelerator for different phases of the research life cycle, and which is specially designed to aid the ideation process. Acceleron guides researchers through the formulation of a comprehensive research proposal, encompassing a novel research problem. The proposals motivation is validated for novelty by identifying gaps in the existing literature and suggesting a plausible list of techniques to solve the proposed problem. We leverage the reasoning and domain-specific skills of Large Language Models (LLMs) to create an agent-based architecture incorporating colleague and mentor personas for LLMs. The LLM agents emulate the ideation process undertaken by researchers, engaging researchers in an interactive fashion to aid in the development of the research proposal. Notably, our tool addresses challenges inherent in LLMs, such as hallucinations, implements a two-stage aspect-based retrieval to manage precision-recall trade-offs, and tackles issues of unanswerability. As evaluation, we illustrate the execution of our motivation validation and method synthesis workflows on proposals from the ML and NLP domain, given by 3 distinct researchers. Our observations and evaluations provided by the researchers illustrate the efficacy of the tool in terms of assisting researchers with appropriate inputs at distinct stages and thus leading to improved time efficiency.",
    "authors": [
      "Harshit Nigam",
      "Manasi Patwardhan",
      "Lovekesh Vig",
      "Gautam Shroff"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-07T10:20:06Z",
    "pdf_url": "https://arxiv.org/pdf/2403.04382v1"
  },
  {
    "arxiv_id": "2403.04321v2",
    "entry_id": "http://arxiv.org/abs/2403.04321v2",
    "title": "Discriminative Probing and Tuning for Text-to-Image Generation",
    "summary": "Despite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models.",
    "authors": [
      "Leigang Qu",
      "Wenjie Wang",
      "Yongqi Li",
      "Hanwang Zhang",
      "Liqiang Nie",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "published": "2024-03-07T08:37:33Z",
    "pdf_url": "https://arxiv.org/pdf/2403.04321v2"
  },
  {
    "arxiv_id": "2403.04261v2",
    "entry_id": "http://arxiv.org/abs/2403.04261v2",
    "title": "Advancing Chinese biomedical text mining with community challenges",
    "summary": "Objective: This study aims to review the recent advances in community challenges for biomedical text mining in China. Methods: We collected information of evaluation tasks released in community challenges of biomedical text mining, including task description, dataset description, data source, task type and related links. A systematic summary and comparative analysis were conducted on various biomedical natural language processing tasks, such as named entity recognition, entity normalization, attribute extraction, relation extraction, event extraction, text classification, text similarity, knowledge graph construction, question answering, text generation, and large language model evaluation. Results: We identified 39 evaluation tasks from 6 community challenges that spanned from 2017 to 2023. Our analysis revealed the diverse range of evaluation task types and data sources in biomedical text mining. We explored the potential clinical applications of these community challenge tasks from a translational biomedical informatics perspective. We compared with their English counterparts, and discussed the contributions, limitations, lessons and guidelines of these community challenges, while highlighting future directions in the era of large language models. Conclusion: Community challenge evaluation competitions have played a crucial role in promoting technology innovation and fostering interdisciplinary collaboration in the field of biomedical text mining. These challenges provide valuable platforms for researchers to develop state-of-the-art solutions.",
    "authors": [
      "Hui Zong",
      "Rongrong Wu",
      "Jiaxue Cha",
      "Weizhe Feng",
      "Erman Wu",
      "Jiakun Li",
      "Aibin Shao",
      "Liang Tao",
      "Zuofeng Li",
      "Buzhou Tang",
      "Bairong Shen"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-03-07T06:52:51Z",
    "pdf_url": "https://arxiv.org/pdf/2403.04261v2"
  },
  {
    "arxiv_id": "2403.13830v1",
    "entry_id": "http://arxiv.org/abs/2403.13830v1",
    "title": "Bridging Text and Molecule: A Survey on Multimodal Frameworks for Molecule",
    "summary": "Artificial intelligence has demonstrated immense potential in scientific research. Within molecular science, it is revolutionizing the traditional computer-aided paradigm, ushering in a new era of deep learning. With recent progress in multimodal learning and natural language processing, an emerging trend has targeted at building multimodal frameworks to jointly model molecules with textual domain knowledge. In this paper, we present the first systematic survey on multimodal frameworks for molecules research. Specifically,we begin with the development of molecular deep learning and point out the necessity to involve textual modality. Next, we focus on recent advances in text-molecule alignment methods, categorizing current models into two groups based on their architectures and listing relevant pre-training tasks. Furthermore, we delves into the utilization of large language models and prompting techniques for molecular tasks and present significant applications in drug discovery. Finally, we discuss the limitations in this field and highlight several promising directions for future research.",
    "authors": [
      "Yi Xiao",
      "Xiangxin Zhou",
      "Qiang Liu",
      "Liang Wang"
    ],
    "categories": [
      "q-bio.BM",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-03-07T03:03:13Z",
    "pdf_url": "https://arxiv.org/pdf/2403.13830v1"
  },
  {
    "arxiv_id": "2403.04105v3",
    "entry_id": "http://arxiv.org/abs/2403.04105v3",
    "title": "Natural Language Processing in the Patent Domain: A Survey",
    "summary": "Patents, which encapsulate crucial technical and legal information in text form and referenced drawings, present a rich domain for natural language processing (NLP) applications. As NLP technologies evolve, large language models (LLMs) have demonstrated outstanding capabilities in general text processing and generation tasks. However, the application of LLMs in the patent domain remains under-explored and under-developed due to the complexity of patents, particularly their language and legal framework. Understanding the unique characteristics of patent documents and related research in the patent domain becomes essential for researchers to apply these tools effectively. Therefore, this paper aims to equip NLP researchers with the essential knowledge to navigate this complex domain efficiently. We introduce the relevant fundamental aspects of patents to provide solid background information. In addition, we systematically break down the structural and linguistic characteristics unique to patents and map out how NLP can be leveraged for patent analysis and generation. Moreover, we demonstrate the spectrum of text-based and multimodal patent-related tasks, including nine patent analysis and four patent generation tasks.",
    "authors": [
      "Lekang Jiang",
      "Stephan Goetz"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-06T23:17:16Z",
    "pdf_url": "https://arxiv.org/pdf/2403.04105v3"
  },
  {
    "arxiv_id": "2403.03699v1",
    "entry_id": "http://arxiv.org/abs/2403.03699v1",
    "title": "Model Parallelism on Distributed Infrastructure: A Literature Review from Theory to LLM Case-Studies",
    "summary": "Neural networks have become a cornerstone of machine learning. As the trend for these to get more and more complex continues, so does the underlying hardware and software infrastructure for training and deployment. In this survey we answer three research questions: \"What types of model parallelism exist?\", \"What are the challenges of model parallelism?\", and \"What is a modern use-case of model parallelism?\" We answer the first question by looking at how neural networks can be parallelised and expressing these as operator graphs while exploring the available dimensions. The dimensions along which neural networks can be parallelised are intra-operator and inter-operator. We answer the second question by collecting and listing both implementation challenges for the types of parallelism, as well as the problem of optimally partitioning the operator graph. We answer the last question by collecting and listing how parallelism is applied in modern multi-billion parameter transformer networks, to the extend that this is possible with the limited information shared about these networks.",
    "authors": [
      "Felix Brakel",
      "Uraz Odyurt",
      "Ana-Lucia Varbanescu"
    ],
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2024-03-06T13:29:00Z",
    "pdf_url": "https://arxiv.org/pdf/2403.03699v1"
  },
  {
    "arxiv_id": "2403.02990v4",
    "entry_id": "http://arxiv.org/abs/2403.02990v4",
    "title": "Data Augmentation using Large Language Models: Data Perspectives, Learning Paradigms and Challenges",
    "summary": "In the rapidly evolving field of large language models (LLMs), data augmentation (DA) has emerged as a pivotal technique for enhancing model performance by diversifying training examples without the need for additional data collection. This survey explores the transformative impact of LLMs on DA, particularly addressing the unique challenges and opportunities they present in the context of natural language processing (NLP) and beyond. From both data and learning perspectives, we examine various strategies that utilize LLMs for data augmentation, including a novel exploration of learning paradigms where LLM-generated data is used for diverse forms of further training. Additionally, this paper highlights the primary open challenges faced in this domain, ranging from controllable data augmentation to multi-modal data augmentation. This survey highlights a paradigm shift introduced by LLMs in DA, and aims to serve as a comprehensive guide for researchers and practitioners.",
    "authors": [
      "Bosheng Ding",
      "Chengwei Qin",
      "Ruochen Zhao",
      "Tianze Luo",
      "Xinze Li",
      "Guizhen Chen",
      "Wenhan Xia",
      "Junjie Hu",
      "Anh Tuan Luu",
      "Shafiq Joty"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-05T14:11:54Z",
    "pdf_url": "https://arxiv.org/pdf/2403.02990v4"
  },
  {
    "arxiv_id": "2403.02901v3",
    "entry_id": "http://arxiv.org/abs/2403.02901v3",
    "title": "A Comprehensive Survey on Process-Oriented Automatic Text Summarization with Exploration of LLM-Based Methods",
    "summary": "Automatic Text Summarization (ATS), utilizing Natural Language Processing (NLP) algorithms, aims to create concise and accurate summaries, thereby significantly reducing the human effort required in processing large volumes of text. ATS has drawn considerable interest in both academic and industrial circles. Many studies have been conducted in the past to survey ATS methods; however, they generally lack practicality for real-world implementations, as they often categorize previous methods from a theoretical standpoint. Moreover, the advent of Large Language Models (LLMs) has altered conventional ATS methods. In this survey, we aim to 1) provide a comprehensive overview of ATS from a ``Process-Oriented Schema'' perspective, which is best aligned with real-world implementations; 2) comprehensively review the latest LLM-based ATS works; and 3) deliver an up-to-date survey of ATS, bridging the two-year gap in the literature. To the best of our knowledge, this is the first survey to specifically investigate LLM-based ATS methods.",
    "authors": [
      "Yang Zhang",
      "Hanlei Jin",
      "Dan Meng",
      "Jun Wang",
      "Jinghua Tan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-05T12:11:07Z",
    "pdf_url": "https://arxiv.org/pdf/2403.02901v3"
  },
  {
    "arxiv_id": "2403.02760v2",
    "entry_id": "http://arxiv.org/abs/2403.02760v2",
    "title": "Emerging Synergies Between Large Language Models and Machine Learning in Ecommerce Recommendations",
    "summary": "With the boom of e-commerce and web applications, recommender systems have become an important part of our daily lives, providing personalized recommendations based on the user's preferences. Although deep neural networks (DNNs) have made significant progress in improving recommendation systems by simulating the interaction between users and items and incorporating their textual information, these DNN-based approaches still have some limitations, such as the difficulty of effectively understanding users' interests and capturing textual information. It is not possible to generalize to different seen/unseen recommendation scenarios and reason about their predictions. At the same time, the emergence of large language models (LLMs), represented by ChatGPT and GPT-4, has revolutionized the fields of natural language processing (NLP) and artificial intelligence (AI) due to their superior capabilities in the basic tasks of language understanding and generation, and their impressive generalization and reasoning capabilities. As a result, recent research has sought to harness the power of LLM to improve recommendation systems. Given the rapid development of this research direction in the field of recommendation systems, there is an urgent need for a systematic review of existing LLM-driven recommendation systems for researchers and practitioners in related fields to gain insight into. More specifically, we first introduced a representative approach to learning user and item representations using LLM as a feature encoder. We then reviewed the latest advances in LLMs techniques for collaborative filtering enhanced recommendation systems from the three paradigms of pre-training, fine-tuning, and prompting. Finally, we had a comprehensive discussion on the future direction of this emerging field.",
    "authors": [
      "Xiaonan Xu",
      "Yichao Wu",
      "Penghao Liang",
      "Yuhang He",
      "Han Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-03-05T08:31:00Z",
    "pdf_url": "https://arxiv.org/pdf/2403.02760v2"
  },
  {
    "arxiv_id": "2403.15412v5",
    "entry_id": "http://arxiv.org/abs/2403.15412v5",
    "title": "Towards Measuring and Modeling \"Culture\" in LLMs: A Survey",
    "summary": "We present a survey of more than 90 recent papers that aim to study cultural representation and inclusion in large language models (LLMs). We observe that none of the studies explicitly define \"culture, which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of \"culture\". We call these aspects the proxies of culture, and organize them across two dimensions of demographic and semantic proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of ``culture,'' such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness of probing techniques and situated studies on the impact of cultural mis- and under-representation in LLM-based applications.",
    "authors": [
      "Muhammad Farid Adilazuarda",
      "Sagnik Mukherjee",
      "Pradhyumna Lavania",
      "Siddhant Singh",
      "Alham Fikri Aji",
      "Jacki O'Neill",
      "Ashutosh Modi",
      "Monojit Choudhury"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-03-05T08:29:36Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15412v5"
  },
  {
    "arxiv_id": "2403.02613v1",
    "entry_id": "http://arxiv.org/abs/2403.02613v1",
    "title": "Large Language Models and Video Games: A Preliminary Scoping Review",
    "summary": "Large language models (LLMs) hold interesting potential for the design, development, and research of video games. Building on the decades of prior research on generative AI in games, many researchers have sped to investigate the power and potential of LLMs for games. Given the recent spike in LLM-related research in games, there is already a wealth of relevant research to survey. In order to capture a snapshot of the state of LLM research in games, and to help lay the foundation for future work, we carried out an initial scoping review of relevant papers published so far. In this paper, we review 76 papers published between 2022 to early 2024 on LLMs and video games, with key focus areas in game AI, game development, narrative, and game research and reviews. Our paper provides an early state of the field and lays the groundwork for future research and reviews on this topic.",
    "authors": [
      "Penny Sweetser"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-03-05T03:04:35Z",
    "pdf_url": "https://arxiv.org/pdf/2403.02613v1"
  },
  {
    "arxiv_id": "2403.02574v1",
    "entry_id": "http://arxiv.org/abs/2403.02574v1",
    "title": "ChatCite: LLM Agent with Human Workflow Guidance for Comparative Literature Summary",
    "summary": "The literature review is an indispensable step in the research process. It provides the benefit of comprehending the research problem and understanding the current research situation while conducting a comparative analysis of prior works. However, literature summary is challenging and time consuming. The previous LLM-based studies on literature review mainly focused on the complete process, including literature retrieval, screening, and summarization. However, for the summarization step, simple CoT method often lacks the ability to provide extensive comparative summary. In this work, we firstly focus on the independent literature summarization step and introduce ChatCite, an LLM agent with human workflow guidance for comparative literature summary. This agent, by mimicking the human workflow, first extracts key elements from relevant literature and then generates summaries using a Reflective Incremental Mechanism. In order to better evaluate the quality of the generated summaries, we devised a LLM-based automatic evaluation metric, G-Score, in refer to the human evaluation criteria. The ChatCite agent outperformed other models in various dimensions in the experiments. The literature summaries generated by ChatCite can also be directly used for drafting literature reviews.",
    "authors": [
      "Yutong Li",
      "Lu Chen",
      "Aiwei Liu",
      "Kai Yu",
      "Lijie Wen"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-03-05T01:13:56Z",
    "pdf_url": "https://arxiv.org/pdf/2403.02574v1"
  },
  {
    "arxiv_id": "2403.01748v3",
    "entry_id": "http://arxiv.org/abs/2403.01748v3",
    "title": "NeuSpeech: Decode Neural signal as Speech",
    "summary": "Decoding language from brain dynamics is an important open direction in the realm of brain-computer interface (BCI), especially considering the rapid growth of large language models. Compared to invasive-based signals which require electrode implantation surgery, non-invasive neural signals (e.g. EEG, MEG) have attracted increasing attention considering their safety and generality. However, the exploration is not adequate in three aspects: 1) previous methods mainly focus on EEG but none of the previous works address this problem on MEG with better signal quality; 2) prior works have predominantly used $``teacher-forcing\"$ during generative decoding, which is impractical; 3) prior works are mostly $``BART-based\"$ not fully auto-regressive, which performs better in other sequence tasks. In this paper, we explore the brain-to-text translation of MEG signals in a speech-decoding formation. Here we are the first to investigate a cross-attention-based ``whisper\" model for generating text directly from MEG signals without teacher forcing. Our model achieves impressive BLEU-1 scores of 60.30 and 52.89 without pretraining $\\&$ teacher-forcing on two major datasets ($\\textit{GWilliams}$ and $\\textit{Schoffelen}$). This paper conducts a comprehensive review to understand how speech decoding formation performs on the neural decoding tasks, including pretraining initialization, training $\\&$ evaluation set splitting, augmentation, and scaling law. Code is available at https://github.com/NeuSpeech/NeuSpeech1$.",
    "authors": [
      "Yiqian Yang",
      "Yiqun Duan",
      "Qiang Zhang",
      "Hyejeong Jo",
      "Jinni Zhou",
      "Won Hee Lee",
      "Renjing Xu",
      "Hui Xiong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-04T05:55:01Z",
    "pdf_url": "https://arxiv.org/pdf/2403.01748v3"
  },
  {
    "arxiv_id": "2403.01152v1",
    "entry_id": "http://arxiv.org/abs/2403.01152v1",
    "title": "A Survey of AI-generated Text Forensic Systems: Detection, Attribution, and Characterization",
    "summary": "We have witnessed lately a rapid proliferation of advanced Large Language Models (LLMs) capable of generating high-quality text. While these LLMs have revolutionized text generation across various domains, they also pose significant risks to the information ecosystem, such as the potential for generating convincing propaganda, misinformation, and disinformation at scale. This paper offers a review of AI-generated text forensic systems, an emerging field addressing the challenges of LLM misuses. We present an overview of the existing efforts in AI-generated text forensics by introducing a detailed taxonomy, focusing on three primary pillars: detection, attribution, and characterization. These pillars enable a practical understanding of AI-generated text, from identifying AI-generated content (detection), determining the specific AI model involved (attribution), and grouping the underlying intents of the text (characterization). Furthermore, we explore available resources for AI-generated text forensics research and discuss the evolving challenges and future directions of forensic systems in an AI era.",
    "authors": [
      "Tharindu Kumarage",
      "Garima Agrawal",
      "Paras Sheth",
      "Raha Moraffah",
      "Aman Chadha",
      "Joshua Garland",
      "Huan Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-03-02T09:39:13Z",
    "pdf_url": "https://arxiv.org/pdf/2403.01152v1"
  },
  {
    "arxiv_id": "2403.00420v2",
    "entry_id": "http://arxiv.org/abs/2403.00420v2",
    "title": "Robust Deep Reinforcement Learning Through Adversarial Attacks and Training : A Survey",
    "summary": "Deep Reinforcement Learning (DRL) is a subfield of machine learning for training autonomous agents that take sequential actions across complex environments. Despite its significant performance in well-known environments, it remains susceptible to minor condition variations, raising concerns about its reliability in real-world applications. To improve usability, DRL must demonstrate trustworthiness and robustness. A way to improve the robustness of DRL to unknown changes in the environmental conditions and possible perturbations is through Adversarial Training, by training the agent against well-suited adversarial attacks on the observations and the dynamics of the environment. Addressing this critical issue, our work presents an in-depth analysis of contemporary adversarial attack and training methodologies, systematically categorizing them and comparing their objectives and operational mechanisms.",
    "authors": [
      "Lucas Schott",
      "Josephine Delas",
      "Hatem Hajri",
      "Elies Gherbi",
      "Reda Yaich",
      "Nora Boulahia-Cuppens",
      "Frederic Cuppens",
      "Sylvain Lamprier"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-03-01T10:16:46Z",
    "pdf_url": "https://arxiv.org/pdf/2403.00420v2"
  },
  {
    "arxiv_id": "2403.00157v1",
    "entry_id": "http://arxiv.org/abs/2403.00157v1",
    "title": "Privacy-Preserving Distributed Optimization and Learning",
    "summary": "Distributed optimization and learning has recently garnered great attention due to its wide applications in sensor networks, smart grids, machine learning, and so forth. Despite rapid development, existing distributed optimization and learning algorithms require each agent to exchange messages with its neighbors, which may expose sensitive information and raise significant privacy concerns. In this survey paper, we overview privacy-preserving distributed optimization and learning methods. We first discuss cryptography, differential privacy, and other techniques that can be used for privacy preservation and indicate their pros and cons for privacy protection in distributed optimization and learning. We believe that among these approaches, differential privacy is most promising due to its low computational and communication complexities, which are extremely appealing for modern learning based applications with high dimensions of optimization variables. We then introduce several differential-privacy algorithms that can simultaneously ensure privacy and optimization accuracy. Moreover, we provide example applications in several machine learning problems to confirm the real-world effectiveness of these algorithms. Finally, we highlight some challenges in this research domain and discuss future directions.",
    "authors": [
      "Ziqin Chen",
      "Yongqiang Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.CR",
      "cs.GT"
    ],
    "published": "2024-02-29T22:18:05Z",
    "pdf_url": "https://arxiv.org/pdf/2403.00157v1"
  },
  {
    "arxiv_id": "2402.19366v3",
    "entry_id": "http://arxiv.org/abs/2402.19366v3",
    "title": "Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency",
    "summary": "The ever-increasing workload of digital forensic labs raises concerns about law enforcement's ability to conduct both cyber-related and non-cyber-related investigations promptly. Consequently, this article explores the potential and usefulness of integrating Large Language Models (LLMs) into digital forensic investigations to address challenges such as bias, explainability, censorship, resource-intensive infrastructure, and ethical and legal considerations. A comprehensive literature review is carried out, encompassing existing digital forensic models, tools, LLMs, deep learning techniques, and the use of LLMs in investigations. The review identifies current challenges within existing digital forensic processes and explores both the obstacles and the possibilities of incorporating LLMs. In conclusion, the study states that the adoption of LLMs in digital forensics, with appropriate constraints, has the potential to improve investigation efficiency, improve traceability, and alleviate the technical and judicial barriers faced by law enforcement entities.",
    "authors": [
      "Akila Wickramasekara",
      "Frank Breitinger",
      "Mark Scanlon"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-02-29T17:13:44Z",
    "pdf_url": "https://arxiv.org/pdf/2402.19366v3"
  },
  {
    "arxiv_id": "2402.19348v2",
    "entry_id": "http://arxiv.org/abs/2402.19348v2",
    "title": "Deep Learning for Cross-Domain Data Fusion in Urban Computing: Taxonomy, Advances, and Outlook",
    "summary": "As cities continue to burgeon, Urban Computing emerges as a pivotal discipline for sustainable development by harnessing the power of cross-domain data fusion from diverse sources (e.g., geographical, traffic, social media, and environmental data) and modalities (e.g., spatio-temporal, visual, and textual modalities). Recently, we are witnessing a rising trend that utilizes various deep-learning methods to facilitate cross-domain data fusion in smart cities. To this end, we propose the first survey that systematically reviews the latest advancements in deep learning-based data fusion methods tailored for urban computing. Specifically, we first delve into data perspective to comprehend the role of each modality and data source. Secondly, we classify the methodology into four primary categories: feature-based, alignment-based, contrast-based, and generation-based fusion methods. Thirdly, we further categorize multi-modal urban applications into seven types: urban planning, transportation, economy, public safety, society, environment, and energy. Compared with previous surveys, we focus more on the synergy of deep learning methods with urban computing applications. Furthermore, we shed light on the interplay between Large Language Models (LLMs) and urban computing, postulating future research directions that could revolutionize the field. We firmly believe that the taxonomy, progress, and prospects delineated in our survey stand poised to significantly enrich the research community. The summary of the comprehensive and up-to-date paper list can be found at https://github.com/yoshall/Awesome-Multimodal-Urban-Computing.",
    "authors": [
      "Xingchen Zou",
      "Yibo Yan",
      "Xixuan Hao",
      "Yuehong Hu",
      "Haomin Wen",
      "Erdong Liu",
      "Junbo Zhang",
      "Yong Li",
      "Tianrui Li",
      "Yu Zheng",
      "Yuxuan Liang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-02-29T16:56:23Z",
    "pdf_url": "https://arxiv.org/pdf/2402.19348v2"
  },
  {
    "arxiv_id": "2402.18659v5",
    "entry_id": "http://arxiv.org/abs/2402.18659v5",
    "title": "Large Language Models and Games: A Survey and Roadmap",
    "summary": "Recent years have seen an explosive increase in research on large language models (LLMs), and accompanying public engagement on the topic. While starting as a niche area within natural language processing, LLMs have shown remarkable potential across a broad range of applications and domains, including games. This paper surveys the current state of the art across the various applications of LLMs in and for games, and identifies the different roles LLMs can take within a game. Importantly, we discuss underexplored areas and promising directions for future uses of LLMs in games and we reconcile the potential and limitations of LLMs within the games domain. As the first comprehensive survey and roadmap at the intersection of LLMs and games, we are hopeful that this paper will serve as the basis for groundbreaking research and innovation in this exciting new field.",
    "authors": [
      "Roberto Gallotta",
      "Graham Todd",
      "Marvin Zammit",
      "Sam Earle",
      "Antonios Liapis",
      "Julian Togelius",
      "Georgios N. Yannakakis"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-02-28T19:09:08Z",
    "pdf_url": "https://arxiv.org/pdf/2402.18659v5"
  },
  {
    "arxiv_id": "2402.18144v1",
    "entry_id": "http://arxiv.org/abs/2402.18144v1",
    "title": "Random Silicon Sampling: Simulating Human Sub-Population Opinion Using a Large Language Model Based on Group-Level Demographic Information",
    "summary": "Large language models exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose \"random silicon sampling,\" a method to emulate the opinions of the human population sub-group. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mirroring a group's opinion using only demographic distribution and elucidate the effect of social biases in language models on such simulations.",
    "authors": [
      "Seungjong Sun",
      "Eungu Lee",
      "Dongyan Nan",
      "Xiangying Zhao",
      "Wonbyung Lee",
      "Bernard J. Jansen",
      "Jang Hyun Kim"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-02-28T08:09:14Z",
    "pdf_url": "https://arxiv.org/pdf/2402.18144v1"
  },
  {
    "arxiv_id": "2403.05578v1",
    "entry_id": "http://arxiv.org/abs/2403.05578v1",
    "title": "Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners",
    "summary": "Text-to-image models such as stable diffusion have opened a plethora of opportunities for generating art. Recent literature has surveyed the use of text-to-image models for enhancing the work of many creative artists. Many e-commerce platforms employ a manual process to generate the banners, which is time-consuming and has limitations of scalability. In this work, we demonstrate the use of text-to-image models for generating personalized web banners with dynamic content for online shoppers based on their interactions. The novelty in this approach lies in converting users' interaction data to meaningful prompts without human intervention. To this end, we utilize a large language model (LLM) to systematically extract a tuple of attributes from item meta-information. The attributes are then passed to a text-to-image model via prompt engineering to generate images for the banner. Our results show that the proposed approach can create high-quality personalized banners for users.",
    "authors": [
      "Shanu Vashishtha",
      "Abhinav Prakash",
      "Lalitesh Morishetti",
      "Kaushiki Nag",
      "Yokila Arora",
      "Sushant Kumar",
      "Kannan Achan"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-02-28T07:56:04Z",
    "pdf_url": "https://arxiv.org/pdf/2403.05578v1"
  },
  {
    "arxiv_id": "2402.18041v1",
    "entry_id": "http://arxiv.org/abs/2402.18041v1",
    "title": "Datasets for Large Language Models: A Comprehensive Survey",
    "summary": "This paper embarks on an exploration into the Large Language Model (LLM) datasets, which play a crucial role in the remarkable advancements of LLMs. The datasets serve as the foundational infrastructure analogous to a root system that sustains and nurtures the development of LLMs. Consequently, examination of these datasets emerges as a critical topic in research. In order to address the current lack of a comprehensive overview and thorough analysis of LLM datasets, and to gain insights into their current status and future trends, this survey consolidates and categorizes the fundamental aspects of LLM datasets from five perspectives: (1) Pre-training Corpora; (2) Instruction Fine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5) Traditional Natural Language Processing (NLP) Datasets. The survey sheds light on the prevailing challenges and points out potential avenues for future investigation. Additionally, a comprehensive review of the existing available dataset resources is also provided, including statistics from 444 datasets, covering 8 language categories and spanning 32 domains. Information from 20 dimensions is incorporated into the dataset statistics. The total data size surveyed surpasses 774.5 TB for pre-training corpora and 700M instances for other datasets. We aim to present the entire landscape of LLM text datasets, serving as a comprehensive reference for researchers in this field and contributing to future studies. Related resources are available at: https://github.com/lmmlzn/Awesome-LLMs-Datasets.",
    "authors": [
      "Yang Liu",
      "Jiahuan Cao",
      "Chongyu Liu",
      "Kai Ding",
      "Lianwen Jin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-28T04:35:51Z",
    "pdf_url": "https://arxiv.org/pdf/2402.18041v1"
  },
  {
    "arxiv_id": "2402.18013v2",
    "entry_id": "http://arxiv.org/abs/2402.18013v2",
    "title": "A Survey on Recent Advances in LLM-Based Multi-turn Dialogue Systems",
    "summary": "This survey provides a comprehensive review of research on multi-turn dialogue systems, with a particular focus on multi-turn dialogue systems based on large language models (LLMs). This paper aims to (a) give a summary of existing LLMs and approaches for adapting LLMs to downstream tasks; (b) elaborate recent advances in multi-turn dialogue systems, covering both LLM-based open-domain dialogue (ODD) and task-oriented dialogue (TOD) systems, along with datasets and evaluation metrics; (c) discuss some future emphasis and recent research problems arising from the development of LLMs and the increasing demands on multi-turn dialogue systems.",
    "authors": [
      "Zihao Yi",
      "Jiarui Ouyang",
      "Zhe Xu",
      "Yuwen Liu",
      "Tianhao Liao",
      "Haohao Luo",
      "Ying Shen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-28T03:16:44Z",
    "pdf_url": "https://arxiv.org/pdf/2402.18013v2"
  },
  {
    "arxiv_id": "2402.18005v2",
    "entry_id": "http://arxiv.org/abs/2402.18005v2",
    "title": "A Sentiment Consolidation Framework for Meta-Review Generation",
    "summary": "Modern natural language generation systems with Large Language Models (LLMs) exhibit the capability to generate a plausible summary of multiple documents; however, it is uncertain if they truly possess the capability of information consolidation to generate summaries, especially on documents with opinionated information. We focus on meta-review generation, a form of sentiment summarisation for the scientific domain. To make scientific sentiment summarization more grounded, we hypothesize that human meta-reviewers follow a three-layer framework of sentiment consolidation to write meta-reviews. Based on the framework, we propose novel prompting methods for LLMs to generate meta-reviews and evaluation metrics to assess the quality of generated meta-reviews. Our framework is validated empirically as we find that prompting LLMs based on the framework -- compared with prompting them with simple instructions -- generates better meta-reviews.",
    "authors": [
      "Miao Li",
      "Jey Han Lau",
      "Eduard Hovy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-28T02:40:09Z",
    "pdf_url": "https://arxiv.org/pdf/2402.18005v2"
  },
  {
    "arxiv_id": "2402.17270v2",
    "entry_id": "http://arxiv.org/abs/2402.17270v2",
    "title": "Multi-Agent, Human-Agent and Beyond: A Survey on Cooperation in Social Dilemmas",
    "summary": "The study of cooperation within social dilemmas has long been a fundamental topic across various disciplines, including computer science and social science. Recent advancements in Artificial Intelligence (AI) have significantly reshaped this field, offering fresh insights into understanding and enhancing cooperation. This survey examines three key areas at the intersection of AI and cooperation in social dilemmas. First, focusing on multi-agent cooperation, we review the intrinsic and external motivations that support cooperation among rational agents, and the methods employed to develop effective strategies against diverse opponents. Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with humans and the human biases towards AI agents. Third, we review the emergent field of leveraging AI agents to enhance cooperation among humans. We conclude by discussing future research avenues, such as using large language models, establishing unified theoretical frameworks, revisiting existing theories of human cooperation, and exploring multiple real-world applications.",
    "authors": [
      "Chunjiang Mu",
      "Hao Guo",
      "Yang Chen",
      "Chen Shen",
      "Shuyue Hu",
      "Zhen Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2024-02-27T07:31:30Z",
    "pdf_url": "https://arxiv.org/pdf/2402.17270v2"
  },
  {
    "arxiv_id": "2402.16968v1",
    "entry_id": "http://arxiv.org/abs/2402.16968v1",
    "title": "A Survey of Large Language Models in Cybersecurity",
    "summary": "Large Language Models (LLMs) have quickly risen to prominence due to their ability to perform at or close to the state-of-the-art in a variety of fields while handling natural language. An important field of research is the application of such models at the cybersecurity context. This survey aims to identify where in the field of cybersecurity LLMs have already been applied, the ways in which they are being used and their limitations in the field. Finally, suggestions are made on how to improve such limitations and what can be expected from these systems once these limitations are overcome.",
    "authors": [
      "Gabriel de Jesus Coelho da Silva",
      "Carlos Becker Westphall"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2024-02-26T19:06:02Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16968v1"
  },
  {
    "arxiv_id": "2402.16827v3",
    "entry_id": "http://arxiv.org/abs/2402.16827v3",
    "title": "A Survey on Data Selection for Language Models",
    "summary": "A major factor in the recent success of large language models is the use of enormous and ever-growing text datasets for unsupervised pre-training. However, naively training a model on all available data may not be optimal (or feasible), as the quality of available text data can vary. Filtering out data can also decrease the carbon footprint and financial costs of training models by reducing the amount of training required. Data selection methods aim to determine which candidate data points to include in the training dataset and how to appropriately sample from the selected data points. The promise of improved data selection methods has caused the volume of research in the area to rapidly expand. However, because deep learning is mostly driven by empirical evidence and experimentation on large-scale data is expensive, few organizations have the resources for extensive data selection research. Consequently, knowledge of effective data selection practices has become concentrated within a few organizations, many of which do not openly share their findings and methodologies. To narrow this gap in knowledge, we present a comprehensive review of existing literature on data selection methods and related research areas, providing a taxonomy of existing approaches. By describing the current landscape of research, this work aims to accelerate progress in data selection by establishing an entry point for new and established researchers. Additionally, throughout this review we draw attention to noticeable holes in the literature and conclude the paper by proposing promising avenues for future research.",
    "authors": [
      "Alon Albalak",
      "Yanai Elazar",
      "Sang Michael Xie",
      "Shayne Longpre",
      "Nathan Lambert",
      "Xinyi Wang",
      "Niklas Muennighoff",
      "Bairu Hou",
      "Liangming Pan",
      "Haewon Jeong",
      "Colin Raffel",
      "Shiyu Chang",
      "Tatsunori Hashimoto",
      "William Yang Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-26T18:54:35Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16827v3"
  },
  {
    "arxiv_id": "2402.16786v2",
    "entry_id": "http://arxiv.org/abs/2402.16786v2",
    "title": "Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models",
    "summary": "Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing constrained evaluation paradigm for values and opinions in LLMs and explore more realistic unconstrained evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT forces models to comply with the PCT's multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs.",
    "authors": [
      "Paul Röttger",
      "Valentin Hofmann",
      "Valentina Pyatkin",
      "Musashi Hinck",
      "Hannah Rose Kirk",
      "Hinrich Schütze",
      "Dirk Hovy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-26T18:00:49Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16786v2"
  },
  {
    "arxiv_id": "2402.16751v3",
    "entry_id": "http://arxiv.org/abs/2402.16751v3",
    "title": "Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems",
    "summary": "Understanding citizens' values in participatory systems is crucial for citizen-centric policy-making. We envision a hybrid participatory system where participants make choices and provide motivations for those choices, and AI agents estimate their value preferences by interacting with them. We focus on situations where a conflict is detected between participants' choices and motivations, and propose methods for estimating value preferences while addressing detected inconsistencies by interacting with the participants. We operationalize the philosophical stance that \"valuing is deliberatively consequential.\" That is, if a participant's choice is based on a deliberation of value preferences, the value preferences can be observed in the motivation the participant provides for the choice. Thus, we propose and compare value preferences estimation methods that prioritize the values estimated from motivations over the values estimated from choices alone. Then, we introduce a disambiguation strategy that combines Natural Language Processing and Active Learning to address the detected inconsistencies between choices and motivations. We evaluate the proposed methods on a dataset of a large-scale survey on energy transition. The results show that explicitly addressing inconsistencies between choices and motivations improves the estimation of an individual's value preferences. The disambiguation strategy does not show substantial improvements when compared to similar baselines--however, we discuss how the novelty of the approach can open new research avenues and propose improvements to address the current limitations.",
    "authors": [
      "Enrico Liscio",
      "Luciano C. Siebert",
      "Catholijn M. Jonker",
      "Pradeep K. Murukannaiah"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-26T17:16:28Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16751v3"
  },
  {
    "arxiv_id": "2402.16929v2",
    "entry_id": "http://arxiv.org/abs/2402.16929v2",
    "title": "LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming Language",
    "summary": "LLMs have demonstrated commendable performance across diverse domains. Nevertheless, formulating high-quality prompts to instruct LLMs proficiently poses a challenge for non-AI experts. Existing research in prompt engineering suggests somewhat scattered optimization principles and designs empirically dependent prompt optimizers. Unfortunately, these endeavors lack a structured design template, incurring high learning costs and resulting in low reusability. In addition, it is not conducive to the iterative updating of prompts. Inspired by structured reusable programming languages, we propose LangGPT, a dual-layer prompt design framework as the programming language for LLMs. LangGPT has an easy-to-learn normative structure and provides an extended structure for migration and reuse. Experiments illustrate that LangGPT significantly enhances the performance of LLMs. Moreover, the case study shows that LangGPT leads LLMs to generate higher-quality responses. Furthermore, we analyzed the ease of use and reusability of LangGPT through a user survey in our online community.",
    "authors": [
      "Ming Wang",
      "Yuanzhong Liu",
      "Xiaoyu Liang",
      "Songlian Li",
      "Yijie Huang",
      "Xiaoming Zhang",
      "Sijia Shen",
      "Chaofeng Guan",
      "Daling Wang",
      "Shi Feng",
      "Huaiwen Zhang",
      "Yifei Zhang",
      "Minghui Zheng",
      "Chi Zhang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "published": "2024-02-26T15:05:16Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16929v2"
  },
  {
    "arxiv_id": "2402.16363v6",
    "entry_id": "http://arxiv.org/abs/2402.16363v6",
    "title": "LLM Inference Unveiled: Survey and Roofline Model Insights",
    "summary": "The field of efficient Large Language Model (LLM) inference is rapidly evolving, presenting a unique blend of opportunities and challenges. Although the field has expanded and is vibrant, there hasn't been a concise framework that analyzes the various methods of LLM Inference to provide a clear understanding of this domain. Our survey stands out from traditional literature reviews by not only summarizing the current state of research but also by introducing a framework based on roofline model for systematic analysis of LLM inference techniques. This framework identifies the bottlenecks when deploying LLMs on hardware devices and provides a clear understanding of practical problems, such as why LLMs are memory-bound, how much memory and computation they need, and how to choose the right hardware. We systematically collate the latest advancements in efficient LLM inference, covering crucial areas such as model compression (e.g., Knowledge Distillation and Quantization), algorithm improvements (e.g., Early Exit and Mixture-of-Expert), and both hardware and system-level enhancements. Our survey stands out by analyzing these methods with roofline model, helping us understand their impact on memory access and computation. This distinctive approach not only showcases the current research landscape but also delivers valuable insights for practical implementation, positioning our work as an indispensable resource for researchers new to the field as well as for those seeking to deepen their understanding of efficient LLM deployment. The analyze tool, LLM-Viewer, is open-sourced.",
    "authors": [
      "Zhihang Yuan",
      "Yuzhang Shang",
      "Yang Zhou",
      "Zhen Dong",
      "Zhe Zhou",
      "Chenhao Xue",
      "Bingzhe Wu",
      "Zhikai Li",
      "Qingyi Gu",
      "Yong Jae Lee",
      "Yan Yan",
      "Beidi Chen",
      "Guangyu Sun",
      "Kurt Keutzer"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-26T07:33:05Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16363v6"
  },
  {
    "arxiv_id": "2402.16269v1",
    "entry_id": "http://arxiv.org/abs/2402.16269v1",
    "title": "From Large Language Models and Optimization to Decision Optimization CoPilot: A Research Manifesto",
    "summary": "Significantly simplifying the creation of optimization models for real-world business problems has long been a major goal in applying mathematical optimization more widely to important business and societal decisions. The recent capabilities of Large Language Models (LLMs) present a timely opportunity to achieve this goal. Therefore, we propose research at the intersection of LLMs and optimization to create a Decision Optimization CoPilot (DOCP) - an AI tool designed to assist any decision maker, interacting in natural language to grasp the business problem, subsequently formulating and solving the corresponding optimization model. This paper outlines our DOCP vision and identifies several fundamental requirements for its implementation. We describe the state of the art through a literature survey and experiments using ChatGPT. We show that a) LLMs already provide substantial novel capabilities relevant to a DOCP, and b) major research challenges remain to be addressed. We also propose possible research directions to overcome these gaps. We also see this work as a call to action to bring together the LLM and optimization communities to pursue our vision, thereby enabling much more widespread improved decision-making.",
    "authors": [
      "Segev Wasserkrug",
      "Leonard Boussioux",
      "Dick den Hertog",
      "Farzaneh Mirzazadeh",
      "Ilker Birbil",
      "Jannis Kurtz",
      "Donato Maragno"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "math.OC"
    ],
    "published": "2024-02-26T03:10:11Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16269v1"
  },
  {
    "arxiv_id": "2402.16142v1",
    "entry_id": "http://arxiv.org/abs/2402.16142v1",
    "title": "From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility",
    "summary": "This groundbreaking study explores the expanse of Large Language Models (LLMs), such as Generative Pre-Trained Transformer (GPT) and Bidirectional Encoder Representations from Transformers (BERT) across varied domains ranging from technology, finance, healthcare to education. Despite their established prowess in Natural Language Processing (NLP), these LLMs have not been systematically examined for their impact on domains such as fitness, and holistic well-being, urban planning, climate modelling as well as disaster management. This review paper, in addition to furnishing a comprehensive analysis of the vast expanse and extent of LLMs' utility in diverse domains, recognizes the research gaps and realms where the potential of LLMs is yet to be harnessed. This study uncovers innovative ways in which LLMs can leave a mark in the fields like fitness and wellbeing, urban planning, climate modelling and disaster response which could inspire future researches and applications in the said avenues.",
    "authors": [
      "Pravneet Kaur",
      "Gautam Siddharth Kashyap",
      "Ankit Kumar",
      "Md Tabrez Nafis",
      "Sandeep Kumar",
      "Vikrant Shokeen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-25T16:47:59Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16142v1"
  },
  {
    "arxiv_id": "2402.16035v1",
    "entry_id": "http://arxiv.org/abs/2402.16035v1",
    "title": "Text Understanding and Generation Using Transformer Models for Intelligent E-commerce Recommendations",
    "summary": "With the rapid development of artificial intelligence technology, Transformer structural pre-training model has become an important tool for large language model (LLM) tasks. In the field of e-commerce, these models are especially widely used, from text understanding to generating recommendation systems, which provide powerful technical support for improving user experience and optimizing service processes. This paper reviews the core application scenarios of Transformer pre-training model in e-commerce text understanding and recommendation generation, including but not limited to automatic generation of product descriptions, sentiment analysis of user comments, construction of personalized recommendation system and automated processing of customer service conversations. Through a detailed analysis of the model's working principle, implementation process, and application effects in specific cases, this paper emphasizes the unique advantages of pre-trained models in understanding complex user intentions and improving the quality of recommendations. In addition, the challenges and improvement directions for the future are also discussed, such as how to further improve the generalization ability of the model, the ability to handle large-scale data sets, and technical strategies to protect user privacy. Ultimately, the paper points out that the application of Transformer structural pre-training models in e-commerce has not only driven technological innovation, but also brought substantial benefits to merchants and consumers, and looking forward, these models will continue to play a key role in e-commerce and beyond.",
    "authors": [
      "Yafei Xiang",
      "Hanyi Yu",
      "Yulu Gong",
      "Shuning Huo",
      "Mengran Zhu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-25T09:19:11Z",
    "pdf_url": "https://arxiv.org/pdf/2402.16035v1"
  },
  {
    "arxiv_id": "2402.15589v2",
    "entry_id": "http://arxiv.org/abs/2402.15589v2",
    "title": "LLMs as Meta-Reviewers' Assistants: A Case Study",
    "summary": "One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves assimilating diverse opinions from multiple expert peers, formulating one's self-judgment as a senior expert, and then summarizing all these perspectives into a concise holistic overview to make an overall recommendation. This process is time-consuming and can be compromised by human factors like fatigue, inconsistency, missing tiny details, etc. Given the latest major developments in Large Language Models (LLMs), it is very compelling to rigorously study whether LLMs can help metareviewers perform this important task better. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to assist meta-reviewers in better comprehending multiple experts perspectives by generating a controlled multi-perspective summary (MPS) of their opinions. To achieve this, we prompt three LLMs with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the MPSs generated by the LLMs and report our findings.",
    "authors": [
      "Eftekhar Hossain",
      "Sanjeev Kumar Sinha",
      "Naman Bansal",
      "Alex Knipper",
      "Souvika Sarkar",
      "John Salvador",
      "Yash Mahajan",
      "Sri Guttikonda",
      "Mousumi Akter",
      "Md. Mahadi Hassan",
      "Matthew Freestone",
      "Matthew C. Williams",
      "Dongji Feng",
      "Santu Karmaker"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2024-02-23T20:14:16Z",
    "pdf_url": "https://arxiv.org/pdf/2402.15589v2"
  },
  {
    "arxiv_id": "2402.15116v1",
    "entry_id": "http://arxiv.org/abs/2402.15116v1",
    "title": "Large Multimodal Agents: A Survey",
    "summary": "Large language models (LLMs) have achieved superior performance in powering text-based AI agents, endowing them with decision-making and reasoning abilities akin to humans. Concurrently, there is an emerging research trend focused on extending these LLM-powered AI agents into the multimodal domain. This extension enables AI agents to interpret and respond to diverse multimodal user queries, thereby handling more intricate and nuanced tasks. In this paper, we conduct a systematic review of LLM-driven multimodal agents, which we refer to as large multimodal agents ( LMAs for short). First, we introduce the essential components involved in developing LMAs and categorize the current body of research into four distinct types. Subsequently, we review the collaborative frameworks integrating multiple LMAs , enhancing collective efficacy. One of the critical challenges in this field is the diverse evaluation methods used across existing studies, hindering effective comparison among different LMAs . Therefore, we compile these evaluation methodologies and establish a comprehensive framework to bridge the gaps. This framework aims to standardize evaluations, facilitating more meaningful comparisons. Concluding our review, we highlight the extensive applications of LMAs and propose possible future research directions. Our discussion aims to provide valuable insights and guidelines for future research in this rapidly evolving field. An up-to-date resource list is available at https://github.com/jun0wanan/awesome-large-multimodal-agents.",
    "authors": [
      "Junlin Xie",
      "Zhihong Chen",
      "Ruifei Zhang",
      "Xiang Wan",
      "Guanbin Li"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-23T06:04:23Z",
    "pdf_url": "https://arxiv.org/pdf/2402.15116v1"
  },
  {
    "arxiv_id": "2402.14304v2",
    "entry_id": "http://arxiv.org/abs/2402.14304v2",
    "title": "Vision-Language Navigation with Embodied Intelligence: A Survey",
    "summary": "As a long-term vision in the field of artificial intelligence, the core goal of embodied intelligence is to improve the perception, understanding, and interaction capabilities of agents and the environment. Vision-language navigation (VLN), as a critical research path to achieve embodied intelligence, focuses on exploring how agents use natural language to communicate effectively with humans, receive and understand instructions, and ultimately rely on visual information to achieve accurate navigation. VLN integrates artificial intelligence, natural language processing, computer vision, and robotics. This field faces technical challenges but shows potential for application such as human-computer interaction. However, due to the complex process involved from language understanding to action execution, VLN faces the problem of aligning visual information and language instructions, improving generalization ability, and many other challenges. This survey systematically reviews the research progress of VLN and details the research direction of VLN with embodied intelligence. After a detailed summary of its system architecture and research based on methods and commonly used benchmark datasets, we comprehensively analyze the problems and challenges faced by current research and explore the future development direction of this field, aiming to provide a practical reference for researchers.",
    "authors": [
      "Peng Gao",
      "Peng Wang",
      "Feng Gao",
      "Fei Wang",
      "Ruyue Yuan"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-02-22T05:45:17Z",
    "pdf_url": "https://arxiv.org/pdf/2402.14304v2"
  },
  {
    "arxiv_id": "2402.14873v3",
    "entry_id": "http://arxiv.org/abs/2402.14873v3",
    "title": "Technical Report on the Pangram AI-Generated Text Classifier",
    "summary": "We present Pangram Text, a transformer-based neural network trained to distinguish text written by large language models from text written by humans. Pangram Text outperforms zero-shot methods such as DetectGPT as well as leading commercial AI detection tools with over 38 times lower error rates on a comprehensive benchmark comprised of 10 text domains (student writing, creative writing, scientific writing, books, encyclopedias, news, email, scientific papers, short-form Q&A) and 8 open- and closed-source large language models. We propose a training algorithm, hard negative mining with synthetic mirrors, that enables our classifier to achieve orders of magnitude lower false positive rates on high-data domains such as reviews. Finally, we show that Pangram Text is not biased against nonnative English speakers and generalizes to domains and models unseen during training.",
    "authors": [
      "Bradley Emi",
      "Max Spero"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-21T17:13:41Z",
    "pdf_url": "https://arxiv.org/pdf/2402.14873v3"
  },
  {
    "arxiv_id": "2402.13598v2",
    "entry_id": "http://arxiv.org/abs/2402.13598v2",
    "title": "User-LLM: Efficient LLM Contextualization with User Embeddings",
    "summary": "Large language models (LLMs) have achieved remarkable success across various domains, but effectively incorporating complex and potentially noisy user timeline data into LLMs remains a challenge. Current approaches often involve translating user timelines into text descriptions before feeding them to LLMs, which can be inefficient and may not fully capture the nuances of user behavior. Inspired by how LLMs are effectively integrated with images through direct embeddings, we propose User-LLM, a novel framework that leverages user embeddings to directly contextualize LLMs with user history interactions. These embeddings, generated by a user encoder pretrained using self-supervised learning on diverse user interactions, capture latent user behaviors and interests as well as their evolution over time. We integrate these user embeddings with LLMs through cross-attention, enabling LLMs to dynamically adapt their responses based on the context of a user's past actions and preferences.\n  Our approach achieves significant efficiency gains by representing user timelines directly as embeddings, leading to substantial inference speedups of up to 78.1X. Comprehensive experiments on MovieLens, Amazon Review, and Google Local Review datasets demonstrate that User-LLM outperforms text-prompt-based contextualization on tasks requiring deep user understanding, with improvements of up to 16.33%, particularly excelling on long sequences that capture subtle shifts in user behavior. Furthermore, the incorporation of Perceiver layers streamlines the integration between user encoders and LLMs, yielding additional computational savings.",
    "authors": [
      "Lin Ning",
      "Luyang Liu",
      "Jiaxing Wu",
      "Neo Wu",
      "Devora Berlowitz",
      "Sushant Prakash",
      "Bradley Green",
      "Shawn O'Banion",
      "Jun Xie"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-02-21T08:03:27Z",
    "pdf_url": "https://arxiv.org/pdf/2402.13598v2"
  },
  {
    "arxiv_id": "2402.13517v2",
    "entry_id": "http://arxiv.org/abs/2402.13517v2",
    "title": "Round Trip Translation Defence against Large Language Model Jailbreaking Attacks",
    "summary": "Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence\n  This version of the article has been accepted for publication, after peer review (when applicable) but is not the Version of Record and does not reflect post-acceptance improvements, or any corrections. The Version of Record is available online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this Accepted Version is subject to the publisher's Accepted Manuscript terms of use https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms",
    "authors": [
      "Canaan Yung",
      "Hadi Mohaghegh Dolatabadi",
      "Sarah Erfani",
      "Christopher Leckie"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-21T03:59:52Z",
    "pdf_url": "https://arxiv.org/pdf/2402.13517v2"
  },
  {
    "arxiv_id": "2404.07214v4",
    "entry_id": "http://arxiv.org/abs/2404.07214v4",
    "title": "Exploring the Frontier of Vision-Language Models: A Survey of Current Methodologies and Future Directions",
    "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the trajectory of the AI revolution. Nevertheless, these LLMs exhibit a notable limitation, as they are primarily adept at processing textual information. To address this constraint, researchers have endeavored to integrate visual capabilities with LLMs, resulting in the emergence of Vision-Language Models (VLMs). These advanced models are instrumental in tackling more intricate tasks such as image captioning and visual question answering. In our comprehensive survey paper, we delve into the key advancements within the realm of VLMs. Our classification organizes VLMs into three distinct categories: models dedicated to vision-language understanding, models that process multimodal inputs to generate unimodal (textual) outputs and models that both accept and produce multimodal inputs and outputs.This classification is based on their respective capabilities and functionalities in processing and generating various modalities of data.We meticulously dissect each model, offering an extensive analysis of its foundational architecture, training data sources, as well as its strengths and limitations wherever possible, providing readers with a comprehensive understanding of its essential components. We also analyzed the performance of VLMs in various benchmark datasets. By doing so, we aim to offer a nuanced understanding of the diverse landscape of VLMs. Additionally, we underscore potential avenues for future research in this dynamic domain, anticipating further breakthroughs and advancements.",
    "authors": [
      "Akash Ghosh",
      "Arkadeep Acharya",
      "Sriparna Saha",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-20T18:57:34Z",
    "pdf_url": "https://arxiv.org/pdf/2404.07214v4"
  },
  {
    "arxiv_id": "2402.12794v1",
    "entry_id": "http://arxiv.org/abs/2402.12794v1",
    "title": "Autonomous Reality Modelling for Cultural Heritage Sites employing cooperative quadrupedal robots and unmanned aerial vehicles",
    "summary": "Nowadays, the use of advanced sensors, such as terrestrial 3D laser scanners, mobile LiDARs and Unmanned Aerial Vehicles (UAV) photogrammetric imaging, has become the prevalent practice for 3D Reality Modeling and digitization of large-scale monuments of Cultural Heritage (CH). In practice, this process is heavily related to the expertise of the surveying team, handling the laborious planning and time-consuming execution of the 3D mapping process that is tailored to the specific requirements and constraints of each site. To minimize human intervention, this paper introduces a novel methodology for autonomous 3D Reality Modeling for CH monuments by employing au-tonomous biomimetic quadrupedal robotic agents and UAVs equipped with the appropriate sensors. These autonomous robotic agents carry out the 3D RM process in a systematic and repeatable ap-proach. The outcomes of this automated process may find applications in digital twin platforms, facilitating secure monitoring and management of cultural heritage sites and spaces, in both indoor and outdoor environments.",
    "authors": [
      "Nikolaos Giakoumidis",
      "Christos-Nikolaos Anagnostopoulos"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-02-20T08:08:07Z",
    "pdf_url": "https://arxiv.org/pdf/2402.12794v1"
  },
  {
    "arxiv_id": "2402.14851v2",
    "entry_id": "http://arxiv.org/abs/2402.14851v2",
    "title": "$R^3$: \"This is My SQL, Are You With Me?\" A Consensus-Based Multi-Agent System for Text-to-SQL Tasks",
    "summary": "Large Language Models (LLMs) have demonstrated strong performance on various tasks. To unleash their power on the Text-to-SQL task, we propose $R^3$ (Review-Rebuttal-Revision), a consensus-based multi-agent system for Text-to-SQL tasks. $R^3$ outperforms the existing single LLM Text-to-SQL systems as well as the multi-agent Text-to-SQL systems by $1.3\\%$ to $8.1\\%$ on Spider and Bird. Surprisingly, we find that for Llama-3-8B, $R^3$ outperforms chain-of-thought prompting by over 20\\%, even outperforming GPT-3.5 on the development set of Spider.",
    "authors": [
      "Hanchen Xia",
      "Feng Jiang",
      "Naihao Deng",
      "Cunxiang Wang",
      "Guojiang Zhao",
      "Rada Mihalcea",
      "Yue Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "published": "2024-02-20T03:57:55Z",
    "pdf_url": "https://arxiv.org/pdf/2402.14851v2"
  },
  {
    "arxiv_id": "2402.12451v2",
    "entry_id": "http://arxiv.org/abs/2402.12451v2",
    "title": "The Revolution of Multimodal Large Language Models: A Survey",
    "summary": "Connecting text and visual modalities plays an essential role in generative intelligence. For this reason, inspired by the success of large language models, significant research efforts are being devoted to the development of Multimodal Large Language Models (MLLMs). These models can seamlessly integrate visual and textual modalities, while providing a dialogue-based interface and instruction-following capabilities. In this paper, we provide a comprehensive review of recent visual-based MLLMs, analyzing their architectural choices, multimodal alignment strategies, and training techniques. We also conduct a detailed analysis of these models across a wide range of tasks, including visual grounding, image generation and editing, visual understanding, and domain-specific applications. Additionally, we compile and describe training datasets and evaluation benchmarks, conducting comparisons among existing models in terms of performance and computational requirements. Overall, this survey offers a comprehensive overview of the current state of the art, laying the groundwork for future MLLMs.",
    "authors": [
      "Davide Caffagni",
      "Federico Cocchi",
      "Luca Barsellotti",
      "Nicholas Moratelli",
      "Sara Sarto",
      "Lorenzo Baraldi",
      "Lorenzo Baraldi",
      "Marcella Cornia",
      "Rita Cucchiara"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "published": "2024-02-19T19:01:01Z",
    "pdf_url": "https://arxiv.org/pdf/2402.12451v2"
  },
  {
    "arxiv_id": "2403.15401v3",
    "entry_id": "http://arxiv.org/abs/2403.15401v3",
    "title": "Large Language Model for Mental Health: A Systematic Review",
    "summary": "Large language models (LLMs) have attracted significant attention for potential applications in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to evaluate the usage of LLMs in mental health, focusing on their strengths and limitations in early screening, digital interventions, and clinical applications. Adhering to PRISMA guidelines, we searched PubMed, IEEE Xplore, Scopus, JMIR, and ACM using keywords: 'mental health OR mental illness OR mental disorder OR psychiatry' AND 'large language models'. We included articles published between January 1, 2017, and April 30, 2024, excluding non-English articles. 30 articles were evaluated, which included research on mental health conditions and suicidal ideation detection through text (n=15), usage of LLMs for mental health conversational agents (CAs) (n=7), and other applications and evaluations of LLMs in mental health (n=18). LLMs exhibit substantial effectiveness in detecting mental health issues and providing accessible, de-stigmatized eHealth services. However, the current risks associated with the clinical use might surpass their benefits. The study identifies several significant issues: the lack of multilingual datasets annotated by experts, concerns about the accuracy and reliability of the content generated, challenges in interpretability due to the 'black box' nature of LLMs, and persistent ethical dilemmas. These include the lack of a clear ethical framework, concerns about data privacy, and the potential for over-reliance on LLMs by both therapists and patients, which could compromise traditional medical practice. Despite these issues, the rapid development of LLMs underscores their potential as new clinical aids, emphasizing the need for continued research and development in this area.",
    "authors": [
      "Zhijun Guo",
      "Alvina Lai",
      "Johan Hilge Thygesen",
      "Joseph Farrington",
      "Thomas Keen",
      "Kezhi Li"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-19T17:58:41Z",
    "pdf_url": "https://arxiv.org/pdf/2403.15401v3"
  },
  {
    "arxiv_id": "2402.12121v2",
    "entry_id": "http://arxiv.org/abs/2402.12121v2",
    "title": "IRR: Image Review Ranking Framework for Evaluating Vision-Language Models",
    "summary": "Large-scale Vision-Language Models (LVLMs) process both images and text, excelling in multimodal tasks such as image captioning and description generation. However, while these models excel at generating factual content, their ability to generate and evaluate texts reflecting perspectives on the same image, depending on the context, has not been sufficiently explored. To address this, we propose IRR: Image Review Rank, a novel evaluation framework designed to assess critic review texts from multiple perspectives. IRR evaluates LVLMs by measuring how closely their judgments align with human interpretations. We validate it using a dataset of images from 15 categories, each with five critic review texts and annotated rankings in both English and Japanese, totaling over 2,000 data instances. The datasets are available at https://hf.co/datasets/naist-nlp/Wiki-ImageReview1.0. Our results indicate that, although LVLMs exhibited consistent performance across languages, their correlation with human annotations was insufficient, highlighting the need for further advancements. These findings highlight the limitations of current evaluation methods and the need for approaches that better capture human reasoning in Vision & Language tasks.",
    "authors": [
      "Kazuki Hayashi",
      "Kazuma Onishi",
      "Toma Suzuki",
      "Yusuke Ide",
      "Seiji Gobara",
      "Shigeki Saito",
      "Yusuke Sakai",
      "Hidetaka Kamigaito",
      "Katsuhiko Hayashi",
      "Taro Watanabe"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "published": "2024-02-19T13:16:10Z",
    "pdf_url": "https://arxiv.org/pdf/2402.12121v2"
  },
  {
    "arxiv_id": "2402.11892v2",
    "entry_id": "http://arxiv.org/abs/2402.11892v2",
    "title": "Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing",
    "summary": "In this paper, we propose shifting the focus of robustness evaluation for Neural Program Repair (NPR) techniques toward naturally-occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes (1) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations, and (2) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60% of these transformations are deemed natural, while 20% are considered unnatural, with strong agreement among annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing. Next, we conduct natural robustness testing on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Additionally, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases on NPR evaluation introduced by limited datasets. Finally, we propose an LLM-based metric to automate the assessment of transformation naturalness, ensuring the scalability of natural robustness testing.",
    "authors": [
      "Thanh Le-Cong",
      "Dat Nguyen",
      "Bach Le",
      "Toby Murray"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-02-19T07:07:44Z",
    "pdf_url": "https://arxiv.org/pdf/2402.11892v2"
  },
  {
    "arxiv_id": "2403.00784v1",
    "entry_id": "http://arxiv.org/abs/2403.00784v1",
    "title": "Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges",
    "summary": "Recent years have witnessed a substantial increase in the use of deep learning to solve various natural language processing (NLP) problems. Early deep learning models were constrained by their sequential or unidirectional nature, such that they struggled to capture the contextual relationships across text inputs. The introduction of bidirectional encoder representations from transformers (BERT) leads to a robust encoder for the transformer model that can understand the broader context and deliver state-of-the-art performance across various NLP tasks. This has inspired researchers and practitioners to apply BERT to practical problems, such as information retrieval (IR). A survey that focuses on a comprehensive analysis of prevalent approaches that apply pretrained transformer encoders like BERT to IR can thus be useful for academia and the industry. In light of this, we revisit a variety of BERT-based methods in this survey, cover a wide range of techniques of IR, and group them into six high-level categories: (i) handling long documents, (ii) integrating semantic information, (iii) balancing effectiveness and efficiency, (iv) predicting the weights of terms, (v) query expansion, and (vi) document expansion. We also provide links to resources, including datasets and toolkits, for BERT-based IR systems. A key highlight of our survey is the comparison between BERT's encoder-based models and the latest generative Large Language Models (LLMs), such as ChatGPT, which rely on decoders. Despite the popularity of LLMs, we find that for specific tasks, finely tuned BERT encoders still outperform, and at a lower deployment cost. Finally, we summarize the comprehensive outcomes of the survey and suggest directions for future research in the area.",
    "authors": [
      "Jiajia Wang",
      "Jimmy X. Huang",
      "Xinhui Tu",
      "Junmei Wang",
      "Angela J. Huang",
      "Md Tahmid Rahman Laskar",
      "Amran Bhuiyan"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-18T23:22:40Z",
    "pdf_url": "https://arxiv.org/pdf/2403.00784v1"
  },
  {
    "arxiv_id": "2402.14837v1",
    "entry_id": "http://arxiv.org/abs/2402.14837v1",
    "title": "An Empirical Categorization of Prompting Techniques for Large Language Models: A Practitioner's Guide",
    "summary": "Due to rapid advancements in the development of Large Language Models (LLMs), programming these models with prompts has recently gained significant attention. However, the sheer number of available prompt engineering techniques creates an overwhelming landscape for practitioners looking to utilize these tools. For the most efficient and effective use of LLMs, it is important to compile a comprehensive list of prompting techniques and establish a standardized, interdisciplinary categorization framework. In this survey, we examine some of the most well-known prompting techniques from both academic and practical viewpoints and classify them into seven distinct categories. We present an overview of each category, aiming to clarify their unique contributions and showcase their practical applications in real-world examples in order to equip fellow practitioners with a structured framework for understanding and categorizing prompting techniques tailored to their specific domains. We believe that this approach will help simplify the complex landscape of prompt engineering and enable more effective utilization of LLMs in various applications. By providing practitioners with a systematic approach to prompt categorization, we aim to assist in navigating the intricacies of effective prompt design for conversational pre-trained LLMs and inspire new possibilities in their respective fields.",
    "authors": [
      "Oluwole Fagbohun",
      "Rachel M. Harrison",
      "Anton Dereventsov"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-02-18T23:03:56Z",
    "pdf_url": "https://arxiv.org/pdf/2402.14837v1"
  },
  {
    "arxiv_id": "2402.11518v2",
    "entry_id": "http://arxiv.org/abs/2402.11518v2",
    "title": "Large Language Model-driven Meta-structure Discovery in Heterogeneous Information Network",
    "summary": "Heterogeneous information networks (HIN) have gained increasing popularity in recent years for capturing complex relations between diverse types of nodes. Meta-structures are proposed as a useful tool to identify the important patterns in HINs, but hand-crafted meta-structures pose significant challenges for scaling up, drawing wide research attention towards developing automatic search algorithms. Previous efforts primarily focused on searching for meta-structures with good empirical performance, overlooking the importance of human comprehensibility and generalizability. To address this challenge, we draw inspiration from the emergent reasoning abilities of large language models (LLMs). We propose ReStruct, a meta-structure search framework that integrates LLM reasoning into the evolutionary procedure. ReStruct uses a grammar translator to encode the meta-structures into natural language sentences, and leverages the reasoning power of LLMs to evaluate their semantic feasibility. Besides, ReStruct also employs performance-oriented evolutionary operations. These two competing forces allow ReStruct to jointly optimize the semantic explainability and empirical performance of meta-structures. Furthermore, ReStruct contains a differential LLM explainer to generate and refine natural language explanations for the discovered meta-structures by reasoning through the search history. Experiments on eight representative HIN datasets demonstrate that ReStruct achieves state-of-the-art performance in both recommendation and node classification tasks. Moreover, a survey study involving 73 graduate students shows that the discovered meta-structures and generated explanations by ReStruct are substantially more comprehensible. Our code and questionnaire are available at https://github.com/LinChen-65/ReStruct.",
    "authors": [
      "Lin Chen",
      "Fengli Xu",
      "Nian Li",
      "Zhenyu Han",
      "Meng Wang",
      "Yong Li",
      "Pan Hui"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-02-18T09:21:12Z",
    "pdf_url": "https://arxiv.org/pdf/2402.11518v2"
  },
  {
    "arxiv_id": "2402.11291v3",
    "entry_id": "http://arxiv.org/abs/2402.11291v3",
    "title": "Puzzle Solving using Reasoning of Large Language Models: A Survey",
    "summary": "Exploring the capabilities of Large Language Models (LLMs) in puzzle solving unveils critical insights into their potential and challenges in AI, marking a significant step towards understanding their applicability in complex reasoning tasks. This survey leverages a unique taxonomy -- dividing puzzles into rule-based and rule-less categories -- to critically assess LLMs through various methodologies, including prompting techniques, neuro-symbolic approaches, and fine-tuning. Through a critical review of relevant datasets and benchmarks, we assess LLMs' performance, identifying significant challenges in complex puzzle scenarios. Our findings highlight the disparity between LLM capabilities and human-like reasoning, particularly in those requiring advanced logical inference. The survey underscores the necessity for novel strategies and richer datasets to advance LLMs' puzzle-solving proficiency and contribute to AI's logical reasoning and creative problem-solving advancements.",
    "authors": [
      "Panagiotis Giadikiaroglou",
      "Maria Lymperaiou",
      "Giorgos Filandrianos",
      "Giorgos Stamou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-17T14:19:38Z",
    "pdf_url": "https://arxiv.org/pdf/2402.11291v3"
  },
  {
    "arxiv_id": "2402.11073v3",
    "entry_id": "http://arxiv.org/abs/2402.11073v3",
    "title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators",
    "summary": "With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.",
    "authors": [
      "Jingwei Ni",
      "Minjing Shi",
      "Dominik Stammbach",
      "Mrinmaya Sachan",
      "Elliott Ash",
      "Markus Leippold"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-16T20:59:57Z",
    "pdf_url": "https://arxiv.org/pdf/2402.11073v3"
  },
  {
    "arxiv_id": "2402.11068v2",
    "entry_id": "http://arxiv.org/abs/2402.11068v2",
    "title": "Large Language Models for Causal Discovery: Current Landscape and Future Directions",
    "summary": "Causal discovery (CD) and Large Language Models (LLMs) have emerged as transformative fields in artificial intelligence that have evolved largely independently. While CD specializes in uncovering cause-effect relationships from data, and LLMs excel at natural language processing and generation, their integration presents unique opportunities for advancing causal understanding. This survey examines how LLMs are transforming CD across three key dimensions: direct causal extraction from text, integration of domain knowledge into statistical methods, and refinement of causal structures. We systematically analyze approaches that leverage LLMs for CD tasks, highlighting their innovative use of metadata and natural language for causal inference. Our analysis reveals both LLMs' potential to enhance traditional CD methods and their current limitations as imperfect expert systems. We identify key research gaps, outline evaluation frameworks and benchmarks for LLM-based causal discovery, and advocate future research efforts for leveraging LLMs in causality research. As the first comprehensive examination of the synergy between LLMs and CD, this work lays the groundwork for future advances in the field.",
    "authors": [
      "Guangya Wan",
      "Yunsheng Lu",
      "Yuqi Wu",
      "Mengxuan Hu",
      "Sheng Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-16T20:48:53Z",
    "pdf_url": "https://arxiv.org/pdf/2402.11068v2"
  },
  {
    "arxiv_id": "2402.10659v7",
    "entry_id": "http://arxiv.org/abs/2402.10659v7",
    "title": "Network Formation and Dynamics Among Multi-LLMs",
    "summary": "Social networks profoundly influence how humans form opinions, exchange information, and organize collectively. As large language models (LLMs) are increasingly embedded into social and professional environments, it is critical to understand whether their interactions approximate human-like network dynamics. We develop a framework to study the network formation behaviors of multiple LLM agents and benchmark them against human decisions. Across synthetic and real-world settings, including friendship, telecommunication, and employment networks, we find that LLMs consistently reproduce fundamental micro-level principles such as preferential attachment, triadic closure, and homophily, as well as macro-level properties including community structure and small-world effects. Importantly, the relative emphasis of these principles adapts to context: for example, LLMs favor homophily in friendship networks but heterophily in organizational settings, mirroring patterns of social mobility. A controlled human-subject survey confirms strong alignment between LLMs and human participants in link-formation decisions. These results establish that LLMs can serve as powerful tools for social simulation and synthetic data generation, while also raising critical questions about bias, fairness, and the design of AI systems that participate in human networks.",
    "authors": [
      "Marios Papachristou",
      "Yuan Yuan"
    ],
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2024-02-16T13:10:14Z",
    "pdf_url": "https://arxiv.org/pdf/2402.10659v7"
  },
  {
    "arxiv_id": "2402.10409v1",
    "entry_id": "http://arxiv.org/abs/2402.10409v1",
    "title": "Understanding Survey Paper Taxonomy about Large Language Models via Graph Representation Learning",
    "summary": "As new research on Large Language Models (LLMs) continues, it is difficult to keep up with new research and models. To help researchers synthesize the new research many have written survey papers, but even those have become numerous. In this paper, we develop a method to automatically assign survey papers to a taxonomy. We collect the metadata of 144 LLM survey papers and explore three paradigms to classify papers within the taxonomy. Our work indicates that leveraging graph structure information on co-category graphs can significantly outperform the language models in two paradigms; pre-trained language models' fine-tuning and zero-shot/few-shot classifications using LLMs. We find that our model surpasses an average human recognition level and that fine-tuning LLMs using weak labels generated by a smaller model, such as the GCN in this study, can be more effective than using ground-truth labels, revealing the potential of weak-to-strong generalization in the taxonomy classification task.",
    "authors": [
      "Jun Zhuang",
      "Casey Kennington"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-02-16T02:21:59Z",
    "pdf_url": "https://arxiv.org/pdf/2402.10409v1"
  },
  {
    "arxiv_id": "2402.10350v1",
    "entry_id": "http://arxiv.org/abs/2402.10350v1",
    "title": "Large Language Models for Forecasting and Anomaly Detection: A Systematic Literature Review",
    "summary": "This systematic literature review comprehensively examines the application of Large Language Models (LLMs) in forecasting and anomaly detection, highlighting the current state of research, inherent challenges, and prospective future directions. LLMs have demonstrated significant potential in parsing and analyzing extensive datasets to identify patterns, predict future events, and detect anomalous behavior across various domains. However, this review identifies several critical challenges that impede their broader adoption and effectiveness, including the reliance on vast historical datasets, issues with generalizability across different contexts, the phenomenon of model hallucinations, limitations within the models' knowledge boundaries, and the substantial computational resources required. Through detailed analysis, this review discusses potential solutions and strategies to overcome these obstacles, such as integrating multimodal data, advancements in learning methodologies, and emphasizing model explainability and computational efficiency. Moreover, this review outlines critical trends that are likely to shape the evolution of LLMs in these fields, including the push toward real-time processing, the importance of sustainable modeling practices, and the value of interdisciplinary collaboration. Conclusively, this review underscores the transformative impact LLMs could have on forecasting and anomaly detection while emphasizing the need for continuous innovation, ethical considerations, and practical solutions to realize their full potential.",
    "authors": [
      "Jing Su",
      "Chufeng Jiang",
      "Xin Jin",
      "Yuxin Qiao",
      "Tingsong Xiao",
      "Hongda Ma",
      "Rong Wei",
      "Zhi Jing",
      "Jiajun Xu",
      "Junhong Lin"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-02-15T22:43:02Z",
    "pdf_url": "https://arxiv.org/pdf/2402.10350v1"
  },
  {
    "arxiv_id": "2402.09939v1",
    "entry_id": "http://arxiv.org/abs/2402.09939v1",
    "title": "Generative AI in the Construction Industry: A State-of-the-art Analysis",
    "summary": "The construction industry is a vital sector of the global economy, but it faces many productivity challenges in various processes, such as design, planning, procurement, inspection, and maintenance. Generative artificial intelligence (AI), which can create novel and realistic data or content, such as text, image, video, or code, based on some input or prior knowledge, offers innovative and disruptive solutions to address these challenges. However, there is a gap in the literature on the current state, opportunities, and challenges of generative AI in the construction industry. This study aims to fill this gap by providing a state-of-the-art analysis of generative AI in construction, with three objectives: (1) to review and categorize the existing and emerging generative AI opportunities and challenges in the construction industry; (2) to propose a framework for construction firms to build customized generative AI solutions using their own data, comprising steps such as data collection, dataset curation, training custom large language model (LLM), model evaluation, and deployment; and (3) to demonstrate the framework via a case study of developing a generative model for querying contract documents. The results show that retrieval augmented generation (RAG) improves the baseline LLM by 5.2, 9.4, and 4.8% in terms of quality, relevance, and reproducibility. This study provides academics and construction professionals with a comprehensive analysis and practical framework to guide the adoption of generative AI techniques to enhance productivity, quality, safety, and sustainability across the construction industry.",
    "authors": [
      "Ridwan Taiwo",
      "Idris Temitope Bello",
      "Sulemana Fatoama Abdulai",
      "Abdul-Mugis Yussif",
      "Babatunde Abiodun Salami",
      "Abdullahi Saka",
      "Tarek Zayed"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-02-15T13:39:55Z",
    "pdf_url": "https://arxiv.org/pdf/2402.09939v1"
  },
  {
    "arxiv_id": "2402.09921v1",
    "entry_id": "http://arxiv.org/abs/2402.09921v1",
    "title": "Identifying and modelling cognitive biases in mobility choices",
    "summary": "This report presents results from an M1 internship dedicated to agent-based modelling and simulation of daily mobility choices. This simulation is intended to be realistic enough to serve as a basis for a serious game about the mobility transition. In order to ensure this level of realism, we conducted a survey to measure if real mobility choices are made rationally, or how biased they are. Results analysed here show that various biases could play a role in decisions. We then propose an implementation in a GAMA agent-based simulation.",
    "authors": [
      "Chloe Conrad",
      "Carole Adam"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2024-02-15T12:58:27Z",
    "pdf_url": "https://arxiv.org/pdf/2402.09921v1"
  },
  {
    "arxiv_id": "2402.09748v1",
    "entry_id": "http://arxiv.org/abs/2402.09748v1",
    "title": "Model Compression and Efficient Inference for Large Language Models: A Survey",
    "summary": "Transformer based large language models have achieved tremendous success. However, the significant memory and computational costs incurred during the inference process make it challenging to deploy large models on resource-constrained devices. In this paper, we investigate compression and efficient inference methods for large language models from an algorithmic perspective. Regarding taxonomy, similar to smaller models, compression and acceleration algorithms for large language models can still be categorized into quantization, pruning, distillation, compact architecture design, dynamic networks. However, Large language models have two prominent characteristics compared to smaller models: (1) Most of compression algorithms require finetuning or even retraining the model after compression. The most notable aspect of large models is the very high cost associated with model finetuning or training. Therefore, many algorithms for large models, such as quantization and pruning, start to explore tuning-free algorithms. (2) Large models emphasize versatility and generalization rather than performance on a single task. Hence, many algorithms, such as knowledge distillation, focus on how to preserving their versatility and generalization after compression. Since these two characteristics were not very pronounced in early large models, we further distinguish large language models into medium models and ``real'' large models. Additionally, we also provide an introduction to some mature frameworks for efficient inference of large models, which can support basic compression or acceleration algorithms, greatly facilitating model deployment for users.",
    "authors": [
      "Wenxiao Wang",
      "Wei Chen",
      "Yicong Luo",
      "Yongliu Long",
      "Zhengkai Lin",
      "Liye Zhang",
      "Binbin Lin",
      "Deng Cai",
      "Xiaofei He"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.PF"
    ],
    "published": "2024-02-15T06:58:30Z",
    "pdf_url": "https://arxiv.org/pdf/2402.09748v1"
  },
  {
    "arxiv_id": "2402.09579v2",
    "entry_id": "http://arxiv.org/abs/2402.09579v2",
    "title": "Advancing Building Energy Modeling with Large Language Models: Exploration and Case Studies",
    "summary": "The rapid progression in artificial intelligence has facilitated the emergence of large language models like ChatGPT, offering potential applications extending into specialized engineering modeling, especially physics-based building energy modeling. This paper investigates the innovative integration of large language models with building energy modeling software, focusing specifically on the fusion of ChatGPT with EnergyPlus. A literature review is first conducted to reveal a growing trend of incorporating large language models in engineering modeling, albeit limited research on their application in building energy modeling. We underscore the potential of large language models in addressing building energy modeling challenges and outline potential applications including simulation input generation, simulation output analysis and visualization, conducting error analysis, co-simulation, simulation knowledge extraction and training, and simulation optimization. Three case studies reveal the transformative potential of large language models in automating and optimizing building energy modeling tasks, underscoring the pivotal role of artificial intelligence in advancing sustainable building practices and energy efficiency. The case studies demonstrate that selecting the right large language model techniques is essential to enhance performance and reduce engineering efforts. The findings advocate a multidisciplinary approach in future artificial intelligence research, with implications extending beyond building energy modeling to other specialized engineering modeling.",
    "authors": [
      "Liang Zhang",
      "Zhelun Chen",
      "Vitaly Ford"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-02-14T21:02:07Z",
    "pdf_url": "https://arxiv.org/pdf/2402.09579v2"
  },
  {
    "arxiv_id": "2402.09563v2",
    "entry_id": "http://arxiv.org/abs/2402.09563v2",
    "title": "ABIDES-Economist: Agent-Based Simulator of Economic Systems with Learning Agents",
    "summary": "We present ABIDES-Economist, an agent-based simulator for economic systems that includes heterogeneous households, firms, a central bank, and a government. Agent behavior can be defined using domain-specific behavioral rules or learned through reinforcement learning by specifying their objectives. We integrate reinforcement learning capabilities for all agents using the OpenAI Gym environment framework for the multi-agent system. To enhance the realism of our model, we base agent parameters and action spaces on economic literature and real U.S. economic data. To tackle the challenges of calibrating heterogeneous agent-based economic models, we conduct a comprehensive survey of stylized facts related to both microeconomic and macroeconomic time series data. We then validate ABIDES-Economist by demonstrating its ability to generate simulated data that aligns with the relevant stylized facts for the economic scenario under consideration, following the learning of all agent behaviors via reinforcement learning. Specifically, we train our economic agents' policies under two broad configurations. The first configuration demonstrates that the learned economic agents produce system data consistent with macroeconomic and microeconomic stylized facts. The second configuration illustrates the utility of the validated simulation platform in designing regulatory policies for the central bank and government. These policies outperform standard rule-based approaches from the literature, which often overlook agent heterogeneity, shocks, and agent adaptability.",
    "authors": [
      "Kshama Dwarakanath",
      "Tucker Balch",
      "Svitlana Vyetrenko"
    ],
    "categories": [
      "cs.MA",
      "econ.GN"
    ],
    "published": "2024-02-14T20:26:52Z",
    "pdf_url": "https://arxiv.org/pdf/2402.09563v2"
  },
  {
    "arxiv_id": "2402.09283v3",
    "entry_id": "http://arxiv.org/abs/2402.09283v3",
    "title": "Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey",
    "summary": "Large Language Models (LLMs) are now commonplace in conversation applications. However, their risks of misuse for generating harmful responses have raised serious societal concerns and spurred recent research on LLM conversation safety. Therefore, in this survey, we provide a comprehensive overview of recent studies, covering three critical aspects of LLM conversation safety: attacks, defenses, and evaluations. Our goal is to provide a structured summary that enhances understanding of LLM conversation safety and encourages further investigation into this important subject. For easy reference, we have categorized all the studies mentioned in this survey according to our taxonomy, available at: https://github.com/niconi19/LLM-conversation-safety.",
    "authors": [
      "Zhichen Dong",
      "Zhanhui Zhou",
      "Chao Yang",
      "Jing Shao",
      "Yu Qiao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-02-14T16:14:03Z",
    "pdf_url": "https://arxiv.org/pdf/2402.09283v3"
  },
  {
    "arxiv_id": "2402.08905v1",
    "entry_id": "http://arxiv.org/abs/2402.08905v1",
    "title": "Time preference, wealth and utility inequality: A microeconomic interaction and dynamic macroeconomic model connection approach",
    "summary": "Based on interactions between individuals and others and references to social norms, this study reveals the impact of heterogeneity in time preference on wealth distribution and inequality. We present a novel approach that connects the interactions between microeconomic agents that generate heterogeneity to the dynamic equations for capital and consumption in macroeconomic models. Using this approach, we estimate the impact of changes in the discount rate due to microeconomic interactions on capital, consumption and utility and the degree of inequality. The results show that intercomparisons with others regarding consumption significantly affect capital, i.e. wealth inequality. Furthermore, the impact on utility is never small and social norms can reduce this impact. Our supporting evidence shows that the quantitative results of inequality calculations correspond to survey data from cohort and cross-cultural studies. This study's micro-macro connection approach can be deployed to connect microeconomic interactions, such as exchange, interest and debt, redistribution, mutual aid and time preference, to dynamic macroeconomic models.",
    "authors": [
      "Takeshi Kato"
    ],
    "categories": [
      "econ.GN",
      "cs.MA",
      "physics.soc-ph"
    ],
    "published": "2024-02-14T02:41:36Z",
    "pdf_url": "https://arxiv.org/pdf/2402.08905v1"
  },
  {
    "arxiv_id": "2402.08761v1",
    "entry_id": "http://arxiv.org/abs/2402.08761v1",
    "title": "JAMDEC: Unsupervised Authorship Obfuscation using Constrained Decoding over Small Language Models",
    "summary": "The permanence of online content combined with the enhanced authorship identification techniques calls for stronger computational methods to protect the identity and privacy of online authorship when needed, e.g., blind reviews for scientific papers, anonymous online reviews, or anonymous interactions in the mental health forums. In this paper, we propose an unsupervised inference-time approach to authorship obfuscation to address the unique challenges of authorship obfuscation: lack of supervision data for diverse authorship and domains, and the need for a sufficient level of revision beyond simple paraphrasing to obfuscate the authorship, all the while preserving the original content and fluency.\n  We introduce JAMDEC, a user-controlled, inference-time algorithm for authorship obfuscation that can be in principle applied to any text and authorship. Our approach builds on small language models such as GPT2-XL in order to help avoid disclosing the original content to proprietary LLM's APIs, while also reducing the performance gap between small and large language models via algorithmic enhancement. The key idea behind our approach is to boost the creative power of smaller language models through constrained decoding, while also allowing for user-specified controls and flexibility. Experimental results demonstrate that our approach based on GPT2-XL outperforms previous state-of-the-art methods based on comparably small models, while performing competitively against GPT3.5 175B, a propriety model that is two orders of magnitudes larger.",
    "authors": [
      "Jillian Fisher",
      "Ximing Lu",
      "Jaehun Jung",
      "Liwei Jiang",
      "Zaid Harchaoui",
      "Yejin Choi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-13T19:54:29Z",
    "pdf_url": "https://arxiv.org/pdf/2402.08761v1"
  },
  {
    "arxiv_id": "2402.08565v2",
    "entry_id": "http://arxiv.org/abs/2402.08565v2",
    "title": "Artificial Intelligence for Literature Reviews: Opportunities and Challenges",
    "summary": "This manuscript presents a comprehensive review of the use of Artificial Intelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous and organised methodology that assesses and integrates previous research on a given topic. Numerous tools have been developed to assist and partially automate the SLR process. The increasing role of AI in this field shows great potential in providing more effective support for researchers, moving towards the semi-automatic creation of literature reviews. Our study focuses on how AI techniques are applied in the semi-automation of SLRs, specifically in the screening and extraction phases. We examine 21 leading SLR tools using a framework that combines 23 traditional features with 11 AI features. We also analyse 11 recent tools that leverage large language models for searching the literature and assisting academic writing. Finally, the paper discusses current trends in the field, outlines key research challenges, and suggests directions for future research.",
    "authors": [
      "Francisco Bolanos",
      "Angelo Salatino",
      "Francesco Osborne",
      "Enrico Motta"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "published": "2024-02-13T16:05:51Z",
    "pdf_url": "https://arxiv.org/pdf/2402.08565v2"
  },
  {
    "arxiv_id": "2402.08323v1",
    "entry_id": "http://arxiv.org/abs/2402.08323v1",
    "title": "Mapping the Ethics of Generative AI: A Comprehensive Scoping Review",
    "summary": "The advent of generative artificial intelligence and the widespread adoption of it in society engendered intensive debates about its ethical implications and risks. These risks often differ from those associated with traditional discriminative machine learning. To synthesize the recent discourse and map its normative concepts, we conducted a scoping review on the ethics of generative artificial intelligence, including especially large language models and text-to-image models. Our analysis provides a taxonomy of 378 normative issues in 19 topic areas and ranks them according to their prevalence in the literature. The study offers a comprehensive overview for scholars, practitioners, or policymakers, condensing the ethical debates surrounding fairness, safety, harmful content, hallucinations, privacy, interaction risks, security, alignment, societal impacts, and others. We discuss the results, evaluate imbalances in the literature, and explore unsubstantiated risk scenarios.",
    "authors": [
      "Thilo Hagendorff"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-02-13T09:38:17Z",
    "pdf_url": "https://arxiv.org/pdf/2402.08323v1"
  },
  {
    "arxiv_id": "2402.08132v2",
    "entry_id": "http://arxiv.org/abs/2402.08132v2",
    "title": "On the Resurgence of Recurrent Models for Long Sequences -- Survey and Research Opportunities in the Transformer Era",
    "summary": "A longstanding challenge for the Machine Learning community is the one of developing models that are capable of processing and learning from very long sequences of data. The outstanding results of Transformers-based networks (e.g., Large Language Models) promotes the idea of parallel attention as the key to succeed in such a challenge, obfuscating the role of classic sequential processing of Recurrent Models. However, in the last few years, researchers who were concerned by the quadratic complexity of self-attention have been proposing a novel wave of neural models, which gets the best from the two worlds, i.e., Transformers and Recurrent Nets. Meanwhile, Deep Space-State Models emerged as robust approaches to function approximation over time, thus opening a new perspective in learning from sequential data, followed by many people in the field and exploited to implement a special class of (linear) Recurrent Neural Networks. This survey is aimed at providing an overview of these trends framed under the unifying umbrella of Recurrence. Moreover, it emphasizes novel research opportunities that become prominent when abandoning the idea of processing long sequences whose length is known-in-advance for the more realistic setting of potentially infinite-length sequences, thus intersecting the field of lifelong-online learning from streamed data.",
    "authors": [
      "Matteo Tiezzi",
      "Michele Casoni",
      "Alessandro Betti",
      "Tommaso Guidi",
      "Marco Gori",
      "Stefano Melacci"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-02-12T23:55:55Z",
    "pdf_url": "https://arxiv.org/pdf/2402.08132v2"
  },
  {
    "arxiv_id": "2402.07744v2",
    "entry_id": "http://arxiv.org/abs/2402.07744v2",
    "title": "Towards Unified Alignment Between Agents, Humans, and Environment",
    "summary": "The rapid progress of foundation models has led to the prosperity of autonomous agents, which leverage the universal capabilities of foundation models to conduct reasoning, decision-making, and environmental interaction. However, the efficacy of agents remains limited when operating in intricate, realistic environments. In this work, we introduce the principles of $\\mathbf{U}$nified $\\mathbf{A}$lignment for $\\mathbf{A}$gents ($\\mathbf{UA}^2$), which advocate for the simultaneous alignment of agents with human intentions, environmental dynamics, and self-constraints such as the limitation of monetary budgets. From the perspective of $\\mathbf{UA}^2$, we review the current agent research and highlight the neglected factors in existing agent benchmarks and method candidates. We also conduct proof-of-concept studies by introducing realistic features to WebShop, including user profiles to demonstrate intentions, personalized reranking for complex environmental dynamics, and runtime cost statistics to reflect self-constraints. We then follow the principles of $\\mathbf{UA}^2$ to propose an initial design of our agent, and benchmark its performance with several candidate baselines in the retrofitted WebShop. The extensive experimental results further prove the importance of the principles of $\\mathbf{UA}^2$. Our research sheds light on the next steps of autonomous agent research with improved general problem-solving abilities.",
    "authors": [
      "Zonghan Yang",
      "An Liu",
      "Zijun Liu",
      "Kaiming Liu",
      "Fangzhou Xiong",
      "Yile Wang",
      "Zeyuan Yang",
      "Qingyuan Hu",
      "Xinrui Chen",
      "Zhenhe Zhang",
      "Fuwen Luo",
      "Zhicheng Guo",
      "Peng Li",
      "Yang Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-12T16:14:22Z",
    "pdf_url": "https://arxiv.org/pdf/2402.07744v2"
  },
  {
    "arxiv_id": "2402.07127v3",
    "entry_id": "http://arxiv.org/abs/2402.07127v3",
    "title": "Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation",
    "summary": "Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale \"in-the-wild\" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for robotic manipulation. The survey summarizes video-based learning approaches, analyses their benefits over standard datasets, survey metrics, and benchmarks, and discusses open challenges and future directions in this nascent domain at the intersection of computer vision, natural language processing, and robot learning.",
    "authors": [
      "Chrisantus Eze",
      "Christopher Crick"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-02-11T08:41:42Z",
    "pdf_url": "https://arxiv.org/pdf/2402.07127v3"
  },
  {
    "arxiv_id": "2402.18590v3",
    "entry_id": "http://arxiv.org/abs/2402.18590v3",
    "title": "Exploring the Impact of Large Language Models on Recommender Systems: An Extensive Review",
    "summary": "The paper underscores the significance of Large Language Models (LLMs) in reshaping recommender systems, attributing their value to unique reasoning abilities absent in traditional recommenders. Unlike conventional systems lacking direct user interaction data, LLMs exhibit exceptional proficiency in recommending items, showcasing their adeptness in comprehending intricacies of language. This marks a fundamental paradigm shift in the realm of recommendations. Amidst the dynamic research landscape, researchers actively harness the language comprehension and generation capabilities of LLMs to redefine the foundations of recommendation tasks. The investigation thoroughly explores the inherent strengths of LLMs within recommendation frameworks, encompassing nuanced contextual comprehension, seamless transitions across diverse domains, adoption of unified approaches, holistic learning strategies leveraging shared data reservoirs, transparent decision-making, and iterative improvements. Despite their transformative potential, challenges persist, including sensitivity to input prompts, occasional misinterpretations, and unforeseen recommendations, necessitating continuous refinement and evolution in LLM-driven recommender systems.",
    "authors": [
      "Arpita Vats",
      "Vinija Jain",
      "Rahul Raja",
      "Aman Chadha"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-02-11T00:24:17Z",
    "pdf_url": "https://arxiv.org/pdf/2402.18590v3"
  },
  {
    "arxiv_id": "2402.06196v3",
    "entry_id": "http://arxiv.org/abs/2402.06196v3",
    "title": "Large Language Models: A Survey",
    "summary": "Large Language Models (LLMs) have drawn a lot of attention due to their strong performance on a wide range of natural language tasks, since the release of ChatGPT in November 2022. LLMs' ability of general-purpose language understanding and generation is acquired by training billions of model's parameters on massive amounts of text data, as predicted by scaling laws \\cite{kaplan2020scaling,hoffmann2022training}. The research area of LLMs, while very recent, is evolving rapidly in many different ways. In this paper, we review some of the most prominent LLMs, including three popular LLM families (GPT, LLaMA, PaLM), and discuss their characteristics, contributions and limitations. We also give an overview of techniques developed to build, and augment LLMs. We then survey popular datasets prepared for LLM training, fine-tuning, and evaluation, review widely used LLM evaluation metrics, and compare the performance of several popular LLMs on a set of representative benchmarks. Finally, we conclude the paper by discussing open challenges and future research directions.",
    "authors": [
      "Shervin Minaee",
      "Tomas Mikolov",
      "Narjes Nikzad",
      "Meysam Chenaghlu",
      "Richard Socher",
      "Xavier Amatriain",
      "Jianfeng Gao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-09T05:37:09Z",
    "pdf_url": "https://arxiv.org/pdf/2402.06196v3"
  },
  {
    "arxiv_id": "2402.10946v3",
    "entry_id": "http://arxiv.org/abs/2402.10946v3",
    "title": "CultureLLM: Incorporating Cultural Differences into Large Language Models",
    "summary": "Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM significantly outperforms various counterparts such as GPT-3.5 (by 8.1%) and Gemini Pro (by 9.5%) with comparable performance to GPT-4 or even better. Our human study shows that the generated samples are semantically equivalent to the original samples, providing an effective solution for LLMs augmentation. Code is released at https://github.com/Scarelette/CultureLLM.",
    "authors": [
      "Cheng Li",
      "Mengzhou Chen",
      "Jindong Wang",
      "Sunayana Sitaram",
      "Xing Xie"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-02-09T04:02:43Z",
    "pdf_url": "https://arxiv.org/pdf/2402.10946v3"
  },
  {
    "arxiv_id": "2402.07940v1",
    "entry_id": "http://arxiv.org/abs/2402.07940v1",
    "title": "LLMs Among Us: Generative AI Participating in Digital Discourse",
    "summary": "The emergence of Large Language Models (LLMs) has great potential to reshape the landscape of many social media platforms. While this can bring promising opportunities, it also raises many threats, such as biases and privacy concerns, and may contribute to the spread of propaganda by malicious actors. We developed the \"LLMs Among Us\" experimental framework on top of the Mastodon social media platform for bot and human participants to communicate without knowing the ratio or nature of bot and human participants. We built 10 personas with three different LLMs, GPT-4, LLama 2 Chat, and Claude. We conducted three rounds of the experiment and surveyed participants after each round to measure the ability of LLMs to pose as human participants without human detection. We found that participants correctly identified the nature of other users in the experiment only 42% of the time despite knowing the presence of both bots and humans. We also found that the choice of persona had substantially more impact on human perception than the choice of mainstream LLMs.",
    "authors": [
      "Kristina Radivojevic",
      "Nicholas Clark",
      "Paul Brenner"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "published": "2024-02-08T19:21:33Z",
    "pdf_url": "https://arxiv.org/pdf/2402.07940v1"
  },
  {
    "arxiv_id": "2402.05741v2",
    "entry_id": "http://arxiv.org/abs/2402.05741v2",
    "title": "Real-World Robot Applications of Foundation Models: A Review",
    "summary": "Recent developments in foundation models, like Large Language Models (LLMs) and Vision-Language Models (VLMs), trained on extensive data, facilitate flexible application across different tasks and modalities. Their impact spans various fields, including healthcare, education, and robotics. This paper provides an overview of the practical application of foundation models in real-world robotics, with a primary emphasis on the replacement of specific components within existing robot systems. The summary encompasses the perspective of input-output relationships in foundation models, as well as their role in perception, motion planning, and control within the field of robotics. This paper concludes with a discussion of future challenges and implications for practical robot applications.",
    "authors": [
      "Kento Kawaharazuka",
      "Tatsuya Matsushima",
      "Andrew Gambardella",
      "Jiaxian Guo",
      "Chris Paxton",
      "Andy Zeng"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-02-08T15:19:50Z",
    "pdf_url": "https://arxiv.org/pdf/2402.05741v2"
  },
  {
    "arxiv_id": "2402.05636v2",
    "entry_id": "http://arxiv.org/abs/2402.05636v2",
    "title": "The Impact of AI Tool on Engineering at ANZ Bank An Empirical Study on GitHub Copilot within Corporate Environment",
    "summary": "The increasing popularity of AI, particularly Large Language Models (LLMs), has significantly impacted various domains, including Software Engineering. This study explores the integration of AI tools in software engineering practices within a large organization. We focus on ANZ Bank, which employs over 5000 engineers covering all aspects of the software development life cycle. This paper details an experiment conducted using GitHub Copilot, a notable AI tool, within a controlled environment to evaluate its effectiveness in real-world engineering tasks. Additionally, this paper shares initial findings on the productivity improvements observed after GitHub Copilot was adopted on a large scale, with about 1000 engineers using it. ANZ Bank's six-week experiment with GitHub Copilot included two weeks of preparation and four weeks of active testing. The study evaluated participant sentiment and the tool's impact on productivity, code quality, and security. Initially, participants used GitHub Copilot for proposed use-cases, with their feedback gathered through regular surveys. In the second phase, they were divided into Control and Copilot groups, each tackling the same Python challenges, and their experiences were again surveyed. Results showed a notable boost in productivity and code quality with GitHub Copilot, though its impact on code security remained inconclusive. Participant responses were overall positive, confirming GitHub Copilot's effectiveness in large-scale software engineering environments. Early data from 1000 engineers also indicated a significant increase in productivity and job satisfaction.",
    "authors": [
      "Sayan Chatterjee",
      "Ching Louis Liu",
      "Gareth Rowland",
      "Tim Hogarth"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-02-08T12:47:57Z",
    "pdf_url": "https://arxiv.org/pdf/2402.05636v2"
  },
  {
    "arxiv_id": "2402.05391v4",
    "entry_id": "http://arxiv.org/abs/2402.05391v4",
    "title": "Knowledge Graphs Meet Multi-Modal Learning: A Comprehensive Survey",
    "summary": "Knowledge Graphs (KGs) play a pivotal role in advancing various AI applications, with the semantic web community's exploration into multi-modal dimensions unlocking new avenues for innovation. In this survey, we carefully review over 300 articles, focusing on KG-aware research in two principal aspects: KG-driven Multi-Modal (KG4MM) learning, where KGs support multi-modal tasks, and Multi-Modal Knowledge Graph (MM4KG), which extends KG studies into the MMKG realm. We begin by defining KGs and MMKGs, then explore their construction progress. Our review includes two primary task categories: KG-aware multi-modal learning tasks, such as Image Classification and Visual Question Answering, and intrinsic MMKG tasks like Multi-modal Knowledge Graph Completion and Entity Alignment, highlighting specific research trajectories. For most of these tasks, we provide definitions, evaluation benchmarks, and additionally outline essential insights for conducting relevant research. Finally, we discuss current challenges and identify emerging trends, such as progress in Large Language Modeling and Multi-modal Pre-training strategies. This survey aims to serve as a comprehensive reference for researchers already involved in or considering delving into KG and multi-modal learning research, offering insights into the evolving landscape of MMKG research and supporting future work.",
    "authors": [
      "Zhuo Chen",
      "Yichi Zhang",
      "Yin Fang",
      "Yuxia Geng",
      "Lingbing Guo",
      "Xiang Chen",
      "Qian Li",
      "Wen Zhang",
      "Jiaoyan Chen",
      "Yushan Zhu",
      "Jiaqi Li",
      "Xiaoze Liu",
      "Jeff Z. Pan",
      "Ningyu Zhang",
      "Huajun Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2024-02-08T04:04:36Z",
    "pdf_url": "https://arxiv.org/pdf/2402.05391v4"
  },
  {
    "arxiv_id": "2402.14590v1",
    "entry_id": "http://arxiv.org/abs/2402.14590v1",
    "title": "Scaling Up LLM Reviews for Google Ads Content Moderation",
    "summary": "Large language models (LLMs) are powerful tools for content moderation, but their inference costs and latency make them prohibitive for casual use on large datasets, such as the Google Ads repository. This study proposes a method for scaling up LLM reviews for content moderation in Google Ads. First, we use heuristics to select candidates via filtering and duplicate removal, and create clusters of ads for which we select one representative ad per cluster. We then use LLMs to review only the representative ads. Finally, we propagate the LLM decisions for the representative ads back to their clusters. This method reduces the number of reviews by more than 3 orders of magnitude while achieving a 2x recall compared to a baseline non-LLM model. The success of this approach is a strong function of the representations used in clustering and label propagation; we found that cross-modal similarity representations yield better results than uni-modal representations.",
    "authors": [
      "Wei Qiao",
      "Tushar Dogra",
      "Otilia Stretcu",
      "Yu-Han Lyu",
      "Tiantian Fang",
      "Dongjin Kwon",
      "Chun-Ta Lu",
      "Enming Luo",
      "Yuan Wang",
      "Chih-Chun Chia",
      "Ariel Fuxman",
      "Fangzhou Wang",
      "Ranjay Krishna",
      "Mehmet Tek"
    ],
    "categories": [
      "cs.IR",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-07T23:47:02Z",
    "pdf_url": "https://arxiv.org/pdf/2402.14590v1"
  },
  {
    "arxiv_id": "2402.04580v2",
    "entry_id": "http://arxiv.org/abs/2402.04580v2",
    "title": "A Comprehensive Survey of Cross-Domain Policy Transfer for Embodied Agents",
    "summary": "The burgeoning fields of robot learning and embodied AI have triggered an increasing demand for large quantities of data. However, collecting sufficient unbiased data from the target domain remains a challenge due to costly data collection processes and stringent safety requirements. Consequently, researchers often resort to data from easily accessible source domains, such as simulation and laboratory environments, for cost-effective data acquisition and rapid model iteration. Nevertheless, the environments and embodiments of these source domains can be quite different from their target domain counterparts, underscoring the need for effective cross-domain policy transfer approaches. In this paper, we conduct a systematic review of existing cross-domain policy transfer methods. Through a nuanced categorization of domain gaps, we encapsulate the overarching insights and design considerations of each problem setting. We also provide a high-level discussion about the key methodologies used in cross-domain policy transfer problems. Lastly, we summarize the open challenges that lie beyond the capabilities of current paradigms and discuss potential future directions in this field.",
    "authors": [
      "Haoyi Niu",
      "Jianming Hu",
      "Guyue Zhou",
      "Xianyuan Zhan"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-02-07T04:43:41Z",
    "pdf_url": "https://arxiv.org/pdf/2402.04580v2"
  },
  {
    "arxiv_id": "2403.09676v1",
    "entry_id": "http://arxiv.org/abs/2403.09676v1",
    "title": "Unmasking the Shadows of AI: Investigating Deceptive Capabilities in Large Language Models",
    "summary": "This research critically navigates the intricate landscape of AI deception, concentrating on deceptive behaviours of Large Language Models (LLMs). My objective is to elucidate this issue, examine the discourse surrounding it, and subsequently delve into its categorization and ramifications. The essay initiates with an evaluation of the AI Safety Summit 2023 (ASS) and introduction of LLMs, emphasising multidimensional biases that underlie their deceptive behaviours.The literature review covers four types of deception categorised: Strategic deception, Imitation, Sycophancy, and Unfaithful Reasoning, along with the social implications and risks they entail. Lastly, I take an evaluative stance on various aspects related to navigating the persistent challenges of the deceptive AI. This encompasses considerations of international collaborative governance, the reconfigured engagement of individuals with AI, proposal of practical adjustments, and specific elements of digital education.",
    "authors": [
      "Linge Guo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-07T00:21:46Z",
    "pdf_url": "https://arxiv.org/pdf/2403.09676v1"
  },
  {
    "arxiv_id": "2402.04247v5",
    "entry_id": "http://arxiv.org/abs/2402.04247v5",
    "title": "Risks of AI Scientists: Prioritizing Safeguarding Over Autonomy",
    "summary": "AI scientists powered by large language models have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, these agents also introduce novel vulnerabilities that require careful consideration for safety. However, there has been limited comprehensive exploration of these vulnerabilities. This perspective examines vulnerabilities in AI scientists, shedding light on potential risks associated with their misuse, and emphasizing the need for safety measures. We begin by providing an overview of the potential risks inherent to AI scientists, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we explore the underlying causes of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding AI scientists and advocate for the development of improved models, robust benchmarks, and comprehensive regulations.",
    "authors": [
      "Xiangru Tang",
      "Qiao Jin",
      "Kunlun Zhu",
      "Tongxin Yuan",
      "Yichi Zhang",
      "Wangchunshu Zhou",
      "Meng Qu",
      "Yilun Zhao",
      "Jian Tang",
      "Zhuosheng Zhang",
      "Arman Cohan",
      "Zhiyong Lu",
      "Mark Gerstein"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-06T18:54:07Z",
    "pdf_url": "https://arxiv.org/pdf/2402.04247v5"
  },
  {
    "arxiv_id": "2402.03962v3",
    "entry_id": "http://arxiv.org/abs/2402.03962v3",
    "title": "Position: Stop Making Unscientific AGI Performance Claims",
    "summary": "Developments in the field of Artificial Intelligence (AI), and particularly large language models (LLMs), have created a 'perfect storm' for observing 'sparks' of Artificial General Intelligence (AGI) that are spurious. Like simpler models, LLMs distill meaningful representations in their latent embeddings that have been shown to correlate with external variables. Nonetheless, the correlation of such representations has often been linked to human-like intelligence in the latter but not the former. We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers: all of them successfully distill information that can be used to predict latent or external variables and yet none of them have previously been linked to AGI. We argue and empirically demonstrate that the finding of meaningful patterns in latent spaces of models cannot be seen as evidence in favor of AGI. Additionally, we review literature from the social sciences that shows that humans are prone to seek such patterns and anthropomorphize. We conclude that both the methodological setup and common public image of AI are ideal for the misinterpretation that correlations between model representations and some variables of interest are 'caused' by the model's understanding of underlying 'ground truth' relationships. We, therefore, call for the academic community to exercise extra caution, and to be keenly aware of principles of academic integrity, in interpreting and communicating about AI research outcomes.",
    "authors": [
      "Patrick Altmeyer",
      "Andrew M. Demetriou",
      "Antony Bartlett",
      "Cynthia C. S. Liem"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-06T12:42:21Z",
    "pdf_url": "https://arxiv.org/pdf/2402.03962v3"
  },
  {
    "arxiv_id": "2403.00765v1",
    "entry_id": "http://arxiv.org/abs/2403.00765v1",
    "title": "An Architecture for Unattended Containerized (Deep) Reinforcement Learning with Webots",
    "summary": "As data science applications gain adoption across industries, the tooling landscape matures to facilitate the life cycle of such applications and provide solutions to the challenges involved to boost the productivity of the people involved. Reinforcement learning with agents in a 3D world could still face challenges: the knowledge required to use a simulation software as well as the utilization of a standalone simulation software in unattended training pipelines.\n  In this paper we review tools and approaches to train reinforcement learning agents for robots in 3D worlds with respect to the robot Robotino and argue that the separation of the simulation environment for creators of virtual worlds and the model development environment for data scientists is not a well covered topic. Often both are the same and data scientists require knowledge of the simulation software to work directly with their APIs. Moreover, sometimes creators of virtual worlds and data scientists even work on the same files. We want to contribute to that topic by describing an approach where data scientists don't require knowledge about the simulation software. Our approach uses the standalone simulation software Webots, the Robot Operating System to communicate with simulated robots as well as the simulation software itself and container technology to separate the simulation from the model development environment. We put emphasize on the APIs the data scientists work with and the use of a standalone simulation software in unattended training pipelines. We show the parts that are specific to the Robotino and the robot task to learn.",
    "authors": [
      "Tobias Haubold",
      "Petra Linke"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-02-06T12:08:01Z",
    "pdf_url": "https://arxiv.org/pdf/2403.00765v1"
  },
  {
    "arxiv_id": "2402.03927v2",
    "entry_id": "http://arxiv.org/abs/2402.03927v2",
    "title": "Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs",
    "summary": "Natural Language Processing (NLP) research is increasingly focusing on the use of Large Language Models (LLMs), with some of the most popular ones being either fully or partially closed-source. The lack of access to model details, especially regarding training data, has repeatedly raised concerns about data contamination among researchers. Several attempts have been made to address this issue, but they are limited to anecdotal evidence and trial and error. Additionally, they overlook the problem of \\emph{indirect} data leaking, where models are iteratively improved by using data coming from users. In this work, we conduct the first systematic analysis of work using OpenAI's GPT-3.5 and GPT-4, the most prominently used LLMs today, in the context of data contamination. By analysing 255 papers and considering OpenAI's data usage policy, we extensively document the amount of data leaked to these models during the first year after the model's release. We report that these models have been globally exposed to $\\sim$4.7M samples from 263 benchmarks. At the same time, we document a number of evaluation malpractices emerging in the reviewed papers, such as unfair or missing baseline comparisons and reproducibility issues. We release our results as a collaborative project on https://leak-llm.github.io/, where other researchers can contribute to our efforts.",
    "authors": [
      "Simone Balloccu",
      "Patrícia Schmidtová",
      "Mateusz Lango",
      "Ondřej Dušek"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-06T11:54:23Z",
    "pdf_url": "https://arxiv.org/pdf/2402.03927v2"
  },
  {
    "arxiv_id": "2402.07927v2",
    "entry_id": "http://arxiv.org/abs/2402.07927v2",
    "title": "A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications",
    "summary": "Prompt engineering has emerged as an indispensable technique for extending the capabilities of large language models (LLMs) and vision-language models (VLMs). This approach leverages task-specific instructions, known as prompts, to enhance model efficacy without modifying the core model parameters. Rather than updating the model parameters, prompts allow seamless integration of pre-trained models into downstream tasks by eliciting desired model behaviors solely based on the given prompt. Prompts can be natural language instructions that provide context to guide the model or learned vector representations that activate relevant knowledge. This burgeoning field has enabled success across various applications, from question-answering to commonsense reasoning. However, there remains a lack of systematic organization and understanding of the diverse prompt engineering methods and techniques. This survey paper addresses the gap by providing a structured overview of recent advancements in prompt engineering, categorized by application area. For each prompting approach, we provide a summary detailing the prompting methodology, its applications, the models involved, and the datasets utilized. We also delve into the strengths and limitations of each approach and include a taxonomy diagram and table summarizing datasets, models, and critical points of each prompting technique. This systematic analysis enables a better understanding of this rapidly developing field and facilitates future research by illuminating open challenges and opportunities for prompt engineering.",
    "authors": [
      "Pranab Sahoo",
      "Ayush Kumar Singh",
      "Sriparna Saha",
      "Vinija Jain",
      "Samrat Mondal",
      "Aman Chadha"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2024-02-05T19:49:13Z",
    "pdf_url": "https://arxiv.org/pdf/2402.07927v2"
  },
  {
    "arxiv_id": "2402.03182v1",
    "entry_id": "http://arxiv.org/abs/2402.03182v1",
    "title": "Empowering Time Series Analysis with Large Language Models: A Survey",
    "summary": "Recently, remarkable progress has been made over large language models (LLMs), demonstrating their unprecedented capability in varieties of natural language tasks. However, completely training a large general-purpose model from the scratch is challenging for time series analysis, due to the large volumes and varieties of time series data, as well as the non-stationarity that leads to concept drift impeding continuous model adaptation and re-training. Recent advances have shown that pre-trained LLMs can be exploited to capture complex dependencies in time series data and facilitate various applications. In this survey, we provide a systematic overview of existing methods that leverage LLMs for time series analysis. Specifically, we first state the challenges and motivations of applying language models in the context of time series as well as brief preliminaries of LLMs. Next, we summarize the general pipeline for LLM-based time series analysis, categorize existing methods into different groups (i.e., direct query, tokenization, prompt design, fine-tune, and model integration), and highlight the key ideas within each group. We also discuss the applications of LLMs for both general and spatial-temporal time series data, tailored to specific domains. Finally, we thoroughly discuss future research opportunities to empower time series analysis with LLMs.",
    "authors": [
      "Yushan Jiang",
      "Zijie Pan",
      "Xikun Zhang",
      "Sahil Garg",
      "Anderson Schneider",
      "Yuriy Nevmyvaka",
      "Dongjin Song"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-02-05T16:46:35Z",
    "pdf_url": "https://arxiv.org/pdf/2402.03182v1"
  },
  {
    "arxiv_id": "2402.03177v1",
    "entry_id": "http://arxiv.org/abs/2402.03177v1",
    "title": "CIDAR: Culturally Relevant Instruction Dataset For Arabic",
    "summary": "Instruction tuning has emerged as a prominent methodology for teaching Large Language Models (LLMs) to follow instructions. However, current instruction datasets predominantly cater to English or are derived from English-dominated LLMs, resulting in inherent biases toward Western culture. This bias significantly impacts the linguistic structures of non-English languages such as Arabic, which has a distinct grammar reflective of the diverse cultures across the Arab region. This paper addresses this limitation by introducing CIDAR: https://hf.co/datasets/arbml/CIDAR, the first open Arabic instruction-tuning dataset culturally-aligned by human reviewers. CIDAR contains 10,000 instruction and output pairs that represent the Arab region. We discuss the cultural relevance of CIDAR via the analysis and comparison to other models fine-tuned on other datasets. Our experiments show that CIDAR can help enrich research efforts in aligning LLMs with the Arabic culture. All the code is available at https://github.com/ARBML/CIDAR.",
    "authors": [
      "Zaid Alyafeai",
      "Khalid Almubarak",
      "Ahmed Ashraf",
      "Deema Alnuhait",
      "Saied Alshahrani",
      "Gubran A. Q. Abdulrahman",
      "Gamil Ahmed",
      "Qais Gawah",
      "Zead Saleh",
      "Mustafa Ghaleb",
      "Yousef Ali",
      "Maged S. Al-Shaibani"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-05T16:44:17Z",
    "pdf_url": "https://arxiv.org/pdf/2402.03177v1"
  },
  {
    "arxiv_id": "2403.08802v4",
    "entry_id": "http://arxiv.org/abs/2403.08802v4",
    "title": "Governance of Generative Artificial Intelligence for Companies",
    "summary": "Generative Artificial Intelligence (GenAI), specifically large language models(LLMs) like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. Although numerous frameworks for governance of AI exist, it is not clear to what extent they apply to GenAI. Our review paper fills this gap by surveying recent works with the purpose of better understanding fundamental characteristics of GenAI and adjusting prior frameworks specifically towards GenAI governance within companies. To do so, it extends Nickerson's framework development processes to include prior conceptualizations. Our framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities as well as mitigate risks associated with GenAI integration. Our research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of GenAI adoption and highlighting research gaps.",
    "authors": [
      "Johannes Schneider",
      "Pauline Kuss",
      "Rene Abraham",
      "Christian Meske"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-02-05T14:20:19Z",
    "pdf_url": "https://arxiv.org/pdf/2403.08802v4"
  },
  {
    "arxiv_id": "2402.02968v2",
    "entry_id": "http://arxiv.org/abs/2402.02968v2",
    "title": "Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives",
    "summary": "Foundation models have indeed made a profound impact on various fields, emerging as pivotal components that significantly shape the capabilities of intelligent systems. In the context of intelligent vehicles, leveraging the power of foundation models has proven to be transformative, offering notable advancements in visual understanding. Equipped with multi-modal and multi-task learning capabilities, multi-modal multi-task visual understanding foundation models (MM-VUFMs) effectively process and fuse data from diverse modalities and simultaneously handle various driving-related tasks with powerful adaptability, contributing to a more holistic understanding of the surrounding scene. In this survey, we present a systematic analysis of MM-VUFMs specifically designed for road scenes. Our objective is not only to provide a comprehensive overview of common practices, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques, but also to highlight their advanced capabilities in diverse learning paradigms. These paradigms include open-world understanding, efficient transfer for road scenes, continual learning, interactive and generative capability. Moreover, we provide insights into key challenges and future trends, such as closed-loop driving systems, interpretability, embodied driving agents, and world models. To facilitate researchers in staying abreast of the latest developments in MM-VUFMs for road scenes, we have established a continuously updated repository at https://github.com/rolsheng/MM-VUFM4DS",
    "authors": [
      "Sheng Luo",
      "Wei Chen",
      "Wanxin Tian",
      "Rui Liu",
      "Luanxuan Hou",
      "Xiubao Zhang",
      "Haifeng Shen",
      "Ruiqi Wu",
      "Shuyi Geng",
      "Yi Zhou",
      "Ling Shao",
      "Yi Yang",
      "Bojun Gao",
      "Qun Li",
      "Guobin Wu"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2024-02-05T12:47:09Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02968v2"
  },
  {
    "arxiv_id": "2402.05964v2",
    "entry_id": "http://arxiv.org/abs/2402.05964v2",
    "title": "A Survey on Transformer Compression",
    "summary": "Transformer plays a vital role in the realms of natural language processing (NLP) and computer vision (CV), specially for constructing large language models (LLM) and large vision models (LVM). Model compression methods reduce the memory and computational cost of Transformer, which is a necessary step to implement large language/vision models on practical devices. Given the unique architecture of Transformer, featuring alternative attention and feedforward neural network (FFN) modules, specific compression techniques are usually required. The efficiency of these compression methods is also paramount, as retraining large models on the entire training dataset is usually impractical. This survey provides a comprehensive review of recent compression methods, with a specific focus on their application to Transformer-based models. The compression methods are primarily categorized into pruning, quantization, knowledge distillation, and efficient architecture design (Mamba, RetNet, RWKV, etc.). In each category, we discuss compression methods for both language and vision tasks, highlighting common underlying principles. Finally, we delve into the relation between various compression methods, and discuss further directions in this domain.",
    "authors": [
      "Yehui Tang",
      "Yunhe Wang",
      "Jianyuan Guo",
      "Zhijun Tu",
      "Kai Han",
      "Hailin Hu",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-02-05T12:16:28Z",
    "pdf_url": "https://arxiv.org/pdf/2402.05964v2"
  },
  {
    "arxiv_id": "2402.02716v1",
    "entry_id": "http://arxiv.org/abs/2402.02716v1",
    "title": "Understanding the planning of LLM agents: A survey",
    "summary": "As Large Language Models (LLMs) have shown significant intelligence, the progress to leverage LLMs as planning modules of autonomous agents has attracted more attention. This survey provides the first systematic view of LLM-based agents planning, covering recent works aiming to improve planning ability. We provide a taxonomy of existing works on LLM-Agent planning, which can be categorized into Task Decomposition, Plan Selection, External Module, Reflection and Memory. Comprehensive analyses are conducted for each direction, and further challenges for the field of research are discussed.",
    "authors": [
      "Xu Huang",
      "Weiwen Liu",
      "Xiaolong Chen",
      "Xingmei Wang",
      "Hao Wang",
      "Defu Lian",
      "Yasheng Wang",
      "Ruiming Tang",
      "Enhong Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-05T04:25:24Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02716v1"
  },
  {
    "arxiv_id": "2402.02619v10",
    "entry_id": "http://arxiv.org/abs/2402.02619v10",
    "title": "Understanding Addition and Subtraction in Transformers",
    "summary": "Transformers are widely deployed in large language models (LLMs), yet most models still fail on basic arithmetic tasks such as multidigit addition. In contrast, we show that small transformers trained from scratch can solve n-digit addition and subtraction with 99.999% accuracy. Building directly on prior work that uncovered addition circuits, we extend the analysis to subtraction and present a unified mechanistic account based on cascading carry and borrow circuits. Using a suite of 49 trained models, we apply systematic ablations and node-level constraints to validate the learned mechanisms and release a reproducible interpretability toolkit for studying arithmetic circuits. Finally, surveying 180 publicly available LLMs, we find that only 7% can reliably perform addition, underscoring the gap between specialized small models and general-purpose LLMs. Our results show that arithmetic can be implemented exactly by tiny transformers, offering a tractable case study for mechanistic interpretability and a cautionary contrast with the persistent arithmetic failures of much larger models.",
    "authors": [
      "Philip Quirke",
      "Clement Neo",
      "Fazl Barez"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2024-02-04T21:33:18Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02619v10"
  },
  {
    "arxiv_id": "2402.02420v3",
    "entry_id": "http://arxiv.org/abs/2402.02420v3",
    "title": "Factuality of Large Language Models: A Survey",
    "summary": "Large language models (LLMs), especially when instruction-tuned for chat, have become part of our daily lives, freeing people from the process of searching, extracting, and integrating information from multiple sources by offering a straightforward answer to a variety of questions in a single place. Unfortunately, in many cases, LLM responses are factually incorrect, which limits their applicability in real-world scenarios. As a result, research on evaluating and improving the factuality of LLMs has attracted a lot of attention recently. In this survey, we critically analyze existing work with the aim to identify the major challenges and their associated causes, pointing out to potential solutions for improving the factuality of LLMs, and analyzing the obstacles to automated factuality evaluation for open-ended text generation. We further offer an outlook on where future research should go.",
    "authors": [
      "Yuxia Wang",
      "Minghan Wang",
      "Muhammad Arslan Manzoor",
      "Fei Liu",
      "Georgi Georgiev",
      "Rocktim Jyoti Das",
      "Preslav Nakov"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-02-04T09:36:31Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02420v3"
  },
  {
    "arxiv_id": "2402.02385v1",
    "entry_id": "http://arxiv.org/abs/2402.02385v1",
    "title": "A Survey on Robotics with Foundation Models: toward Embodied AI",
    "summary": "While the exploration for embodied AI has spanned multiple decades, it remains a persistent challenge to endow agents with human-level intelligence, including perception, learning, reasoning, decision-making, control, and generalization capabilities, so that they can perform general-purpose tasks in open, unstructured, and dynamic environments. Recent advances in computer vision, natural language processing, and multi-modality learning have shown that the foundation models have superhuman capabilities for specific tasks. They not only provide a solid cornerstone for integrating basic modules into embodied AI systems but also shed light on how to scale up robot learning from a methodological perspective. This survey aims to provide a comprehensive and up-to-date overview of foundation models in robotics, focusing on autonomous manipulation and encompassing high-level planning and low-level control. Moreover, we showcase their commonly used datasets, simulators, and benchmarks. Importantly, we emphasize the critical challenges intrinsic to this field and delineate potential avenues for future research, contributing to advancing the frontier of academic and industrial discourse.",
    "authors": [
      "Zhiyuan Xu",
      "Kun Wu",
      "Junjie Wen",
      "Jinming Li",
      "Ning Liu",
      "Zhengping Che",
      "Jian Tang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2024-02-04T07:55:01Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02385v1"
  },
  {
    "arxiv_id": "2402.05952v1",
    "entry_id": "http://arxiv.org/abs/2402.05952v1",
    "title": "Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques",
    "summary": "The integration of Large Language Models (LLMs) with Graph Representation Learning (GRL) marks a significant evolution in analyzing complex data structures. This collaboration harnesses the sophisticated linguistic capabilities of LLMs to improve the contextual understanding and adaptability of graph models, thereby broadening the scope and potential of GRL. Despite a growing body of research dedicated to integrating LLMs into the graph domain, a comprehensive review that deeply analyzes the core components and operations within these models is notably lacking. Our survey fills this gap by proposing a novel taxonomy that breaks down these models into primary components and operation techniques from a novel technical perspective. We further dissect recent literature into two primary components including knowledge extractors and organizers, and two operation techniques including integration and training stratigies, shedding light on effective model design and training strategies. Additionally, we identify and explore potential future research avenues in this nascent yet underexplored field, proposing paths for continued progress.",
    "authors": [
      "Qiheng Mao",
      "Zemin Liu",
      "Chenghao Liu",
      "Zhuo Li",
      "Jianling Sun"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-04T05:51:14Z",
    "pdf_url": "https://arxiv.org/pdf/2402.05952v1"
  },
  {
    "arxiv_id": "2402.05121v3",
    "entry_id": "http://arxiv.org/abs/2402.05121v3",
    "title": "Large Language Model for Table Processing: A Survey",
    "summary": "Tables, typically two-dimensional and structured to store large amounts of data, are essential in daily activities like database queries, spreadsheet manipulations, web table question answering, and image table information extraction. Automating these table-centric tasks with Large Language Models (LLMs) or Visual Language Models (VLMs) offers significant public benefits, garnering interest from academia and industry. This survey provides a comprehensive overview of table-related tasks, examining both user scenarios and technical aspects. It covers traditional tasks like table question answering as well as emerging fields such as spreadsheet manipulation and table data analysis. We summarize the training techniques for LLMs and VLMs tailored for table processing. Additionally, we discuss prompt engineering, particularly the use of LLM-powered agents, for various table-related tasks. Finally, we highlight several challenges, including diverse user input when serving and slow thinking using chain-of-thought.",
    "authors": [
      "Weizheng Lu",
      "Jing Zhang",
      "Ju Fan",
      "Zihao Fu",
      "Yueguo Chen",
      "Xiaoyong Du"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-04T00:47:53Z",
    "pdf_url": "https://arxiv.org/pdf/2402.05121v3"
  },
  {
    "arxiv_id": "2402.02244v3",
    "entry_id": "http://arxiv.org/abs/2402.02244v3",
    "title": "Beyond the Limits: A Survey of Techniques to Extend the Context Length in Large Language Models",
    "summary": "Recently, large language models (LLMs) have shown remarkable capabilities including understanding context, engaging in logical reasoning, and generating responses. However, this is achieved at the expense of stringent computational and memory requirements, hindering their ability to effectively support long input sequences. This survey provides an inclusive review of the recent techniques and methods devised to extend the sequence length in LLMs, thereby enhancing their capacity for long-context understanding. In particular, we review and categorize a wide range of techniques including architectural modifications, such as modified positional encoding and altered attention mechanisms, which are designed to enhance the processing of longer sequences while avoiding a proportional increase in computational requirements. The diverse methodologies investigated in this study can be leveraged across different phases of LLMs, i.e., training, fine-tuning and inference. This enables LLMs to efficiently process extended sequences. The limitations of the current methodologies is discussed in the last section along with the suggestions for future research directions, underscoring the importance of sequence length in the continued advancement of LLMs.",
    "authors": [
      "Xindi Wang",
      "Mahsa Salmani",
      "Parsa Omidi",
      "Xiangyu Ren",
      "Mehdi Rezagholizadeh",
      "Armaghan Eshaghi"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-03T19:20:02Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02244v3"
  },
  {
    "arxiv_id": "2402.02218v1",
    "entry_id": "http://arxiv.org/abs/2402.02218v1",
    "title": "Machine Intelligence in Africa: a survey",
    "summary": "In the last 5 years, the availability of large audio datasets in African countries has opened unlimited opportunities to build machine intelligence (MI) technologies that are closer to the people and speak, learn, understand, and do businesses in local languages, including for those who cannot read and write. Unfortunately, these audio datasets are not fully exploited by current MI tools, leaving several Africans out of MI business opportunities. Additionally, many state-of-the-art MI models are not culture-aware, and the ethics of their adoption indexes are questionable. The lack thereof is a major drawback in many applications in Africa. This paper summarizes recent developments in machine intelligence in Africa from a multi-layer multiscale and culture-aware ethics perspective, showcasing MI use cases in 54 African countries through 400 articles on MI research, industry, government actions, as well as uses in art, music, the informal economy, and small businesses in Africa. The survey also opens discussions on the reliability of MI rankings and indexes in the African continent as well as algorithmic definitions of unclear terms used in MI.",
    "authors": [
      "Allahsera Auguste Tapo",
      "Ali Traore",
      "Sidy Danioko",
      "Hamidou Tembine"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-02-03T17:27:14Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02218v1"
  },
  {
    "arxiv_id": "2402.02047v4",
    "entry_id": "http://arxiv.org/abs/2402.02047v4",
    "title": "Calibration and Correctness of Language Models for Code",
    "summary": "Machine learning models are widely used, but can also often be wrong. Users would benefit from a reliable indication of whether a given output from a given model should be trusted, so a rational decision can be made whether to use the output or not. For example, outputs can be associated with a confidence measure; if this confidence measure is strongly associated with likelihood of correctness, then the model is said to be well-calibrated.\n  A well-calibrated confidence measure can serve as a basis for rational, graduated decision-making on how much review and care is needed when using generated code. Calibration has so far been studied in mostly non-generative (e.g. classification) settings, especially in software engineering. However, generated code can quite often be wrong: Given generated code, developers must decide whether to use directly, use after varying intensity of careful review, or discard model-generated code. Thus, calibration is vital in generative settings.\n  We make several contributions. We develop a framework for evaluating the calibration of code-generating models. We consider several tasks, correctness criteria, datasets, and approaches, and find that, by and large, generative code models we test are not well-calibrated out of the box. We then show how calibration can be improved using standard methods, such as Platt scaling. Since Platt scaling relies on the prior availability of correctness data, we evaluate the applicability and generalizability of Platt scaling in software engineering, discuss settings where it has good potential for practical use, and settings where it does not. Our contributions will lead to better-calibrated decision-making in the current use of code generated by language models, and offers a framework for future research to further improve calibration methods for generative models in software engineering.",
    "authors": [
      "Claudio Spiess",
      "David Gros",
      "Kunal Suresh Pai",
      "Michael Pradel",
      "Md Rafiqul Islam Rabin",
      "Amin Alipour",
      "Susmit Jha",
      "Prem Devanbu",
      "Toufique Ahmed"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2024-02-03T05:52:28Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02047v4"
  },
  {
    "arxiv_id": "2402.02025v2",
    "entry_id": "http://arxiv.org/abs/2402.02025v2",
    "title": "A Survey of Constraint Formulations in Safe Reinforcement Learning",
    "summary": "Safety is critical when applying reinforcement learning (RL) to real-world problems. As a result, safe RL has emerged as a fundamental and powerful paradigm for optimizing an agent's policy while incorporating notions of safety. A prevalent safe RL approach is based on a constrained criterion, which seeks to maximize the expected cumulative reward subject to specific safety constraints. Despite recent effort to enhance safety in RL, a systematic understanding of the field remains difficult. This challenge stems from the diversity of constraint representations and little exploration of their interrelations. To bridge this knowledge gap, we present a comprehensive review of representative constraint formulations, along with a curated selection of algorithms designed specifically for each formulation. In addition, we elucidate the theoretical underpinnings that reveal the mathematical mutual relations among common problem formulations. We conclude with a discussion of the current state and future directions of safe reinforcement learning research.",
    "authors": [
      "Akifumi Wachi",
      "Xun Shen",
      "Yanan Sui"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-02-03T04:40:31Z",
    "pdf_url": "https://arxiv.org/pdf/2402.02025v2"
  },
  {
    "arxiv_id": "2402.01968v2",
    "entry_id": "http://arxiv.org/abs/2402.01968v2",
    "title": "A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions",
    "summary": "Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for context-aware systems, with each phase of the process encompassing diverse approaches drawn from various application domains such as collision avoidance in autonomous driving, disaster relief management, utility management, supply chain management, human-AI interaction, and others. Finally, we discuss the existing challenges of context-aware multi-agent systems and provide future research directions in this field.",
    "authors": [
      "Hung Du",
      "Srikanth Thudumu",
      "Rajesh Vasa",
      "Kon Mouzakis"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-02-03T00:27:22Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01968v2"
  },
  {
    "arxiv_id": "2402.01874v1",
    "entry_id": "http://arxiv.org/abs/2402.01874v1",
    "title": "The RL/LLM Taxonomy Tree: Reviewing Synergies Between Reinforcement Learning and Large Language Models",
    "summary": "In this work, we review research studies that combine Reinforcement Learning (RL) and Large Language Models (LLMs), two areas that owe their momentum to the development of deep neural networks. We propose a novel taxonomy of three main classes based on the way that the two model types interact with each other. The first class, RL4LLM, includes studies where RL is leveraged to improve the performance of LLMs on tasks related to Natural Language Processing. L4LLM is divided into two sub-categories depending on whether RL is used to directly fine-tune an existing LLM or to improve the prompt of the LLM. In the second class, LLM4RL, an LLM assists the training of an RL model that performs a task that is not inherently related to natural language. We further break down LLM4RL based on the component of the RL training framework that the LLM assists or replaces, namely reward shaping, goal generation, and policy function. Finally, in the third class, RL+LLM, an LLM and an RL agent are embedded in a common planning framework without either of them contributing to training or fine-tuning of the other. We further branch this class to distinguish between studies with and without natural language feedback. We use this taxonomy to explore the motivations behind the synergy of LLMs and RL and explain the reasons for its success, while pinpointing potential shortcomings and areas where further research is needed, as well as alternative methodologies that serve the same goal.",
    "authors": [
      "Moschoula Pternea",
      "Prerna Singh",
      "Abir Chakraborty",
      "Yagna Oruganti",
      "Mirco Milletari",
      "Sayli Bapat",
      "Kebei Jiang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2024-02-02T20:01:15Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01874v1"
  },
  {
    "arxiv_id": "2402.01864v2",
    "entry_id": "http://arxiv.org/abs/2402.01864v2",
    "title": "(A)I Am Not a Lawyer, But...: Engaging Legal Experts towards Responsible LLM Policies for Legal Advice",
    "summary": "Large language models (LLMs) are increasingly capable of providing users with advice in a wide range of professional domains, including legal advice. However, relying on LLMs for legal queries raises concerns due to the significant expertise required and the potential real-world consequences of the advice. To explore \\textit{when} and \\textit{why} LLMs should or should not provide advice to users, we conducted workshops with 20 legal experts using methods inspired by case-based reasoning. The provided realistic queries (\"cases\") allowed experts to examine granular, situation-specific concerns and overarching technical and legal constraints, producing a concrete set of contextual considerations for LLM developers. By synthesizing the factors that impacted LLM response appropriateness, we present a 4-dimension framework: (1) User attributes and behaviors, (2) Nature of queries, (3) AI capabilities, and (4) Social impacts. We share experts' recommendations for LLM response strategies, which center around helping users identify `right questions to ask' and relevant information rather than providing definitive legal judgments. Our findings reveal novel legal considerations, such as unauthorized practice of law, confidentiality, and liability for inaccurate advice, that have been overlooked in the literature. The case-based deliberation method enabled us to elicit fine-grained, practice-informed insights that surpass those from de-contextualized surveys or speculative principles. These findings underscore the applicability of our method for translating domain-specific professional knowledge and practices into policies that can guide LLM behavior in a more responsible direction.",
    "authors": [
      "Inyoung Cheong",
      "King Xia",
      "K. J. Kevin Feng",
      "Quan Ze Chen",
      "Amy X. Zhang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2024-02-02T19:35:34Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01864v2"
  },
  {
    "arxiv_id": "2402.01830v3",
    "entry_id": "http://arxiv.org/abs/2402.01830v3",
    "title": "PiCO: Peer Review in LLMs based on the Consistency Optimization",
    "summary": "Existing large language models (LLMs) evaluation methods typically focus on testing the performance on some closed-environment and domain-specific benchmarks with human annotations. In this paper, we explore a novel unsupervised evaluation direction, utilizing peer-review mechanisms to measure LLMs automatically. In this setting, both open-source and closed-source LLMs lie in the same environment, capable of answering unlabeled questions and evaluating each other, where each LLM's response score is jointly determined by other anonymous ones. To obtain the ability hierarchy among these models, we assign each LLM a learnable capability parameter to adjust the final ranking. We formalize it as a constrained optimization problem, intending to maximize the consistency of each LLM's capabilities and scores. The key assumption behind is that high-level LLM can evaluate others' answers more accurately than low-level ones, while higher-level LLM can also achieve higher response scores. Moreover, we propose three metrics called PEN, CIN, and LIS to evaluate the gap in aligning human rankings. We perform experiments on multiple datasets with these metrics, validating the effectiveness of the proposed approach.",
    "authors": [
      "Kun-Peng Ning",
      "Shuo Yang",
      "Yu-Yang Liu",
      "Jia-Yu Yao",
      "Zhen-Hui Liu",
      "Yong-Hong Tian",
      "Yibing Song",
      "Li Yuan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-02-02T18:49:26Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01830v3"
  },
  {
    "arxiv_id": "2402.01602v1",
    "entry_id": "http://arxiv.org/abs/2402.01602v1",
    "title": "Foundation Model Sherpas: Guiding Foundation Models through Knowledge and Reasoning",
    "summary": "Foundation models (FMs) such as large language models have revolutionized the field of AI by showing remarkable performance in various tasks. However, they exhibit numerous limitations that prevent their broader adoption in many real-world systems, which often require a higher bar for trustworthiness and usability. Since FMs are trained using loss functions aimed at reconstructing the training corpus in a self-supervised manner, there is no guarantee that the model's output aligns with users' preferences for a specific task at hand. In this survey paper, we propose a conceptual framework that encapsulates different modes by which agents could interact with FMs and guide them suitably for a set of tasks, particularly through knowledge augmentation and reasoning. Our framework elucidates agent role categories such as updating the underlying FM, assisting with prompting the FM, and evaluating the FM output. We also categorize several state-of-the-art approaches into agent interaction protocols, highlighting the nature and extent of involvement of the various agent roles. The proposed framework provides guidance for future directions to further realize the power of FMs in practical AI systems.",
    "authors": [
      "Debarun Bhattacharjya",
      "Junkyu Lee",
      "Don Joven Agravante",
      "Balaji Ganesan",
      "Radu Marinescu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-02-02T18:00:35Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01602v1"
  },
  {
    "arxiv_id": "2402.01439v1",
    "entry_id": "http://arxiv.org/abs/2402.01439v1",
    "title": "From Words to Molecules: A Survey of Large Language Models in Chemistry",
    "summary": "In recent years, Large Language Models (LLMs) have achieved significant success in natural language processing (NLP) and various interdisciplinary areas. However, applying LLMs to chemistry is a complex task that requires specialized domain knowledge. This paper provides a thorough exploration of the nuanced methodologies employed in integrating LLMs into the field of chemistry, delving into the complexities and innovations at this interdisciplinary juncture. Specifically, our analysis begins with examining how molecular information is fed into LLMs through various representation and tokenization methods. We then categorize chemical LLMs into three distinct groups based on the domain and modality of their input data, and discuss approaches for integrating these inputs for LLMs. Furthermore, this paper delves into the pretraining objectives with adaptations to chemical LLMs. After that, we explore the diverse applications of LLMs in chemistry, including novel paradigms for their application in chemistry tasks. Finally, we identify promising research directions, including further integration with chemical knowledge, advancements in continual learning, and improvements in model interpretability, paving the way for groundbreaking developments in the field.",
    "authors": [
      "Chang Liao",
      "Yemin Yu",
      "Yu Mei",
      "Ying Wei"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM",
      "q-bio.QM"
    ],
    "published": "2024-02-02T14:30:48Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01439v1"
  },
  {
    "arxiv_id": "2402.01364v2",
    "entry_id": "http://arxiv.org/abs/2402.01364v2",
    "title": "Continual Learning for Large Language Models: A Survey",
    "summary": "Large language models (LLMs) are not amenable to frequent re-training, due to high training costs arising from their massive scale. However, updates are necessary to endow LLMs with new skills and keep them up-to-date with rapidly evolving human knowledge. This paper surveys recent works on continual learning for LLMs. Due to the unique nature of LLMs, we catalog continue learning techniques in a novel multi-staged categorization scheme, involving continual pretraining, instruction tuning, and alignment. We contrast continual learning for LLMs with simpler adaptation methods used in smaller models, as well as with other enhancement strategies like retrieval-augmented generation and model editing. Moreover, informed by a discussion of benchmarks and evaluation, we identify several challenges and future work directions for this crucial task.",
    "authors": [
      "Tongtong Wu",
      "Linhao Luo",
      "Yuan-Fang Li",
      "Shirui Pan",
      "Thuy-Trang Vu",
      "Gholamreza Haffari"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-02T12:34:09Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01364v2"
  },
  {
    "arxiv_id": "2402.06647v1",
    "entry_id": "http://arxiv.org/abs/2402.06647v1",
    "title": "A Survey on Large Language Model Hallucination via a Creativity Perspective",
    "summary": "Hallucinations in large language models (LLMs) are always seen as limitations. However, could they also be a source of creativity? This survey explores this possibility, suggesting that hallucinations may contribute to LLM application by fostering creativity. This survey begins with a review of the taxonomy of hallucinations and their negative impact on LLM reliability in critical applications. Then, through historical examples and recent relevant theories, the survey explores the potential creative benefits of hallucinations in LLMs. To elucidate the value and evaluation criteria of this connection, we delve into the definitions and assessment methods of creativity. Following the framework of divergent and convergent thinking phases, the survey systematically reviews the literature on transforming and harnessing hallucinations for creativity in LLMs. Finally, the survey discusses future research directions, emphasizing the need to further explore and refine the application of hallucinations in creative processes within LLMs.",
    "authors": [
      "Xuhui Jiang",
      "Yuxing Tian",
      "Fengrui Hua",
      "Chengjin Xu",
      "Yuanzhuo Wang",
      "Jian Guo"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-02-02T12:21:04Z",
    "pdf_url": "https://arxiv.org/pdf/2402.06647v1"
  },
  {
    "arxiv_id": "2402.01801v3",
    "entry_id": "http://arxiv.org/abs/2402.01801v3",
    "title": "Large Language Models for Time Series: A Survey",
    "summary": "Large Language Models (LLMs) have seen significant use in domains such as natural language processing and computer vision. Going beyond text, image and graphics, LLMs present a significant potential for analysis of time series data, benefiting domains such as climate, IoT, healthcare, traffic, audio and finance. This survey paper provides an in-depth exploration and a detailed taxonomy of the various methodologies employed to harness the power of LLMs for time series analysis. We address the inherent challenge of bridging the gap between LLMs' original text data training and the numerical nature of time series data, and explore strategies for transferring and distilling knowledge from LLMs to numerical time series analysis. We detail various methodologies, including (1) direct prompting of LLMs, (2) time series quantization, (3) aligning techniques, (4) utilization of the vision modality as a bridging mechanism, and (5) the combination of LLMs with tools. Additionally, this survey offers a comprehensive overview of the existing multimodal time series and text datasets and delves into the challenges and future opportunities of this emerging field. We maintain an up-to-date Github repository which includes all the papers and datasets discussed in the survey.",
    "authors": [
      "Xiyuan Zhang",
      "Ranak Roy Chowdhury",
      "Rajesh K. Gupta",
      "Jingbo Shang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-02T07:24:35Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01801v3"
  },
  {
    "arxiv_id": "2402.01799v2",
    "entry_id": "http://arxiv.org/abs/2402.01799v2",
    "title": "Faster and Lighter LLMs: A Survey on Current Challenges and Way Forward",
    "summary": "Despite the impressive performance of LLMs, their widespread adoption faces challenges due to substantial computational and memory requirements during inference. Recent advancements in model compression and system-level optimization methods aim to enhance LLM inference. This survey offers an overview of these methods, emphasizing recent developments. Through experiments on LLaMA(/2)-7B, we evaluate various compression techniques, providing practical insights for efficient LLM deployment in a unified setting. The empirical analysis on LLaMA(/2)-7B highlights the effectiveness of these methods. Drawing from survey insights, we identify current limitations and discuss potential future directions to improve LLM inference efficiency. We release the codebase to reproduce the results presented in this paper at https://github.com/nyunAI/Faster-LLM-Survey",
    "authors": [
      "Arnav Chavan",
      "Raghav Magazine",
      "Shubham Kushwaha",
      "Mérouane Debbah",
      "Deepak Gupta"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-02-02T06:29:34Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01799v2"
  },
  {
    "arxiv_id": "2402.18587v1",
    "entry_id": "http://arxiv.org/abs/2402.18587v1",
    "title": "At the Dawn of Generative AI Era: A Tutorial-cum-Survey on New Frontiers in 6G Wireless Intelligence",
    "summary": "The majority of data-driven wireless research leans heavily on discriminative AI (DAI) that requires vast real-world datasets. Unlike the DAI, Generative AI (GenAI) pertains to generative models (GMs) capable of discerning the underlying data distribution, patterns, and features of the input data. This makes GenAI a crucial asset in wireless domain wherein real-world data is often scarce, incomplete, costly to acquire, and hard to model or comprehend. With these appealing attributes, GenAI can replace or supplement DAI methods in various capacities. Accordingly, this combined tutorial-survey paper commences with preliminaries of 6G and wireless intelligence by outlining candidate 6G applications and services, presenting a taxonomy of state-of-the-art DAI models, exemplifying prominent DAI use cases, and elucidating the multifaceted ways through which GenAI enhances DAI. Subsequently, we present a tutorial on GMs by spotlighting seminal examples such as generative adversarial networks, variational autoencoders, flow-based GMs, diffusion-based GMs, generative transformers, large language models, to name a few. Contrary to the prevailing belief that GenAI is a nascent trend, our exhaustive review of approximately 120 technical papers demonstrates the scope of research across core wireless research areas, including physical layer design; network optimization, organization, and management; network traffic analytics; cross-layer network security; and localization & positioning. Furthermore, we outline the central role of GMs in pioneering areas of 6G network research, including semantic/THz/near-field communications, ISAC, extremely large antenna arrays, digital twins, AI-generated content services, mobile edge computing and edge AI, adversarial ML, and trustworthy AI. Lastly, we shed light on the multifarious challenges ahead, suggesting potential strategies and promising remedies.",
    "authors": [
      "Abdulkadir Celik",
      "Ahmed M. Eltawil"
    ],
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-02-02T06:23:25Z",
    "pdf_url": "https://arxiv.org/pdf/2402.18587v1"
  },
  {
    "arxiv_id": "2402.01105v4",
    "entry_id": "http://arxiv.org/abs/2402.01105v4",
    "title": "A Survey for Foundation Models in Autonomous Driving",
    "summary": "The advent of foundation models has revolutionized the fields of natural language processing and computer vision, paving the way for their application in autonomous driving (AD). This survey presents a comprehensive review of more than 40 research papers, demonstrating the role of foundation models in enhancing AD. Large language models contribute to planning and simulation in AD, particularly through their proficiency in reasoning, code generation and translation. In parallel, vision foundation models are increasingly adapted for critical tasks such as 3D object detection and tracking, as well as creating realistic driving scenarios for simulation and testing. Multi-modal foundation models, integrating diverse inputs, exhibit exceptional visual understanding and spatial reasoning, crucial for end-to-end AD. This survey not only provides a structured taxonomy, categorizing foundation models based on their modalities and functionalities within the AD domain but also delves into the methods employed in current research. It identifies the gaps between existing foundation models and cutting-edge AD approaches, thereby charting future research directions and proposing a roadmap for bridging these gaps.",
    "authors": [
      "Haoxiang Gao",
      "Zhongruo Wang",
      "Yaqian Li",
      "Kaiwen Long",
      "Ming Yang",
      "Yiqing Shen"
    ],
    "categories": [
      "cs.LG",
      "cs.CV",
      "cs.RO"
    ],
    "published": "2024-02-02T02:44:59Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01105v4"
  },
  {
    "arxiv_id": "2402.01788v2",
    "entry_id": "http://arxiv.org/abs/2402.01788v2",
    "title": "LitLLM: A Toolkit for Scientific Literature Review",
    "summary": "Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-factual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our project page including the demo and toolkit can be accessed here: https://litllm.github.io",
    "authors": [
      "Shubham Agarwal",
      "Gaurav Sahu",
      "Abhay Puri",
      "Issam H. Laradji",
      "Krishnamurthy DJ Dvijotham",
      "Jason Stanley",
      "Laurent Charlin",
      "Christopher Pal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-02-02T02:41:28Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01788v2"
  },
  {
    "arxiv_id": "2402.01786v2",
    "entry_id": "http://arxiv.org/abs/2402.01786v2",
    "title": "COA-GPT: Generative Pre-trained Transformers for Accelerated Course of Action Development in Military Operations",
    "summary": "The development of Courses of Action (COAs) in military operations is traditionally a time-consuming and intricate process. Addressing this challenge, this study introduces COA-GPT, a novel algorithm employing Large Language Models (LLMs) for rapid and efficient generation of valid COAs. COA-GPT incorporates military doctrine and domain expertise to LLMs through in-context learning, allowing commanders to input mission information - in both text and image formats - and receive strategically aligned COAs for review and approval. Uniquely, COA-GPT not only accelerates COA development, producing initial COAs within seconds, but also facilitates real-time refinement based on commander feedback. This work evaluates COA-GPT in a military-relevant scenario within a militarized version of the StarCraft II game, comparing its performance against state-of-the-art reinforcement learning algorithms. Our results demonstrate COA-GPT's superiority in generating strategically sound COAs more swiftly, with added benefits of enhanced adaptability and alignment with commander intentions. COA-GPT's capability to rapidly adapt and update COAs during missions presents a transformative potential for military planning, particularly in addressing planning discrepancies and capitalizing on emergent windows of opportunities.",
    "authors": [
      "Vinicius G. Goecks",
      "Nicholas Waytowich"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-02-01T21:51:09Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01786v2"
  },
  {
    "arxiv_id": "2402.00262v1",
    "entry_id": "http://arxiv.org/abs/2402.00262v1",
    "title": "Computational Experiments Meet Large Language Model Based Agents: A Survey and Perspective",
    "summary": "Computational experiments have emerged as a valuable method for studying complex systems, involving the algorithmization of counterfactuals. However, accurately representing real social systems in Agent-based Modeling (ABM) is challenging due to the diverse and intricate characteristics of humans, including bounded rationality and heterogeneity. To address this limitation, the integration of Large Language Models (LLMs) has been proposed, enabling agents to possess anthropomorphic abilities such as complex reasoning and autonomous learning. These agents, known as LLM-based Agent, offer the potential to enhance the anthropomorphism lacking in ABM. Nonetheless, the absence of explicit explainability in LLMs significantly hinders their application in the social sciences. Conversely, computational experiments excel in providing causal analysis of individual behaviors and complex phenomena. Thus, combining computational experiments with LLM-based Agent holds substantial research potential. This paper aims to present a comprehensive exploration of this fusion. Primarily, it outlines the historical development of agent structures and their evolution into artificial societies, emphasizing their importance in computational experiments. Then it elucidates the advantages that computational experiments and LLM-based Agents offer each other, considering the perspectives of LLM-based Agent for computational experiments and vice versa. Finally, this paper addresses the challenges and future trends in this research domain, offering guidance for subsequent related studies.",
    "authors": [
      "Qun Ma",
      "Xiao Xue",
      "Deyu Zhou",
      "Xiangning Yu",
      "Donghua Liu",
      "Xuwen Zhang",
      "Zihan Zhao",
      "Yifan Shen",
      "Peilin Ji",
      "Juanjuan Li",
      "Gang Wang",
      "Wanpeng Ma"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-02-01T01:17:46Z",
    "pdf_url": "https://arxiv.org/pdf/2402.00262v1"
  },
  {
    "arxiv_id": "2402.00260v3",
    "entry_id": "http://arxiv.org/abs/2402.00260v3",
    "title": "Human-mediated Large Language Models for Robotic Intervention in Children with Autism Spectrum Disorders",
    "summary": "The robotic intervention for individuals with Autism Spectrum Disorder (ASD) has generally used pre-defined scripts to deliver verbal content during one-to-one therapy sessions. This practice restricts the use of robots to limited, pre-mediated instructional curricula. In this paper, we increase robot autonomy in one such robotic intervention for children with ASD by implementing perspective-taking teaching. Our approach uses large language models (LLM) to generate verbal content as texts and then deliver it to the child via robotic speech. In the proposed pipeline, we teach perspective-taking through which our robot takes up three roles: initiator, prompter, and reinforcer. We adopted the GPT-2 + BART pipelines to generate social situations, ask questions (as initiator), and give options (as prompter) when required. The robot encourages the child by giving positive reinforcement for correct answers (as a reinforcer). In addition to our technical contribution, we conducted ten-minute sessions with domain experts simulating an actual perspective teaching session, with the researcher acting as a child participant. These sessions validated our robotic intervention pipeline through surveys, including those from NASA TLX and GodSpeed. We used BERTScore to compare our GPT-2 + BART pipeline with an all GPT-2 and found the performance of the former to be better. Based on the responses by the domain experts, the robot session demonstrated higher performance with no additional increase in mental or physical demand, temporal demand, effort, or frustration compared to a no-robot session. We also concluded that the domain experts perceived the robot as ideally safe, likable, and reliable.",
    "authors": [
      "Ruchik Mishra",
      "Karla Conn Welch",
      "Dan O Popa"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2024-02-01T01:09:00Z",
    "pdf_url": "https://arxiv.org/pdf/2402.00260v3"
  },
  {
    "arxiv_id": "2402.00253v2",
    "entry_id": "http://arxiv.org/abs/2402.00253v2",
    "title": "A Survey on Hallucination in Large Vision-Language Models",
    "summary": "Recent development of Large Vision-Language Models (LVLMs) has attracted growing attention within the AI landscape for its practical implementation potential. However, ``hallucination'', or more specifically, the misalignment between factual visual content and corresponding textual generation, poses a significant challenge of utilizing LVLMs. In this comprehensive survey, we dissect LVLM-related hallucinations in an attempt to establish an overview and facilitate future mitigation. Our scrutiny starts with a clarification of the concept of hallucinations in LVLMs, presenting a variety of hallucination symptoms and highlighting the unique challenges inherent in LVLM hallucinations. Subsequently, we outline the benchmarks and methodologies tailored specifically for evaluating hallucinations unique to LVLMs. Additionally, we delve into an investigation of the root causes of these hallucinations, encompassing insights from the training data and model components. We also critically review existing methods for mitigating hallucinations. The open questions and future directions pertaining to hallucinations within LVLMs are discussed to conclude this survey.",
    "authors": [
      "Hanchao Liu",
      "Wenyuan Xue",
      "Yifei Chen",
      "Dapeng Chen",
      "Xiutian Zhao",
      "Ke Wang",
      "Liping Hou",
      "Rongjun Li",
      "Wei Peng"
    ],
    "categories": [
      "cs.CV",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-02-01T00:33:21Z",
    "pdf_url": "https://arxiv.org/pdf/2402.00253v2"
  },
  {
    "arxiv_id": "2401.18070v2",
    "entry_id": "http://arxiv.org/abs/2401.18070v2",
    "title": "Do Language Models Exhibit the Same Cognitive Biases in Problem Solving as Human Learners?",
    "summary": "There is increasing interest in employing large language models (LLMs) as cognitive models. For such purposes, it is central to understand which properties of human cognition are well-modeled by LLMs, and which are not. In this work, we study the biases of LLMs in relation to those known in children when solving arithmetic word problems. Surveying the learning science literature, we posit that the problem-solving process can be split into three distinct steps: text comprehension, solution planning and solution execution. We construct tests for each one in order to understand whether current LLMs display the same cognitive biases as children in these steps. We generate a novel set of word problems for each of these tests, using a neuro-symbolic approach that enables fine-grained control over the problem features. We find evidence that LLMs, with and without instruction-tuning, exhibit human-like biases in both the text-comprehension and the solution-planning steps of the solving process, but not in the final step, in which the arithmetic expressions are executed to obtain the answer.",
    "authors": [
      "Andreas Opedal",
      "Alessandro Stolfo",
      "Haruki Shirakami",
      "Ying Jiao",
      "Ryan Cotterell",
      "Bernhard Schölkopf",
      "Abulhair Saparov",
      "Mrinmaya Sachan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-31T18:48:20Z",
    "pdf_url": "https://arxiv.org/pdf/2401.18070v2"
  },
  {
    "arxiv_id": "2402.01763v4",
    "entry_id": "http://arxiv.org/abs/2402.01763v4",
    "title": "When Large Language Models Meet Vector Databases: A Survey",
    "summary": "This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration's impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.",
    "authors": [
      "Zhi Jing",
      "Yongye Su",
      "Yikun Han",
      "Bo Yuan",
      "Haiyun Xu",
      "Chunjiang Liu",
      "Kehai Chen",
      "Min Zhang"
    ],
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-01-30T23:35:28Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01763v4"
  },
  {
    "arxiv_id": "2402.01761v1",
    "entry_id": "http://arxiv.org/abs/2402.01761v1",
    "title": "Rethinking Interpretability in the Era of Large Language Models",
    "summary": "Interpretable machine learning has exploded as an area of interest over the last decade, sparked by the rise of increasingly large datasets and deep neural networks. Simultaneously, large language models (LLMs) have demonstrated remarkable capabilities across a wide array of tasks, offering a chance to rethink opportunities in interpretable machine learning. Notably, the capability to explain in natural language allows LLMs to expand the scale and complexity of patterns that can be given to a human. However, these new capabilities raise new challenges, such as hallucinated explanations and immense computational costs.\n  In this position paper, we start by reviewing existing methods to evaluate the emerging field of LLM interpretation (both interpreting LLMs and using LLMs for explanation). We contend that, despite their limitations, LLMs hold the opportunity to redefine interpretability with a more ambitious scope across many applications, including in auditing LLMs themselves. We highlight two emerging research priorities for LLM interpretation: using LLMs to directly analyze new datasets and to generate interactive explanations.",
    "authors": [
      "Chandan Singh",
      "Jeevana Priya Inala",
      "Michel Galley",
      "Rich Caruana",
      "Jianfeng Gao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-30T17:38:54Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01761v1"
  },
  {
    "arxiv_id": "2402.00891v1",
    "entry_id": "http://arxiv.org/abs/2402.00891v1",
    "title": "Large Language Models in Cybersecurity: State-of-the-Art",
    "summary": "The rise of Large Language Models (LLMs) has revolutionized our comprehension of intelligence bringing us closer to Artificial Intelligence. Since their introduction, researchers have actively explored the applications of LLMs across diverse fields, significantly elevating capabilities. Cybersecurity, traditionally resistant to data-driven solutions and slow to embrace machine learning, stands out as a domain. This study examines the existing literature, providing a thorough characterization of both defensive and adversarial applications of LLMs within the realm of cybersecurity. Our review not only surveys and categorizes the current landscape but also identifies critical research gaps. By evaluating both offensive and defensive applications, we aim to provide a holistic understanding of the potential risks and opportunities associated with LLM-driven cybersecurity.",
    "authors": [
      "Farzad Nourmohammadzadeh Motlagh",
      "Mehrdad Hajizadeh",
      "Mehryar Majd",
      "Pejman Najafi",
      "Feng Cheng",
      "Christoph Meinel"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-01-30T16:55:25Z",
    "pdf_url": "https://arxiv.org/pdf/2402.00891v1"
  },
  {
    "arxiv_id": "2404.16038v1",
    "entry_id": "http://arxiv.org/abs/2404.16038v1",
    "title": "A Survey on Generative AI and LLM for Video Generation, Understanding, and Streaming",
    "summary": "This paper offers an insightful examination of how currently top-trending AI technologies, i.e., generative artificial intelligence (Generative AI) and large language models (LLMs), are reshaping the field of video technology, including video generation, understanding, and streaming. It highlights the innovative use of these technologies in producing highly realistic videos, a significant leap in bridging the gap between real-world dynamics and digital creation. The study also delves into the advanced capabilities of LLMs in video understanding, demonstrating their effectiveness in extracting meaningful information from visual content, thereby enhancing our interaction with videos. In the realm of video streaming, the paper discusses how LLMs contribute to more efficient and user-centric streaming experiences, adapting content delivery to individual viewer preferences. This comprehensive review navigates through the current achievements, ongoing challenges, and future possibilities of applying Generative AI and LLMs to video-related tasks, underscoring the immense potential these technologies hold for advancing the field of video technology related to multimedia, networking, and AI communities.",
    "authors": [
      "Pengyuan Zhou",
      "Lin Wang",
      "Zhi Liu",
      "Yanbin Hao",
      "Pan Hui",
      "Sasu Tarkoma",
      "Jussi Kangasharju"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "published": "2024-01-30T14:37:10Z",
    "pdf_url": "https://arxiv.org/pdf/2404.16038v1"
  },
  {
    "arxiv_id": "2402.00888v2",
    "entry_id": "http://arxiv.org/abs/2402.00888v2",
    "title": "Security and Privacy Challenges of Large Language Models: A Survey",
    "summary": "Large Language Models (LLMs) have demonstrated extraordinary capabilities and contributed to multiple fields, such as generating and summarizing text, language translation, and question-answering. Nowadays, LLM is becoming a very popular tool in computerized language processing tasks, with the capability to analyze complicated linguistic patterns and provide relevant and appropriate responses depending on the context. While offering significant advantages, these models are also vulnerable to security and privacy attacks, such as jailbreaking attacks, data poisoning attacks, and Personally Identifiable Information (PII) leakage attacks. This survey provides a thorough review of the security and privacy challenges of LLMs for both training data and users, along with the application-based risks in various domains, such as transportation, education, and healthcare. We assess the extent of LLM vulnerabilities, investigate emerging security and privacy attacks for LLMs, and review the potential defense mechanisms. Additionally, the survey outlines existing research gaps in this domain and highlights future research directions.",
    "authors": [
      "Badhan Chandra Das",
      "M. Hadi Amini",
      "Yanzhao Wu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2024-01-30T04:00:54Z",
    "pdf_url": "https://arxiv.org/pdf/2402.00888v2"
  },
  {
    "arxiv_id": "2401.16638v1",
    "entry_id": "http://arxiv.org/abs/2401.16638v1",
    "title": "Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs",
    "summary": "Fine-tuning large pre-trained language models (LLMs) on particular datasets is a commonly employed strategy in Natural Language Processing (NLP) classification tasks. However, this approach usually results in a loss of models generalizability. In this paper, we present a framework that allows for maintaining generalizability, and enhances the performance on the downstream task by utilizing task-specific context attribution. We show that a linear transformation of the text representation from any transformer model using the task-specific concept operator results in a projection onto the latent concept space, referred to as context attribution in this paper. The specific concept operator is optimized during the supervised learning stage via novel loss functions. The proposed framework demonstrates that context attribution of the text representation for each task objective can improve the capacity of the discriminator function and thus achieve better performance for the classification task. Experimental results on three datasets, namely HateXplain, IMDB reviews, and Social Media Attributions, illustrate that the proposed model attains superior accuracy and generalizability. Specifically, for the non-fine-tuned BERT on the HateXplain dataset, we observe 8% improvement in accuracy and 10% improvement in F1-score. Whereas for the IMDB dataset, fine-tuned state-of-the-art XLNet is outperformed by 1% for both accuracy and F1-score. Furthermore, in an out-of-domain cross-dataset test, DistilBERT fine-tuned on the IMDB dataset in conjunction with the proposed model improves the F1-score on the HateXplain dataset by 7%. For the Social Media Attributions dataset of YouTube comments, we observe 5.2% increase in F1-metric. The proposed framework is implemented with PyTorch and provided open-source on GitHub.",
    "authors": [
      "Stepan Tytarenko",
      "Mohammad Ruhul Amin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-30T00:23:29Z",
    "pdf_url": "https://arxiv.org/pdf/2401.16638v1"
  },
  {
    "arxiv_id": "2401.16310v4",
    "entry_id": "http://arxiv.org/abs/2401.16310v4",
    "title": "An Insight into Security Code Review with LLMs: Capabilities, Obstacles, and Influential Factors",
    "summary": "Security code review is a time-consuming and labor-intensive process typically requiring integration with automated security defect detection tools. However, existing security analysis tools struggle with poor generalization, high false positive rates, and coarse detection granularity. Large Language Models (LLMs) have been considered promising candidates for addressing those challenges. In this study, we conducted an empirical study to explore the potential of LLMs in detecting security defects during code review. Specifically, we evaluated the performance of six LLMs under five different prompts and compared them with state-of-the-art static analysis tools. We also performed linguistic and regression analyses for the best-performing LLM to identify quality problems in its responses and factors influencing its performance. Our findings showthat: (1) existing pre-trained LLMs have limited capability in security code review but significantly outperformthe state-of-the-art static analysis tools. (2) GPT-4 performs best among all LLMs when provided with a CWE list for reference. (3) GPT-4 frequently generates verbose or non-compliant responses with the task requirements given in the prompts. (4) GPT-4 is more adept at identifying security defects in code files with fewer tokens, containing functional logic, or written by developers with less involvement in the project.",
    "authors": [
      "Jiaxin Yu",
      "Peng Liang",
      "Yujia Fu",
      "Amjed Tahir",
      "Mojtaba Shahin",
      "Chong Wang",
      "Yangxiao Cai"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2024-01-29T17:13:44Z",
    "pdf_url": "https://arxiv.org/pdf/2401.16310v4"
  },
  {
    "arxiv_id": "2402.01735v2",
    "entry_id": "http://arxiv.org/abs/2402.01735v2",
    "title": "VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models",
    "summary": "Visually Impaired Assistance (VIA) aims to automatically help the visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (VIALM). In this task, given an image illustrating the physical environments and a linguistic request from a VI user, VIALM aims to output step-by-step guidance to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities in VIA. The results indicate that while LMs can potentially benefit VIA, their output cannot be well environment-grounded (i.e., 25.7% GPT-4's responses) and lacks fine-grained guidance (i.e., 32.1% GPT-4's responses).",
    "authors": [
      "Yi Zhao",
      "Yilin Zhang",
      "Rong Xiang",
      "Jing Li",
      "Hillming Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2024-01-29T08:28:32Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01735v2"
  },
  {
    "arxiv_id": "2402.01726v2",
    "entry_id": "http://arxiv.org/abs/2402.01726v2",
    "title": "AI Does Not Alter Perceptions of Text Messages",
    "summary": "For many people, anxiety, depression, and other social and mental factors can make composing text messages an active challenge. To remedy this problem, large language models (LLMs) may yet prove to be the perfect tool to assist users that would otherwise find texting difficult or stressful. However, despite rapid uptake in LLM usage, considerations for their assistive usage in text message composition have not been explored. A primary concern regarding LLM usage is that poor public sentiment regarding AI introduces the possibility that its usage may harm perceptions of AI-assisted text messages, making usage counter-productive. To (in)validate this possibility, we explore how the belief that a text message did or did not receive AI assistance in composition alters its perceived tone, clarity, and ability to convey intent. In this study, we survey the perceptions of 26 participants on 18 randomly labeled pre-composed text messages. In analyzing the participants' ratings of message tone, clarity, and ability to convey intent, we find that there is no statistically significant evidence that the belief that AI is utilized alters recipient perceptions. This provides hopeful evidence that LLM-based text message composition assistance can be implemented without the risk of counter-productive outcomes.",
    "authors": [
      "N'yoma Diamond"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2024-01-27T14:32:12Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01726v2"
  },
  {
    "arxiv_id": "2401.15422v2",
    "entry_id": "http://arxiv.org/abs/2401.15422v2",
    "title": "A Survey on Data Augmentation in Large Model Era",
    "summary": "Large models, encompassing large language and diffusion models, have shown exceptional promise in approximating human-level intelligence, garnering significant interest from both academic and industrial spheres. However, the training of these large models necessitates vast quantities of high-quality data, and with continuous updates to these models, the existing reservoir of high-quality data may soon be depleted. This challenge has catalyzed a surge in research focused on data augmentation methods. Leveraging large models, these data augmentation techniques have outperformed traditional approaches. This paper offers an exhaustive review of large model-driven data augmentation methods, adopting a comprehensive perspective. We begin by establishing a classification of relevant studies into three main categories: image augmentation, text augmentation, and paired data augmentation. Following this, we delve into various data post-processing techniques pertinent to large model-based data augmentation. Our discussion then expands to encompass the array of applications for these data augmentation methods within natural language processing, computer vision, and audio signal processing. We proceed to evaluate the successes and limitations of large model-based data augmentation across different scenarios. Concluding our review, we highlight prospective challenges and avenues for future exploration in the field of data augmentation. Our objective is to furnish researchers with critical insights, ultimately contributing to the advancement of more sophisticated large models. We consistently maintain the related open-source materials at: https://github.com/MLGroup-JLU/LLM-data-aug-survey.",
    "authors": [
      "Yue Zhou",
      "Chenlu Guo",
      "Xu Wang",
      "Yi Chang",
      "Yuan Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-01-27T14:19:33Z",
    "pdf_url": "https://arxiv.org/pdf/2401.15422v2"
  },
  {
    "arxiv_id": "2401.15347v1",
    "entry_id": "http://arxiv.org/abs/2401.15347v1",
    "title": "A Comprehensive Survey of Compression Algorithms for Language Models",
    "summary": "How can we compress language models without sacrificing accuracy? The number of compression algorithms for language models is rapidly growing to benefit from remarkable advances of recent language models without side effects due to the gigantic size of language models, such as increased carbon emissions and expensive maintenance fees. While numerous compression algorithms have shown remarkable progress in compressing language models, it ironically becomes challenging to capture emerging trends and identify the fundamental concepts underlying them due to the excessive number of algorithms. In this paper, we survey and summarize diverse compression algorithms including pruning, quantization, knowledge distillation, low-rank approximation, parameter sharing, and efficient architecture design. We not only summarize the overall trend of diverse compression algorithms but also select representative algorithms and provide in-depth analyses of them. We discuss the value of each category of compression algorithms, and the desired properties of low-cost compression algorithms which have a significant impact due to the emergence of large language models. Finally, we introduce promising future research topics based on our survey results.",
    "authors": [
      "Seungcheol Park",
      "Jaehyeon Choi",
      "Sojin Lee",
      "U Kang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-27T08:38:56Z",
    "pdf_url": "https://arxiv.org/pdf/2401.15347v1"
  },
  {
    "arxiv_id": "2401.15127v3",
    "entry_id": "http://arxiv.org/abs/2401.15127v3",
    "title": "Evaluation of LLM Chatbots for OSINT-based Cyber Threat Awareness",
    "summary": "Knowledge sharing about emerging threats is crucial in the rapidly advancing field of cybersecurity and forms the foundation of Cyber Threat Intelligence (CTI). In this context, Large Language Models are becoming increasingly significant in the field of cybersecurity, presenting a wide range of opportunities. This study surveys the performance of ChatGPT, GPT4all, Dolly, Stanford Alpaca, Alpaca-LoRA, Falcon, and Vicuna chatbots in binary classification and Named Entity Recognition (NER) tasks performed using Open Source INTelligence (OSINT). We utilize well-established data collected in previous research from Twitter to assess the competitiveness of these chatbots when compared to specialized models trained for those tasks. In binary classification experiments, Chatbot GPT-4 as a commercial model achieved an acceptable F1 score of 0.94, and the open-source GPT4all model achieved an F1 score of 0.90. However, concerning cybersecurity entity recognition, all evaluated chatbots have limitations and are less effective. This study demonstrates the capability of chatbots for OSINT binary classification and shows that they require further improvement in NER to effectively replace specially trained models. Our results shed light on the limitations of the LLM chatbots when compared to specialized models, and can help researchers improve chatbots technology with the objective to reduce the required effort to integrate machine learning in OSINT-based CTI tools.",
    "authors": [
      "Samaneh Shafee",
      "Alysson Bessani",
      "Pedro M. Ferreira"
    ],
    "categories": [
      "cs.CR",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-01-26T13:15:24Z",
    "pdf_url": "https://arxiv.org/pdf/2401.15127v3"
  },
  {
    "arxiv_id": "2402.03349v1",
    "entry_id": "http://arxiv.org/abs/2402.03349v1",
    "title": "When Geoscience Meets Generative AI and Large Language Models: Foundations, Trends, and Future Challenges",
    "summary": "Generative Artificial Intelligence (GAI) represents an emerging field that promises the creation of synthetic data and outputs in different modalities. GAI has recently shown impressive results across a large spectrum of applications ranging from biology, medicine, education, legislation, computer science, and finance. As one strives for enhanced safety, efficiency, and sustainability, generative AI indeed emerges as a key differentiator and promises a paradigm shift in the field. This paper explores the potential applications of generative AI and large language models in geoscience. The recent developments in the field of machine learning and deep learning have enabled the generative model's utility for tackling diverse prediction problems, simulation, and multi-criteria decision-making challenges related to geoscience and Earth system dynamics. This survey discusses several GAI models that have been used in geoscience comprising generative adversarial networks (GANs), physics-informed neural networks (PINNs), and generative pre-trained transformer (GPT)-based structures. These tools have helped the geoscience community in several applications, including (but not limited to) data generation/augmentation, super-resolution, panchromatic sharpening, haze removal, restoration, and land surface changing. Some challenges still remain such as ensuring physical interpretation, nefarious use cases, and trustworthiness. Beyond that, GAI models show promises to the geoscience community, especially with the support to climate change, urban science, atmospheric science, marine science, and planetary science through their extraordinary ability to data-driven modeling and uncertainty quantification.",
    "authors": [
      "Abdenour Hadid",
      "Tanujit Chakraborty",
      "Daniel Busby"
    ],
    "categories": [
      "physics.geo-ph",
      "cs.AI",
      "cs.LG",
      "physics.ao-ph"
    ],
    "published": "2024-01-25T12:03:50Z",
    "pdf_url": "https://arxiv.org/pdf/2402.03349v1"
  },
  {
    "arxiv_id": "2401.14043v3",
    "entry_id": "http://arxiv.org/abs/2401.14043v3",
    "title": "Towards Goal-oriented Prompt Engineering for Large Language Models: A Survey",
    "summary": "Large Language Models (LLMs) have shown prominent performance in various downstream tasks and prompt engineering plays a pivotal role in optimizing LLMs' performance. This paper, not only as an overview of current prompt engineering methods, but also aims to highlight the limitation of designing prompts based on an anthropomorphic assumption that expects LLMs to think like humans. From our review of 50 representative studies, we demonstrate that a goal-oriented prompt formulation, which guides LLMs to follow established human logical thinking, significantly improves the performance of LLMs. Furthermore, We introduce a novel taxonomy that categorizes goal-oriented prompting methods into five interconnected stages and we demonstrate the broad applicability of our framework. With four future directions proposed, we hope to further emphasize the power and potential of goal-oriented prompt engineering in all fields.",
    "authors": [
      "Haochen Li",
      "Jonathan Leung",
      "Zhiqi Shen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-25T09:47:55Z",
    "pdf_url": "https://arxiv.org/pdf/2401.14043v3"
  },
  {
    "arxiv_id": "2401.13912v1",
    "entry_id": "http://arxiv.org/abs/2401.13912v1",
    "title": "A Survey of Deep Learning and Foundation Models for Time Series Forecasting",
    "summary": "Deep Learning has been successfully applied to many application domains, yet its advantages have been slow to emerge for time series forecasting. For example, in the well-known Makridakis (M) Competitions, hybrids of traditional statistical or machine learning techniques have only recently become the top performers. With the recent architectural advances in deep learning being applied to time series forecasting (e.g., encoder-decoders with attention, transformers, and graph neural networks), deep learning has begun to show significant advantages. Still, in the area of pandemic prediction, there remain challenges for deep learning models: the time series is not long enough for effective training, unawareness of accumulated scientific knowledge, and interpretability of the model. To this end, the development of foundation models (large deep learning models with extensive pre-training) allows models to understand patterns and acquire knowledge that can be applied to new related problems before extensive training data becomes available. Furthermore, there is a vast amount of knowledge available that deep learning models can tap into, including Knowledge Graphs and Large Language Models fine-tuned with scientific domain knowledge. There is ongoing research examining how to utilize or inject such knowledge into deep learning models. In this survey, several state-of-the-art modeling techniques are reviewed, and suggestions for further work are provided.",
    "authors": [
      "John A. Miller",
      "Mohammed Aldosari",
      "Farah Saeed",
      "Nasid Habib Barna",
      "Subas Rana",
      "I. Budak Arpinar",
      "Ninghao Liu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-01-25T03:14:07Z",
    "pdf_url": "https://arxiv.org/pdf/2401.13912v1"
  },
  {
    "arxiv_id": "2401.14423v4",
    "entry_id": "http://arxiv.org/abs/2401.14423v4",
    "title": "Prompt Design and Engineering: Introduction and Advanced Methods",
    "summary": "Prompt design and engineering has rapidly become essential for maximizing the potential of large language models. In this paper, we introduce core concepts, advanced techniques like Chain-of-Thought and Reflection, and the principles behind building LLM-based agents. Finally, we provide a survey of tools for prompt engineers.",
    "authors": [
      "Xavier Amatriain"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2024-01-24T06:20:18Z",
    "pdf_url": "https://arxiv.org/pdf/2401.14423v4"
  },
  {
    "arxiv_id": "2401.12874v2",
    "entry_id": "http://arxiv.org/abs/2401.12874v2",
    "title": "From Understanding to Utilization: A Survey on Explainability for Large Language Models",
    "summary": "Explainability for Large Language Models (LLMs) is a critical yet challenging aspect of natural language processing. As LLMs are increasingly integral to diverse applications, their \"black-box\" nature sparks significant concerns regarding transparency and ethical use. This survey underscores the imperative for increased explainability in LLMs, delving into both the research on explainability and the various methodologies and tasks that utilize an understanding of these models. Our focus is primarily on pre-trained Transformer-based LLMs, such as LLaMA family, which pose distinctive interpretability challenges due to their scale and complexity. In terms of existing methods, we classify them into local and global analyses, based on their explanatory objectives. When considering the utilization of explainability, we explore several compelling methods that concentrate on model editing, control generation, and model enhancement. Additionally, we examine representative evaluation metrics and datasets, elucidating their advantages and limitations. Our goal is to reconcile theoretical and empirical understanding with practical implementation, proposing exciting avenues for explanatory techniques and their applications in the LLMs era.",
    "authors": [
      "Haoyan Luo",
      "Lucia Specia"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-23T16:09:53Z",
    "pdf_url": "https://arxiv.org/pdf/2401.12874v2"
  },
  {
    "arxiv_id": "2402.00045v7",
    "entry_id": "http://arxiv.org/abs/2402.00045v7",
    "title": "Detecting Multimedia Generated by Large AI Models: A Survey",
    "summary": "The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) and beyond detection (adding attributes like generalizability, robustness, and interpretability to detectors). Additionally, we have presented a brief overview of generation mechanisms, public datasets, online detection tools, and evaluation metrics to provide a valuable resource for researchers and practitioners in this field. Most importantly, we offer a focused analysis from a social media perspective to highlight their broader societal impact. Furthermore, we identify current challenges in detection and propose directions for future research that address unexplored, ongoing, and emerging issues in detecting multimedia generated by LAIMs. Our aim for this survey is to fill an academic gap and contribute to global AI security efforts, helping to ensure the integrity of information in the digital realm. The project link is https://github.com/Purdue-M2/Detect-LAIM-generated-Multimedia-Survey.",
    "authors": [
      "Li Lin",
      "Neeraj Gupta",
      "Yue Zhang",
      "Hainan Ren",
      "Chun-Hao Liu",
      "Feng Ding",
      "Xin Wang",
      "Xin Li",
      "Luisa Verdoliva",
      "Shu Hu"
    ],
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-22T15:08:19Z",
    "pdf_url": "https://arxiv.org/pdf/2402.00045v7"
  },
  {
    "arxiv_id": "2401.11888v1",
    "entry_id": "http://arxiv.org/abs/2401.11888v1",
    "title": "Multimodal Deep Learning of Word-of-Mouth Text and Demographics to Predict Customer Rating: Handling Consumer Heterogeneity in Marketing",
    "summary": "In the marketing field, understanding consumer heterogeneity, which is the internal or psychological difference among consumers that cannot be captured by behavioral logs, has long been a critical challenge. However, a number of consumers today usually post their evaluation on the specific product on the online platform, which can be the valuable source of such unobservable differences among consumers. Several previous studies have shown the validity of the analysis on text modality, but on the other hand, such analyses may not necessarily demonstrate sufficient predictive accuracy for text alone, as they may not include information readily available from cross-sectional data, such as consumer profile data. In addition, recent advances in machine learning techniques, such as large-scale language models (LLMs) and multimodal learning have made it possible to deal with the various kind of dataset simultaneously, including textual data and the traditional cross-sectional data, and the joint representations can be effectively obtained from multiple modalities. Therefore, this study constructs a product evaluation model that takes into account consumer heterogeneity by multimodal learning of online product reviews and consumer profile information. We also compare multiple models using different modalities or hyper-parameters to demonstrate the robustness of multimodal learning in marketing analysis.",
    "authors": [
      "Junichiro Niimi"
    ],
    "categories": [
      "cs.CE",
      "cs.LG"
    ],
    "published": "2024-01-22T12:28:50Z",
    "pdf_url": "https://arxiv.org/pdf/2401.11888v1"
  },
  {
    "arxiv_id": "2402.01680v2",
    "entry_id": "http://arxiv.org/abs/2402.01680v2",
    "title": "Large Language Model based Multi-Agents: A Survey of Progress and Challenges",
    "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide array of tasks. Due to the impressive planning and reasoning abilities of LLMs, they have been used as autonomous agents to do many tasks automatically. Recently, based on the development of using one LLM as a single planning or decision-making agent, LLM-based multi-agent systems have achieved considerable progress in complex problem-solving and world simulation. To provide the community with an overview of this dynamic field, we present this survey to offer an in-depth discussion on the essential aspects of multi-agent systems based on LLMs, as well as the challenges. Our goal is for readers to gain substantial insights on the following questions: What domains and environments do LLM-based multi-agents simulate? How are these agents profiled and how do they communicate? What mechanisms contribute to the growth of agents' capacities? For those interested in delving into this field of study, we also summarize the commonly used datasets or benchmarks for them to have convenient access. To keep researchers updated on the latest studies, we maintain an open-source GitHub repository, dedicated to outlining the research on LLM-based multi-agent systems.",
    "authors": [
      "Taicheng Guo",
      "Xiuying Chen",
      "Yaqi Wang",
      "Ruidi Chang",
      "Shichao Pei",
      "Nitesh V. Chawla",
      "Olaf Wiest",
      "Xiangliang Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2024-01-21T23:36:14Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01680v2"
  },
  {
    "arxiv_id": "2401.11624v5",
    "entry_id": "http://arxiv.org/abs/2401.11624v5",
    "title": "In-context Learning with Retrieved Demonstrations for Language Models: A Survey",
    "summary": "Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training procedures, and inference algorithms.",
    "authors": [
      "Man Luo",
      "Xin Xu",
      "Yue Liu",
      "Panupong Pasupat",
      "Mehran Kazemi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2024-01-21T23:34:42Z",
    "pdf_url": "https://arxiv.org/pdf/2401.11624v5"
  },
  {
    "arxiv_id": "2403.08774v2",
    "entry_id": "http://arxiv.org/abs/2403.08774v2",
    "title": "Discussion of Loop Expansion and Introduction of Series Cutting Functions to Local Potential Approximation: Complexity Analysis Using Green's Functions, Cutting Of Nth-Order Social Interactions For Progressive Safety",
    "summary": "In this study, we focus on the aforementioned paper, \"Examination Kubo-Matsubara Green's Function Of The Edwards-Anderson Model: Extreme Value Information Flow Of Nth-Order Interpolated Extrapolation Of Zero Phenomena Using The Replica Method (2024)\". This paper also applies theoretical physics methods to better understand the filter bubble phenomenon, focusing in particular on loop expansions and truncation functions. Using the loop expansion method, the complexity of social interactions during the occurrence of filter bubbles will be discussed in order to introduce series, express mathematically, and evaluate the impact of these interactions. We analyze the interactions between agents and their time evolution using a variety of Green's functions, including delayed Green's functions, advanced Green's functions, and causal Green's functions, to capture the dynamic response of the system through local potential approximations. In addition, we apply truncation functions and truncation techniques to ensure incremental safety and evaluate the long-term stability of the system. This approach will enable a better understanding of the mechanisms of filter bubble generation and dissolution, and discuss insights into their prevention and management. This research explores the possibilities of applying theoretical physics frameworks to social science problems and examines methods for analyzing the complex dynamics of information flow and opinion formation in digital society.This paper is partially an attempt to utilize \"Generative AI\" and was written with educational intent. There are currently no plans for it to become a peer-reviewed paper.",
    "authors": [
      "Yasuko Kawahata"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.AI"
    ],
    "published": "2024-01-21T15:03:17Z",
    "pdf_url": "https://arxiv.org/pdf/2403.08774v2"
  },
  {
    "arxiv_id": "2401.11314v2",
    "entry_id": "http://arxiv.org/abs/2401.11314v2",
    "title": "CodeAid: Evaluating a Classroom Deployment of an LLM-based Programming Assistant that Balances Student and Educator Needs",
    "summary": "Timely, personalized feedback is essential for students learning programming. LLM-powered tools like ChatGPT offer instant support, but reveal direct answers with code, which may hinder deep conceptual engagement. We developed CodeAid, an LLM-powered programming assistant delivering helpful, technically correct responses, without revealing code solutions. CodeAid answers conceptual questions, generates pseudo-code with line-by-line explanations, and annotates student's incorrect code with fix suggestions. We deployed CodeAid in a programming class of 700 students for a 12-week semester. A thematic analysis of 8,000 usages of CodeAid was performed, further enriched by weekly surveys, and 22 student interviews. We then interviewed eight programming educators to gain further insights. Our findings reveal four design considerations for future educational AI assistants: D1) exploiting AI's unique benefits; D2) simplifying query formulation while promoting cognitive engagement; D3) avoiding direct responses while encouraging motivated learning; and D4) maintaining transparency and control for students to asses and steer AI responses.",
    "authors": [
      "Majeed Kazemitabaar",
      "Runlong Ye",
      "Xiaoning Wang",
      "Austin Z. Henley",
      "Paul Denny",
      "Michelle Craig",
      "Tovi Grossman"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2024-01-20T20:14:42Z",
    "pdf_url": "https://arxiv.org/pdf/2401.11314v2"
  },
  {
    "arxiv_id": "2401.10825v3",
    "entry_id": "http://arxiv.org/abs/2401.10825v3",
    "title": "Recent Advances in Named Entity Recognition: A Comprehensive Survey and Comparative Study",
    "summary": "Named Entity Recognition seeks to extract substrings within a text that name real-world objects and to determine their type (for example, whether they refer to persons or organizations). In this survey, we first present an overview of recent popular approaches, including advancements in Transformer-based methods and Large Language Models (LLMs) that have not had much coverage in other surveys. In addition, we discuss reinforcement learning and graph-based approaches, highlighting their role in enhancing NER performance. Second, we focus on methods designed for datasets with scarce annotations. Third, we evaluate the performance of the main NER implementations on a variety of datasets with differing characteristics (as regards their domain, their size, and their number of classes). We thus provide a deep comparison of algorithms that have never been considered together. Our experiments shed some light on how the characteristics of datasets affect the behavior of the methods we compare.",
    "authors": [
      "Imed Keraghel",
      "Stanislas Morbieu",
      "Mohamed Nadif"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-01-19T17:21:05Z",
    "pdf_url": "https://arxiv.org/pdf/2401.10825v3"
  },
  {
    "arxiv_id": "2401.10965v1",
    "entry_id": "http://arxiv.org/abs/2401.10965v1",
    "title": "Decentralizing Coordination in Open Vehicle Fleets for Scalable and Dynamic Task Allocation",
    "summary": "One of the major challenges in the coordination of large, open, collaborative, and commercial vehicle fleets is dynamic task allocation. Self-concerned individually rational vehicle drivers have both local and global objectives, which require coordination using some fair and efficient task allocation method. In this paper, we review the literature on scalable and dynamic task allocation focusing on deterministic and dynamic two-dimensional linear assignment problems. We focus on multiagent system representation of open vehicle fleets where dynamically appearing vehicles are represented by software agents that should be allocated to a set of dynamically appearing tasks. We give a comparison and critical analysis of recent research results focusing on centralized, distributed, and decentralized solution approaches. Moreover, we propose mathematical models for dynamic versions of the following assignment problems well known in combinatorial optimization: the assignment problem, bottleneck assignment problem, fair matching problem, dynamic minimum deviation assignment problem, $\\sum_{k}$-assignment problem, the semiassignment problem, the assignment problem with side constraints, and the assignment problem while recognizing agent qualification; all while considering the main aspect of open vehicle fleets: random arrival of tasks and vehicles (agents) that may become available after assisting previous tasks or by participating in the fleet at times based on individual interest.",
    "authors": [
      "Marin Lujak",
      "Stefano Giordani",
      "Andrea Omicini",
      "Sascha Ossowski"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2024-01-19T12:47:27Z",
    "pdf_url": "https://arxiv.org/pdf/2401.10965v1"
  },
  {
    "arxiv_id": "2401.10415v2",
    "entry_id": "http://arxiv.org/abs/2401.10415v2",
    "title": "Can Large Language Model Summarizers Adapt to Diverse Scientific Communication Goals?",
    "summary": "In this work, we investigate the controllability of large language models (LLMs) on scientific summarization tasks. We identify key stylistic and content coverage factors that characterize different types of summaries such as paper reviews, abstracts, and lay summaries. By controlling stylistic features, we find that non-fine-tuned LLMs outperform humans in the MuP review generation task, both in terms of similarity to reference summaries and human preferences. Also, we show that we can improve the controllability of LLMs with keyword-based classifier-free guidance (CFG) while achieving lexical overlap comparable to strong fine-tuned baselines on arXiv and PubMed. However, our results also indicate that LLMs cannot consistently generate long summaries with more than 8 sentences. Furthermore, these models exhibit limited capacity to produce highly abstractive lay summaries. Although LLMs demonstrate strong generic summarization competency, sophisticated content control without costly fine-tuning remains an open problem for domain-specific applications.",
    "authors": [
      "Marcio Fonseca",
      "Shay B. Cohen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-18T23:00:54Z",
    "pdf_url": "https://arxiv.org/pdf/2401.10415v2"
  },
  {
    "arxiv_id": "2401.10364v1",
    "entry_id": "http://arxiv.org/abs/2401.10364v1",
    "title": "Using LLM such as ChatGPT for Designing and Implementing a RISC Processor: Execution,Challenges and Limitations",
    "summary": "This paper discusses the feasibility of using Large Language Models LLM for code generation with a particular application in designing an RISC. The paper also reviews the associated steps such as parsing, tokenization, encoding, attention mechanism, sampling the tokens and iterations during code generation. The generated code for the RISC components is verified through testbenches and hardware implementation on a FPGA board. Four metric parameters Correct output on the first iteration, Number of errors embedded in the code, Number of trials required to achieve the code and Failure to generate the code after three iterations, are used to compare the efficiency of using LLM in programming. In all the cases, the generated code had significant errors and human intervention was always required to fix the bugs. LLM can therefore be used to complement a programmer code design.",
    "authors": [
      "Shadeeb Hossain",
      "Aayush Gohil",
      "Yizhou Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AR",
      "cs.SE"
    ],
    "published": "2024-01-18T20:14:10Z",
    "pdf_url": "https://arxiv.org/pdf/2401.10364v1"
  },
  {
    "arxiv_id": "2401.10034v3",
    "entry_id": "http://arxiv.org/abs/2401.10034v3",
    "title": "Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap",
    "summary": "Large language models (LLMs) have not only revolutionized natural language processing but also extended their prowess to various domains, marking a significant stride towards artificial general intelligence. The interplay between LLMs and evolutionary algorithms (EAs), despite differing in objectives and methodologies, share a common pursuit of applicability in complex problems. Meanwhile, EA can provide an optimization framework for LLM's further enhancement under black-box settings, empowering LLM with flexible global search capacities. On the other hand, the abundant domain knowledge inherent in LLMs could enable EA to conduct more intelligent searches. Furthermore, the text processing and generative capabilities of LLMs would aid in deploying EAs across a wide range of tasks. Based on these complementary advantages, this paper provides a thorough review and a forward-looking roadmap, categorizing the reciprocal inspiration into two main avenues: LLM-enhanced EA and EA-enhanced LLM. Some integrated synergy methods are further introduced to exemplify the complementarity between LLMs and EAs in diverse scenarios, including code generation, software engineering, neural architecture search, and various generation tasks. As the first comprehensive review focused on the EA research in the era of LLMs, this paper provides a foundational stepping stone for understanding the collaborative potential of LLMs and EAs. The identified challenges and future directions offer guidance for researchers and practitioners to unlock the full potential of this innovative collaboration in propelling advancements in optimization and artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/wuxingyu-ai/LLM4EC.",
    "authors": [
      "Xingyu Wu",
      "Sheng-hao Wu",
      "Jibin Wu",
      "Liang Feng",
      "Kay Chen Tan"
    ],
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-01-18T14:58:17Z",
    "pdf_url": "https://arxiv.org/pdf/2401.10034v3"
  },
  {
    "arxiv_id": "2401.09890v1",
    "entry_id": "http://arxiv.org/abs/2401.09890v1",
    "title": "A Survey on Hardware Accelerators for Large Language Models",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools for natural language processing tasks, revolutionizing the field with their ability to understand and generate human-like text. As the demand for more sophisticated LLMs continues to grow, there is a pressing need to address the computational challenges associated with their scale and complexity. This paper presents a comprehensive survey on hardware accelerators designed to enhance the performance and energy efficiency of Large Language Models. By examining a diverse range of accelerators, including GPUs, FPGAs, and custom-designed architectures, we explore the landscape of hardware solutions tailored to meet the unique computational demands of LLMs. The survey encompasses an in-depth analysis of architecture, performance metrics, and energy efficiency considerations, providing valuable insights for researchers, engineers, and decision-makers aiming to optimize the deployment of LLMs in real-world applications.",
    "authors": [
      "Christoforos Kachris"
    ],
    "categories": [
      "cs.AR",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-01-18T11:05:03Z",
    "pdf_url": "https://arxiv.org/pdf/2401.09890v1"
  },
  {
    "arxiv_id": "2401.09615v2",
    "entry_id": "http://arxiv.org/abs/2401.09615v2",
    "title": "Learning Shortcuts: On the Misleading Promise of NLU in Language Models",
    "summary": "The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.",
    "authors": [
      "Geetanjali Bihani",
      "Julia Taylor Rayz"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-17T21:55:15Z",
    "pdf_url": "https://arxiv.org/pdf/2401.09615v2"
  },
  {
    "arxiv_id": "2401.09555v1",
    "entry_id": "http://arxiv.org/abs/2401.09555v1",
    "title": "Improving Classification Performance With Human Feedback: Label a few, we label the rest",
    "summary": "In the realm of artificial intelligence, where a vast majority of data is unstructured, obtaining substantial amounts of labeled data to train supervised machine learning models poses a significant challenge. To address this, we delve into few-shot and active learning, where are goal is to improve AI models with human feedback on a few labeled examples. This paper focuses on understanding how a continuous feedback loop can refine models, thereby enhancing their accuracy, recall, and precision through incremental human input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and SetFit, we aim to analyze the efficacy of using a limited number of labeled examples to substantially improve model accuracy. We benchmark this approach on the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to prove that with just a few labeled examples, we are able to surpass the accuracy of zero shot large language models to provide enhanced text classification performance. We demonstrate that rather than needing to manually label millions of rows of data, we just need to label a few and the model can effectively predict the rest.",
    "authors": [
      "Natan Vidra",
      "Thomas Clifford",
      "Katherine Jijo",
      "Eden Chung",
      "Liang Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-01-17T19:13:05Z",
    "pdf_url": "https://arxiv.org/pdf/2401.09555v1"
  },
  {
    "arxiv_id": "2401.08960v2",
    "entry_id": "http://arxiv.org/abs/2401.08960v2",
    "title": "From User Surveys to Telemetry-Driven AI Agents: Exploring the Potential of Personalized Productivity Solutions",
    "summary": "Information workers increasingly struggle with productivity challenges in modern workplaces, facing difficulties in managing time and effectively utilizing workplace analytics data for behavioral improvement. Despite the availability of productivity metrics through enterprise tools, workers often fail to translate this data into actionable insights. We present a comprehensive, user-centric approach to address these challenges through AI-based productivity agents tailored to users' needs. Utilizing a two-phase method, we first conducted a survey with 363 participants, exploring various aspects of productivity, communication style, agent approach, personality traits, personalization, and privacy. Drawing on the survey insights, we developed a GPT-4 powered personalized productivity agent that utilizes telemetry data gathered via Viva Insights from information workers to provide tailored assistance. We compared its performance with alternative productivity-assistive tools, such as dashboard and narrative, in a study involving 40 participants. Our findings highlight the importance of user-centric design, adaptability, and the balance between personalization and privacy in AI-assisted productivity tools. By building on these insights, our work provides important guidance for developing more effective productivity solutions, ultimately leading to optimized efficiency and user experiences for information workers.",
    "authors": [
      "Subigya Nepal",
      "Javier Hernandez",
      "Talie Massachi",
      "Kael Rowan",
      "Judith Amores",
      "Jina Suh",
      "Gonzalo Ramos",
      "Brian Houck",
      "Shamsi T. Iqbal",
      "Mary Czerwinski"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2024-01-17T04:20:10Z",
    "pdf_url": "https://arxiv.org/pdf/2401.08960v2"
  },
  {
    "arxiv_id": "2401.09491v2",
    "entry_id": "http://arxiv.org/abs/2401.09491v2",
    "title": "Memory, Space, and Planning: Multiscale Predictive Representations",
    "summary": "Memory is inherently entangled with prediction and planning. Flexible behavior in biological and artificial agents depends on the interplay of learning from the past and predicting the future in ever-changing environments. This chapter reviews computational, behavioral, and neural evidence suggesting these processes rely on learning the relational structure of experiences, known as cognitive maps, and draws two key takeaways. First, that these memory structures are organized as multiscale, compact predictive representations in hippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that such predictive memory structures are crucial to the complementary functions of the hippocampus and PFC, both for enabling a recall of detailed and coherent past episodes as well as generalizing experiences at varying scales for efficient prediction and planning. These insights advance our understanding of memory and planning mechanisms in the brain and hold significant implications for advancing artificial intelligence systems.",
    "authors": [
      "Ida Momennejad"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-01-16T21:46:43Z",
    "pdf_url": "https://arxiv.org/pdf/2401.09491v2"
  },
  {
    "arxiv_id": "2401.08825v1",
    "entry_id": "http://arxiv.org/abs/2401.08825v1",
    "title": "AiGen-FoodReview: A Multimodal Dataset of Machine-Generated Restaurant Reviews and Images on Social Media",
    "summary": "Online reviews in the form of user-generated content (UGC) significantly impact consumer decision-making. However, the pervasive issue of not only human fake content but also machine-generated content challenges UGC's reliability. Recent advances in Large Language Models (LLMs) may pave the way to fabricate indistinguishable fake generated content at a much lower cost. Leveraging OpenAI's GPT-4-Turbo and DALL-E-2 models, we craft AiGen-FoodReview, a multi-modal dataset of 20,144 restaurant review-image pairs divided into authentic and machine-generated. We explore unimodal and multimodal detection models, achieving 99.80% multimodal accuracy with FLAVA. We use attributes from readability and photographic theories to score reviews and images, respectively, demonstrating their utility as hand-crafted features in scalable and interpretable detection models, with comparable performance. The paper contributes by open-sourcing the dataset and releasing fake review detectors, recommending its use in unimodal and multimodal fake review detection tasks, and evaluating linguistic and visual features in synthetic versus authentic data.",
    "authors": [
      "Alessandro Gambetti",
      "Qiwei Han"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2024-01-16T20:57:36Z",
    "pdf_url": "https://arxiv.org/pdf/2401.08825v1"
  },
  {
    "arxiv_id": "2401.08358v1",
    "entry_id": "http://arxiv.org/abs/2401.08358v1",
    "title": "Hallucination Detection and Hallucination Mitigation: An Investigation",
    "summary": "Large language models (LLMs), including ChatGPT, Bard, and Llama, have achieved remarkable successes over the last two years in a range of different applications. In spite of these successes, there exist concerns that limit the wide application of LLMs. A key problem is the problem of hallucination. Hallucination refers to the fact that in addition to correct responses, LLMs can also generate seemingly correct but factually incorrect responses. This report aims to present a comprehensive review of the current literature on both hallucination detection and hallucination mitigation. We hope that this report can serve as a good reference for both engineers and researchers who are interested in LLMs and applying them to real world tasks.",
    "authors": [
      "Junliang Luo",
      "Tianyu Li",
      "Di Wu",
      "Michael Jenkin",
      "Steve Liu",
      "Gregory Dudek"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-16T13:36:07Z",
    "pdf_url": "https://arxiv.org/pdf/2401.08358v1"
  },
  {
    "arxiv_id": "2401.08092v2",
    "entry_id": "http://arxiv.org/abs/2401.08092v2",
    "title": "A Survey of Resource-efficient LLM and Multimodal Foundation Models",
    "summary": "Large foundation models, including large language models (LLMs), vision transformers (ViTs), diffusion, and LLM-based multimodal models, are revolutionizing the entire machine learning lifecycle, from training to deployment. However, the substantial advancements in versatility and performance these models offer come at a significant cost in terms of hardware resources. To support the growth of these large models in a scalable and environmentally sustainable way, there has been a considerable focus on developing resource-efficient strategies. This survey delves into the critical importance of such research, examining both algorithmic and systemic aspects. It offers a comprehensive analysis and valuable insights gleaned from existing literature, encompassing a broad array of topics from cutting-edge model architectures and training/serving algorithms to practical system designs and implementations. The goal of this survey is to provide an overarching understanding of how current approaches are tackling the resource challenges posed by large foundation models and to potentially inspire future breakthroughs in this field.",
    "authors": [
      "Mengwei Xu",
      "Wangsong Yin",
      "Dongqi Cai",
      "Rongjie Yi",
      "Daliang Xu",
      "Qipeng Wang",
      "Bingyang Wu",
      "Yihao Zhao",
      "Chen Yang",
      "Shihe Wang",
      "Qiyang Zhang",
      "Zhenyan Lu",
      "Li Zhang",
      "Shangguang Wang",
      "Yuanchun Li",
      "Yunxin Liu",
      "Xin Jin",
      "Xuanzhe Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2024-01-16T03:35:26Z",
    "pdf_url": "https://arxiv.org/pdf/2401.08092v2"
  },
  {
    "arxiv_id": "2401.07518v4",
    "entry_id": "http://arxiv.org/abs/2401.07518v4",
    "title": "Survey of Natural Language Processing for Education: Taxonomy, Systematic Review, and Future Trends",
    "summary": "Natural Language Processing (NLP) aims to analyze text or speech via techniques in the computer science field. It serves applications in the domains of healthcare, commerce, education, and so on. Particularly, NLP has been widely applied to the education domain and its applications have enormous potential to help teaching and learning. In this survey, we review recent advances in NLP with a focus on solving problems relevant to the education domain. In detail, we begin with introducing the related background and the real-world scenarios in education to which NLP techniques could contribute. Then, we present a taxonomy of NLP in the education domain and highlight typical NLP applications including question answering, question construction, automated assessment, and error correction. Next, we illustrate the task definition, challenges, and corresponding cutting-edge techniques based on the above taxonomy. In particular, LLM-involved methods are included for discussion due to the wide usage of LLMs in diverse NLP applications. After that, we showcase some off-the-shelf demonstrations in this domain, which are designed for educators or researchers. At last, we conclude with five promising directions for future research, including generalization over subjects and languages, deployed LLM-based systems for education, adaptive learning for teaching and learning, interpretability for education, and ethical consideration of NLP techniques. We organize all relevant datasets and papers in the open-available Github Link for better review https://github.com/LiXinyuan1015/NLP-for-Education.",
    "authors": [
      "Yunshi Lan",
      "Xinyuan Li",
      "Hanyue Du",
      "Xuesong Lu",
      "Ming Gao",
      "Weining Qian",
      "Aoying Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-15T07:48:42Z",
    "pdf_url": "https://arxiv.org/pdf/2401.07518v4"
  },
  {
    "arxiv_id": "2401.07510v3",
    "entry_id": "http://arxiv.org/abs/2401.07510v3",
    "title": "Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering",
    "summary": "ChatGPT explores a strategic blueprint of question answering (QA) in delivering medical diagnosis, treatment recommendations, and other healthcare support. This is achieved through the increasing incorporation of medical domain data via natural language processing (NLP) and multimodal paradigms. By transitioning the distribution of text, images, videos, and other modalities from the general domain to the medical domain, these techniques have expedited the progress of medical domain question answering (MDQA). They bridge the gap between human natural language and sophisticated medical domain knowledge or expert manual annotations, handling large-scale, diverse, unbalanced, or even unlabeled data analysis scenarios in medical contexts. Central to our focus is the utilizing of language models and multimodal paradigms for medical question answering, aiming to guide the research community in selecting appropriate mechanisms for their specific medical research requirements. Specialized tasks such as unimodal-related question answering, reading comprehension, reasoning, diagnosis, relation extraction, probability modeling, and others, as well as multimodal-related tasks like vision question answering, image caption, cross-modal retrieval, report summarization, and generation, are discussed in detail. Each section delves into the intricate specifics of the respective method under consideration. This paper highlights the structures and advancements of medical domain explorations against general domain methods, emphasizing their applications across different tasks and datasets. It also outlines current challenges and opportunities for future medical domain research, paving the way for continued innovation and application in this rapidly evolving field.",
    "authors": [
      "Qing Li",
      "Lei Li",
      "Yu Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-15T07:21:16Z",
    "pdf_url": "https://arxiv.org/pdf/2401.07510v3"
  },
  {
    "arxiv_id": "2401.07187v3",
    "entry_id": "http://arxiv.org/abs/2401.07187v3",
    "title": "A Survey on Statistical Theory of Deep Learning: Approximation, Training Dynamics, and Generative Models",
    "summary": "In this article, we review the literature on statistical theories of neural networks from three perspectives: approximation, training dynamics and generative models. In the first part, results on excess risks for neural networks are reviewed in the nonparametric framework of regression (and classification in Appendix~{\\color{blue}B}). These results rely on explicit constructions of neural networks, leading to fast convergence rates of excess risks. Nonetheless, their underlying analysis only applies to the global minimizer in the highly non-convex landscape of deep neural networks. This motivates us to review the training dynamics of neural networks in the second part. Specifically, we review papers that attempt to answer ``how the neural network trained via gradient-based methods finds the solution that can generalize well on unseen data.'' In particular, two well-known paradigms are reviewed: the Neural Tangent Kernel (NTK) paradigm, and Mean-Field (MF) paradigm. Last but not least, we review the most recent theoretical advancements in generative models including Generative Adversarial Networks (GANs), diffusion models, and in-context learning (ICL) in the Large Language Models (LLMs) from two perpsectives reviewed previously, i.e., approximation and training dynamics.",
    "authors": [
      "Namjoon Suh",
      "Guang Cheng"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "math.ST"
    ],
    "published": "2024-01-14T02:30:19Z",
    "pdf_url": "https://arxiv.org/pdf/2401.07187v3"
  },
  {
    "arxiv_id": "2401.06967v1",
    "entry_id": "http://arxiv.org/abs/2401.06967v1",
    "title": "NHANES-GCP: Leveraging the Google Cloud Platform and BigQuery ML for reproducible machine learning with data from the National Health and Nutrition Examination Survey",
    "summary": "Summary: NHANES, the National Health and Nutrition Examination Survey, is a program of studies led by the Centers for Disease Control and Prevention (CDC) designed to assess the health and nutritional status of adults and children in the United States (U.S.). NHANES data is frequently used by biostatisticians and clinical scientists to study health trends across the U.S., but every analysis requires extensive data management and cleaning before use and this repetitive data engineering collectively costs valuable research time and decreases the reproducibility of analyses. Here, we introduce NHANES-GCP, a Cloud Development Kit for Terraform (CDKTF) Infrastructure-as-Code (IaC) and Data Build Tool (dbt) resources built on the Google Cloud Platform (GCP) that automates the data engineering and management aspects of working with NHANES data. With current GCP pricing, NHANES-GCP costs less than $2 to run and less than $15/yr of ongoing costs for hosting the NHANES data, all while providing researchers with clean data tables that can readily be integrated for large-scale analyses. We provide examples of leveraging BigQuery ML to carry out the process of selecting data, integrating data, training machine learning and statistical models, and generating results all from a single SQL-like query. NHANES-GCP is designed to enhance the reproducibility of analyses and create a well-engineered NHANES data resource for statistics, machine learning, and fine-tuning Large Language Models (LLMs).\n  Availability and implementation\" NHANES-GCP is available at https://github.com/In-Vivo-Group/NHANES-GCP",
    "authors": [
      "B. Ross Katz",
      "Abdul Khan",
      "James York-Winegar",
      "Alexander J. Titus"
    ],
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "stat.AP"
    ],
    "published": "2024-01-13T03:41:54Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06967v1"
  },
  {
    "arxiv_id": "2401.06676v1",
    "entry_id": "http://arxiv.org/abs/2401.06676v1",
    "title": "LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase",
    "summary": "Recommendation systems are ubiquitous, from Spotify playlist suggestions to Amazon product suggestions. Nevertheless, depending on the methodology or the dataset, these systems typically fail to capture user preferences and generate general recommendations. Recent advancements in Large Language Models (LLM) offer promising results for analyzing user queries. However, employing these models to capture user preferences and efficiency remains an open question. In this paper, we propose LLMRS, an LLM-based zero-shot recommender system where we employ pre-trained LLM to encode user reviews into a review score and generate user-tailored recommendations. We experimented with LLMRS on a real-world dataset, the Amazon product reviews, for software purchase use cases. The results show that LLMRS outperforms the ranking-based baseline model while successfully capturing meaningful information from product reviews, thereby providing more reliable recommendations.",
    "authors": [
      "Angela John",
      "Theophilus Aidoo",
      "Hamayoon Behmanush",
      "Irem B. Gunduz",
      "Hewan Shrestha",
      "Maxx Richard Rahman",
      "Wolfgang Maaß"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2024-01-12T16:33:17Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06676v1"
  },
  {
    "arxiv_id": "2401.10279v1",
    "entry_id": "http://arxiv.org/abs/2401.10279v1",
    "title": "A systematic review of geospatial location embedding approaches in large language models: A path to spatial AI systems",
    "summary": "Geospatial Location Embedding (GLE) helps a Large Language Model (LLM) assimilate and analyze spatial data. GLE emergence in Geospatial Artificial Intelligence (GeoAI) is precipitated by the need for deeper geospatial awareness in our complex contemporary spaces and the success of LLMs in extracting deep meaning in Generative AI. We searched Google Scholar, Science Direct, and arXiv for papers on geospatial location embedding and LLM and reviewed articles focused on gaining deeper spatial \"knowing\" through LLMs. We screened 304 titles, 30 abstracts, and 18 full-text papers that reveal four GLE themes - Entity Location Embedding (ELE), Document Location Embedding (DLE), Sequence Location Embedding (SLE), and Token Location Embedding (TLE). Synthesis is tabular and narrative, including a dialogic conversation between \"Space\" and \"LLM.\" Though GLEs aid spatial understanding by superimposing spatial data, they emphasize the need to advance in the intricacies of spatial modalities and generalized reasoning. GLEs signal the need for a Spatial Foundation/Language Model (SLM) that embeds spatial knowing within the model architecture. The SLM framework advances Spatial Artificial Intelligence Systems (SPAIS), establishing a Spatial Vector Space (SVS) that maps to physical space. The resulting spatially imbued Language Model is unique. It simultaneously represents actual space and an AI-capable space, paving the way for AI native geo storage, analysis, and multi-modality as the basis for Spatial Artificial Intelligence Systems (SPAIS).",
    "authors": [
      "Sean Tucker"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2024-01-12T12:43:33Z",
    "pdf_url": "https://arxiv.org/pdf/2401.10279v1"
  },
  {
    "arxiv_id": "2401.06831v1",
    "entry_id": "http://arxiv.org/abs/2401.06831v1",
    "title": "A Survey on the Applications of Frontier AI, Foundation Models, and Large Language Models to Intelligent Transportation Systems",
    "summary": "This survey paper explores the transformative influence of frontier AI, foundation models, and Large Language Models (LLMs) in the realm of Intelligent Transportation Systems (ITS), emphasizing their integral role in advancing transportation intelligence, optimizing traffic management, and contributing to the realization of smart cities. Frontier AI refers to the forefront of AI technology, encompassing the latest advancements, innovations, and experimental techniques in the field, especially AI foundation models and LLMs. Foundation models, like GPT-4, are large, general-purpose AI models that provide a base for a wide range of applications. They are characterized by their versatility and scalability. LLMs are obtained from finetuning foundation models with a specific focus on processing and generating natural language. They excel in tasks like language understanding, text generation, translation, and summarization. By leveraging vast textual data, including traffic reports and social media interactions, LLMs extract critical insights, fostering the evolution of ITS. The survey navigates the dynamic synergy between LLMs and ITS, delving into applications in traffic management, integration into autonomous vehicles, and their role in shaping smart cities. It provides insights into ongoing research, innovations, and emerging trends, aiming to inspire collaboration at the intersection of language, intelligence, and mobility for safer, more efficient, and sustainable transportation systems. The paper further surveys interactions between LLMs and various aspects of ITS, exploring roles in traffic management, facilitating autonomous vehicles, and contributing to smart city development, while addressing challenges brought by frontier AI and foundation models. This paper offers valuable inspiration for future research and innovation in the transformative domain of intelligent transportation.",
    "authors": [
      "Mohamed R. Shoaib",
      "Heba M. Emara",
      "Jun Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-12T10:29:48Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06831v1"
  },
  {
    "arxiv_id": "2401.05778v1",
    "entry_id": "http://arxiv.org/abs/2401.05778v1",
    "title": "Risk Taxonomy, Mitigation, and Assessment Benchmarks of Large Language Model Systems",
    "summary": "Large language models (LLMs) have strong capabilities in solving diverse natural language processing tasks. However, the safety and security issues of LLM systems have become the major obstacle to their widespread application. Many studies have extensively investigated risks in LLM systems and developed the corresponding mitigation strategies. Leading-edge enterprises such as OpenAI, Google, Meta, and Anthropic have also made lots of efforts on responsible LLMs. Therefore, there is a growing need to organize the existing studies and establish comprehensive taxonomies for the community. In this paper, we delve into four essential modules of an LLM system, including an input module for receiving prompts, a language model trained on extensive corpora, a toolchain module for development and deployment, and an output module for exporting LLM-generated content. Based on this, we propose a comprehensive taxonomy, which systematically analyzes potential risks associated with each module of an LLM system and discusses the corresponding mitigation strategies. Furthermore, we review prevalent benchmarks, aiming to facilitate the risk assessment of LLM systems. We hope that this paper can help LLM participants embrace a systematic perspective to build their responsible LLM systems.",
    "authors": [
      "Tianyu Cui",
      "Yanling Wang",
      "Chuanpu Fu",
      "Yong Xiao",
      "Sijia Li",
      "Xinhao Deng",
      "Yunpeng Liu",
      "Qinglin Zhang",
      "Ziyi Qiu",
      "Peiyang Li",
      "Zhixing Tan",
      "Junwu Xiong",
      "Xinyu Kong",
      "Zujie Wen",
      "Ke Xu",
      "Qi Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-11T09:29:56Z",
    "pdf_url": "https://arxiv.org/pdf/2401.05778v1"
  },
  {
    "arxiv_id": "2401.06807v1",
    "entry_id": "http://arxiv.org/abs/2401.06807v1",
    "title": "An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue Assistant",
    "summary": "In recent times, there has been an increasing awareness about imminent environmental challenges, resulting in people showing a stronger dedication to taking care of the environment and nurturing green life. The current $19.6 billion indoor gardening industry, reflective of this growing sentiment, not only signifies a monetary value but also speaks of a profound human desire to reconnect with the natural world. However, several recent surveys cast a revealing light on the fate of plants within our care, with more than half succumbing primarily due to the silent menace of improper care. Thus, the need for accessible expertise capable of assisting and guiding individuals through the intricacies of plant care has become paramount more than ever. In this work, we make the very first attempt at building a plant care assistant, which aims to assist people with plant(-ing) concerns through conversations. We propose a plant care conversational dataset named Plantational, which contains around 1K dialogues between users and plant care experts. Our end-to-end proposed approach is two-fold : (i) We first benchmark the dataset with the help of various large language models (LLMs) and visual language model (VLM) by studying the impact of instruction tuning (zero-shot and few-shot prompting) and fine-tuning techniques on this task; (ii) finally, we build EcoSage, a multi-modal plant care assisting dialogue generation framework, incorporating an adapter-based modality infusion using a gated mechanism. We performed an extensive examination (both automated and manual evaluation) of the performance exhibited by various LLMs and VLM in the generation of the domain-specific dialogue responses to underscore the respective strengths and weaknesses of these diverse models.",
    "authors": [
      "Mohit Tomar",
      "Abhisek Tiwari",
      "Tulika Saha",
      "Prince Jha",
      "Sriparna Saha"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-10T19:06:35Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06807v1"
  },
  {
    "arxiv_id": "2401.06805v2",
    "entry_id": "http://arxiv.org/abs/2401.06805v2",
    "title": "Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning",
    "summary": "Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence (AGI) with abstract reasoning ability is the goal of next-generation AI. Recent advancements in Large Language Models (LLMs), along with the emerging field of Multimodal Large Language Models (MLLMs), have demonstrated impressive capabilities across a wide range of multimodal tasks and applications. Particularly, various MLLMs, each with distinct model architectures, training data, and training stages, have been evaluated across a broad range of MLLM benchmarks. These studies have, to varying degrees, revealed different aspects of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs have not been systematically investigated. In this survey, we comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions. We believe our survey establishes a solid base and sheds light on this important topic, multimodal reasoning.",
    "authors": [
      "Yiqi Wang",
      "Wentao Chen",
      "Xiaotian Han",
      "Xudong Lin",
      "Haiteng Zhao",
      "Yongfei Liu",
      "Bohan Zhai",
      "Jianbo Yuan",
      "Quanzeng You",
      "Hongxia Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-10T15:29:21Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06805v2"
  },
  {
    "arxiv_id": "2401.05459v2",
    "entry_id": "http://arxiv.org/abs/2401.05459v2",
    "title": "Personal LLM Agents: Insights and Survey about the Capability, Efficiency and Security",
    "summary": "Since the advent of personal computing devices, intelligent personal assistants (IPAs) have been one of the key technologies that researchers and engineers have focused on, aiming to help users efficiently obtain information and execute tasks, and provide users with more intelligent, convenient, and rich interaction experiences. With the development of smartphones and IoT, computing and sensing devices have become ubiquitous, greatly expanding the boundaries of IPAs. However, due to the lack of capabilities such as user intent understanding, task planning, tool using, and personal data management etc., existing IPAs still have limited practicality and scalability. Recently, the emergence of foundation models, represented by large language models (LLMs), brings new opportunities for the development of IPAs. With the powerful semantic understanding and reasoning capabilities, LLM can enable intelligent agents to solve complex problems autonomously. In this paper, we focus on Personal LLM Agents, which are LLM-based agents that are deeply integrated with personal data and personal devices and used for personal assistance. We envision that Personal LLM Agents will become a major software paradigm for end-users in the upcoming era. To realize this vision, we take the first step to discuss several important questions about Personal LLM Agents, including their architecture, capability, efficiency and security. We start by summarizing the key components and design choices in the architecture of Personal LLM Agents, followed by an in-depth analysis of the opinions collected from domain experts. Next, we discuss several key challenges to achieve intelligent, efficient and secure Personal LLM Agents, followed by a comprehensive survey of representative solutions to address these challenges.",
    "authors": [
      "Yuanchun Li",
      "Hao Wen",
      "Weijun Wang",
      "Xiangyu Li",
      "Yizhen Yuan",
      "Guohong Liu",
      "Jiacheng Liu",
      "Wenxing Xu",
      "Xiang Wang",
      "Yi Sun",
      "Rui Kong",
      "Yile Wang",
      "Hanfei Geng",
      "Jian Luan",
      "Xuefeng Jin",
      "Zilong Ye",
      "Guanjing Xiong",
      "Fan Zhang",
      "Xiang Li",
      "Mengwei Xu",
      "Zhijun Li",
      "Peng Li",
      "Yang Liu",
      "Ya-Qin Zhang",
      "Yunxin Liu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-01-10T09:25:45Z",
    "pdf_url": "https://arxiv.org/pdf/2401.05459v2"
  },
  {
    "arxiv_id": "2401.04934v1",
    "entry_id": "http://arxiv.org/abs/2401.04934v1",
    "title": "Fully Decentralized Cooperative Multi-Agent Reinforcement Learning: A Survey",
    "summary": "Cooperative multi-agent reinforcement learning is a powerful tool to solve many real-world cooperative tasks, but restrictions of real-world applications may require training the agents in a fully decentralized manner. Due to the lack of information about other agents, it is challenging to derive algorithms that can converge to the optimal joint policy in a fully decentralized setting. Thus, this research area has not been thoroughly studied. In this paper, we seek to systematically review the fully decentralized methods in two settings: maximizing a shared reward of all agents and maximizing the sum of individual rewards of all agents, and discuss open questions and future research directions.",
    "authors": [
      "Jiechuan Jiang",
      "Kefan Su",
      "Zongqing Lu"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-10T05:07:42Z",
    "pdf_url": "https://arxiv.org/pdf/2401.04934v1"
  },
  {
    "arxiv_id": "2401.04334v1",
    "entry_id": "http://arxiv.org/abs/2401.04334v1",
    "title": "Large Language Models for Robotics: Opportunities, Challenges, and Perspectives",
    "summary": "Large language models (LLMs) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, LLMs harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only LLMs often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of LLMs and multimodal LLMs into various robotic tasks. Additionally, we propose a framework that utilizes multimodal GPT-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that GPT-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of LLMs and multimodal LLMs across a variety of robotic tasks enriches the understanding of LLM-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.",
    "authors": [
      "Jiaqi Wang",
      "Zihao Wu",
      "Yiwei Li",
      "Hanqi Jiang",
      "Peng Shu",
      "Enze Shi",
      "Huawen Hu",
      "Chong Ma",
      "Yiheng Liu",
      "Xuhui Wang",
      "Yincheng Yao",
      "Xuan Liu",
      "Huaqin Zhao",
      "Zhengliang Liu",
      "Haixing Dai",
      "Lin Zhao",
      "Bao Ge",
      "Xiang Li",
      "Tianming Liu",
      "Shu Zhang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2024-01-09T03:22:16Z",
    "pdf_url": "https://arxiv.org/pdf/2401.04334v1"
  },
  {
    "arxiv_id": "2401.06796v1",
    "entry_id": "http://arxiv.org/abs/2401.06796v1",
    "title": "AI Hallucinations: A Misnomer Worth Clarifying",
    "summary": "As large language models continue to advance in Artificial Intelligence (AI), text generation systems have been shown to suffer from a problematic phenomenon termed often as \"hallucination.\" However, with AI's increasing presence across various domains including medicine, concerns have arisen regarding the use of the term itself. In this study, we conducted a systematic review to identify papers defining \"AI hallucination\" across fourteen databases. We present and analyze definitions obtained across all databases, categorize them based on their applications, and extract key points within each category. Our results highlight a lack of consistency in how the term is used, but also help identify several alternative terms in the literature. We discuss implications of these and call for a more unified effort to bring consistency to an important contemporary AI issue that can affect multiple domains significantly.",
    "authors": [
      "Negar Maleki",
      "Balaji Padmanabhan",
      "Kaushik Dutta"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-09T01:49:41Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06796v1"
  },
  {
    "arxiv_id": "2401.05443v1",
    "entry_id": "http://arxiv.org/abs/2401.05443v1",
    "title": "LLM4PLC: Harnessing Large Language Models for Verifiable Programming of PLCs in Industrial Control Systems",
    "summary": "Although Large Language Models (LLMs) have established pre-dominance in automated code generation, they are not devoid of shortcomings. The pertinent issues primarily relate to the absence of execution guarantees for generated code, a lack of explainability, and suboptimal support for essential but niche programming languages. State-of-the-art LLMs such as GPT-4 and LLaMa2 fail to produce valid programs for Industrial Control Systems (ICS) operated by Programmable Logic Controllers (PLCs). We propose LLM4PLC, a user-guided iterative pipeline leveraging user feedback and external verification tools including grammar checkers, compilers and SMV verifiers to guide the LLM's generation. We further enhance the generation potential of LLM by employing Prompt Engineering and model fine-tuning through the creation and usage of LoRAs. We validate this system using a FischerTechnik Manufacturing TestBed (MFTB), illustrating how LLMs can evolve from generating structurally flawed code to producing verifiably correct programs for industrial applications. We run a complete test suite on GPT-3.5, GPT-4, Code Llama-7B, a fine-tuned Code Llama-7B model, Code Llama-34B, and a fine-tuned Code Llama-34B model. The proposed pipeline improved the generation success rate from 47% to 72%, and the Survey-of-Experts code quality from 2.25/10 to 7.75/10. To promote open research, we share the complete experimental setup, the LLM Fine-Tuning Weights, and the video demonstrations of the different programs on our dedicated webpage.",
    "authors": [
      "Mohamad Fakih",
      "Rahul Dharmaji",
      "Yasamin Moghaddas",
      "Gustavo Quiros Araya",
      "Oluwatosin Ogundare",
      "Mohammad Abdullah Al Faruque"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "published": "2024-01-08T23:52:42Z",
    "pdf_url": "https://arxiv.org/pdf/2401.05443v1"
  },
  {
    "arxiv_id": "2401.06795v2",
    "entry_id": "http://arxiv.org/abs/2401.06795v2",
    "title": "AI and Generative AI for Research Discovery and Summarization",
    "summary": "AI and generative AI tools, including chatbots like ChatGPT that rely on large language models (LLMs), have burst onto the scene this year, creating incredible opportunities to increase work productivity and improve our lives. Statisticians and data scientists have begun experiencing the benefits from the availability of these tools in numerous ways, such as the generation of programming code from text prompts to analyze data or fit statistical models. One area that these tools can make a substantial impact is in research discovery and summarization. Standalone tools and plugins to chatbots are being developed that allow researchers to more quickly find relevant literature than pre-2023 search tools. Furthermore, generative AI tools have improved to the point where they can summarize and extract the key points from research articles in succinct language. Finally, chatbots based on highly parameterized LLMs can be used to simulate abductive reasoning, which provides researchers the ability to make connections among related technical topics, which can also be used for research discovery. We review the developments in AI and generative AI for research discovery and summarization, and propose directions where these types of tools are likely to head in the future that may be of interest to statistician and data scientists.",
    "authors": [
      "Mark Glickman",
      "Yi Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-08T18:42:55Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06795v2"
  },
  {
    "arxiv_id": "2401.04057v1",
    "entry_id": "http://arxiv.org/abs/2401.04057v1",
    "title": "Unveiling Bias in Fairness Evaluations of Large Language Models: A Critical Literature Review of Music and Movie Recommendation Systems",
    "summary": "The rise of generative artificial intelligence, particularly Large Language Models (LLMs), has intensified the imperative to scrutinize fairness alongside accuracy. Recent studies have begun to investigate fairness evaluations for LLMs within domains such as recommendations. Given that personalization is an intrinsic aspect of recommendation systems, its incorporation into fairness assessments is paramount. Yet, the degree to which current fairness evaluation frameworks account for personalization remains unclear. Our comprehensive literature review aims to fill this gap by examining how existing frameworks handle fairness evaluations of LLMs, with a focus on the integration of personalization factors. Despite an exhaustive collection and analysis of relevant works, we discovered that most evaluations overlook personalization, a critical facet of recommendation systems, thereby inadvertently perpetuating unfair practices. Our findings shed light on this oversight and underscore the urgent need for more nuanced fairness evaluations that acknowledge personalization. Such improvements are vital for fostering equitable development within the AI community.",
    "authors": [
      "Chandan Kumar Sah",
      "Lian Xiaoli",
      "Muhammad Mirajul Islam"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2024-01-08T17:57:29Z",
    "pdf_url": "https://arxiv.org/pdf/2401.04057v1"
  },
  {
    "arxiv_id": "2401.03910v1",
    "entry_id": "http://arxiv.org/abs/2401.03910v1",
    "title": "A Philosophical Introduction to Language Models -- Part I: Continuity With Classic Debates",
    "summary": "Large language models like GPT-4 have achieved remarkable proficiency in a broad spectrum of language-based tasks, some of which are traditionally associated with hallmarks of human intelligence. This has prompted ongoing disagreements about the extent to which we can meaningfully ascribe any kind of linguistic or cognitive competence to language models. Such questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. This article -- the first part of two companion papers -- serves both as a primer on language models for philosophers, and as an opinionated survey of their significance in relation to classic debates in the philosophy cognitive science, artificial intelligence, and linguistics. We cover topics such as compositionality, language acquisition, semantic competence, grounding, world models, and the transmission of cultural knowledge. We argue that the success of language models challenges several long-held assumptions about artificial neural networks. However, we also highlight the need for further empirical investigation to better understand their internal mechanisms. This sets the stage for the companion paper (Part II), which turns to novel empirical methods for probing the inner workings of language models, and new philosophical questions prompted by their latest developments.",
    "authors": [
      "Raphaël Millière",
      "Cameron Buckner"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-08T14:12:31Z",
    "pdf_url": "https://arxiv.org/pdf/2401.03910v1"
  },
  {
    "arxiv_id": "2401.06792v2",
    "entry_id": "http://arxiv.org/abs/2401.06792v2",
    "title": "LightHouse: A Survey of AGI Hallucination",
    "summary": "With the development of artificial intelligence, large-scale models have become increasingly intelligent. However, numerous studies indicate that hallucinations within these large models are a bottleneck hindering the development of AI research. In the pursuit of achieving strong artificial intelligence, a significant volume of research effort is being invested in the AGI (Artificial General Intelligence) hallucination research. Previous explorations have been conducted in researching hallucinations within LLMs (Large Language Models). As for multimodal AGI, research on hallucinations is still in an early stage. To further the progress of research in the domain of hallucinatory phenomena, we present a bird's eye view of hallucinations in AGI, summarizing the current work on AGI hallucinations and proposing some directions for future research.",
    "authors": [
      "Feng Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-08T03:52:40Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06792v2"
  },
  {
    "arxiv_id": "2401.03568v2",
    "entry_id": "http://arxiv.org/abs/2401.03568v2",
    "title": "Agent AI: Surveying the Horizons of Multimodal Interaction",
    "summary": "Multi-modal AI systems will likely become a ubiquitous presence in our everyday lives. A promising approach to making these systems more interactive is to embody them as agents within physical and virtual environments. At present, systems leverage existing foundation models as the basic building blocks for the creation of embodied agents. Embedding agents within such environments facilitates the ability of models to process and interpret visual and contextual data, which is critical for the creation of more sophisticated and context-aware AI systems. For example, a system that can perceive user actions, human behavior, environmental objects, audio expressions, and the collective sentiment of a scene can be used to inform and direct agent responses within the given environment. To accelerate research on agent-based multimodal intelligence, we define \"Agent AI\" as a class of interactive systems that can perceive visual stimuli, language inputs, and other environmentally-grounded data, and can produce meaningful embodied actions. In particular, we explore systems that aim to improve agents based on next-embodied action prediction by incorporating external knowledge, multi-sensory inputs, and human feedback. We argue that by developing agentic AI systems in grounded environments, one can also mitigate the hallucinations of large foundation models and their tendency to generate environmentally incorrect outputs. The emerging field of Agent AI subsumes the broader embodied and agentic aspects of multimodal interactions. Beyond agents acting and interacting in the physical world, we envision a future where people can easily create any virtual reality or simulated scene and interact with agents embodied within the virtual environment.",
    "authors": [
      "Zane Durante",
      "Qiuyuan Huang",
      "Naoki Wake",
      "Ran Gong",
      "Jae Sung Park",
      "Bidipta Sarkar",
      "Rohan Taori",
      "Yusuke Noda",
      "Demetri Terzopoulos",
      "Yejin Choi",
      "Katsushi Ikeuchi",
      "Hoi Vo",
      "Li Fei-Fei",
      "Jianfeng Gao"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-01-07T19:11:18Z",
    "pdf_url": "https://arxiv.org/pdf/2401.03568v2"
  },
  {
    "arxiv_id": "2401.03454v1",
    "entry_id": "http://arxiv.org/abs/2401.03454v1",
    "title": "Computational Argumentation-based Chatbots: a Survey",
    "summary": "Chatbots are conversational software applications designed to interact dialectically with users for a plethora of different purposes. Surprisingly, these colloquial agents have only recently been coupled with computational models of arguments (i.e. computational argumentation), whose aim is to formalise, in a machine-readable format, the ordinary exchange of information that characterises human communications. Chatbots may employ argumentation with different degrees and in a variety of manners. The present survey sifts through the literature to review papers concerning this kind of argumentation-based bot, drawing conclusions about the benefits and drawbacks that this approach entails in comparison with standard chatbots, while also envisaging possible future development and integration with the Transformer-based architecture and state-of-the-art Large Language models.",
    "authors": [
      "Federico Castagna",
      "Nadin Kokciyan",
      "Isabel Sassoon",
      "Simon Parsons",
      "Elizabeth Sklar"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-01-07T11:20:42Z",
    "pdf_url": "https://arxiv.org/pdf/2401.03454v1"
  },
  {
    "arxiv_id": "2401.03428v1",
    "entry_id": "http://arxiv.org/abs/2401.03428v1",
    "title": "Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects",
    "summary": "Intelligent agents stand out as a potential path toward artificial general intelligence (AGI). Thus, researchers have dedicated significant effort to diverse implementations for them. Benefiting from recent progress in large language models (LLMs), LLM-based agents that use universal natural language as an interface exhibit robust generalization capabilities across various applications -- from serving as autonomous general-purpose task assistants to applications in coding, social, and economic domains, LLM-based agents offer extensive exploration opportunities. This paper surveys current research to provide an in-depth overview of LLM-based intelligent agents within single-agent and multi-agent systems. It covers their definitions, research frameworks, and foundational components such as their composition, cognitive and planning methods, tool utilization, and responses to environmental feedback. We also delve into the mechanisms of deploying LLM-based agents in multi-agent systems, including multi-role collaboration, message passing, and strategies to alleviate communication issues between agents. The discussions also shed light on popular datasets and application scenarios. We conclude by envisioning prospects for LLM-based agents, considering the evolving landscape of AI and natural language processing.",
    "authors": [
      "Yuheng Cheng",
      "Ceyao Zhang",
      "Zhengwen Zhang",
      "Xiangrui Meng",
      "Sirui Hong",
      "Wenhao Li",
      "Zihao Wang",
      "Zekai Wang",
      "Feng Yin",
      "Junhua Zhao",
      "Xiuqiang He"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2024-01-07T09:08:24Z",
    "pdf_url": "https://arxiv.org/pdf/2401.03428v1"
  },
  {
    "arxiv_id": "2401.08438v2",
    "entry_id": "http://arxiv.org/abs/2401.08438v2",
    "title": "CogGPT: Unleashing the Power of Cognitive Dynamics on Large Language Models",
    "summary": "Cognitive dynamics are pivotal to advance human understanding of the world. Recent advancements in large language models (LLMs) reveal their potential for cognitive simulation. However, these LLM-based cognitive studies primarily focus on static modeling, overlooking the dynamic nature of cognition. To bridge this gap, we propose the concept of the cognitive dynamics of LLMs and present a corresponding task with the inspiration of longitudinal studies. Towards the task, we develop CogBench, a novel benchmark to assess the cognitive dynamics of LLMs and validate it through participant surveys. We also design two evaluation metrics for CogBench, including Authenticity and Rationality. Recognizing the inherent static nature of LLMs, we introduce CogGPT for the task, which features an innovative iterative cognitive mechanism aimed at enhancing lifelong cognitive dynamics. Empirical results demonstrate the superiority of CogGPT over existing methods, particularly in its ability to facilitate role-specific cognitive dynamics under continuous information flows.",
    "authors": [
      "Yaojia Lv",
      "Haojie Pan",
      "Zekun Wang",
      "Jiafeng Liang",
      "Yuanxing Liu",
      "Ruiji Fu",
      "Ming Liu",
      "Zhongyuan Wang",
      "Bing Qin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-06T03:59:59Z",
    "pdf_url": "https://arxiv.org/pdf/2401.08438v2"
  },
  {
    "arxiv_id": "2401.02843v3",
    "entry_id": "http://arxiv.org/abs/2401.02843v3",
    "title": "Thousands of AI Authors on the Future of AI",
    "summary": "In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey).\n  Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that \"substantial\" or \"extreme\" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more.",
    "authors": [
      "Katja Grace",
      "Harlan Stewart",
      "Julia Fabienne Sandkühler",
      "Stephen Thomas",
      "Ben Weinstein-Raun",
      "Jan Brauner",
      "Richard C. Korzekwa"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2024-01-05T14:53:09Z",
    "pdf_url": "https://arxiv.org/pdf/2401.02843v3"
  },
  {
    "arxiv_id": "2401.02500v2",
    "entry_id": "http://arxiv.org/abs/2401.02500v2",
    "title": "On the Prospects of Incorporating Large Language Models (LLMs) in Automated Planning and Scheduling (APS)",
    "summary": "Automated Planning and Scheduling is among the growing areas in Artificial Intelligence (AI) where mention of LLMs has gained popularity. Based on a comprehensive review of 126 papers, this paper investigates eight categories based on the unique applications of LLMs in addressing various aspects of planning problems: language translation, plan generation, model construction, multi-agent planning, interactive planning, heuristics optimization, tool integration, and brain-inspired planning. For each category, we articulate the issues considered and existing gaps. A critical insight resulting from our review is that the true potential of LLMs unfolds when they are integrated with traditional symbolic planners, pointing towards a promising neuro-symbolic approach. This approach effectively combines the generative aspects of LLMs with the precision of classical planning methods. By synthesizing insights from existing literature, we underline the potential of this integration to address complex planning challenges. Our goal is to encourage the ICAPS community to recognize the complementary strengths of LLMs and symbolic planners, advocating for a direction in automated planning that leverages these synergistic capabilities to develop more advanced and intelligent planning systems.",
    "authors": [
      "Vishal Pallagani",
      "Kaushik Roy",
      "Bharath Muppasani",
      "Francesco Fabiano",
      "Andrea Loreggia",
      "Keerthiram Murugesan",
      "Biplav Srivastava",
      "Francesca Rossi",
      "Lior Horesh",
      "Amit Sheth"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2024-01-04T19:22:09Z",
    "pdf_url": "https://arxiv.org/pdf/2401.02500v2"
  },
  {
    "arxiv_id": "2401.02349v2",
    "entry_id": "http://arxiv.org/abs/2401.02349v2",
    "title": "A Survey Analyzing Generalization in Deep Reinforcement Learning",
    "summary": "Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to large language models, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will formalize and analyze generalization in deep reinforcement learning. We will explain the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their generalization capabilities. Furthermore, we will categorize and explain the manifold solution approaches to increase generalization, and overcome overfitting in deep reinforcement learning policies. From exploration to adversarial analysis and from regularization to robustness our paper provides an analysis on a wide range of subfields within deep reinforcement learning with a broad scope and in-depth view. We believe our study can provide a compact guideline for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with higher generalization skills.",
    "authors": [
      "Ezgi Korkmaz"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2024-01-04T16:45:01Z",
    "pdf_url": "https://arxiv.org/pdf/2401.02349v2"
  },
  {
    "arxiv_id": "2401.01519v4",
    "entry_id": "http://arxiv.org/abs/2401.01519v4",
    "title": "Exploring the Frontiers of LLMs in Psychological Applications: A Comprehensive Review",
    "summary": "This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT are transforming psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologies in psychology, the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations. Researchers should responsibly use LLMs in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, the article provides a comprehensive overview of the current state of LLMs in psychology, exploring potential benefits and challenges. It serves as a call to action for researchers to leverage LLMs' advantages responsibly while addressing associated risks.",
    "authors": [
      "Luoma Ke",
      "Song Tong",
      "Peng Cheng",
      "Kaiping Peng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2024-01-03T03:01:29Z",
    "pdf_url": "https://arxiv.org/pdf/2401.01519v4"
  },
  {
    "arxiv_id": "2401.01286v5",
    "entry_id": "http://arxiv.org/abs/2401.01286v5",
    "title": "A Comprehensive Study of Knowledge Editing for Large Language Models",
    "summary": "Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the knowledge editing problem and then provide a comprehensive review of cutting-edge approaches. Drawing inspiration from educational and cognitive research theories, we propose a unified categorization criterion that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge. Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches. Additionally, we provide an in-depth analysis of knowledge location, which can give a deeper understanding of the knowledge structures inherent within LLMs. Finally, we discuss several potential applications of knowledge editing, outlining its broad and impactful implications.",
    "authors": [
      "Ningyu Zhang",
      "Yunzhi Yao",
      "Bozhong Tian",
      "Peng Wang",
      "Shumin Deng",
      "Mengru Wang",
      "Zekun Xi",
      "Shengyu Mao",
      "Jintian Zhang",
      "Yuansheng Ni",
      "Siyuan Cheng",
      "Ziwen Xu",
      "Xin Xu",
      "Jia-Chen Gu",
      "Yong Jiang",
      "Pengjun Xie",
      "Fei Huang",
      "Lei Liang",
      "Zhiqiang Zhang",
      "Xiaowei Zhu",
      "Jun Zhou",
      "Huajun Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2024-01-02T16:54:58Z",
    "pdf_url": "https://arxiv.org/pdf/2401.01286v5"
  },
  {
    "arxiv_id": "2401.01262v2",
    "entry_id": "http://arxiv.org/abs/2401.01262v2",
    "title": "Fairness Certification for Natural Language Processing and Large Language Models",
    "summary": "Natural Language Processing (NLP) plays an important role in our daily lives, particularly due to the enormous progress of Large Language Models (LLM). However, NLP has many fairness-critical use cases, e.g., as an expert system in recruitment or as an LLM-based tutor in education. Since NLP is based on human language, potentially harmful biases can diffuse into NLP systems and produce unfair results, discriminate against minorities or generate legal issues. Hence, it is important to develop a fairness certification for NLP approaches. We follow a qualitative research approach towards a fairness certification for NLP. In particular, we have reviewed a large body of literature on algorithmic fairness, and we have conducted semi-structured expert interviews with a wide range of experts from that area. We have systematically devised six fairness criteria for NLP, which can be further refined into 18 sub-categories. Our criteria offer a foundation for operationalizing and testing processes to certify fairness, both from the perspective of the auditor and the audited organization.",
    "authors": [
      "Vincent Freiberger",
      "Erik Buchmann"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2024-01-02T16:09:36Z",
    "pdf_url": "https://arxiv.org/pdf/2401.01262v2"
  },
  {
    "arxiv_id": "2402.01642v1",
    "entry_id": "http://arxiv.org/abs/2402.01642v1",
    "title": "Detection of Machine-Generated Text: Literature Survey",
    "summary": "Since language models produce fake text quickly and easily, there is an oversupply of such content in the public domain. The degree of sophistication and writing style has reached a point where differentiating between human authored and machine-generated content is nearly impossible. As a result, works generated by language models rather than human authors have gained significant media attention and stirred controversy.Concerns regarding the possible influence of advanced language models on society have also arisen, needing a fuller knowledge of these processes. Natural language generation (NLG) and generative pre-trained transformer (GPT) models have revolutionized a variety of sectors: the scope not only permeated throughout journalism and customer service but also reached academia. To mitigate the hazardous implications that may arise from the use of these models, preventative measures must be implemented, such as providing human agents with the capacity to distinguish between artificially made and human composed texts utilizing automated systems and possibly reverse-engineered language models. Furthermore, to ensure a balanced and responsible approach, it is critical to have a full grasp of the socio-technological ramifications of these breakthroughs. This literature survey aims to compile and synthesize accomplishments and developments in the aforementioned work, while also identifying future prospects. It also gives an overview of machine-generated text trends and explores the larger societal implications. Ultimately, this survey intends to contribute to the development of robust and effective approaches for resolving the issues connected with the usage and detection of machine-generated text by exploring the interplay between the capabilities of language models and their possible implications.",
    "authors": [
      "Dmytro Valiaiev"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2024-01-02T01:44:15Z",
    "pdf_url": "https://arxiv.org/pdf/2402.01642v1"
  },
  {
    "arxiv_id": "2401.02984v3",
    "entry_id": "http://arxiv.org/abs/2401.02984v3",
    "title": "Large Language Models in Mental Health Care: a Scoping Review",
    "summary": "Objectieve:This review aims to deliver a comprehensive analysis of Large Language Models (LLMs) utilization in mental health care, evaluating their effectiveness, identifying challenges, and exploring their potential for future application. Materials and Methods: A systematic search was performed across multiple databases including PubMed, Web of Science, Google Scholar, arXiv, medRxiv, and PsyArXiv in November 2023. The review includes all types of original research, regardless of peer-review status, published or disseminated between October 1, 2019, and December 2, 2023. Studies were included without language restrictions if they employed LLMs developed after T5 and directly investigated research questions within mental health care settings. Results: Out of an initial 313 articles, 34 were selected based on their relevance to LLMs applications in mental health care and the rigor of their reported outcomes. The review identified various LLMs applications in mental health care, including diagnostics, therapy, and enhancing patient engagement. Key challenges highlighted were related to data availability and reliability, the nuanced handling of mental states, and effective evaluation methods. While LLMs showed promise in improving accuracy and accessibility, significant gaps in clinical applicability and ethical considerations were noted. Conclusion: LLMs hold substantial promise for enhancing mental health care. For their full potential to be realized, emphasis must be placed on developing robust datasets, development and evaluation frameworks, ethical guidelines, and interdisciplinary collaborations to address current limitations.",
    "authors": [
      "Yining Hua",
      "Fenglin Liu",
      "Kailai Yang",
      "Zehan Li",
      "Hongbin Na",
      "Yi-han Sheu",
      "Peilin Zhou",
      "Lauren V. Moran",
      "Sophia Ananiadou",
      "David A. Clifton",
      "Andrew Beam",
      "John Torous"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-01T17:35:52Z",
    "pdf_url": "https://arxiv.org/pdf/2401.02984v3"
  },
  {
    "arxiv_id": "2401.00689v1",
    "entry_id": "http://arxiv.org/abs/2401.00689v1",
    "title": "Large language model for Bible sentiment analysis: Sermon on the Mount",
    "summary": "The revolution of natural language processing via large language models has motivated its use in multidisciplinary areas that include social sciences and humanities and more specifically, comparative religion. Sentiment analysis provides a mechanism to study the emotions expressed in text. Recently, sentiment analysis has been used to study and compare translations of the Bhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we use sentiment analysis for studying selected chapters of the Bible. These chapters are known as the Sermon on the Mount. We utilize a pre-trained language model for sentiment analysis by reviewing five translations of the Sermon on the Mount, which include the King James version, the New International Version, the New Revised Standard Version, the Lamsa Version, and the Basic English Version. We provide a chapter-by-chapter and verse-by-verse comparison using sentiment and semantic analysis and review the major sentiments expressed. Our results highlight the varying sentiments across the chapters and verses. We found that the vocabulary of the respective translations is significantly different. We detected different levels of humour, optimism, and empathy in the respective chapters that were used by Jesus to deliver his message.",
    "authors": [
      "Mahek Vora",
      "Tom Blau",
      "Vansh Kachhwal",
      "Ashu M. G. Solo",
      "Rohitash Chandra"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2024-01-01T07:35:29Z",
    "pdf_url": "https://arxiv.org/pdf/2401.00689v1"
  },
  {
    "arxiv_id": "2401.00625v4",
    "entry_id": "http://arxiv.org/abs/2401.00625v4",
    "title": "Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models",
    "summary": "The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between various resources and corresponding optimization techniques. A standardized set of evaluation metrics and datasets is also presented to facilitate consistent and fair comparisons across different models and techniques. By offering a comprehensive overview of the current sota and identifying open research avenues, this survey serves as a foundational reference for researchers and practitioners, aiding them in developing more sustainable and efficient LLMs in a rapidly evolving landscape.",
    "authors": [
      "Guangji Bai",
      "Zheng Chai",
      "Chen Ling",
      "Shiyu Wang",
      "Jiaying Lu",
      "Nan Zhang",
      "Tingwei Shi",
      "Ziyang Yu",
      "Mengdan Zhu",
      "Yifei Zhang",
      "Xinyuan Song",
      "Carl Yang",
      "Yue Cheng",
      "Liang Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2024-01-01T01:12:42Z",
    "pdf_url": "https://arxiv.org/pdf/2401.00625v4"
  }
]