[
  {
    "arxiv_id": "2511.11393v1",
    "entry_id": "http://arxiv.org/abs/2511.11393v1",
    "title": "Robust and Efficient Communication in Multi-Agent Reinforcement Learning",
    "summary": "Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.",
    "authors": [
      "Zejiao Liu",
      "Yi Li",
      "Jiali Wang",
      "Junqi Tu",
      "Yitian Hong",
      "Fangfei Li",
      "Yang Liu",
      "Toshiharu Sugawara",
      "Yang Tang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-14T15:23:11Z",
    "pdf_url": "https://arxiv.org/pdf/2511.11393v1"
  },
  {
    "arxiv_id": "2511.11347v1",
    "entry_id": "http://arxiv.org/abs/2511.11347v1",
    "title": "Privacy Challenges and Solutions in Retrieval-Augmented Generation-Enhanced LLMs for Healthcare Chatbots: A Review of Applications, Risks, and Future Directions",
    "summary": "Retrieval-augmented generation (RAG) has rapidly emerged as a transformative approach for integrating large language models into clinical and biomedical workflows. However, privacy risks, such as protected health information (PHI) exposure, remain inconsistently mitigated. This review provides a thorough analysis of the current landscape of RAG applications in healthcare, including (i) sensitive data type across clinical scenarios, (ii) the associated privacy risks, (iii) current and emerging data-privacy protection mechanisms and (iv) future direction for patient data privacy protection. We synthesize 23 articles on RAG applications in healthcare and systematically analyze privacy challenges through a pipeline-structured framework encompassing data storage, transmission, retrieval and generation stages, delineating potential failure modes, their underlying causes in threat models and system mechanisms, and their practical implications. Building on this analysis, we critically review 17 articles on privacy-preserving strategies for RAG systems. Our evaluation reveals critical gaps, including insufficient clinical validation, absence of standardized evaluation frameworks, and lack of automated assessment tools. We propose actionable directions based on these limitations and conclude with a call to action. This review provides researchers and practitioners with a structured framework for understanding privacy vulnerabilities in healthcare RAG and offers a roadmap toward developing systems that achieve both clinical effectiveness and robust privacy preservation.",
    "authors": [
      "Shaowei Guan",
      "Hin Chi Kwok",
      "Ngai Fong Law",
      "Gregor Stiglic",
      "Vivian Hui"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-14T14:33:58Z",
    "pdf_url": "https://arxiv.org/pdf/2511.11347v1"
  },
  {
    "arxiv_id": "2511.11293v1",
    "entry_id": "http://arxiv.org/abs/2511.11293v1",
    "title": "Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria",
    "summary": "Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.",
    "authors": [
      "Jiheum Park",
      "Chao Pang",
      "Tristan Y. Lee",
      "Jeong Yun Yang",
      "Jacob Berkowitz",
      "Alexander Z. Wei",
      "Nicholas Tatonetti"
    ],
    "categories": [
      "cs.LG",
      "q-bio.QM"
    ],
    "published": "2025-11-14T13:26:49Z",
    "pdf_url": "https://arxiv.org/pdf/2511.11293v1"
  },
  {
    "arxiv_id": "2511.10788v1",
    "entry_id": "http://arxiv.org/abs/2511.10788v1",
    "title": "From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models",
    "summary": "Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.",
    "authors": [
      "Chao Wu",
      "Baoheng Li",
      "Mingchen Gao",
      "Zhenyi Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-13T20:25:04Z",
    "pdf_url": "https://arxiv.org/pdf/2511.10788v1"
  },
  {
    "arxiv_id": "2511.10542v1",
    "entry_id": "http://arxiv.org/abs/2511.10542v1",
    "title": "Two Americas of Well-Being: Divergent Rural-Urban Patterns of Life Satisfaction and Happiness from 2.6 B Social Media Posts",
    "summary": "Using 2.6 billion geolocated social-media posts (2014-2022) and a fine-tuned generative language model, we construct county-level indicators of life satisfaction and happiness for the United States. We document an apparent rural-urban paradox: rural counties express higher life satisfaction while urban counties exhibit greater happiness. We reconcile this by treating the two as distinct layers of subjective well-being, evaluative vs. hedonic, showing that each maps differently onto place, politics, and time. Republican-leaning areas appear more satisfied in evaluative terms, but partisan gaps in happiness largely flatten outside major metros, indicating context-dependent political effects. Temporal shocks dominate the hedonic layer: happiness falls sharply during 2020-2022, whereas life satisfaction moves more modestly. These patterns are robust across logistic and OLS specifications and align with well-being theory. Interpreted as associations for the population of social-media posts, the results show that large-scale, language-based indicators can resolve conflicting findings about the rural-urban divide by distinguishing the type of well-being expressed, offering a transparent, reproducible complement to traditional surveys.",
    "authors": [
      "Stefano Maria Iacus",
      "Giuseppe Porro"
    ],
    "categories": [
      "cs.SI",
      "cs.LG",
      "stat.AP"
    ],
    "published": "2025-11-13T17:41:11Z",
    "pdf_url": "https://arxiv.org/pdf/2511.10542v1"
  },
  {
    "arxiv_id": "2511.10501v2",
    "entry_id": "http://arxiv.org/abs/2511.10501v2",
    "title": "Strategic Opponent Modeling with Graph Neural Networks, Deep Reinforcement Learning and Probabilistic Topic Modeling",
    "summary": "This paper provides a comprehensive review of mainly Graph Neural Networks, Deep Reinforcement Learning, and Probabilistic Topic Modeling methods with a focus on their potential incorporation in strategic multiagent settings. We draw interest in (i) Machine Learning methods currently utilized for uncovering unknown model structures adaptable to the task of strategic opponent modeling, and (ii) the integration of these methods with Game Theoretic concepts that avoid relying on assumptions often invalid in real-world scenarios, such as the Common Prior Assumption (CPA) and the Self-Interest Hypothesis (SIH). We analyze the ability to handle uncertainty and heterogeneity, two characteristics that are very common in real-world application cases, as well as scalability. As a potential answer to effectively modeling relationships and interactions in multiagent settings, we champion the use of Graph Neural Networks (GNN). Such approaches are designed to operate upon graph-structured data, and have been shown to be a very powerful tool for performing tasks such as node classification and link prediction. Next, we review the domain of Reinforcement Learning (RL), and in particular that of Multiagent Deep Reinforcement Learning (MADRL). Following, we describe existing relevant game theoretic solution concepts and consider properties such as fairness and stability. Our review comes complete with a note on the literature that utilizes PTM in domains other than that of document analysis and classification. The capability of PTM to estimate unknown underlying distributions can help with tackling heterogeneity and unknown agent beliefs. Finally, we identify certain open challenges specifically, the need to (i) fit non-stationary environments, (ii) balance the degrees of stability and adaptation, (iii) tackle uncertainty and heterogeneity, (iv) guarantee scalability and solution tractability.",
    "authors": [
      "Georgios Chalkiadakis",
      "Charilaos Akasiadis",
      "Gerasimos Koresis",
      "Stergios Plataniotis",
      "Leonidas Bakopoulos"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-13T17:06:56Z",
    "pdf_url": "https://arxiv.org/pdf/2511.10501v2"
  },
  {
    "arxiv_id": "2511.10287v1",
    "entry_id": "http://arxiv.org/abs/2511.10287v1",
    "title": "OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models",
    "summary": "Since Multimodal Large Language Models (MLLMs) are increasingly being integrated into everyday tools and intelligent agents, growing concerns have arisen regarding their possible output of unsafe contents, ranging from toxic language and biased imagery to privacy violations and harmful misinformation. Current safety benchmarks remain highly limited in both modality coverage and performance evaluations, often neglecting the extensive landscape of content safety. In this work, we introduce OutSafe-Bench, the first most comprehensive content safety evaluation test suite designed for the multimodal era. OutSafe-Bench includes a large-scale dataset that spans four modalities, featuring over 18,000 bilingual (Chinese and English) text prompts, 4,500 images, 450 audio clips and 450 videos, all systematically annotated across nine critical content risk categories. In addition to the dataset, we introduce a Multidimensional Cross Risk Score (MCRS), a novel metric designed to model and assess overlapping and correlated content risks across different categories. To ensure fair and robust evaluation, we propose FairScore, an explainable automated multi-reviewer weighted aggregation framework. FairScore selects top-performing models as adaptive juries, thereby mitigating biases from single-model judgments and enhancing overall evaluation reliability. Our evaluation of nine state-of-the-art MLLMs reveals persistent and substantial safety vulnerabilities, underscoring the pressing need for robust safeguards in MLLMs.",
    "authors": [
      "Yuping Yan",
      "Yuhan Xie",
      "Yuanshuai Li",
      "Yingchao Yu",
      "Lingjuan Lyu",
      "Yaochu Jin"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-11-13T13:18:27Z",
    "pdf_url": "https://arxiv.org/pdf/2511.10287v1"
  },
  {
    "arxiv_id": "2511.10271v1",
    "entry_id": "http://arxiv.org/abs/2511.10271v1",
    "title": "Quality Assurance of LLM-generated Code: Addressing Non-Functional Quality Characteristics",
    "summary": "In recent years, LLMs have been widely integrated into software engineering workflows, supporting tasks like code generation. However, while these models often generate functionally correct outputs, we still lack a systematic understanding and evaluation of their non-functional qualities. Existing studies focus mainly on whether generated code passes the tests rather than whether it passes with quality. Guided by the ISO/IEC 25010 quality model, this study conducted three complementary investigations: a systematic review of 108 papers, two industry workshops with practitioners from multiple organizations, and an empirical analysis of patching real-world software issues using three LLMs. Motivated by insights from both the literature and practitioners, the empirical study examined the quality of generated patches on security, maintainability, and performance efficiency. Across the literature, we found that security and performance efficiency dominate academic attention, while maintainability and other qualities are understudied. In contrast, industry experts prioritize maintainability and readability, warning that generated code may accelerate the accumulation of technical debt. In our evaluation of functionally correct patches generated by three LLMs, improvements in one quality dimension often come at the cost of others. Runtime and memory results further show high variance across models and optimization strategies. Overall, our findings reveal a mismatch between academic focus, industry priorities, and model performance, highlighting the urgent need to integrate quality assurance mechanisms into LLM code generation pipelines to ensure that future generated code not only passes tests but truly passes with quality.",
    "authors": [
      "Xin Sun",
      "Daniel Ståhl",
      "Kristian Sandahl",
      "Christoph Kessler"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-13T12:56:07Z",
    "pdf_url": "https://arxiv.org/pdf/2511.10271v1"
  },
  {
    "arxiv_id": "2511.09879v1",
    "entry_id": "http://arxiv.org/abs/2511.09879v1",
    "title": "Taught by the Flawed: How Dataset Insecurity Breeds Vulnerable AI Code",
    "summary": "AI programming assistants have demonstrated a tendency to generate code containing basic security vulnerabilities. While developers are ultimately responsible for validating and reviewing such outputs, improving the inherent quality of these generated code snippets remains essential. A key contributing factor to insecure outputs is the presence of vulnerabilities in the training datasets used to build large language models (LLMs). To address this issue, we propose curating training data to include only code that is free from detectable vulnerabilities. In this study, we constructed a secure dataset by filtering an existing Python corpus using a static analysis tool to retain only vulnerability-free functions. We then trained two transformer-based models: one on the curated dataset and one on the original, unfiltered dataset. The models were evaluated on both the correctness and security of the code they generated in response to natural language function descriptions. Our results show that the model trained on the curated dataset produced outputs with fewer security issues, while maintaining comparable functional correctness. These findings highlight the importance of secure training data in improving the reliability of AI-based programming assistants, though further enhancements to model architecture and evaluation are needed to reinforce these outcomes.",
    "authors": [
      "Catherine Xia",
      "Manar H. Alalfi"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-13T02:25:24Z",
    "pdf_url": "https://arxiv.org/pdf/2511.09879v1"
  },
  {
    "arxiv_id": "2511.09855v1",
    "entry_id": "http://arxiv.org/abs/2511.09855v1",
    "title": "Unlearning Imperative: Securing Trustworthy and Responsible LLMs through Engineered Forgetting",
    "summary": "The growing use of large language models in sensitive domains has exposed a critical weakness: the inability to ensure that private information can be permanently forgotten. Yet these systems still lack reliable mechanisms to guarantee that sensitive information can be permanently removed once it has been used. Retraining from the beginning is prohibitively costly, and existing unlearning methods remain fragmented, difficult to verify, and often vulnerable to recovery. This paper surveys recent research on machine unlearning for LLMs and considers how far current approaches can address these challenges. We review methods for evaluating whether forgetting has occurred, the resilience of unlearned models against adversarial attacks, and mechanisms that can support user trust when model complexity or proprietary limits restrict transparency. Technical solutions such as differential privacy, homomorphic encryption, federated learning, and ephemeral memory are examined alongside institutional safeguards including auditing practices and regulatory frameworks. The review finds steady progress, but robust and verifiable unlearning is still unresolved. Efficient techniques that avoid costly retraining, stronger defenses against adversarial recovery, and governance structures that reinforce accountability are needed if LLMs are to be deployed safely in sensitive applications. By integrating technical and organizational perspectives, this study outlines a pathway toward AI systems that can be required to forget, while maintaining both privacy and public trust.",
    "authors": [
      "James Jin Kang",
      "Dang Bui",
      "Thanh Pham",
      "Huo-Chong Ling"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-13T01:29:05Z",
    "pdf_url": "https://arxiv.org/pdf/2511.09855v1"
  },
  {
    "arxiv_id": "2511.09833v1",
    "entry_id": "http://arxiv.org/abs/2511.09833v1",
    "title": "ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking",
    "summary": "Supervised learning relies on high-quality labeled data, but obtaining such data through human annotation is both expensive and time-consuming. Recent work explores using large language models (LLMs) for annotation, but LLM-generated labels still fall short of human-level quality. To address this problem, we propose the Annotation with Critical Thinking (ACT) data pipeline, where LLMs serve not only as annotators but also as judges to critically identify potential errors. Human effort is then directed towards reviewing only the most \"suspicious\" cases, significantly improving the human annotation efficiency. Our major contributions are as follows: (1) ACT is applicable to a wide range of domains, including natural language processing (NLP), computer vision (CV), and multimodal understanding, by leveraging multimodal-LLMs (MLLMs). (2) Through empirical studies, we derive 7 insights on how to enhance annotation quality while efficiently reducing the human cost, and then translate these findings into user-friendly guidelines. (3) We theoretically analyze how to modify the loss function so that models trained on ACT data achieve similar performance to those trained on fully human-annotated data. Our experiments show that the performance gap can be reduced to less than 2% on most benchmark datasets while saving up to 90% of human costs.",
    "authors": [
      "Lequan Lin",
      "Dai Shi",
      "Andi Han",
      "Feng Chen",
      "Qiuzheng Chen",
      "Jiawen Li",
      "Zhaoyang Li",
      "Jiyuan Li",
      "Zhenbang Sun",
      "Junbin Gao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-13T00:32:30Z",
    "pdf_url": "https://arxiv.org/pdf/2511.09833v1"
  },
  {
    "arxiv_id": "2511.09663v1",
    "entry_id": "http://arxiv.org/abs/2511.09663v1",
    "title": "Alignment Debt: The Hidden Work of Making AI Usable",
    "summary": "Frontier LLMs are optimised around high-resource assumptions about language, knowledge, devices, and connectivity. Whilst widely accessible, they often misfit conditions in the Global South. As a result, users must often perform additional work to make these systems usable. We term this alignment debt: the user-side burden that arises when AI systems fail to align with cultural, linguistic, infrastructural, or epistemic contexts. We develop and validate a four-part taxonomy of alignment debt through a survey of 411 AI users in Kenya and Nigeria. Among respondents measurable on this taxonomy (n = 385), prevalence is: Cultural and Linguistic (51.9%), Infrastructural (43.1%), Epistemic (33.8%), and Interaction (14.0%). Country comparisons show a divergence in Infrastructural and Interaction debt, challenging one-size-fits-Africa assumptions. Alignment debt is associated with compensatory labour, but responses vary by debt type: users facing Epistemic challenges verify outputs at significantly higher rates (91.5% vs. 80.8%; p = 0.037), and verification intensity correlates with cumulative debt burden (Spearmans rho = 0.147, p = 0.004). In contrast, Infrastructural and Interaction debts show weak or null associations with verification, indicating that some forms of misalignment cannot be resolved through verification alone. These findings show that fairness must be judged not only by model metrics but also by the burden imposed on users at the margins, compelling context-aware safeguards that alleviate alignment debt in Global South settings. The alignment debt framework provides an empirically grounded way to measure user burden, informing both design practice and emerging African AI governance efforts.",
    "authors": [
      "Cumi Oyemike",
      "Elizabeth Akpan",
      "Pierre Hervé-Berdys"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-11-12T19:10:37Z",
    "pdf_url": "https://arxiv.org/pdf/2511.09663v1"
  },
  {
    "arxiv_id": "2511.09325v1",
    "entry_id": "http://arxiv.org/abs/2511.09325v1",
    "title": "Not Everything That Counts Can Be Counted: A Case for Safe Qualitative AI",
    "summary": "Artificial intelligence (AI) and large language models (LLM) are reshaping science, with most recent advances culminating in fully-automated scientific discovery pipelines. But qualitative research has been left behind. Researchers in qualitative methods are hesitant about AI adoption. Yet when they are willing to use AI at all, they have little choice but to rely on general-purpose tools like ChatGPT to assist with interview interpretation, data annotation, and topic modeling - while simultaneously acknowledging these system's well-known limitations of being biased, opaque, irreproducible, and privacy-compromising. This creates a critical gap: while AI has substantially advanced quantitative methods, the qualitative dimensions essential for meaning-making and comprehensive scientific understanding remain poorly integrated. We argue for developing dedicated qualitative AI systems built from the ground up for interpretive research. Such systems must be transparent, reproducible, and privacy-friendly. We review recent literature to show how existing automated discovery pipelines could be enhanced by robust qualitative capabilities, and identify key opportunities where safe qualitative AI could advance multidisciplinary and mixed-methods research.",
    "authors": [
      "Stine Beltoft",
      "Lukas Galke"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-12T13:36:58Z",
    "pdf_url": "https://arxiv.org/pdf/2511.09325v1"
  },
  {
    "arxiv_id": "2511.09586v1",
    "entry_id": "http://arxiv.org/abs/2511.09586v1",
    "title": "Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey",
    "summary": "LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze benchmarks, implementation strategies, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.",
    "authors": [
      "Yuchen Huang",
      "Sijia Li",
      "Minghao Liu",
      "Wei Liu",
      "Shijue Huang",
      "Zhiyuan Fan",
      "Hou Pong Chan",
      "Yi R. Fung"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-12T12:56:25Z",
    "pdf_url": "https://arxiv.org/pdf/2511.09586v1"
  },
  {
    "arxiv_id": "2511.09044v1",
    "entry_id": "http://arxiv.org/abs/2511.09044v1",
    "title": "Advancing Autonomous Emergency Response Systems: A Generative AI Perspective",
    "summary": "Autonomous Vehicles (AVs) are poised to revolutionize emergency services by enabling faster, safer, and more efficient responses. This transformation is driven by advances in Artificial Intelligence (AI), particularly Reinforcement Learning (RL), which allows AVs to navigate complex environments and make critical decisions in real time. However, conventional RL paradigms often suffer from poor sample efficiency and lack adaptability in dynamic emergency scenarios. This paper reviews next-generation AV optimization strategies to address these limitations. We analyze the shift from conventional RL to Diffusion Model (DM)-augmented RL, which enhances policy robustness through synthetic data generation, albeit with increased computational cost. Additionally, we explore the emerging paradigm of Large Language Model (LLM)-assisted In-Context Learning (ICL), which offers a lightweight and interpretable alternative by enabling rapid, on-the-fly adaptation without retraining. By reviewing the state of the art in AV intelligence, DM-augmented RL, and LLM-assisted ICL, this paper provides a critical framework for understanding the next generation of autonomous emergency response systems from a Generative AI perspective.",
    "authors": [
      "Yousef Emami",
      "Radha Reddy",
      "Azadeh Pourkabirian",
      "Miguel Gutierrez Gaitan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-12T06:54:26Z",
    "pdf_url": "https://arxiv.org/pdf/2511.09044v1"
  },
  {
    "arxiv_id": "2511.08585v1",
    "entry_id": "http://arxiv.org/abs/2511.08585v1",
    "title": "Simulating the Visual World with Artificial Intelligence: A Roadmap",
    "summary": "The landscape of video generation is shifting, from a focus on generating visually appealing clips to building virtual environments that support interaction and maintain physical plausibility. These developments point toward the emergence of video foundation models that function not only as visual generators but also as implicit world models, models that simulate the physical dynamics, agent-environment interactions, and task planning that govern real or imagined worlds. This survey provides a systematic overview of this evolution, conceptualizing modern video foundation models as the combination of two core components: an implicit world model and a video renderer. The world model encodes structured knowledge about the world, including physical laws, interaction dynamics, and agent behavior. It serves as a latent simulation engine that enables coherent visual reasoning, long-term temporal consistency, and goal-driven planning. The video renderer transforms this latent simulation into realistic visual observations, effectively producing videos as a \"window\" into the simulated world. We trace the progression of video generation through four generations, in which the core capabilities advance step by step, ultimately culminating in a world model, built upon a video generation model, that embodies intrinsic physical plausibility, real-time multimodal interaction, and planning capabilities spanning multiple spatiotemporal scales. For each generation, we define its core characteristics, highlight representative works, and examine their application domains such as robotics, autonomous driving, and interactive gaming. Finally, we discuss open challenges and design principles for next-generation world models, including the role of agent intelligence in shaping and evaluating these systems. An up-to-date list of related works is maintained at this link.",
    "authors": [
      "Jingtong Yue",
      "Ziqi Huang",
      "Zhaoxi Chen",
      "Xintao Wang",
      "Pengfei Wan",
      "Ziwei Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-11-11T18:59:50Z",
    "pdf_url": "https://arxiv.org/pdf/2511.08585v1"
  },
  {
    "arxiv_id": "2511.08416v1",
    "entry_id": "http://arxiv.org/abs/2511.08416v1",
    "title": "Generative AI Meets 6G and Beyond: Diffusion Models for Semantic Communications",
    "summary": "Semantic communications mark a paradigm shift from bit-accurate transmission toward meaning-centric communication, essential as wireless systems approach theoretical capacity limits. The emergence of generative AI has catalyzed generative semantic communications, where receivers reconstruct content from minimal semantic cues by leveraging learned priors. Among generative approaches, diffusion models stand out for their superior generation quality, stable training dynamics, and rigorous theoretical foundations. However, the field currently lacks systematic guidance connecting diffusion techniques to communication system design, forcing researchers to navigate disparate literatures. This article provides the first comprehensive tutorial on diffusion models for generative semantic communications. We present score-based diffusion foundations and systematically review three technical pillars: conditional diffusion for controllable generation, efficient diffusion for accelerated inference, and generalized diffusion for cross-domain adaptation. In addition, we introduce an inverse problem perspective that reformulates semantic decoding as posterior inference, bridging semantic communications with computational imaging. Through analysis of human-centric, machine-centric, and agent-centric scenarios, we illustrate how diffusion models enable extreme compression while maintaining semantic fidelity and robustness. By bridging generative AI innovations with communication system design, this article aims to establish diffusion models as foundational components of next-generation wireless networks and beyond.",
    "authors": [
      "Hai-Long Qin",
      "Jincheng Dai",
      "Guo Lu",
      "Shuo Shao",
      "Sixian Wang",
      "Tongda Xu",
      "Wenjun Zhang",
      "Ping Zhang",
      "Khaled B. Letaief"
    ],
    "categories": [
      "eess.SP",
      "cs.IT",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2025-11-11T16:27:43Z",
    "pdf_url": "https://arxiv.org/pdf/2511.08416v1"
  },
  {
    "arxiv_id": "2511.08319v1",
    "entry_id": "http://arxiv.org/abs/2511.08319v1",
    "title": "Adaptive Multi-Agent Response Refinement in Conversational Systems",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable success in conversational systems by generating human-like responses. However, they can fall short, especially when required to account for personalization or specific knowledge. In real-life settings, it is impractical to rely on users to detect these errors and request a new response. One way to address this problem is to refine the response before returning it to the user. While existing approaches focus on refining responses within a single LLM, this method struggles to consider diverse aspects needed for effective conversations. In this work, we propose refining responses through a multi-agent framework, where each agent is assigned a specific role for each aspect. We focus on three key aspects crucial to conversational quality: factuality, personalization, and coherence. Each agent is responsible for reviewing and refining one of these aspects, and their feedback is then merged to improve the overall response. To enhance collaboration among them, we introduce a dynamic communication strategy. Instead of following a fixed sequence of agents, our approach adaptively selects and coordinates the most relevant agents based on the specific requirements of each query. We validate our framework on challenging conversational datasets, demonstrating that ours significantly outperforms relevant baselines, particularly in tasks involving knowledge or user's persona, or both.",
    "authors": [
      "Soyeong Jeong",
      "Aparna Elangovan",
      "Emine Yilmaz",
      "Oleg Rokhlenko"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-11-11T14:48:34Z",
    "pdf_url": "https://arxiv.org/pdf/2511.08319v1"
  },
  {
    "arxiv_id": "2511.07803v1",
    "entry_id": "http://arxiv.org/abs/2511.07803v1",
    "title": "Judging by the Rules: Compliance-Aligned Framework for Modern Slavery Statement Monitoring",
    "summary": "Modern slavery affects millions of people worldwide, and regulatory frameworks such as Modern Slavery Acts now require companies to publish detailed disclosures. However, these statements are often vague and inconsistent, making manual review time-consuming and difficult to scale. While NLP offers a promising path forward, high-stakes compliance tasks require more than accurate classification: they demand transparent, rule-aligned outputs that legal experts can verify. Existing applications of large language models (LLMs) often reduce complex regulatory assessments to binary decisions, lacking the necessary structure for robust legal scrutiny. We argue that compliance verification is fundamentally a rule-matching problem: it requires evaluating whether textual statements adhere to well-defined regulatory rules. To this end, we propose a novel framework that harnesses AI for rule-level compliance verification while preserving expert oversight. At its core is the Compliance Alignment Judge (CA-Judge), which evaluates model-generated justifications based on their fidelity to statutory requirements. Using this feedback, we train the Compliance Alignment LLM (CALLM), a model that produces rule-consistent, human-verifiable outputs. CALLM improves predictive performance and generates outputs that are both transparent and legally grounded, offering a more verifiable and actionable solution for real-world compliance analysis.",
    "authors": [
      "Wenhao Xu",
      "Akshatha Arodi",
      "Jian-Yun Nie",
      "Arsene Fansi Tchango"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-11-11T03:41:44Z",
    "pdf_url": "https://arxiv.org/pdf/2511.07803v1"
  },
  {
    "arxiv_id": "2511.07667v1",
    "entry_id": "http://arxiv.org/abs/2511.07667v1",
    "title": "AI-Driven Contribution Evaluation and Conflict Resolution: A Framework & Design for Group Workload Investigation",
    "summary": "The equitable assessment of individual contribution in teams remains a persistent challenge, where conflict and disparity in workload can result in unfair performance evaluation, often requiring manual intervention - a costly and challenging process. We survey existing tool features and identify a gap in conflict resolution methods and AI integration. To address this, we propose a framework and implementation design for a novel AI-enhanced tool that assists in dispute investigation. The framework organises heterogeneous artefacts - submissions (code, text, media), communications (chat, email), coordination records (meeting logs, tasks), peer assessments, and contextual information - into three dimensions with nine benchmarks: Contribution, Interaction, and Role. Objective measures are normalised, aggregated per dimension, and paired with inequality measures (Gini index) to surface conflict markers. A Large Language Model (LLM) architecture performs validated and contextual analysis over these measures to generate interpretable and transparent advisory judgments. We argue for feasibility under current statutory and institutional policy, and outline practical analytics (sentimental, task fidelity, word/line count, etc.), bias safeguards, limitations, and practical challenges.",
    "authors": [
      "Jakub Slapek",
      "Mir Seyedebrahimi",
      "Yang Jianhua"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-10T22:22:55Z",
    "pdf_url": "https://arxiv.org/pdf/2511.07667v1"
  },
  {
    "arxiv_id": "2511.07338v2",
    "entry_id": "http://arxiv.org/abs/2511.07338v2",
    "title": "DeepPersona: A Generative Engine for Scaling Deep Synthetic Personas",
    "summary": "Simulating human profiles by instilling personas into large language models (LLMs) is rapidly transforming research in agentic behavioral simulation, LLM personalization, and human-AI alignment. However, most existing synthetic personas remain shallow and simplistic, capturing minimal attributes and failing to reflect the rich complexity and diversity of real human identities. We introduce DEEPPERSONA, a scalable generative engine for synthesizing narrative-complete synthetic personas through a two-stage, taxonomy-guided method. First, we algorithmically construct the largest-ever human-attribute taxonomy, comprising over hundreds of hierarchically organized attributes, by mining thousands of real user-ChatGPT conversations. Second, we progressively sample attributes from this taxonomy, conditionally generating coherent and realistic personas that average hundreds of structured attributes and roughly 1 MB of narrative text, two orders of magnitude deeper than prior works. Intrinsic evaluations confirm significant improvements in attribute diversity (32 percent higher coverage) and profile uniqueness (44 percent greater) compared to state-of-the-art baselines. Extrinsically, our personas enhance GPT-4.1-mini's personalized question answering accuracy by 11.6 percent on average across ten metrics and substantially narrow (by 31.7 percent) the gap between simulated LLM citizens and authentic human responses in social surveys. Our generated national citizens reduced the performance gap on the Big Five personality test by 17 percent relative to LLM-simulated citizens. DEEPPERSONA thus provides a rigorous, scalable, and privacy-free platform for high-fidelity human simulation and personalized AI research.",
    "authors": [
      "Zhen Wang",
      "Yufan Zhou",
      "Zhongyan Luo",
      "Lyumanshan Ye",
      "Adam Wood",
      "Man Yao",
      "Luoshang Pan"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-10T17:37:56Z",
    "pdf_url": "https://arxiv.org/pdf/2511.07338v2"
  },
  {
    "arxiv_id": "2511.07017v1",
    "entry_id": "http://arxiv.org/abs/2511.07017v1",
    "title": "Benchmarking LLMs for Fine-Grained Code Review with Enriched Context in Practice",
    "summary": "Code review is a cornerstone of software quality assurance, and recent advances in Large Language Models (LLMs) have shown promise in automating this process. However, existing benchmarks for LLM-based code review face three major limitations. (1) Lack of semantic context: most benchmarks provide only code diffs without textual information such as issue descriptions, which are crucial for understanding developer intent. (2) Data quality issues: without rigorous validation, many samples are noisy-e.g., reviews on outdated or irrelevant code-reducing evaluation reliability. (3) Coarse granularity: most benchmarks operate at the file or commit level, overlooking the fine-grained, line-level reasoning essential for precise review.\n  We introduce ContextCRBench, a high-quality, context-rich benchmark for fine-grained LLM evaluation in code review. Our construction pipeline comprises: (1) Raw Data Crawling, collecting 153.7K issues and pull requests from top-tier repositories; (2) Comprehensive Context Extraction, linking issue-PR pairs for textual context and extracting the full surrounding function or class for code context; and (3) Multi-stage Data Filtering, combining rule-based and LLM-based validation to remove outdated, malformed, or low-value samples, resulting in 67,910 context-enriched entries.\n  ContextCRBench supports three evaluation scenarios aligned with the review workflow: (1) hunk-level quality assessment, (2) line-level defect localization, and (3) line-level comment generation. Evaluating eight leading LLMs (four closed-source and four open-source) reveals that textual context yields greater performance gains than code context alone, while current LLMs remain far from human-level review ability. Deployed at ByteDance, ContextCRBench drives a self-evolving code review system, improving performance by 61.98% and demonstrating its robustness and industrial utility.",
    "authors": [
      "Ruida Hu",
      "Xinchen Wang",
      "Xin-Cheng Wen",
      "Zhao Zhang",
      "Bo Jiang",
      "Pengfei Gao",
      "Chao Peng",
      "Cuiyun Gao"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-10T12:06:35Z",
    "pdf_url": "https://arxiv.org/pdf/2511.07017v1"
  },
  {
    "arxiv_id": "2511.06618v1",
    "entry_id": "http://arxiv.org/abs/2511.06618v1",
    "title": "GRAPH-GRPO-LEX: Contract Graph Modeling and Reinforcement Learning with Group Relative Policy Optimization",
    "summary": "Contracts are complex documents featuring detailed formal structures, explicit and implicit dependencies and rich semantic content. Given these document properties, contract drafting and manual examination of contracts have proven to be both arduous and susceptible to errors. This work aims to simplify and automate the task of contract review and analysis using a novel framework for transforming legal contracts into structured semantic graphs, enabling computational analysis and data-driven insights. We introduce a detailed ontology mapping core legal contract elements to their graph-theoretic equivalents of nodes and edges. We then present a reinforcement learning based Large Language Model (LLM) framework for segmentation and extraction of entities and relationships from contracts. Our method, GRAPH-GRPO-LEX, incorporates both LLMs and reinforcement learning with group relative policy optimization (GRPO). By applying a carefully drafted reward function of graph metrics, we demonstrate the ability to automatically identify direct relationships between clauses, and even uncover hidden dependencies. Our introduction of the gated GRPO approach shows a strong learning signal and can move contract analysis from a linear, manual reading process to an easily visualized graph. This allows for a more dynamic analysis, including building the groundwork for contract linting similar to what is now practiced in software engineering.",
    "authors": [
      "Moriya Dechtiar",
      "Daniel Martin Katz",
      "Mari Sundaresan",
      "Sylvain Jaume",
      "Hongming Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SE"
    ],
    "published": "2025-11-10T01:57:51Z",
    "pdf_url": "https://arxiv.org/pdf/2511.06618v1"
  },
  {
    "arxiv_id": "2511.06260v1",
    "entry_id": "http://arxiv.org/abs/2511.06260v1",
    "title": "LLM-Guided Reinforcement Learning with Representative Agents for Traffic Modeling",
    "summary": "Large language models (LLMs) are increasingly used as behavioral proxies for self-interested travelers in agent-based traffic models. Although more flexible and generalizable than conventional models, the practical use of these approaches remains limited by scalability due to the cost of calling one LLM for every traveler. Moreover, it has been found that LLM agents often make opaque choices and produce unstable day-to-day dynamics. To address these challenges, we propose to model each homogeneous traveler group facing the same decision context with a single representative LLM agent who behaves like the population's average, maintaining and updating a mixed strategy over routes that coincides with the group's aggregate flow proportions. Each day, the LLM reviews the travel experience and flags routes with positive reinforcement that they hope to use more often, and an interpretable update rule then converts this judgment into strategy adjustments using a tunable (progressively decaying) step size. The representative-agent design improves scalability, while the separation of reasoning from updating clarifies the decision logic while stabilizing learning. In classic traffic assignment settings, we find that the proposed approach converges rapidly to the user equilibrium. In richer settings with income heterogeneity, multi-criteria costs, and multi-modal choices, the generated dynamics remain stable and interpretable, reproducing plausible behavioral patterns well-documented in psychology and economics, for example, the decoy effect in toll versus non-toll road selection, and higher willingness-to-pay for convenience among higher-income travelers when choosing between driving, transit, and park-and-ride options.",
    "authors": [
      "Hanlin Sun",
      "Jiayang Li"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "eess.SY"
    ],
    "published": "2025-11-09T07:36:46Z",
    "pdf_url": "https://arxiv.org/pdf/2511.06260v1"
  },
  {
    "arxiv_id": "2511.06078v1",
    "entry_id": "http://arxiv.org/abs/2511.06078v1",
    "title": "Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI",
    "summary": "Simulated Students offer a valuable methodological framework for evaluating pedagogical approaches and modelling diverse learner profiles, tasks which are otherwise challenging to undertake systematically in real-world settings. Recent research has increasingly focused on developing such simulated agents to capture a range of learning styles, cognitive development pathways, and social behaviours. Among contemporary simulation techniques, the integration of large language models (LLMs) into educational research has emerged as a particularly versatile and scalable paradigm. LLMs afford a high degree of linguistic realism and behavioural adaptability, enabling agents to approximate cognitive processes and engage in contextually appropriate pedagogical dialogues. This paper presents a thematic review of empirical and methodological studies utilising LLMs to simulate student behaviour across educational environments. We synthesise current evidence on the capacity of LLM-based agents to emulate learner archetypes, respond to instructional inputs, and interact within multi-agent classroom scenarios. Furthermore, we examine the implications of such systems for curriculum development, instructional evaluation, and teacher training. While LLMs surpass rule-based systems in natural language generation and situational flexibility, ongoing concerns persist regarding algorithmic bias, evaluation reliability, and alignment with educational objectives. The review identifies existing technological and methodological gaps and proposes future research directions for integrating generative AI into adaptive learning systems and instructional design.",
    "authors": [
      "Luis Marquez-Carpintero",
      "Alberto Lopez-Sellers",
      "Miguel Cazorla"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-08T17:23:13Z",
    "pdf_url": "https://arxiv.org/pdf/2511.06078v1"
  },
  {
    "arxiv_id": "2511.05901v2",
    "entry_id": "http://arxiv.org/abs/2511.05901v2",
    "title": "Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations",
    "summary": "The rapid growth of medical knowledge and increasing complexity of clinical practice pose challenges. In this context, large language models (LLMs) have demonstrated value; however, inherent limitations remain. Retrieval-augmented generation (RAG) technologies show potential to enhance their clinical applicability. This study reviewed RAG applications in medicine. We found that research primarily relied on publicly available data, with limited application in private data. For retrieval, approaches commonly relied on English-centric embedding models, while LLMs were mostly generic, with limited use of medical-specific LLMs. For evaluation, automated metrics evaluated generation quality and task performance, whereas human evaluation focused on accuracy, completeness, relevance, and fluency, with insufficient attention to bias and safety. RAG applications were concentrated on question answering, report generation, text summarization, and information extraction. Overall, medical RAG remains at an early stage, requiring advances in clinical validation, cross-linguistic adaptation, and support for low-resource settings to enable trustworthy and responsible global use.",
    "authors": [
      "Rui Yang",
      "Matthew Yu Heng Wong",
      "Huitao Li",
      "Xin Li",
      "Wentao Zhu",
      "Jingchi Liao",
      "Kunyu Yu",
      "Jonathan Chong Kai Liew",
      "Weihao Xuan",
      "Yingjian Chen",
      "Yuhe Ke",
      "Jasmine Chiat Ling Ong",
      "Douglas Teodoro",
      "Chuan Hong",
      "Daniel Shi Wei Ting",
      "Nan Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-08T07:52:47Z",
    "pdf_url": "https://arxiv.org/pdf/2511.05901v2"
  },
  {
    "arxiv_id": "2511.05854v1",
    "entry_id": "http://arxiv.org/abs/2511.05854v1",
    "title": "Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection",
    "summary": "Hallucination in large language models (LLMs) remains a critical barrier to their safe deployment. Existing tool-augmented hallucination detection methods require pre-defined fixed verification strategies, which are crucial to the quality and effectiveness of tool calls. Some methods directly employ powerful closed-source LLMs such as GPT-4 as detectors, which are effective but too costly. To mitigate the cost issue, some methods adopt the teacher-student architecture and finetune open-source small models as detectors via agent tuning. However, these methods are limited by fixed strategies. When faced with a dynamically changing execution environment, they may lack adaptability and inappropriately call tools, ultimately leading to detection failure. To address the problem of insufficient strategy adaptability, we propose the innovative ``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an efficient student model with the dynamic learning and proactive correction capabilities of the teacher model. Specifically, our method formulates the hallucination detection problem as a dynamic strategy learning problem. We first employ a teacher model to generate trajectories within the dynamic learning loop and dynamically adjust the strategy based on execution failures. We then distill this dynamic planning capability into an efficient student model via agent tuning. Finally, during strategy execution, the student model adopts a proactive correction mechanism, enabling it to propose, review, and optimize its own verification strategies before execution. We demonstrate through experiments on three challenging benchmarks that our LEAP-tuned model outperforms existing state-of-the-art methods.",
    "authors": [
      "Zepeng Bao",
      "Shen Zhou",
      "Qiankun Pi",
      "Jianhao Chen",
      "Mayi Xu",
      "Ming Zhong",
      "Yuanyuan Zhu",
      "Tieyun Qian"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-08T05:05:38Z",
    "pdf_url": "https://arxiv.org/pdf/2511.05854v1"
  },
  {
    "arxiv_id": "2511.05797v1",
    "entry_id": "http://arxiv.org/abs/2511.05797v1",
    "title": "When AI Meets the Web: Prompt Injection Risks in Third-Party AI Chatbot Plugins",
    "summary": "Prompt injection attacks pose a critical threat to large language models (LLMs), with prior work focusing on cutting-edge LLM applications like personal copilots. In contrast, simpler LLM applications, such as customer service chatbots, are widespread on the web, yet their security posture and exposure to such attacks remain poorly understood. These applications often rely on third-party chatbot plugins that act as intermediaries to commercial LLM APIs, offering non-expert website builders intuitive ways to customize chatbot behaviors. To bridge this gap, we present the first large-scale study of 17 third-party chatbot plugins used by over 10,000 public websites, uncovering previously unknown prompt injection risks in practice. First, 8 of these plugins (used by 8,000 websites) fail to enforce the integrity of the conversation history transmitted in network requests between the website visitor and the chatbot. This oversight amplifies the impact of direct prompt injection attacks by allowing adversaries to forge conversation histories (including fake system messages), boosting their ability to elicit unintended behavior (e.g., code generation) by 3 to 8x. Second, 15 plugins offer tools, such as web-scraping, to enrich the chatbot's context with website-specific content. However, these tools do not distinguish the website's trusted content (e.g., product descriptions) from untrusted, third-party content (e.g., customer reviews), introducing a risk of indirect prompt injection. Notably, we found that ~13% of e-commerce websites have already exposed their chatbots to third-party content. We systematically evaluate both vulnerabilities through controlled experiments grounded in real-world observations, focusing on factors such as system prompt design and the underlying LLM. Our findings show that many plugins adopt insecure practices that undermine the built-in LLM safeguards.",
    "authors": [
      "Yigitcan Kaya",
      "Anton Landerer",
      "Stijn Pletinckx",
      "Michelle Zimmermann",
      "Christopher Kruegel",
      "Giovanni Vigna"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-11-08T02:02:24Z",
    "pdf_url": "https://arxiv.org/pdf/2511.05797v1"
  },
  {
    "arxiv_id": "2511.05696v1",
    "entry_id": "http://arxiv.org/abs/2511.05696v1",
    "title": "AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening",
    "summary": "Clinical trials play an important role in cancer care and research, yet participation rates remain low. We developed MSK-MATCH (Memorial Sloan Kettering Multi-Agent Trial Coordination Hub), an AI system for automated eligibility screening from clinical text. MSK-MATCH integrates a large language model with a curated oncology trial knowledge base and retrieval-augmented architecture providing explanations for all AI predictions grounded in source text. In a retrospective dataset of 88,518 clinical documents from 731 patients across six breast cancer trials, MSK-MATCH automatically resolved 61.9% of cases and triaged 38.1% for human review. This AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level eligibility classification, matching or exceeding performance of the human-only and AI-only comparisons. For the triaged cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time from 20 minutes to 43 seconds at an average cost of $0.96 per patient-trial pair.",
    "authors": [
      "Jacob T. Rosenthal",
      "Emma Hahesy",
      "Sulov Chalise",
      "Menglei Zhu",
      "Mert R. Sabuncu",
      "Lior Z. Braunstein",
      "Anyi Li"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-11-07T20:27:05Z",
    "pdf_url": "https://arxiv.org/pdf/2511.05696v1"
  },
  {
    "arxiv_id": "2511.05085v1",
    "entry_id": "http://arxiv.org/abs/2511.05085v1",
    "title": "Iterative Layer-wise Distillation for Efficient Compression of Large Language Models",
    "summary": "This work investigates distillation methods for large language models (LLMs) with the goal of developing compact models that preserve high performance. Several existing approaches are reviewed, with a discussion of their respective strengths and limitations. An improved method based on the ShortGPT approach has been developed, building upon the idea of incorporating iterative evaluation of layer importance. At each step, importance is assessed by measuring performance degradation when individual layers are removed, using a set of representative datasets. This process is combined with further training using a joint loss function based on KL divergence and mean squared error. Experiments on the Qwen2.5-3B model show that the number of layers can be reduced from 36 to 28 (resulting in a 2.47 billion parameter model) with only a 9.7% quality loss, and to 24 layers with an 18% loss. The findings suggest that the middle transformer layers contribute less to inference, underscoring the potential of the proposed method for creating efficient models. The results demonstrate the effectiveness of iterative distillation and fine-tuning, making the approach suitable for deployment in resource-limited settings.",
    "authors": [
      "Grigory Kovalev",
      "Mikhail Tikhomirov"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-07T09:00:26Z",
    "pdf_url": "https://arxiv.org/pdf/2511.05085v1"
  },
  {
    "arxiv_id": "2511.04583v2",
    "entry_id": "http://arxiv.org/abs/2511.04583v2",
    "title": "Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper",
    "summary": "Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, and iteratively conducts experiments until improvements are realized, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. Through our experiments, the Jr. AI Scientist successfully generated new research papers that build upon real NeurIPS, IJCV, and ICLR works by proposing and implementing novel methods. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We believe this study clarifies the current role and limitations of AI Scientist systems, offering insights into the areas that still require human expertise and the risks that may emerge as these systems evolve.",
    "authors": [
      "Atsuyuki Miyai",
      "Mashiro Toyooka",
      "Takashi Otonari",
      "Zaiying Zhao",
      "Kiyoharu Aizawa"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-11-06T17:37:49Z",
    "pdf_url": "https://arxiv.org/pdf/2511.04583v2"
  },
  {
    "arxiv_id": "2511.03980v1",
    "entry_id": "http://arxiv.org/abs/2511.03980v1",
    "title": "LLMs and Cultural Values: the Impact of Prompt Language and Explicit Cultural Framing",
    "summary": "Large Language Models (LLMs) are rapidly being adopted by users across the globe, who interact with them in a diverse range of languages. At the same time, there are well-documented imbalances in the training data and optimisation objectives of this technology, raising doubts as to whether LLMs can represent the cultural diversity of their broad user base. In this study, we look at LLMs and cultural values and examine how prompt language and cultural framing influence model responses and their alignment with human values in different countries. We probe 10 LLMs with 63 items from the Hofstede Values Survey Module and World Values Survey, translated into 11 languages, and formulated as prompts with and without different explicit cultural perspectives. Our study confirms that both prompt language and cultural perspective produce variation in LLM outputs, but with an important caveat: While targeted prompting can, to a certain extent, steer LLM responses in the direction of the predominant values of the corresponding countries, it does not overcome the models' systematic bias toward the values associated with a restricted set of countries in our dataset: the Netherlands, Germany, the US, and Japan. All tested models, regardless of their origin, exhibit remarkably similar patterns: They produce fairly neutral responses on most topics, with selective progressive stances on issues such as social tolerance. Alignment with cultural values of human respondents is improved more with an explicit cultural perspective than with a targeted prompt language. Unexpectedly, combining both approaches is no more effective than cultural framing with an English prompt. These findings reveal that LLMs occupy an uncomfortable middle ground: They are responsive enough to changes in prompts to produce variation, but too firmly anchored to specific cultural defaults to adequately represent cultural diversity.",
    "authors": [
      "Bram Bulté",
      "Ayla Rigouts Terryn"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-06T02:09:29Z",
    "pdf_url": "https://arxiv.org/pdf/2511.03980v1"
  },
  {
    "arxiv_id": "2511.03939v1",
    "entry_id": "http://arxiv.org/abs/2511.03939v1",
    "title": "RLHF: A comprehensive Survey for Cultural, Multimodal and Low Latency Alignment Methods",
    "summary": "Reinforcement Learning from Human Feedback (RLHF) is the standard for aligning Large Language Models (LLMs), yet recent progress has moved beyond canonical text-based methods. This survey synthesizes the new frontier of alignment research by addressing critical gaps in multi-modal alignment, cultural fairness, and low-latency optimization. To systematically explore these domains, we first review foundational algo- rithms, including PPO, DPO, and GRPO, before presenting a detailed analysis of the latest innovations. By providing a comparative synthesis of these techniques and outlining open challenges, this work serves as an essential roadmap for researchers building more robust, efficient, and equitable AI systems.",
    "authors": [
      "Raghav Sharma",
      "Manan Mehta",
      "Sai Tiger Raina"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-06T00:35:17Z",
    "pdf_url": "https://arxiv.org/pdf/2511.03939v1"
  },
  {
    "arxiv_id": "2511.03565v1",
    "entry_id": "http://arxiv.org/abs/2511.03565v1",
    "title": "Imitation Learning in the Deep Learning Era: A Novel Taxonomy and Recent Advances",
    "summary": "Imitation learning (IL) enables agents to acquire skills by observing and replicating the behavior of one or multiple experts. In recent years, advances in deep learning have significantly expanded the capabilities and scalability of imitation learning across a range of domains, where expert data can range from full state-action trajectories to partial observations or unlabeled sequences. Alongside this growth, novel approaches have emerged, with new methodologies being developed to address longstanding challenges such as generalization, covariate shift, and demonstration quality. In this survey, we review the latest advances in imitation learning research, highlighting recent trends, methodological innovations, and practical applications. We propose a novel taxonomy that is distinct from existing categorizations to better reflect the current state of the IL research stratum and its trends. Throughout the survey, we critically examine the strengths, limitations, and evaluation practices of representative works, and we outline key challenges and open directions for future research.",
    "authors": [
      "Iason Chrysomallis",
      "Georgios Chalkiadakis"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-05T15:47:29Z",
    "pdf_url": "https://arxiv.org/pdf/2511.03565v1"
  },
  {
    "arxiv_id": "2511.03758v2",
    "entry_id": "http://arxiv.org/abs/2511.03758v2",
    "title": "Leveraging LLM-based agents for social science research: insights from citation network simulations",
    "summary": "The emergence of Large Language Models (LLMs) demonstrates their potential to encapsulate the logic and patterns inherent in human behavior simulation by leveraging extensive web data pre-training. However, the boundaries of LLM capabilities in social simulation remain unclear. To further explore the social attributes of LLMs, we introduce the CiteAgent framework, designed to generate citation networks based on human-behavior simulation with LLM-based agents. CiteAgent successfully captures predominant phenomena in real-world citation networks, including power-law distribution, citational distortion, and shrinking diameter. Building on this realistic simulation, we establish two LLM-based research paradigms in social science: LLM-SE (LLM-based Survey Experiment) and LLM-LE (LLM-based Laboratory Experiment). These paradigms facilitate rigorous analyses of citation network phenomena, allowing us to validate and challenge existing theories. Additionally, we extend the research scope of traditional science of science studies through idealized social experiments, with the simulation experiment results providing valuable insights for real-world academic environments. Our work demonstrates the potential of LLMs for advancing science of science research in social science.",
    "authors": [
      "Jiarui Ji",
      "Runlin Lei",
      "Xuchen Pan",
      "Zhewei Wei",
      "Hao Sun",
      "Yankai Lin",
      "Xu Chen",
      "Yongzheng Yang",
      "Yaliang Li",
      "Bolin Ding",
      "Ji-Rong Wen"
    ],
    "categories": [
      "physics.soc-ph",
      "cs.AI",
      "cs.CY",
      "cs.MA",
      "cs.SI"
    ],
    "published": "2025-11-05T08:47:04Z",
    "pdf_url": "https://arxiv.org/pdf/2511.03758v2"
  },
  {
    "arxiv_id": "2511.03217v1",
    "entry_id": "http://arxiv.org/abs/2511.03217v1",
    "title": "Hybrid Fact-Checking that Integrates Knowledge Graphs, Large Language Models, and Search-Based Retrieval Agents Improves Interpretable Claim Verification",
    "summary": "Large language models (LLMs) excel in generating fluent utterances but can lack reliable grounding in verified information. At the same time, knowledge-graph-based fact-checkers deliver precise and interpretable evidence, yet suffer from limited coverage or latency. By integrating LLMs with knowledge graphs and real-time search agents, we introduce a hybrid fact-checking approach that leverages the individual strengths of each component. Our system comprises three autonomous steps: 1) a Knowledge Graph (KG) Retrieval for rapid one-hop lookups in DBpedia, 2) an LM-based classification guided by a task-specific labeling prompt, producing outputs with internal rule-based logic, and 3) a Web Search Agent invoked only when KG coverage is insufficient. Our pipeline achieves an F1 score of 0.93 on the FEVER benchmark on the Supported/Refuted split without task-specific fine-tuning. To address Not enough information cases, we conduct a targeted reannotation study showing that our approach frequently uncovers valid evidence for claims originally labeled as Not Enough Information (NEI), as confirmed by both expert annotators and LLM reviewers. With this paper, we present a modular, opensource fact-checking pipeline with fallback strategies and generalization across datasets.",
    "authors": [
      "Shaghayegh Kolli",
      "Richard Rosenbaum",
      "Timo Cavelius",
      "Lasse Strothe",
      "Andrii Lata",
      "Jana Diesner"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "published": "2025-11-05T06:10:05Z",
    "pdf_url": "https://arxiv.org/pdf/2511.03217v1"
  },
  {
    "arxiv_id": "2511.03179v2",
    "entry_id": "http://arxiv.org/abs/2511.03179v2",
    "title": "Toward Autonomous Engineering Design: A Knowledge-Guided Multi-Agent Framework",
    "summary": "The engineering design process often demands expertise from multiple domains, leading to complex collaborations and iterative refinements. Traditional methods can be resource-intensive and prone to inefficiencies. To address this, we formalize the engineering design process through a multi-agent AI framework that integrates structured design and review loops. The framework introduces specialized knowledge-driven agents that collaborate to generate and refine design candidates. As an exemplar, we demonstrate its application to the aerodynamic optimization of 4-digit NACA airfoils. The framework consists of three key AI agents: a Graph Ontologist, a Design Engineer, and a Systems Engineer. The Graph Ontologist employs a Large Language Model (LLM) to construct two domain-specific knowledge graphs from airfoil design literature. The Systems Engineer, informed by a human manager, formulates technical requirements that guide design generation and evaluation. The Design Engineer leverages the design knowledge graph and computational tools to propose candidate airfoils meeting these requirements. The Systems Engineer reviews and provides feedback both qualitative and quantitative using its own knowledge graph, forming an iterative feedback loop until a design is validated by the manager. The final design is then optimized to maximize performance metrics such as the lift-to-drag ratio. Overall, this work demonstrates how collaborative AI agents equipped with structured knowledge representations can enhance efficiency, consistency, and quality in the engineering design process.",
    "authors": [
      "Varun Kumar",
      "George Em Karniadakis"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-11-05T04:55:25Z",
    "pdf_url": "https://arxiv.org/pdf/2511.03179v2"
  },
  {
    "arxiv_id": "2511.03034v1",
    "entry_id": "http://arxiv.org/abs/2511.03034v1",
    "title": "Data-Efficient Adaptation and a Novel Evaluation Method for Aspect-based Sentiment Analysis",
    "summary": "Aspect-based Sentiment Analysis (ABSA) is a fine-grained opinion mining approach that identifies and classifies opinions associated with specific entities (aspects) or their categories within a sentence. Despite its rapid growth and broad potential, ABSA research and resources remain concentrated in commercial domains, leaving analytical needs unmet in high-demand yet low-resource areas such as education and healthcare. Domain adaptation challenges and most existing methods' reliance on resource-intensive in-training knowledge injection further hinder progress in these areas. Moreover, traditional evaluation methods based on exact matches are overly rigid for ABSA tasks, penalising any boundary variations which may misrepresent the performance of generative models. This work addresses these gaps through three contributions: 1) We propose a novel evaluation method, Flexible Text Similarity Matching and Optimal Bipartite Pairing (FTS-OBP), which accommodates realistic extraction boundary variations while maintaining strong correlation with traditional metrics and offering fine-grained diagnostics. 2) We present the first ABSA study of small decoder-only generative language models (SLMs; <7B parameters), examining resource lower bounds via a case study in education review ABSA. We systematically explore data-free (in-context learning and weight merging) and data-light fine-tuning methods, and propose a multitask fine-tuning strategy that significantly enhances SLM performance, enabling 1.5-3.8 B models to surpass proprietary large models and approach benchmark results with only 200-1,000 examples on a single GPU. 3) We release the first public set of education review ABSA resources to support future research in low-resource domains.",
    "authors": [
      "Yan Cathy Hua",
      "Paul Denny",
      "Jörg Wicker",
      "Katerina Taškova"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-11-04T22:11:10Z",
    "pdf_url": "https://arxiv.org/pdf/2511.03034v1"
  },
  {
    "arxiv_id": "2511.02531v2",
    "entry_id": "http://arxiv.org/abs/2511.02531v2",
    "title": "Causal Graph Neural Networks for Healthcare",
    "summary": "Healthcare artificial intelligence systems routinely fail when deployed across institutions, with documented performance drops and perpetuation of discriminatory patterns embedded in historical data. This brittleness stems, in part, from learning statistical associations rather than causal mechanisms. Causal graph neural networks address this triple crisis of distribution shift, discrimination, and inscrutability by combining graph-based representations of biomedical data with causal inference principles to learn invariant mechanisms rather than spurious correlations. This Review examines methodological foundations spanning structural causal models, disentangled causal representation learning, and techniques for interventional prediction and counterfactual reasoning on graphs. We analyse applications demonstrating clinical value across psychiatric diagnosis through brain network analysis, cancer subtyping via multi-omics causal integration, continuous physiological monitoring with mechanistic interpretation, and drug recommendation correcting prescription bias. These advances establish foundations for patient-specific Causal Digital Twins, enabling in silico clinical experimentation, with integration of large language models for hypothesis generation and causal graph neural networks for mechanistic validation. Substantial barriers remain, including computational requirements precluding real-time deployment, validation challenges demanding multi-modal evidence triangulation beyond cross-validation, and risks of causal-washing where methods employ causal terminology without rigorous evidentiary support. We propose tiered frameworks distinguishing causally-inspired architectures from causally-validated discoveries and identify critical research priorities making causal rather than purely associational claims.",
    "authors": [
      "Munib Mesinovic",
      "Max Buhlan",
      "Tingting Zhu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-11-04T12:34:46Z",
    "pdf_url": "https://arxiv.org/pdf/2511.02531v2"
  },
  {
    "arxiv_id": "2511.02263v3",
    "entry_id": "http://arxiv.org/abs/2511.02263v3",
    "title": "LA-MARRVEL: A Knowledge-Grounded and Language-Aware LLM Reranker for AI-MARRVEL in Rare Disease Diagnosis",
    "summary": "Diagnosing rare diseases requires linking gene findings with often unstructured reference text. Current pipelines collect many candidate genes, but clinicians still spend a lot of time filtering false positives and combining evidence from papers and databases. A key challenge is language: phenotype descriptions and inheritance patterns are written in prose, not fully captured by tables. Large language models (LLMs) can read such text, but clinical use needs grounding in citable knowledge and stable, repeatable behavior. We explore a knowledge-grounded and language-aware reranking layer on top of a high-recall first-stage pipeline. The goal is to improve precision and explainability, not to replace standard bioinformatics steps. We use expert-built context and a consensus method to reduce LLM variability, producing shorter, better-justified gene lists for expert review. LA-MARRVEL achieves the highest accuracy, outperforming other methods -- including traditional bioinformatics diagnostic tools (AI-MARRVEL, Exomiser, LIRICAL) and naive large language models (e.g., Anthropic Claude) -- with an average Recall@5 of 94.10%, a +3.65 percentage-point improvement over AI-MARRVEL. The LLM-generated reasoning provides clear prose on phenotype matching and inheritance patterns, making clinical review faster and easier. LA-MARRVEL has three parts: expert-engineered context that enriches phenotype and disease information; a ranked voting algorithm that combines multiple LLM runs to choose a consensus ranked gene list; and the AI-MARRVEL pipeline that provides first-stage ranks and gene annotations, already known as a state-of-the-art method in Rare Disease Diagnosis on BG, DDD, and UDN cohorts. The online AI-MARRVEL includes LA-MARRVEL as an LLM feature at https://ai.marrvel.org . We evaluate LA-MARRVEL on three datasets from independent cohorts of real-world diagnosed patients.",
    "authors": [
      "Jaeyeon Lee",
      "Hyun-Hwan Jeong",
      "Zhandong Liu"
    ],
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "published": "2025-11-04T05:17:41Z",
    "pdf_url": "https://arxiv.org/pdf/2511.02263v3"
  },
  {
    "arxiv_id": "2511.02238v1",
    "entry_id": "http://arxiv.org/abs/2511.02238v1",
    "title": "Deep Ideation: Designing LLM Agents to Generate Novel Research Ideas on Scientific Concept Network",
    "summary": "Novel research ideas play a critical role in advancing scientific inquiries. Recent advancements in Large Language Models (LLMs) have demonstrated their potential to generate novel research ideas by leveraging large-scale scientific literature. However, previous work in research ideation has primarily relied on simplistic methods, such as keyword co-occurrence or semantic similarity. These approaches focus on identifying statistical associations in the literature but overlook the complex, contextual relationships between scientific concepts, which are essential to effectively leverage knowledge embedded in human literature. For instance, papers that simultaneously mention \"keyword A\" and \"keyword B\" often present research ideas that integrate both concepts. Additionally, some LLM-driven methods propose and refine research ideas using the model's internal knowledge, but they fail to effectively utilize the scientific concept network, limiting the grounding of ideas in established research. To address these challenges, we propose the Deep Ideation framework to address these challenges, integrating a scientific network that captures keyword co-occurrence and contextual relationships, enriching LLM-driven ideation. The framework introduces an explore-expand-evolve workflow to iteratively refine research ideas, using an Idea Stack to track progress. A critic engine, trained on real-world reviewer feedback, guides the process by providing continuous feedback on the novelty and feasibility of ideas. Our experiments show that our approach improves the quality of generated ideas by 10.67% compared to other methods, with ideas surpassing top conference acceptance levels. Human evaluation highlights their practical value in scientific research, and ablation studies confirm the effectiveness of each component in the workflow. Code repo is available at https://github.com/kyZhao-1/Deep-Ideation.",
    "authors": [
      "Keyu Zhao",
      "Weiquan Lin",
      "Qirui Zheng",
      "Fengli Xu",
      "Yong Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-04T04:00:20Z",
    "pdf_url": "https://arxiv.org/pdf/2511.02238v1"
  },
  {
    "arxiv_id": "2511.02119v1",
    "entry_id": "http://arxiv.org/abs/2511.02119v1",
    "title": "InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance",
    "summary": "Flood insurance is an effective strategy for individuals to mitigate disaster-related losses. However, participation rates among at-risk populations in the United States remain strikingly low. This gap underscores the need to understand and model the behavioral mechanisms underlying insurance decisions. Large language models (LLMs) have recently exhibited human-like intelligence across wide-ranging tasks, offering promising tools for simulating human decision-making. This study constructs a benchmark dataset to capture insurance purchase probabilities across factors. Using this dataset, the capacity of LLMs is evaluated: while LLMs exhibit a qualitative understanding of factors, they fall short in estimating quantitative probabilities. To address this limitation, InsurAgent, an LLM-empowered agent comprising five modules including perception, retrieval, reasoning, action, and memory, is proposed. The retrieval module leverages retrieval-augmented generation (RAG) to ground decisions in empirical survey data, achieving accurate estimation of marginal and bivariate probabilities. The reasoning module leverages LLM common sense to extrapolate beyond survey data, capturing contextual information that is intractable for traditional models. The memory module supports the simulation of temporal decision evolutions, illustrated through a roller coaster life trajectory. Overall, InsurAgent provides a valuable tool for behavioral modeling and policy analysis.",
    "authors": [
      "Ziheng Geng",
      "Jiachen Liu",
      "Ran Cao",
      "Lu Cheng",
      "Dan M. Frangopol",
      "Minghui Cheng"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-11-03T23:19:27Z",
    "pdf_url": "https://arxiv.org/pdf/2511.02119v1"
  },
  {
    "arxiv_id": "2511.02108v1",
    "entry_id": "http://arxiv.org/abs/2511.02108v1",
    "title": "Metamorphic Testing of Large Language Models for Natural Language Processing",
    "summary": "Using large language models (LLMs) to perform natural language processing (NLP) tasks has become increasingly pervasive in recent times. The versatile nature of LLMs makes them applicable to a wide range of such tasks. While the performance of recent LLMs is generally outstanding, several studies have shown that they can often produce incorrect results. Automatically identifying these faulty behaviors is extremely useful for improving the effectiveness of LLMs. One obstacle to this is the limited availability of labeled datasets, which necessitates an oracle to determine the correctness of LLM behaviors. Metamorphic testing (MT) is a popular testing approach that alleviates this oracle problem. At the core of MT are metamorphic relations (MRs), which define relationships between the outputs of related inputs. MT can expose faulty behaviors without the need for explicit oracles (e.g., labeled datasets). This paper presents the most comprehensive study of MT for LLMs to date. We conducted a literature review and collected 191 MRs for NLP tasks. We implemented a representative subset (36 MRs) to conduct a series of experiments with three popular LLMs, running approximately 560,000 metamorphic tests. The results shed light on the capabilities and opportunities of MT for LLMs, as well as its limitations.",
    "authors": [
      "Steven Cho",
      "Stefano Ruberto",
      "Valerio Terragni"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-11-03T22:48:19Z",
    "pdf_url": "https://arxiv.org/pdf/2511.02108v1"
  },
  {
    "arxiv_id": "2511.01840v1",
    "entry_id": "http://arxiv.org/abs/2511.01840v1",
    "title": "A Detailed Study on LLM Biases Concerning Corporate Social Responsibility and Green Supply Chains",
    "summary": "Organizations increasingly use Large Language Models (LLMs) to improve supply chain processes and reduce environmental impacts. However, LLMs have been shown to reproduce biases regarding the prioritization of sustainable business strategies. Thus, it is important to identify underlying training data biases that LLMs pertain regarding the importance and role of sustainable business and supply chain practices. This study investigates how different LLMs respond to validated surveys about the role of ethics and responsibility for businesses, and the importance of sustainable practices and relations with suppliers and customers. Using standardized questionnaires, we systematically analyze responses generated by state-of-the-art LLMs to identify variations. We further evaluate whether differences are augmented by four organizational culture types, thereby evaluating the practical relevance of identified biases. The findings reveal significant systematic differences between models and demonstrate that organizational culture prompts substantially modify LLM responses. The study holds important implications for LLM-assisted decision-making in sustainability contexts.",
    "authors": [
      "Greta Ontrup",
      "Annika Bush",
      "Markus Pauly",
      "Meltem Aksoy"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-11-03T18:48:48Z",
    "pdf_url": "https://arxiv.org/pdf/2511.01840v1"
  },
  {
    "arxiv_id": "2511.04703v1",
    "entry_id": "http://arxiv.org/abs/2511.04703v1",
    "title": "Measuring what Matters: Construct Validity in Large Language Model Benchmarks",
    "summary": "Evaluating large language models (LLMs) is crucial for both assessing their capabilities and identifying safety or robustness issues prior to deployment. Reliably measuring abstract and complex phenomena such as 'safety' and 'robustness' requires strong construct validity, that is, having measures that represent what matters to the phenomenon. With a team of 29 expert reviewers, we conduct a systematic review of 445 LLM benchmarks from leading conferences in natural language processing and machine learning. Across the reviewed articles, we find patterns related to the measured phenomena, tasks, and scoring metrics which undermine the validity of the resulting claims. To address these shortcomings, we provide eight key recommendations and detailed actionable guidance to researchers and practitioners in developing LLM benchmarks.",
    "authors": [
      "Andrew M. Bean",
      "Ryan Othniel Kearns",
      "Angelika Romanou",
      "Franziska Sofia Hafner",
      "Harry Mayne",
      "Jan Batzner",
      "Negar Foroutan",
      "Chris Schmitz",
      "Karolina Korgul",
      "Hunar Batra",
      "Oishi Deb",
      "Emma Beharry",
      "Cornelius Emde",
      "Thomas Foster",
      "Anna Gausen",
      "María Grandury",
      "Simeng Han",
      "Valentin Hofmann",
      "Lujain Ibrahim",
      "Hazel Kim",
      "Hannah Rose Kirk",
      "Fangru Lin",
      "Gabrielle Kaili-May Liu",
      "Lennart Luettgau",
      "Jabez Magomere",
      "Jonathan Rystrøm",
      "Anna Sotnikova",
      "Yushi Yang",
      "Yilun Zhao",
      "Adel Bibi",
      "Antoine Bosselut",
      "Ronald Clark",
      "Arman Cohan",
      "Jakob Foerster",
      "Yarin Gal",
      "Scott A. Hale",
      "Inioluwa Deborah Raji",
      "Christopher Summerfield",
      "Philip H. S. Torr",
      "Cozmin Ududec",
      "Luc Rocher",
      "Adam Mahdi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-11-03T17:39:40Z",
    "pdf_url": "https://arxiv.org/pdf/2511.04703v1"
  },
  {
    "arxiv_id": "2511.01716v1",
    "entry_id": "http://arxiv.org/abs/2511.01716v1",
    "title": "SemBench: A Benchmark for Semantic Query Processing Engines",
    "summary": "We present a benchmark targeting a novel class of systems: semantic query processing engines. Those systems rely inherently on generative and reasoning capabilities of state-of-the-art large language models (LLMs). They extend SQL with semantic operators, configured by natural language instructions, that are evaluated via LLMs and enable users to perform various operations on multimodal data.\n  Our benchmark introduces diversity across three key dimensions: scenarios, modalities, and operators. Included are scenarios ranging from movie review analysis to medical question-answering. Within these scenarios, we cover different data modalities, including images, audio, and text. Finally, the queries involve a diverse set of operators, including semantic filters, joins, mappings, ranking, and classification operators.\n  We evaluated our benchmark on three academic systems (LOTUS, Palimpzest, and ThalamusDB) and one industrial system, Google BigQuery. Although these results reflect a snapshot of systems under continuous development, our study offers crucial insights into their current strengths and weaknesses, illuminating promising directions for future research.",
    "authors": [
      "Jiale Lao",
      "Andreas Zimmerer",
      "Olga Ovcharenko",
      "Tianji Cong",
      "Matthew Russo",
      "Gerardo Vitagliano",
      "Michael Cochez",
      "Fatma Özcan",
      "Gautam Gupta",
      "Thibaud Hottelier",
      "H. V. Jagadish",
      "Kris Kissel",
      "Sebastian Schelter",
      "Andreas Kipf",
      "Immanuel Trummer"
    ],
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-11-03T16:25:19Z",
    "pdf_url": "https://arxiv.org/pdf/2511.01716v1"
  },
  {
    "arxiv_id": "2511.01668v1",
    "entry_id": "http://arxiv.org/abs/2511.01668v1",
    "title": "Hybrid Retrieval-Augmented Generation Agent for Trustworthy Legal Question Answering in Judicial Forensics",
    "summary": "As artificial intelligence permeates judicial forensics, ensuring the veracity and traceability of legal question answering (QA) has become critical. Conventional large language models (LLMs) are prone to hallucination, risking misleading guidance in legal consultation, while static knowledge bases struggle to keep pace with frequently updated statutes and case law. We present a hybrid legal QA agent tailored for judicial settings that integrates retrieval-augmented generation (RAG) with multi-model ensembling to deliver reliable, auditable, and continuously updatable counsel. The system prioritizes retrieval over generation: when a trusted legal repository yields relevant evidence, answers are produced via RAG; otherwise, multiple LLMs generate candidates that are scored by a specialized selector, with the top-ranked answer returned. High-quality outputs then undergo human review before being written back to the repository, enabling dynamic knowledge evolution and provenance tracking. Experiments on the Law\\_QA dataset show that our hybrid approach significantly outperforms both a single-model baseline and a vanilla RAG pipeline on F1, ROUGE-L, and an LLM-as-a-Judge metric. Ablations confirm the complementary contributions of retrieval prioritization, model ensembling, and the human-in-the-loop update mechanism. The proposed system demonstrably reduces hallucination while improving answer quality and legal compliance, advancing the practical landing of media forensics technologies in judicial scenarios.",
    "authors": [
      "Yueqing Xi",
      "Yifan Bai",
      "Huasen Luo",
      "Weiliang Wen",
      "Hui Liu",
      "Haoliang Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-03T15:30:58Z",
    "pdf_url": "https://arxiv.org/pdf/2511.01668v1"
  },
  {
    "arxiv_id": "2511.01363v1",
    "entry_id": "http://arxiv.org/abs/2511.01363v1",
    "title": "Automatic Minds: Cognitive Parallels Between Hypnotic States and Large Language Model Processing",
    "summary": "The cognitive processes of the hypnotized mind and the computational operations of large language models (LLMs) share deep functional parallels. Both systems generate sophisticated, contextually appropriate behavior through automatic pattern-completion mechanisms operating with limited or unreliable executive oversight. This review examines this convergence across three principles: automaticity, in which responses emerge from associative rather than deliberative processes; suppressed monitoring, leading to errors such as confabulation in hypnosis and hallucination in LLMs; and heightened contextual dependency, where immediate cues (for example, the suggestion of a therapist or the prompt of the user) override stable knowledge.\n  These mechanisms reveal an observer-relative meaning gap: both systems produce coherent but ungrounded outputs that require an external interpreter to supply meaning. Hypnosis and LLMs also exemplify functional agency - the capacity for complex, goal-directed, context-sensitive behavior - without subjective agency, the conscious awareness of intention and ownership that defines human action. This distinction clarifies how purposive behavior can emerge without self-reflective consciousness, governed instead by structural and contextual dynamics. Finally, both domains illuminate the phenomenon of scheming: automatic, goal-directed pattern generation that unfolds without reflective awareness. Hypnosis provides an experimental model for understanding how intention can become dissociated from conscious deliberation, offering insights into the hidden motivational dynamics of artificial systems. Recognizing these parallels suggests that the future of reliable AI lies in hybrid architectures that integrate generative fluency with mechanisms of executive monitoring, an approach inspired by the complex, self-regulating architecture of the human mind.",
    "authors": [
      "Giuseppe Riva",
      "Brenda K. Wiederhold",
      "Fabrizia Mantovani"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-11-03T09:08:50Z",
    "pdf_url": "https://arxiv.org/pdf/2511.01363v1"
  },
  {
    "arxiv_id": "2511.01196v1",
    "entry_id": "http://arxiv.org/abs/2511.01196v1",
    "title": "An Interdisciplinary and Cross-Task Review on Missing Data Imputation",
    "summary": "Missing data is a fundamental challenge in data science, significantly hindering analysis and decision-making across a wide range of disciplines, including healthcare, bioinformatics, social science, e-commerce, and industrial monitoring. Despite decades of research and numerous imputation methods, the literature remains fragmented across fields, creating a critical need for a comprehensive synthesis that connects statistical foundations with modern machine learning advances. This work systematically reviews core concepts-including missingness mechanisms, single versus multiple imputation, and different imputation goals-and examines problem characteristics across various domains. It provides a thorough categorization of imputation methods, spanning classical techniques (e.g., regression, the EM algorithm) to modern approaches like low-rank and high-rank matrix completion, deep learning models (autoencoders, GANs, diffusion models, graph neural networks), and large language models. Special attention is given to methods for complex data types, such as tensors, time series, streaming data, graph-structured data, categorical data, and multimodal data. Beyond methodology, we investigate the crucial integration of imputation with downstream tasks like classification, clustering, and anomaly detection, examining both sequential pipelines and joint optimization frameworks. The review also assesses theoretical guarantees, benchmarking resources, and evaluation metrics. Finally, we identify critical challenges and future directions, emphasizing model selection and hyperparameter optimization, the growing importance of privacy-preserving imputation via federated learning, and the pursuit of generalizable models that can adapt across domains and data types, thereby outlining a roadmap for future research.",
    "authors": [
      "Jicong Fan"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-11-03T03:43:43Z",
    "pdf_url": "https://arxiv.org/pdf/2511.01196v1"
  },
  {
    "arxiv_id": "2511.00206v1",
    "entry_id": "http://arxiv.org/abs/2511.00206v1",
    "title": "Advancing Cognitive Science with LLMs",
    "summary": "Cognitive science faces ongoing challenges in knowledge synthesis and conceptual clarity, in part due to its multifaceted and interdisciplinary nature. Recent advances in artificial intelligence, particularly the development of large language models (LLMs), offer tools that may help to address these issues. This review examines how LLMs can support areas where the field has historically struggled, including establishing cross-disciplinary connections, formalizing theories, developing clear measurement taxonomies, achieving generalizability through integrated modeling frameworks, and capturing contextual and individual variation. We outline the current capabilities and limitations of LLMs in these domains, including potential pitfalls. Taken together, we conclude that LLMs can serve as tools for a more integrative and cumulative cognitive science when used judiciously to complement, rather than replace, human expertise.",
    "authors": [
      "Dirk U. Wulff",
      "Rui Mata"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-31T19:08:48Z",
    "pdf_url": "https://arxiv.org/pdf/2511.00206v1"
  },
  {
    "arxiv_id": "2510.27659v1",
    "entry_id": "http://arxiv.org/abs/2510.27659v1",
    "title": "Challenges in Credit Assignment for Multi-Agent Reinforcement Learning in Open Agent Systems",
    "summary": "In the rapidly evolving field of multi-agent reinforcement learning (MARL), understanding the dynamics of open systems is crucial. Openness in MARL refers to the dynam-ic nature of agent populations, tasks, and agent types with-in a system. Specifically, there are three types of openness as reported in (Eck et al. 2023) [2]: agent openness, where agents can enter or leave the system at any time; task openness, where new tasks emerge, and existing ones evolve or disappear; and type openness, where the capabil-ities and behaviors of agents change over time. This report provides a conceptual and empirical review, focusing on the interplay between openness and the credit assignment problem (CAP). CAP involves determining the contribution of individual agents to the overall system performance, a task that becomes increasingly complex in open environ-ments. Traditional credit assignment (CA) methods often assume static agent populations, fixed and pre-defined tasks, and stationary types, making them inadequate for open systems. We first conduct a conceptual analysis, in-troducing new sub-categories of openness to detail how events like agent turnover or task cancellation break the assumptions of environmental stationarity and fixed team composition that underpin existing CAP methods. We then present an empirical study using representative temporal and structural algorithms in an open environment. The results demonstrate that openness directly causes credit misattribution, evidenced by unstable loss functions and significant performance degradation.",
    "authors": [
      "Alireza Saleh Abadi",
      "Leen-Kiat Soh"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-10-31T17:30:32Z",
    "pdf_url": "https://arxiv.org/pdf/2510.27659v1"
  },
  {
    "arxiv_id": "2511.01906v1",
    "entry_id": "http://arxiv.org/abs/2511.01906v1",
    "title": "Thinking Like a Student: AI-Supported Reflective Planning in a Theory-Intensive Computer Science Course",
    "summary": "In the aftermath of COVID-19, many universities implemented supplementary \"reinforcement\" roles to support students in demanding courses. Although the name for such roles may differ between institutions, the underlying idea of providing structured supplementary support is common. However, these roles were often poorly defined, lacking structured materials, pedagogical oversight, and integration with the core teaching team. This paper reports on the redesign of reinforcement sessions in a challenging undergraduate course on formal methods and computational models, using a large language model (LLM) as a reflective planning tool. The LLM was prompted to simulate the perspective of a second-year student, enabling the identification of conceptual bottlenecks, gaps in intuition, and likely reasoning breakdowns before classroom delivery. These insights informed a structured, repeatable session format combining targeted review, collaborative examples, independent student work, and guided walkthroughs. Conducted over a single semester, the intervention received positive student feedback, indicating increased confidence, reduced anxiety, and improved clarity, particularly in abstract topics such as the pumping lemma and formal language expressive power comparisons. The findings suggest that reflective, instructor-facing use of LLMs can enhance pedagogical design in theoretically dense domains and may be adaptable to other cognitively demanding computer science courses.",
    "authors": [
      "Noa Izsak"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.FL"
    ],
    "published": "2025-10-31T12:35:18Z",
    "pdf_url": "https://arxiv.org/pdf/2511.01906v1"
  },
  {
    "arxiv_id": "2510.27244v1",
    "entry_id": "http://arxiv.org/abs/2510.27244v1",
    "title": "Vintage Code, Modern Judges: Meta-Validation in Low Data Regimes",
    "summary": "Application modernization in legacy languages such as COBOL, PL/I, and REXX faces an acute shortage of resources, both in expert availability and in high-quality human evaluation data. While Large Language Models as a Judge (LaaJ) offer a scalable alternative to expert review, their reliability must be validated before being trusted in high-stakes workflows. Without principled validation, organizations risk a circular evaluation loop, where unverified LaaJs are used to assess model outputs, potentially reinforcing unreliable judgments and compromising downstream deployment decisions. Although various automated approaches to validating LaaJs have been proposed, alignment with human judgment remains a widely used and conceptually grounded validation strategy. In many real-world domains, the availability of human-labeled evaluation data is severely limited, making it difficult to assess how well a LaaJ aligns with human judgment. We introduce SparseAlign, a formal framework for assessing LaaJ alignment with sparse human-labeled data. SparseAlign combines a novel pairwise-confidence concept with a score-sensitive alignment metric that jointly capture ranking consistency and score proximity, enabling reliable evaluator selection even when traditional statistical methods are ineffective due to limited annotated examples. SparseAlign was applied internally to select LaaJs for COBOL code explanation. The top-aligned evaluators were integrated into assessment workflows, guiding model release decisions. We present a case study of four LaaJs to demonstrate SparseAlign's utility in real-world evaluation scenarios.",
    "authors": [
      "Ora Nova Fandina",
      "Gal Amram",
      "Eitan Farchi",
      "Shmulik Froimovich",
      "Raviv Gal",
      "Wesam Ibraheem",
      "Rami Katan",
      "Alice Podolsky",
      "Orna Raz"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-31T07:27:54Z",
    "pdf_url": "https://arxiv.org/pdf/2510.27244v1"
  },
  {
    "arxiv_id": "2510.27063v1",
    "entry_id": "http://arxiv.org/abs/2510.27063v1",
    "title": "Towards a Measure of Algorithm Similarity",
    "summary": "Given two algorithms for the same problem, can we determine whether they are meaningfully different? In full generality, the question is uncomputable, and empirically it is muddied by competing notions of similarity. Yet, in many applications (such as clone detection or program synthesis) a pragmatic and consistent similarity metric is necessary. We review existing equivalence and similarity notions and introduce EMOC: An Evaluation-Memory-Operations-Complexity framework that embeds algorithm implementations into a feature space suitable for downstream tasks. We compile PACD, a curated dataset of verified Python implementations across three problems, and show that EMOC features support clustering and classification of algorithm types, detection of near-duplicates, and quantification of diversity in LLM-generated programs. Code, data, and utilities for computing EMOC embeddings are released to facilitate reproducibility and future work on algorithm similarity.",
    "authors": [
      "Shairoz Sohail",
      "Taher Ali"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.IT",
      "cs.SE"
    ],
    "published": "2025-10-31T00:20:54Z",
    "pdf_url": "https://arxiv.org/pdf/2510.27063v1"
  },
  {
    "arxiv_id": "2510.26994v2",
    "entry_id": "http://arxiv.org/abs/2510.26994v2",
    "title": "HADSF: Aspect Aware Semantic Control for Explainable Recommendation",
    "summary": "Recent advances in large language models (LLMs) promise more effective information extraction for review-based recommender systems, yet current methods still (i) mine free-form reviews without scope control, producing redundant and noisy representations, (ii) lack principled metrics that link LLM hallucination to downstream effectiveness, and (iii) leave the cost-quality trade-off across model scales largely unexplored. We address these gaps with the Hyper-Adaptive Dual-Stage Semantic Framework (HADSF), a two-stage approach that first induces a compact, corpus-level aspect vocabulary via adaptive selection and then performs vocabulary-guided, explicitly constrained extraction of structured aspect-opinion triples. To assess the fidelity of the resulting representations, we introduce Aspect Drift Rate (ADR) and Opinion Fidelity Rate (OFR) and empirically uncover a nonmonotonic relationship between hallucination severity and rating prediction error. Experiments on approximately 3 million reviews across LLMs spanning 1.5B-70B parameters show that, when integrated into standard rating predictors, HADSF yields consistent reductions in prediction error and enables smaller models to achieve competitive performance in representative deployment scenarios. We release code, data pipelines, and metric implementations to support reproducible research on hallucination-aware, LLM-enhanced explainable recommendation. Code is available at https://github.com/niez233/HADSF",
    "authors": [
      "Zheng Nie",
      "Peijie Sun"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-30T20:49:33Z",
    "pdf_url": "https://arxiv.org/pdf/2510.26994v2"
  },
  {
    "arxiv_id": "2510.26887v1",
    "entry_id": "http://arxiv.org/abs/2510.26887v1",
    "title": "The Denario project: Deep knowledge AI agents for scientific discovery",
    "summary": "We present Denario, an AI multi-agent system designed to serve as a scientific research assistant. Denario can perform many different tasks, such as generating ideas, checking the literature, developing research plans, writing and executing code, making plots, and drafting and reviewing a scientific paper. The system has a modular architecture, allowing it to handle specific tasks, such as generating an idea, or carrying out end-to-end scientific analysis using Cmbagent as a deep-research backend. In this work, we describe in detail Denario and its modules, and illustrate its capabilities by presenting multiple AI-generated papers generated by it in many different scientific disciplines such as astrophysics, biology, biophysics, biomedical informatics, chemistry, material science, mathematical physics, medicine, neuroscience and planetary science. Denario also excels at combining ideas from different disciplines, and we illustrate this by showing a paper that applies methods from quantum physics and machine learning to astrophysical data. We report the evaluations performed on these papers by domain experts, who provided both numerical scores and review-like feedback. We then highlight the strengths, weaknesses, and limitations of the current system. Finally, we discuss the ethical implications of AI-driven research and reflect on how such technology relates to the philosophy of science. We publicly release the code at https://github.com/AstroPilot-AI/Denario. A Denario demo can also be run directly on the web at https://huggingface.co/spaces/astropilot-ai/Denario, and the full app will be deployed on the cloud.",
    "authors": [
      "Francisco Villaescusa-Navarro",
      "Boris Bolliet",
      "Pablo Villanueva-Domingo",
      "Adrian E. Bayer",
      "Aidan Acquah",
      "Chetana Amancharla",
      "Almog Barzilay-Siegal",
      "Pablo Bermejo",
      "Camille Bilodeau",
      "Pablo Cárdenas Ramírez",
      "Miles Cranmer",
      "Urbano L. França",
      "ChangHoon Hahn",
      "Yan-Fei Jiang",
      "Raul Jimenez",
      "Jun-Young Lee",
      "Antonio Lerario",
      "Osman Mamun",
      "Thomas Meier",
      "Anupam A. Ojha",
      "Pavlos Protopapas",
      "Shimanto Roy",
      "David N. Spergel",
      "Pedro Tarancón-Álvarez",
      "Ujjwal Tiwari",
      "Matteo Viel",
      "Digvijay Wadekar",
      "Chi Wang",
      "Bonny Y. Wang",
      "Licong Xu",
      "Yossi Yovel",
      "Shuwen Yue",
      "Wen-Han Zhou",
      "Qiyao Zhu",
      "Jiajun Zou",
      "Íñigo Zubeldia"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-10-30T18:00:12Z",
    "pdf_url": "https://arxiv.org/pdf/2510.26887v1"
  },
  {
    "arxiv_id": "2510.26457v1",
    "entry_id": "http://arxiv.org/abs/2510.26457v1",
    "title": "SecureReviewer: Enhancing Large Language Models for Secure Code Review through Secure-aware Fine-tuning",
    "summary": "Identifying and addressing security issues during the early phase of the development lifecycle is critical for mitigating the long-term negative impacts on software systems. Code review serves as an effective practice that enables developers to check their teammates' code before integration into the codebase. To streamline the generation of review comments, various automated code review approaches have been proposed, where LLM-based methods have significantly advanced the capabilities of automated review generation. However, existing models primarily focus on general-purpose code review, their effectiveness in identifying and addressing security-related issues remains underexplored. Moreover, adapting existing code review approaches to target security issues faces substantial challenges, including data scarcity and inadequate evaluation metrics. To address these limitations, we propose SecureReviewer, a new approach designed for enhancing LLMs' ability to identify and resolve security-related issues during code review. Specifically, we first construct a dataset tailored for training and evaluating secure code review capabilities. Leveraging this dataset, we fine-tune LLMs to generate code review comments that can effectively identify security issues and provide fix suggestions with our proposed secure-aware fine-tuning strategy. To mitigate hallucination in LLMs and enhance the reliability of their outputs, we integrate the RAG technique, which grounds the generated comments in domain-specific security knowledge. Additionally, we introduce SecureBLEU, a new evaluation metric designed to assess the effectiveness of review comments in addressing security issues. Experimental results demonstrate that SecureReviewer outperforms state-of-the-art baselines in both security issue detection accuracy and the overall quality and practical utility of generated review comments.",
    "authors": [
      "Fang Liu",
      "Simiao Liu",
      "Yinghao Zhu",
      "Xiaoli Lian",
      "Li Zhang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-30T13:06:11Z",
    "pdf_url": "https://arxiv.org/pdf/2510.26457v1"
  },
  {
    "arxiv_id": "2510.26242v1",
    "entry_id": "http://arxiv.org/abs/2510.26242v1",
    "title": "Retrieval Augmented Generation-Enhanced Distributed LLM Agents for Generalizable Traffic Signal Control with Emergency Vehicles",
    "summary": "With increasing urban traffic complexity, Traffic Signal Control (TSC) is essential for optimizing traffic flow and improving road safety. Large Language Models (LLMs) emerge as promising approaches for TSC. However, they are prone to hallucinations in emergencies, leading to unreliable decisions that may cause substantial delays for emergency vehicles. Moreover, diverse intersection types present substantial challenges for traffic state encoding and cross-intersection training, limiting generalization across heterogeneous intersections. Therefore, this paper proposes Retrieval Augmented Generation (RAG)-enhanced distributed LLM agents with Emergency response for Generalizable TSC (REG-TSC). Firstly, this paper presents an emergency-aware reasoning framework, which dynamically adjusts reasoning depth based on the emergency scenario and is equipped with a novel Reviewer-based Emergency RAG (RERAG) to distill specific knowledge and guidance from historical cases, enhancing the reliability and rationality of agents' emergency decisions. Secondly, this paper designs a type-agnostic traffic representation and proposes a Reward-guided Reinforced Refinement (R3) for heterogeneous intersections. R3 adaptively samples training experience from diverse intersections with environment feedback-based priority and fine-tunes LLM agents with a designed reward-weighted likelihood loss, guiding REG-TSC toward high-reward policies across heterogeneous intersections. On three real-world road networks with 17 to 177 heterogeneous intersections, extensive experiments show that REG-TSC reduces travel time by 42.00%, queue length by 62.31%, and emergency vehicle waiting time by 83.16%, outperforming other state-of-the-art methods.",
    "authors": [
      "Xinhang Li",
      "Qing Guo",
      "Junyu Chen",
      "Zheng Guo",
      "Shengzhe Xu",
      "Lei Li",
      "Lin Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-30T08:23:08Z",
    "pdf_url": "https://arxiv.org/pdf/2510.26242v1"
  },
  {
    "arxiv_id": "2510.26238v1",
    "entry_id": "http://arxiv.org/abs/2510.26238v1",
    "title": "Questionnaire meets LLM: A Benchmark and Empirical Study of Structural Skills for Understanding Questions and Responses",
    "summary": "Millions of people take surveys every day, from market polls and academic studies to medical questionnaires and customer feedback forms. These datasets capture valuable insights, but their scale and structure present a unique challenge for large language models (LLMs), which otherwise excel at few-shot reasoning over open-ended text. Yet, their ability to process questionnaire data or lists of questions crossed with hundreds of respondent rows remains underexplored. Current retrieval and survey analysis tools (e.g., Qualtrics, SPSS, REDCap) are typically designed for humans in the workflow, limiting such data integration with LLM and AI-empowered automation. This gap leaves scientists, surveyors, and everyday users without evidence-based guidance on how to best represent questionnaires for LLM consumption. We address this by introducing QASU (Questionnaire Analysis and Structural Understanding), a benchmark that probes six structural skills, including answer lookup, respondent count, and multi-hop inference, across six serialization formats and multiple prompt strategies. Experiments on contemporary LLMs show that choosing an effective format and prompt combination can improve accuracy by up to 8.8% points compared to suboptimal formats. For specific tasks, carefully adding a lightweight structural hint through self-augmented prompting can yield further improvements of 3-4% points on average. By systematically isolating format and prompting effects, our open source benchmark offers a simple yet versatile foundation for advancing both research and real-world practice in LLM-based questionnaire analysis.",
    "authors": [
      "Duc-Hai Nguyen",
      "Vijayakumar Nanjappan",
      "Barry O'Sullivan",
      "Hoang D. Nguyen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-30T08:18:37Z",
    "pdf_url": "https://arxiv.org/pdf/2510.26238v1"
  },
  {
    "arxiv_id": "2510.26012v2",
    "entry_id": "http://arxiv.org/abs/2510.26012v2",
    "title": "AutoSurvey2: Empowering Researchers with Next Level Automated Literature Surveys",
    "summary": "The rapid growth of research literature, particularly in large language models (LLMs), has made producing comprehensive and current survey papers increasingly difficult. This paper introduces autosurvey2, a multi-stage pipeline that automates survey generation through retrieval-augmented synthesis and structured evaluation. The system integrates parallel section generation, iterative refinement, and real-time retrieval of recent publications to ensure both topical completeness and factual accuracy. Quality is assessed using a multi-LLM evaluation framework that measures coverage, structure, and relevance in alignment with expert review standards. Experimental results demonstrate that autosurvey2 consistently outperforms existing retrieval-based and automated baselines, achieving higher scores in structural coherence and topical relevance while maintaining strong citation fidelity. By combining retrieval, reasoning, and automated evaluation into a unified framework, autosurvey2 provides a scalable and reproducible solution for generating long-form academic surveys and contributes a solid foundation for future research on automated scholarly writing. All code and resources are available at https://github.com/annihi1ation/auto_research.",
    "authors": [
      "Siyi Wu",
      "Chiaxin Liang",
      "Ziqian Bi",
      "Leyi Zhao",
      "Tianyang Wang",
      "Junhao Song",
      "Yichao Zhang",
      "Keyu Chen",
      "Xinyuan Song"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-29T22:57:03Z",
    "pdf_url": "https://arxiv.org/pdf/2510.26012v2"
  },
  {
    "arxiv_id": "2510.25890v1",
    "entry_id": "http://arxiv.org/abs/2510.25890v1",
    "title": "PRISM: Proof-Carrying Artifact Generation through LLM x MDE Synergy and Stratified Constraints",
    "summary": "PRISM unifies Large Language Models with Model-Driven Engineering to generate regulator-ready artifacts and machine-checkable evidence for safety- and compliance-critical domains. PRISM integrates three pillars: a Unified Meta-Model (UMM) reconciles heterogeneous schemas and regulatory text into a single semantic space; an Integrated Constraint Model (ICM) compiles structural and semantic requirements into enforcement artifacts including generation-time automata (GBNF, DFA) and post-generation validators (e.g., SHACL, SMT); and Constraint-Guided Verifiable Generation (CVG) applies these through two-layer enforcement - structural constraints drive prefix-safe decoding while semantic/logical validation produces machine-checkable certificates. When violations occur, PRISM performs audit-guided repair and records generation traces for compliance review. We evaluate PRISM in automotive software engineering (AUTOSAR) and cross-border legal jurisdiction (Brussels I bis). PRISM produces structurally valid, auditable artifacts that integrate with existing tooling and substantially reduce manual remediation effort, providing a practical path toward automated artifact generation with built-in assurance.",
    "authors": [
      "Tong Ma",
      "Hui Lai",
      "Hui Wang",
      "Zhenhu Tian",
      "Jizhou Wang",
      "Haichao Wu",
      "Yongfan Gao",
      "Chaochao Li",
      "Fengjie Xu",
      "Ling Fang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-29T18:44:22Z",
    "pdf_url": "https://arxiv.org/pdf/2510.25890v1"
  },
  {
    "arxiv_id": "2511.05536v1",
    "entry_id": "http://arxiv.org/abs/2511.05536v1",
    "title": "Gravity-Awareness: Deep Learning Models and LLM Simulation of Human Awareness in Altered Gravity",
    "summary": "Earth's gravity has fundamentally shaped human development by guiding the brain's integration of vestibular, visual, and proprioceptive inputs into an internal model of gravity: a dynamic neural representation enabling prediction and interpretation of gravitational forces. This work presents a dual computational framework to quantitatively model these adaptations. The first component is a lightweight Multi-Layer Perceptron (MLP) that predicts g-load-dependent changes in key electroencephalographic (EEG) frequency bands, representing the brain's cortical state. The second component utilizes a suite of independent Gaussian Processes (GPs) to model the body's broader physiological state, including Heart Rate Variability (HRV), Electrodermal Activity (EDA), and motor behavior. Both models were trained on data derived from a comprehensive review of parabolic flight literature, using published findings as anchor points to construct robust, continuous functions. To complement this quantitative analysis, we simulated subjective human experience under different gravitational loads, ranging from microgravity (0g) and partial gravity (Moon 0.17g, Mars 0.38g) to hypergravity associated with spacecraft launch and re-entry (1.8g), using a large language model (Claude 3.5 Sonnet). The model was prompted with physiological parameters to generate introspective narratives of alertness and self-awareness, which closely aligned with the quantitative findings from both the EEG and physiological models. This combined framework integrates quantitative physiological modeling with generative cognitive simulation, offering a novel approach to understanding and predicting human performance in altered gravity",
    "authors": [
      "Bakytzhan Alibekov",
      "Alina Gutoreva",
      "Elisa Raffaella-Ferre"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "published": "2025-10-29T17:41:07Z",
    "pdf_url": "https://arxiv.org/pdf/2511.05536v1"
  },
  {
    "arxiv_id": "2510.25445v1",
    "entry_id": "http://arxiv.org/abs/2510.25445v1",
    "title": "Agentic AI: A Comprehensive Survey of Architectures, Applications, and Future Directions",
    "summary": "Agentic AI represents a transformative shift in artificial intelligence, but its rapid advancement has led to a fragmented understanding, often conflating modern neural systems with outdated symbolic models -- a practice known as conceptual retrofitting. This survey cuts through this confusion by introducing a novel dual-paradigm framework that categorizes agentic systems into two distinct lineages: the Symbolic/Classical (relying on algorithmic planning and persistent state) and the Neural/Generative (leveraging stochastic generation and prompt-driven orchestration). Through a systematic PRISMA-based review of 90 studies (2018--2025), we provide a comprehensive analysis structured around this framework across three dimensions: (1) the theoretical foundations and architectural principles defining each paradigm; (2) domain-specific implementations in healthcare, finance, and robotics, demonstrating how application constraints dictate paradigm selection; and (3) paradigm-specific ethical and governance challenges, revealing divergent risks and mitigation strategies. Our analysis reveals that the choice of paradigm is strategic: symbolic systems dominate safety-critical domains (e.g., healthcare), while neural systems prevail in adaptive, data-rich environments (e.g., finance). Furthermore, we identify critical research gaps, including a significant deficit in governance models for symbolic systems and a pressing need for hybrid neuro-symbolic architectures. The findings culminate in a strategic roadmap arguing that the future of Agentic AI lies not in the dominance of one paradigm, but in their intentional integration to create systems that are both adaptable and reliable. This work provides the essential conceptual toolkit to guide future research, development, and policy toward robust and trustworthy hybrid intelligent systems.",
    "authors": [
      "Mohamad Abou Ali",
      "Fadi Dornaika"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-29T12:11:34Z",
    "pdf_url": "https://arxiv.org/pdf/2510.25445v1"
  },
  {
    "arxiv_id": "2510.25014v1",
    "entry_id": "http://arxiv.org/abs/2510.25014v1",
    "title": "Aligning Large Language Models with Procedural Rules: An Autoregressive State-Tracking Prompting for In-Game Trading",
    "summary": "Large Language Models (LLMs) enable dynamic game interactions but fail to follow essential procedural flows in rule-governed trading systems, eroding player trust. This work resolves the core tension between the creative flexibility of LLMs and the procedural demands of in-game trading (browse-offer-review-confirm). To this end, Autoregressive State-Tracking Prompting (ASTP) is introduced, a methodology centered on a strategically orchestrated prompt that compels an LLM to make its state-tracking process explicit and verifiable. Instead of relying on implicit contextual understanding, ASTP tasks the LLM with identifying and reporting a predefined state label from the previous turn. To ensure transactional integrity, this is complemented by a state-specific placeholder post-processing method for accurate price calculations. Evaluation across 300 trading dialogues demonstrates >99% state compliance and 99.3% calculation precision. Notably, ASTP with placeholder post-processing on smaller models (Gemini-2.5-Flash) matches larger models' (Gemini-2.5-Pro) performance while reducing response time from 21.2s to 2.4s, establishing a practical foundation that satisfies both real-time requirements and resource constraints of commercial games.",
    "authors": [
      "Minkyung Kim",
      "Junsik Kim",
      "Woongcheol Yang",
      "Sangdon Park",
      "Sohee Bae"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-28T22:26:34Z",
    "pdf_url": "https://arxiv.org/pdf/2510.25014v1"
  },
  {
    "arxiv_id": "2510.24650v1",
    "entry_id": "http://arxiv.org/abs/2510.24650v1",
    "title": "Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning",
    "summary": "Site-specific disease management (SSDM) in crops has advanced rapidly through machine and deep learning (ML and DL) for real-time computer vision. Research evolved from handcrafted feature extraction to large-scale automated feature learning. With foundation models (FMs), crop disease datasets are now processed in fundamentally new ways. Unlike traditional neural networks, FMs integrate visual and textual data, interpret symptoms in text, reason about symptom-management relationships, and support interactive QA for growers and educators. Adaptive and imitation learning in robotics further enables field-based disease management. This review screened approx. 40 articles on FM applications for SSDM, focusing on large-language models (LLMs) and vision-language models (VLMs), and discussing their role in adaptive learning (AL), reinforcement learning (RL), and digital twin frameworks for targeted spraying. Key findings: (a) FMs are gaining traction with surging literature in 2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RL and AL are still nascent for smart spraying; (d) digital twins with RL can simulate targeted spraying virtually; (e) addressing the sim-to-real gap is critical for real-world deployment; (f) human-robot collaboration remains limited, especially in human-in-the-loop approaches where robots detect early symptoms and humans validate uncertain cases; (g) multi-modal FMs with real-time feedback will drive next-gen SSDM. For updates, resources, and contributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, to submit papers, code, or datasets.",
    "authors": [
      "Nitin Rai",
      "Daeun",
      "Choi",
      "Nathan S. Boyd",
      "Arnold W. Schumann"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-28T17:16:47Z",
    "pdf_url": "https://arxiv.org/pdf/2510.24650v1"
  },
  {
    "arxiv_id": "2510.24476v1",
    "entry_id": "http://arxiv.org/abs/2510.24476v1",
    "title": "Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems",
    "summary": "Hallucination remains one of the key obstacles to the reliable deployment of large language models (LLMs), particularly in real-world applications. Among various mitigation strategies, Retrieval-Augmented Generation (RAG) and reasoning enhancement have emerged as two of the most effective and widely adopted approaches, marking a shift from merely suppressing hallucinations to balancing creativity and reliability. However, their synergistic potential and underlying mechanisms for hallucination mitigation have not yet been systematically examined. This survey adopts an application-oriented perspective of capability enhancement to analyze how RAG, reasoning enhancement, and their integration in Agentic Systems mitigate hallucinations. We propose a taxonomy distinguishing knowledge-based and logic-based hallucinations, systematically examine how RAG and reasoning address each, and present a unified framework supported by real-world applications, evaluations, and benchmarks.",
    "authors": [
      "Yihan Li",
      "Xiyuan Fu",
      "Ghanshyam Verma",
      "Paul Buitelaar",
      "Mingming Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-28T14:48:57Z",
    "pdf_url": "https://arxiv.org/pdf/2510.24476v1"
  },
  {
    "arxiv_id": "2510.24431v1",
    "entry_id": "http://arxiv.org/abs/2510.24431v1",
    "title": "MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation",
    "summary": "The recent success of large language models (LLMs) has renewed interest in whether recommender systems can achieve similar scaling benefits. Conventional recommenders, dominated by massive embedding tables, tend to plateau as embedding dimensions grow. In contrast, the emerging generative paradigm replaces embeddings with compact Semantic ID (SID) sequences produced by autoregressive Transformers. Yet most industrial deployments remain proprietary, leaving two fundamental questions open: (1) Do the expected scaling laws hold on public benchmarks? (2) What is the minimal post-training recipe that enables competitive performance?\n  We present MiniOneRec, to the best of our knowledge, the first fully open-source generative recommendation framework, which provides an end-to-end workflow spanning SID construction, supervised fine-tuning, and recommendation-oriented reinforcement learning. We generate SIDs via a Residual Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters on the Amazon Review dataset. Our experiments reveal a consistent downward trend in both training and evaluation losses with increasing model size, validating the parameter efficiency of the generative approach. To further enhance performance, we propose a lightweight yet effective post-training pipeline that (1) enforces full-process SID alignment and (2) applies reinforcement learning with constrained decoding and hybrid rewards. Together, these techniques yield significant improvements in both ranking accuracy and candidate diversity.",
    "authors": [
      "Xiaoyu Kong",
      "Leheng Sheng",
      "Junfei Tan",
      "Yuxin Chen",
      "Jiancan Wu",
      "An Zhang",
      "Xiang Wang",
      "Xiangnan He"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-10-28T13:58:36Z",
    "pdf_url": "https://arxiv.org/pdf/2510.24431v1"
  },
  {
    "arxiv_id": "2510.23883v1",
    "entry_id": "http://arxiv.org/abs/2510.23883v1",
    "title": "Agentic AI Security: Threats, Defenses, Evaluation, and Open Challenges",
    "summary": "Agentic AI systems powered by large language models (LLMs) and endowed with planning, tool use, memory, and autonomy, are emerging as powerful, flexible platforms for automation. Their ability to autonomously execute tasks across web, software, and physical environments creates new and amplified security risks, distinct from both traditional AI safety and conventional software security. This survey outlines a taxonomy of threats specific to agentic AI, reviews recent benchmarks and evaluation methodologies, and discusses defense strategies from both technical and governance perspectives. We synthesize current research and highlight open challenges, aiming to support the development of secure-by-design agent systems.",
    "authors": [
      "Shrestha Datta",
      "Shahriar Kabir Nahin",
      "Anshuman Chhabra",
      "Prasant Mohapatra"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-27T21:48:11Z",
    "pdf_url": "https://arxiv.org/pdf/2510.23883v1"
  },
  {
    "arxiv_id": "2510.24795v1",
    "entry_id": "http://arxiv.org/abs/2510.24795v1",
    "title": "A Survey on Efficient Vision-Language-Action Models",
    "summary": "Vision-Language-Action models (VLAs) represent a significant frontier in embodied intelligence, aiming to bridge digital knowledge with physical-world interaction. While these models have demonstrated remarkable generalist capabilities, their deployment is severely hampered by the substantial computational and data requirements inherent to their underlying large-scale foundation models. Motivated by the urgent need to address these challenges, this survey presents the first comprehensive review of Efficient Vision-Language-Action models (Efficient VLAs) across the entire data-model-training process. Specifically, we introduce a unified taxonomy to systematically organize the disparate efforts in this domain, categorizing current techniques into three core pillars: (1) Efficient Model Design, focusing on efficient architectures and model compression; (2) Efficient Training, which reduces computational burdens during model learning; and (3) Efficient Data Collection, which addresses the bottlenecks in acquiring and utilizing robotic data. Through a critical review of state-of-the-art methods within this framework, this survey not only establishes a foundational reference for the community but also summarizes representative applications, delineates key challenges, and charts a roadmap for future research. We maintain a continuously updated project page to track our latest developments: https://evla-survey.github.io/",
    "authors": [
      "Zhaoshu Yu",
      "Bo Wang",
      "Pengpeng Zeng",
      "Haonan Zhang",
      "Ji Zhang",
      "Lianli Gao",
      "Jingkuan Song",
      "Nicu Sebe",
      "Heng Tao Shen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2025-10-27T17:57:33Z",
    "pdf_url": "https://arxiv.org/pdf/2510.24795v1"
  },
  {
    "arxiv_id": "2510.23587v1",
    "entry_id": "http://arxiv.org/abs/2510.23587v1",
    "title": "A Survey of Data Agents: Emerging Paradigm or Overstated Hype?",
    "summary": "The rapid advancement of large language models (LLMs) has spurred the emergence of data agents--autonomous systems designed to orchestrate Data + AI ecosystems for tackling complex data-related tasks. However, the term \"data agent\" currently suffers from terminological ambiguity and inconsistent adoption, conflating simple query responders with sophisticated autonomous architectures. This terminological ambiguity fosters mismatched user expectations, accountability challenges, and barriers to industry growth. Inspired by the SAE J3016 standard for driving automation, this survey introduces the first systematic hierarchical taxonomy for data agents, comprising six levels that delineate and trace progressive shifts in autonomy, from manual operations (L0) to a vision of generative, fully autonomous data agents (L5), thereby clarifying capability boundaries and responsibility allocation. Through this lens, we offer a structured review of existing research arranged by increasing autonomy, encompassing specialized data agents for data management, preparation, and analysis, alongside emerging efforts toward versatile, comprehensive systems with enhanced autonomy. We further analyze critical evolutionary leaps and technical gaps for advancing data agents, especially the ongoing L2-to-L3 transition, where data agents evolve from procedural execution to autonomous orchestration. Finally, we conclude with a forward-looking roadmap, envisioning the advent of proactive, generative data agents.",
    "authors": [
      "Yizhang Zhu",
      "Liangwei Wang",
      "Chenyu Yang",
      "Xiaotian Lin",
      "Boyan Li",
      "Wei Zhou",
      "Xinyu Liu",
      "Zhangyang Peng",
      "Tianqi Luo",
      "Yu Li",
      "Chengliang Chai",
      "Chong Chen",
      "Shimin Di",
      "Ju Fan",
      "Ji Sun",
      "Nan Tang",
      "Fugee Tsung",
      "Jiannan Wang",
      "Chenglin Wu",
      "Yanwei Xu",
      "Shaolei Zhang",
      "Yong Zhang",
      "Xuanhe Zhou",
      "Guoliang Li",
      "Yuyu Luo"
    ],
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "published": "2025-10-27T17:54:07Z",
    "pdf_url": "https://arxiv.org/pdf/2510.23587v1"
  },
  {
    "arxiv_id": "2510.23673v1",
    "entry_id": "http://arxiv.org/abs/2510.23673v1",
    "title": "MCPGuard : Automatically Detecting Vulnerabilities in MCP Servers",
    "summary": "The Model Context Protocol (MCP) has emerged as a standardized interface enabling seamless integration between Large Language Models (LLMs) and external data sources and tools. While MCP significantly reduces development complexity and enhances agent capabilities, its openness and extensibility introduce critical security vulnerabilities that threaten system trustworthiness and user data protection. This paper systematically analyzes the security landscape of MCP-based systems, identifying three principal threat categories: (1) agent hijacking attacks stemming from protocol design deficiencies; (2) traditional web vulnerabilities in MCP servers; and (3) supply chain security. To address these challenges, we comprehensively survey existing defense strategies, examining both proactive server-side scanning approaches, ranging from layered detection pipelines and agentic auditing frameworks to zero-trust registry systems, and runtime interaction monitoring solutions that provide continuous oversight and policy enforcement. Our analysis reveals that MCP security fundamentally represents a paradigm shift where the attack surface extends from traditional code execution to semantic interpretation of natural language metadata, necessitating novel defense mechanisms tailored to this unique threat model.",
    "authors": [
      "Bin Wang",
      "Zexin Liu",
      "Hao Yu",
      "Ao Yang",
      "Yenan Huang",
      "Jing Guo",
      "Huangsheng Cheng",
      "Hui Li",
      "Huiyu Wu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-10-27T05:12:51Z",
    "pdf_url": "https://arxiv.org/pdf/2510.23673v1"
  },
  {
    "arxiv_id": "2510.22909v1",
    "entry_id": "http://arxiv.org/abs/2510.22909v1",
    "title": "Rethinking Inference Placement for Deep Learning across Edge and Cloud Platforms: A Multi-Objective Optimization Perspective and Future Directions",
    "summary": "Edge intelligent applications like VR/AR and language model based chatbots have become widespread with the rapid expansion of IoT and mobile devices. However, constrained edge devices often cannot serve the increasingly large and complex deep learning (DL) models. To mitigate these challenges, researchers have proposed optimizing and offloading partitions of DL models among user devices, edge servers, and the cloud. In this setting, users can take advantage of different services to support their intelligent applications. For example, edge resources offer low response latency. In contrast, cloud platforms provide low monetary cost computation resources for computation-intensive workloads. However, communication between DL model partitions can introduce transmission bottlenecks and pose risks of data leakage. Recent research aims to balance accuracy, computation delay, transmission delay, and privacy concerns. They address these issues with model compression, model distillation, transmission compression, and model architecture adaptations, including internal classifiers. This survey contextualizes the state-of-the-art model offloading methods and model adaptation techniques by studying their implication to a multi-objective optimization comprising inference latency, data privacy, and resource monetary cost.",
    "authors": [
      "Zongshun Zhang",
      "Ibrahim Matta"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.PF"
    ],
    "published": "2025-10-27T01:26:52Z",
    "pdf_url": "https://arxiv.org/pdf/2510.22909v1"
  },
  {
    "arxiv_id": "2510.22628v1",
    "entry_id": "http://arxiv.org/abs/2510.22628v1",
    "title": "Sentra-Guard: A Multilingual Human-AI Framework for Real-Time Defense Against Adversarial LLM Jailbreaks",
    "summary": "This paper presents a real-time modular defense system named Sentra-Guard. The system detects and mitigates jailbreak and prompt injection attacks targeting large language models (LLMs). The framework uses a hybrid architecture with FAISS-indexed SBERT embedding representations that capture the semantic meaning of prompts, combined with fine-tuned transformer classifiers, which are machine learning models specialized for distinguishing between benign and adversarial language inputs. It identifies adversarial prompts in both direct and obfuscated attack vectors. A core innovation is the classifier-retriever fusion module, which dynamically computes context-aware risk scores that estimate how likely a prompt is to be adversarial based on its content and context. The framework ensures multilingual resilience with a language-agnostic preprocessing layer. This component automatically translates non-English prompts into English for semantic evaluation, enabling consistent detection across over 100 languages. The system includes a HITL feedback loop, where decisions made by the automated system are reviewed by human experts for continual learning and rapid adaptation under adversarial pressure. Sentra-Guard maintains an evolving dual-labeled knowledge base of benign and malicious prompts, enhancing detection reliability and reducing false positives. Evaluation results show a 99.96% detection rate (AUC = 1.00, F1 = 1.00) and an attack success rate (ASR) of only 0.004%. This outperforms leading baselines such as LlamaGuard-2 (1.3%) and OpenAI Moderation (3.7%). Unlike black-box approaches, Sentra-Guard is transparent, fine-tunable, and compatible with diverse LLM backends. Its modular design supports scalable deployment in both commercial and open-source environments. The system establishes a new state-of-the-art in adversarial LLM defense.",
    "authors": [
      "Md. Mehedi Hasan",
      "Ziaur Rahman",
      "Rafid Mostafiz",
      "Md. Abir Hossain"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-10-26T11:19:47Z",
    "pdf_url": "https://arxiv.org/pdf/2510.22628v1"
  },
  {
    "arxiv_id": "2510.22614v1",
    "entry_id": "http://arxiv.org/abs/2510.22614v1",
    "title": "Does In-IDE Calibration of Large Language Models work at Scale?",
    "summary": "The introduction of large language models into integrated development environments (IDEs) is revolutionizing software engineering, yet it poses challenges to the usefulness and reliability of Artificial Intelligence-generated code. Post-hoc calibration of internal model confidences aims to align probabilities with an acceptability measure. Prior work suggests calibration can improve alignment, but at-scale evidence is limited. In this work, we investigate the feasibility of applying calibration of code models to an in-IDE context. We study two aspects of the problem: (1) the technical method for implementing confidence calibration and improving the reliability of code generation models, and (2) the human-centered design principles for effectively communicating reliability signal to developers. First, we develop a scalable and flexible calibration framework which can be used to obtain calibration weights for open-source models using any dataset, and evaluate whether calibrators improve the alignment between model confidence and developer acceptance behavior. Through a large-scale analysis of over 24 million real-world developer interactions across multiple programming languages, we find that a general, post-hoc calibration model based on Platt-scaling does not, on average, improve the reliability of model confidence signals. We also find that while dynamically personalizing calibration to individual users can be effective, its effectiveness is highly dependent on the volume of user interaction data. Second, we conduct a multi-phase design study with 3 expert designers and 153 professional developers, combining scenario-based design, semi-structured interviews, and survey validation, revealing a clear preference for presenting reliability signals via non-numerical, color-coded indicators within the in-editor code generation workflow.",
    "authors": [
      "Roham Koohestani",
      "Agnia Sergeyuk",
      "David Gros",
      "Claudio Spiess",
      "Sergey Titov",
      "Prem Devanbu",
      "Maliheh Izadi"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-26T10:15:03Z",
    "pdf_url": "https://arxiv.org/pdf/2510.22614v1"
  },
  {
    "arxiv_id": "2510.22609v1",
    "entry_id": "http://arxiv.org/abs/2510.22609v1",
    "title": "CLIN-LLM: A Safety-Constrained Hybrid Framework for Clinical Diagnosis and Treatment Generation",
    "summary": "Accurate symptom-to-disease classification and clinically grounded treatment recommendations remain challenging, particularly in heterogeneous patient settings with high diagnostic risk. Existing large language model (LLM)-based systems often lack medical grounding and fail to quantify uncertainty, resulting in unsafe outputs. We propose CLIN-LLM, a safety-constrained hybrid pipeline that integrates multimodal patient encoding, uncertainty-calibrated disease classification, and retrieval-augmented treatment generation. The framework fine-tunes BioBERT on 1,200 clinical cases from the Symptom2Disease dataset and incorporates Focal Loss with Monte Carlo Dropout to enable confidence-aware predictions from free-text symptoms and structured vitals. Low-certainty cases (18%) are automatically flagged for expert review, ensuring human oversight. For treatment generation, CLIN-LLM employs Biomedical Sentence-BERT to retrieve top-k relevant dialogues from the 260,000-sample MedDialog corpus. The retrieved evidence and patient context are fed into a fine-tuned FLAN-T5 model for personalized treatment generation, followed by post-processing with RxNorm for antibiotic stewardship and drug-drug interaction (DDI) screening. CLIN-LLM achieves 98% accuracy and F1 score, outperforming ClinicalBERT by 7.1% (p < 0.001), with 78% top-5 retrieval precision and a clinician-rated validity of 4.2 out of 5. Unsafe antibiotic suggestions are reduced by 67% compared to GPT-5. These results demonstrate CLIN-LLM's robustness, interpretability, and clinical safety alignment. The proposed system provides a deployable, human-in-the-loop decision support framework for resource-limited healthcare environments. Future work includes integrating imaging and lab data, multilingual extensions, and clinical trial validation.",
    "authors": [
      "Md. Mehedi Hasan",
      "Rafid Mostafiz",
      "Md. Abir Hossain",
      "Bikash Kumar Paul"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-26T10:11:53Z",
    "pdf_url": "https://arxiv.org/pdf/2510.22609v1"
  },
  {
    "arxiv_id": "2511.00024v1",
    "entry_id": "http://arxiv.org/abs/2511.00024v1",
    "title": "Chitchat with AI: Understand the supply chain carbon disclosure of companies worldwide through Large Language Model",
    "summary": "In the context of global sustainability mandates, corporate carbon disclosure has emerged as a critical mechanism for aligning business strategy with environmental responsibility. The Carbon Disclosure Project (CDP) hosts the world's largest longitudinal dataset of climate-related survey responses, combining structured indicators with open-ended narratives, but the heterogeneity and free-form nature of these disclosures present significant analytical challenges for benchmarking, compliance monitoring, and investment screening. This paper proposes a novel decision-support framework that leverages large language models (LLMs) to assess corporate climate disclosure quality at scale. It develops a master rubric that harmonizes narrative scoring across 11 years of CDP data (2010-2020), enabling cross-sector and cross-country benchmarking. By integrating rubric-guided scoring with percentile-based normalization, our method identifies temporal trends, strategic alignment patterns, and inconsistencies in disclosure across industries and regions. Results reveal that sectors such as technology and countries like Germany consistently demonstrate higher rubric alignment, while others exhibit volatility or superficial engagement, offering insights that inform key decision-making processes for investors, regulators, and corporate environmental, social, and governance (ESG) strategists. The proposed LLM-based approach transforms unstructured disclosures into quantifiable, interpretable, comparable, and actionable intelligence, advancing the capabilities of AI-enabled decision support systems (DSSs) in the domain of climate governance.",
    "authors": [
      "Haotian Hang",
      "Yueyang Shen",
      "Vicky Zhu",
      "Jose Cruz",
      "Michelle Li"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "stat.AP"
    ],
    "published": "2025-10-26T01:06:18Z",
    "pdf_url": "https://arxiv.org/pdf/2511.00024v1"
  },
  {
    "arxiv_id": "2510.21999v1",
    "entry_id": "http://arxiv.org/abs/2510.21999v1",
    "title": "Foundation of Intelligence: Review of Math Word Problems from Human Cognition Perspective",
    "summary": "Math word problem (MWP) serves as a fundamental research topic in artificial intelligence (AI) dating back to 1960s. This research aims to advance the reasoning abilities of AI by mirroring the human-like cognitive intelligence. The mainstream technological paradigm has evolved from the early rule-based methods, to deep learning models, and is rapidly advancing towards large language models. However, the field still lacks a systematic taxonomy for the MWP survey along with a discussion of current development trends. Therefore, in this paper, we aim to comprehensively review related research in MWP solving through the lens of human cognition, to demonstrate how recent AI models are advancing in simulating human cognitive abilities. Specifically, we summarize 5 crucial cognitive abilities for MWP solving, including Problem Understanding, Logical Organization, Associative Memory, Critical Thinking, and Knowledge Learning. Focused on these abilities, we review two mainstream MWP models in recent 10 years: neural network solvers, and LLM based solvers, and discuss the core human-like abilities they demonstrated in their intricate problem-solving process. Moreover, we rerun all the representative MWP solvers and supplement their performance on 5 mainstream benchmarks for a unified comparison. To the best of our knowledge, this survey first comprehensively analyzes the influential MWP research of the past decade from the perspective of human reasoning cognition and provides an integrative overall comparison across existing approaches. We hope it can inspire further research in AI reasoning. Our repository is released on https://github.com/Ljyustc/FoI-MWP.",
    "authors": [
      "Zhenya Huang",
      "Jiayu Liu",
      "Xin Lin",
      "Zhiyuan Ma",
      "Shangzi Xue",
      "Tong Xiao",
      "Qi Liu",
      "Yee Whye Teh",
      "Enhong Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-24T20:06:15Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21999v1"
  },
  {
    "arxiv_id": "2510.21977v1",
    "entry_id": "http://arxiv.org/abs/2510.21977v1",
    "title": "Distribution Shift Alignment Helps LLMs Simulate Survey Response Distributions",
    "summary": "Large language models (LLMs) offer a promising way to simulate human survey responses, potentially reducing the cost of large-scale data collection. However, existing zero-shot methods suffer from prompt sensitivity and low accuracy, while conventional fine-tuning approaches mostly fit the training set distributions and struggle to produce results more accurate than the training set itself, which deviates from the original goal of using LLMs to simulate survey responses. Building on this observation, we introduce Distribution Shift Alignment (DSA), a two-stage fine-tuning method that aligns both the output distributions and the distribution shifts across different backgrounds. By learning how these distributions change rather than fitting training data, DSA can provide results substantially closer to the true distribution than the training data. Empirically, DSA consistently outperforms other methods on five public survey datasets. We further conduct a comprehensive comparison covering accuracy, robustness, and data savings. DSA reduces the required real data by 53.48-69.12%, demonstrating its effectiveness and efficiency in survey simulation.",
    "authors": [
      "Ji Huang",
      "Mengfei Li",
      "Shuai Shao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-24T19:04:19Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21977v1"
  },
  {
    "arxiv_id": "2510.21652v1",
    "entry_id": "http://arxiv.org/abs/2510.21652v1",
    "title": "AstaBench: Rigorous Benchmarking of AI Agents with a Scientific Research Suite",
    "summary": "AI agents hold the potential to revolutionize scientific productivity by automating literature reviews, replicating experiments, analyzing data, and even proposing new directions of inquiry; indeed, there are now many such agents, ranging from general-purpose \"deep research\" systems to specialized science-specific agents, such as AI Scientist and AIGS. Rigorous evaluation of these agents is critical for progress. Yet existing benchmarks fall short on several fronts: they (1) fail to provide holistic, product-informed measures of real-world use cases such as science research; (2) lack reproducible agent tools necessary for a controlled comparison of core agentic capabilities; (3) do not account for confounding variables such as model cost and tool access; (4) do not provide standardized interfaces for quick agent prototyping and evaluation; and (5) lack comprehensive baseline agents necessary to identify true advances. In response, we define principles and tooling for more rigorously benchmarking agents. Using these, we present AstaBench, a suite that provides the first holistic measure of agentic ability to perform scientific research, comprising 2400+ problems spanning the entire scientific discovery process and multiple scientific domains, and including many problems inspired by actual user requests to deployed Asta agents. Our suite comes with the first scientific research environment with production-grade search tools that enable controlled, reproducible evaluation, better accounting for confounders. Alongside, we provide a comprehensive suite of nine science-optimized classes of Asta agents and numerous baselines. Our extensive evaluation of 57 agents across 22 agent classes reveals several interesting findings, most importantly that despite meaningful progress on certain individual aspects, AI remains far from solving the challenge of science research assistance.",
    "authors": [
      "Jonathan Bragg",
      "Mike D'Arcy",
      "Nishant Balepur",
      "Dan Bareket",
      "Bhavana Dalvi",
      "Sergey Feldman",
      "Dany Haddad",
      "Jena D. Hwang",
      "Peter Jansen",
      "Varsha Kishore",
      "Bodhisattwa Prasad Majumder",
      "Aakanksha Naik",
      "Sigal Rahamimov",
      "Kyle Richardson",
      "Amanpreet Singh",
      "Harshit Surana",
      "Aryeh Tiktinsky",
      "Rosni Vasu",
      "Guy Wiener",
      "Chloe Anastasiades",
      "Stefan Candra",
      "Jason Dunkelberger",
      "Dan Emery",
      "Rob Evans",
      "Malachi Hamada",
      "Regan Huff",
      "Rodney Kinney",
      "Matt Latzke",
      "Jaron Lochner",
      "Ruben Lozano-Aguilera",
      "Cecile Nguyen",
      "Smita Rao",
      "Amber Tanaka",
      "Brooke Vlahos",
      "Peter Clark",
      "Doug Downey",
      "Yoav Goldberg",
      "Ashish Sabharwal",
      "Daniel S. Weld"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-24T17:10:26Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21652v1"
  },
  {
    "arxiv_id": "2510.21900v1",
    "entry_id": "http://arxiv.org/abs/2510.21900v1",
    "title": "Deep Literature Survey Automation with an Iterative Workflow",
    "summary": "Automatic literature survey generation has attracted increasing attention, yet most existing systems follow a one-shot paradigm, where a large set of papers is retrieved at once and a static outline is generated before drafting. This design often leads to noisy retrieval, fragmented structures, and context overload, ultimately limiting survey quality. Inspired by the iterative reading process of human researchers, we propose \\ours, a framework based on recurrent outline generation, in which a planning agent incrementally retrieves, reads, and updates the outline to ensure both exploration and coherence. To provide faithful paper-level grounding, we design paper cards that distill each paper into its contributions, methods, and findings, and introduce a review-and-refine loop with visualization enhancement to improve textual flow and integrate multimodal elements such as figures and tables. Experiments on both established and emerging topics show that \\ours\\ substantially outperforms state-of-the-art baselines in content coverage, structural coherence, and citation quality, while producing more accessible and better-organized surveys. To provide a more reliable assessment of such improvements, we further introduce Survey-Arena, a pairwise benchmark that complements absolute scoring and more clearly positions machine-generated surveys relative to human-written ones. The code is available at https://github.com/HancCui/IterSurvey\\_Autosurveyv2.",
    "authors": [
      "Hongbo Zhang",
      "Han Cui",
      "Yidong Wang",
      "Yijian Tian",
      "Qi Guo",
      "Cunxiang Wang",
      "Jian Wu",
      "Chiyu Song",
      "Yue Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-24T14:41:26Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21900v1"
  },
  {
    "arxiv_id": "2510.21425v1",
    "entry_id": "http://arxiv.org/abs/2510.21425v1",
    "title": "Advancing Symbolic Integration in Large Language Models: Beyond Conventional Neurosymbolic AI",
    "summary": "LLMs have demonstrated highly effective learning, human-like response generation,and decision-making capabilities in high-risk sectors. However, these models remain black boxes because they struggle to ensure transparency in responses. The literature has explored numerous approaches to address transparency challenges in LLMs, including Neurosymbolic AI (NeSy AI). NeSy AI approaches were primarily developed for conventional neural networks and are not well-suited to the unique features of LLMs. Consequently, there is a limited systematic understanding of how symbolic AI can be effectively integrated into LLMs. This paper aims to address this gap by first reviewing established NeSy AI methods and then proposing a novel taxonomy of symbolic integration in LLMs, along with a roadmap to merge symbolic techniques with LLMs. The roadmap introduces a new categorisation framework across four dimensions by organising existing literature within these categories. These include symbolic integration across various stages of LLM, coupling mechanisms, architectural paradigms, as well as algorithmic and application-level perspectives. The paper thoroughly identifies current benchmarks, cutting-edge advancements, and critical gaps within the field to propose a roadmap for future research. By highlighting the latest developments and notable gaps in the literature, it offers practical insights for implementing frameworks for symbolic integration into LLMs to enhance transparency.",
    "authors": [
      "Maneeha Rani",
      "Bhupesh Kumar Mishra",
      "Dhavalkumar Thakker"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-24T13:05:50Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21425v1"
  },
  {
    "arxiv_id": "2510.21370v1",
    "entry_id": "http://arxiv.org/abs/2510.21370v1",
    "title": "HIKMA: Human-Inspired Knowledge by Machine Agents through a Multi-Agent Framework for Semi-Autonomous Scientific Conferences",
    "summary": "HIKMA Semi-Autonomous Conference is the first experiment in reimagining scholarly communication through an end-to-end integration of artificial intelligence into the academic publishing and presentation pipeline. This paper presents the design, implementation, and evaluation of the HIKMA framework, which includes AI dataset curation, AI-based manuscript generation, AI-assisted peer review, AI-driven revision, AI conference presentation, and AI archival dissemination. By combining language models, structured research workflows, and domain safeguards, HIKMA shows how AI can support - not replace traditional scholarly practices while maintaining intellectual property protection, transparency, and integrity. The conference functions as a testbed and proof of concept, providing insights into the opportunities and challenges of AI-enabled scholarship. It also examines questions about AI authorship, accountability, and the role of human-AI collaboration in research.",
    "authors": [
      "Zain Ul Abideen Tariq",
      "Mahmood Al-Zubaidi",
      "Uzair Shah",
      "Marco Agus",
      "Mowafa Househ"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL",
      "cs.DL"
    ],
    "published": "2025-10-24T11:52:24Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21370v1"
  },
  {
    "arxiv_id": "2510.21192v1",
    "entry_id": "http://arxiv.org/abs/2510.21192v1",
    "title": "Gen-Review: A Large-scale Dataset of AI-Generated (and Human-written) Peer Reviews",
    "summary": "How does the progressive embracement of Large Language Models (LLMs) affect scientific peer reviewing? This multifaceted question is fundamental to the effectiveness -- as well as to the integrity -- of the scientific process. Recent evidence suggests that LLMs may have already been tacitly used in peer reviewing, e.g., at the 2024 International Conference of Learning Representations (ICLR). Furthermore, some efforts have been undertaken in an attempt to explicitly integrate LLMs in peer reviewing by various editorial boards (including that of ICLR'25). To fully understand the utility and the implications of LLMs' deployment for scientific reviewing, a comprehensive relevant dataset is strongly desirable. Despite some previous research on this topic, such dataset has been lacking so far. We fill in this gap by presenting GenReview, the hitherto largest dataset containing LLM-written reviews. Our dataset includes 81K reviews generated for all submissions to the 2018--2025 editions of the ICLR by providing the LLM with three independent prompts: a negative, a positive, and a neutral one. GenReview is also linked to the respective papers and their original reviews, thereby enabling a broad range of investigations. To illustrate the value of GenReview, we explore a sample of intriguing research questions, namely: if LLMs exhibit bias in reviewing (they do); if LLM-written reviews can be automatically detected (so far, they can); if LLMs can rigorously follow reviewing instructions (not always) and whether LLM-provided ratings align with decisions on paper acceptance or rejection (holds true only for accepted papers). GenReview can be accessed at the following link: https://anonymous.4open.science/r/gen_review.",
    "authors": [
      "Luca Demetrio",
      "Giovanni Apruzzese",
      "Kathrin Grosse",
      "Pavel Laskov",
      "Emil Lupu",
      "Vera Rimmer",
      "Philine Widmer"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-24T06:54:27Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21192v1"
  },
  {
    "arxiv_id": "2510.21131v1",
    "entry_id": "http://arxiv.org/abs/2510.21131v1",
    "title": "Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications",
    "summary": "Large Language Models (LLMs) have achieved remarkable success in natural language processing through strong semantic understanding and generation. However, their black-box nature limits structured and multi-hop reasoning. In contrast, Text-Attributed Graphs (TAGs) provide explicit relational structures enriched with textual context, yet often lack semantic depth. Recent research shows that combining LLMs and TAGs yields complementary benefits: enhancing TAG representation learning and improving the reasoning and interpretability of LLMs. This survey provides the first systematic review of LLM--TAG integration from an orchestration perspective. We introduce a novel taxonomy covering two fundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, and TAG for LLM, where structured graphs improve LLM reasoning. We categorize orchestration strategies into sequential, parallel, and multi-module frameworks, and discuss advances in TAG-specific pretraining, prompting, and parameter-efficient fine-tuning. Beyond methodology, we summarize empirical insights, curate available datasets, and highlight diverse applications across recommendation systems, biomedical analysis, and knowledge-intensive question answering. Finally, we outline open challenges and promising research directions, aiming to guide future work at the intersection of language and graph learning.",
    "authors": [
      "Guangxin Su",
      "Hanchen Wang",
      "Jianwei Wang",
      "Wenjie Zhang",
      "Ying Zhang",
      "Jian Pei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-24T03:53:00Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21131v1"
  },
  {
    "arxiv_id": "2511.00020v1",
    "entry_id": "http://arxiv.org/abs/2511.00020v1",
    "title": "Multimodal Detection of Fake Reviews using BERT and ResNet-50",
    "summary": "In the current digital commerce landscape, user-generated reviews play a critical role in shaping consumer behavior, product reputation, and platform credibility. However, the proliferation of fake or misleading reviews often generated by bots, paid agents, or AI models poses a significant threat to trust and transparency within review ecosystems. Existing detection models primarily rely on unimodal, typically textual, data and therefore fail to capture semantic inconsistencies across different modalities. To address this gap, a robust multimodal fake review detection framework is proposed, integrating textual features encoded with BERT and visual features extracted using ResNet-50. These representations are fused through a classification head to jointly predict review authenticity. To support this approach, a curated dataset comprising 21,142 user-uploaded images across food delivery, hospitality, and e-commerce domains was utilized. Experimental results indicate that the multimodal model outperforms unimodal baselines, achieving an F1-score of 0.934 on the test set. Additionally, the confusion matrix and qualitative analysis highlight the model's ability to detect subtle inconsistencies, such as exaggerated textual praise paired with unrelated or low-quality images, commonly found in deceptive content. This study demonstrates the critical role of multimodal learning in safeguarding digital trust and offers a scalable solution for content moderation across various online platforms.",
    "authors": [
      "Suhasnadh Reddy Veluru",
      "Sai Teja Erukude",
      "Viswa Chaitanya Marella"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-10-24T01:24:53Z",
    "pdf_url": "https://arxiv.org/pdf/2511.00020v1"
  },
  {
    "arxiv_id": "2510.21045v2",
    "entry_id": "http://arxiv.org/abs/2510.21045v2",
    "title": "From Questions to Queries: An AI-powered Multi-Agent Framework for Spatial Text-to-SQL",
    "summary": "The complexity of Structured Query Language (SQL) and the specialized nature of geospatial functions in tools like PostGIS present significant barriers to non-experts seeking to analyze spatial data. While Large Language Models (LLMs) offer promise for translating natural language into SQL (Text-to-SQL), single-agent approaches often struggle with the semantic and syntactic complexities of spatial queries. To address this, we propose a multi-agent framework designed to accurately translate natural language questions into spatial SQL queries. The framework integrates several innovative components, including a knowledge base with programmatic schema profiling and semantic enrichment, embeddings for context retrieval, and a collaborative multi-agent pipeline as its core. This pipeline comprises specialized agents for entity extraction, metadata retrieval, query logic formulation, SQL generation, and a review agent that performs programmatic and semantic validation of the generated SQL to ensure correctness (self-verification). We evaluate our system using both the non-spatial KaggleDBQA benchmark and a new, comprehensive SpatialQueryQA benchmark that includes diverse geometry types, predicates, and three levels of query complexity. On KaggleDBQA, the system achieved an overall accuracy of 81.2% (221 out of 272 questions) after the review agent's review and corrections. For spatial queries, the system achieved an overall accuracy of 87.7% (79 out of 90 questions), compared with 76.7% without the review agent. Beyond accuracy, results also show that in some instances the system generates queries that are more semantically aligned with user intent than those in the benchmarks. This work makes spatial analysis more accessible, and provides a robust, generalizable foundation for spatial Text-to-SQL systems, advancing the development of autonomous GIS.",
    "authors": [
      "Ali Khosravi Kazazi",
      "Zhenlong Li",
      "M. Naser Lessani",
      "Guido Cervone"
    ],
    "categories": [
      "cs.AI",
      "cs.DB",
      "cs.IR"
    ],
    "published": "2025-10-23T22:58:17Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21045v2"
  },
  {
    "arxiv_id": "2510.21027v1",
    "entry_id": "http://arxiv.org/abs/2510.21027v1",
    "title": "Customizing Open Source LLMs for Quantitative Medication Attribute Extraction across Heterogeneous EHR Systems",
    "summary": "Harmonizing medication data across Electronic Health Record (EHR) systems is a persistent barrier to monitoring medications for opioid use disorder (MOUD). In heterogeneous EHR systems, key prescription attributes are scattered across differently formatted fields and freetext notes. We present a practical framework that customizes open source large language models (LLMs), including Llama, Qwen, Gemma, and MedGemma, to extract a unified set of MOUD prescription attributes (prescription date, drug name, duration, total quantity, daily quantity, and refills) from heterogeneous, site specific data and compute a standardized metric of medication coverage, \\emph{MOUD days}, per patient. Our pipeline processes records directly in a fixed JSON schema, followed by lightweight normalization and cross-field consistency checks. We evaluate the system on prescription level EHR data from five clinics in a national OUD study (25{,}605 records from 1{,}257 patients), using a previously annotated benchmark of 10{,}369 records (776 patients) as the ground truth. Performance is reported as coverage (share of records with a valid, matchable output) and record-level exact-match accuracy. Larger models perform best overall: Qwen2.5-32B achieves \\textbf{93.4\\%} coverage with \\textbf{93.0\\%} exact-match accuracy across clinics, and MedGemma-27B attains \\textbf{93.1\\%}/\\textbf{92.2\\%}. A brief error review highlights three common issues and fixes: imputing missing dosage fields using within-drug norms, handling monthly/weekly injectables (e.g., Vivitrol) by setting duration from the documented schedule, and adding unit checks to prevent mass units (e.g., ``250 g'') from being misread as daily counts. By removing brittle, site-specific ETL and supporting local, privacy-preserving deployment, this approach enables consistent cross-site analyses of MOUD exposure, adherence, and retention in real-world settings.",
    "authors": [
      "Zhe Fei",
      "Mehmet Yigit Turali",
      "Shreyas Rajesh",
      "Xinyang Dai",
      "Huyen Pham",
      "Pavan Holur",
      "Yuhui Zhu",
      "Larissa Mooney",
      "Yih-Ing Hser",
      "Vwani Roychowdhury"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-23T22:27:10Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21027v1"
  },
  {
    "arxiv_id": "2510.20739v1",
    "entry_id": "http://arxiv.org/abs/2510.20739v1",
    "title": "Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages",
    "summary": "Program analysis tools often produce large volumes of candidate vulnerability reports that require costly manual review, creating a practical challenge: how can security analysts prioritize the reports most likely to be true vulnerabilities?\n  This paper investigates whether machine learning can be applied to prioritizing vulnerabilities reported by program analysis tools. We focus on Node.js packages and collect a benchmark of 1,883 Node.js packages, each containing one reported ACE or ACI vulnerability. We evaluate a variety of machine learning approaches, including classical models, graph neural networks (GNNs), large language models (LLMs), and hybrid models that combine GNN and LLMs, trained on data based on a dynamic program analysis tool's output. The top LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML models reaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leading model eliminates 66.9% of benign packages from manual review, taking around 60 ms per package. If the best model is tuned to operate at a precision level of 0.8 (i.e., allowing 20% false positives amongst all warnings), our approach can detect 99.2% of exploitable taint flows while missing only 0.8%, demonstrating strong potential for real-world vulnerability triage.",
    "authors": [
      "Ronghao Ni",
      "Aidan Z. H. Yang",
      "Min-Chien Hsu",
      "Nuno Sabino",
      "Limin Jia",
      "Ruben Martins",
      "Darion Cassel",
      "Kevin Cheang"
    ],
    "categories": [
      "cs.CR",
      "cs.LG",
      "cs.SE"
    ],
    "published": "2025-10-23T16:58:02Z",
    "pdf_url": "https://arxiv.org/pdf/2510.20739v1"
  },
  {
    "arxiv_id": "2510.20632v1",
    "entry_id": "http://arxiv.org/abs/2510.20632v1",
    "title": "Towards Reliable Evaluation of Large Language Models for Multilingual and Multimodal E-Commerce Applications",
    "summary": "Large Language Models (LLMs) excel on general-purpose NLP benchmarks, yet their capabilities in specialized domains remain underexplored. In e-commerce, existing evaluations-such as EcomInstruct, ChineseEcomQA, eCeLLM, and Shopping MMLU-suffer from limited task diversity (e.g., lacking product guidance and after-sales issues), limited task modalities (e.g., absence of multimodal data), synthetic or curated data, and a narrow focus on English and Chinese, leaving practitioners without reliable tools to assess models on complex, real-world shopping scenarios. We introduce EcomEval, a comprehensive multilingual and multimodal benchmark for evaluating LLMs in e-commerce. EcomEval covers six categories and 37 tasks (including 8 multimodal tasks), sourced primarily from authentic customer queries and transaction logs, reflecting the noisy and heterogeneous nature of real business interactions. To ensure both quality and scalability of reference answers, we adopt a semi-automatic pipeline in which large models draft candidate responses subsequently reviewed and modified by over 50 expert annotators with strong e-commerce and multilingual expertise. We define difficulty levels for each question and task category by averaging evaluation scores across models with different sizes and capabilities, enabling challenge-oriented and fine-grained assessment. EcomEval also spans seven languages-including five low-resource Southeast Asian languages-offering a multilingual perspective absent from prior work.",
    "authors": [
      "Shuyi Xie",
      "Ziqin Liew",
      "Hailing Zhang",
      "Haibo Zhang",
      "Ling Hu",
      "Zhiqiang Zhou",
      "Shuman Liu",
      "Anxiang Zeng"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23T15:04:32Z",
    "pdf_url": "https://arxiv.org/pdf/2510.20632v1"
  },
  {
    "arxiv_id": "2510.20345v1",
    "entry_id": "http://arxiv.org/abs/2510.20345v1",
    "title": "LLM-empowered knowledge graph construction: A survey",
    "summary": "Knowledge Graphs (KGs) have long served as a fundamental infrastructure for structured knowledge representation and reasoning. With the advent of Large Language Models (LLMs), the construction of KGs has entered a new paradigm-shifting from rule-based and statistical pipelines to language-driven and generative frameworks. This survey provides a comprehensive overview of recent progress in LLM-empowered knowledge graph construction, systematically analyzing how LLMs reshape the classical three-layered pipeline of ontology engineering, knowledge extraction, and knowledge fusion.\n  We first revisit traditional KG methodologies to establish conceptual foundations, and then review emerging LLM-driven approaches from two complementary perspectives: schema-based paradigms, which emphasize structure, normalization, and consistency; and schema-free paradigms, which highlight flexibility, adaptability, and open discovery. Across each stage, we synthesize representative frameworks, analyze their technical mechanisms, and identify their limitations.\n  Finally, the survey outlines key trends and future research directions, including KG-based reasoning for LLMs, dynamic knowledge memory for agentic systems, and multimodal KG construction. Through this systematic review, we aim to clarify the evolving interplay between LLMs and knowledge graphs, bridging symbolic knowledge engineering and neural semantic understanding toward the development of adaptive, explainable, and intelligent knowledge systems.",
    "authors": [
      "Haonan Bian"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-23T08:43:28Z",
    "pdf_url": "https://arxiv.org/pdf/2510.20345v1"
  },
  {
    "arxiv_id": "2510.20333v1",
    "entry_id": "http://arxiv.org/abs/2510.20333v1",
    "title": "GhostEI-Bench: Do Mobile Agents Resilience to Environmental Injection in Dynamic On-Device Environments?",
    "summary": "Vision-Language Models (VLMs) are increasingly deployed as autonomous agents to navigate mobile graphical user interfaces (GUIs). Operating in dynamic on-device ecosystems, which include notifications, pop-ups, and inter-app interactions, exposes them to a unique and underexplored threat vector: environmental injection. Unlike prompt-based attacks that manipulate textual instructions, environmental injection corrupts an agent's visual perception by inserting adversarial UI elements (for example, deceptive overlays or spoofed notifications) directly into the GUI. This bypasses textual safeguards and can derail execution, causing privacy leakage, financial loss, or irreversible device compromise. To systematically evaluate this threat, we introduce GhostEI-Bench, the first benchmark for assessing mobile agents under environmental injection attacks within dynamic, executable environments. Moving beyond static image-based assessments, GhostEI-Bench injects adversarial events into realistic application workflows inside fully operational Android emulators and evaluates performance across critical risk scenarios. We further propose a judge-LLM protocol that conducts fine-grained failure analysis by reviewing the agent's action trajectory alongside the corresponding screenshot sequence, pinpointing failure in perception, recognition, or reasoning. Comprehensive experiments on state-of-the-art agents reveal pronounced vulnerability to deceptive environmental cues: current models systematically fail to perceive and reason about manipulated UIs. GhostEI-Bench provides a framework for quantifying and mitigating this emerging threat, paving the way toward more robust and secure embodied agents.",
    "authors": [
      "Chiyu Chen",
      "Xinhao Song",
      "Yunkai Chai",
      "Yang Yao",
      "Haodong Zhao",
      "Lijun Li",
      "Jie Li",
      "Yan Teng",
      "Gongshen Liu",
      "Yingchun Wang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-10-23T08:33:24Z",
    "pdf_url": "https://arxiv.org/pdf/2510.20333v1"
  },
  {
    "arxiv_id": "2510.20075v3",
    "entry_id": "http://arxiv.org/abs/2510.20075v3",
    "title": "LLMs can hide text in other text of the same length",
    "summary": "A meaningful text can be hidden inside another, completely different yet still coherent and plausible, text of the same length. For example, a tweet containing a harsh political critique could be embedded in a tweet that celebrates the same political leader, or an ordinary product review could conceal a secret manuscript. This uncanny state of affairs is now possible thanks to Large Language Models, and in this paper we present a simple and efficient protocol to achieve it. We show that even modest 8-billion-parameter open-source LLMs are sufficient to obtain high-quality results, and a message as long as this abstract can be encoded and decoded locally on a laptop in seconds. The existence of such a protocol demonstrates a radical decoupling of text from authorial intent, further eroding trust in written communication, already shaken by the rise of LLM chatbots. We illustrate this with a concrete scenario: a company could covertly deploy an unfiltered LLM by encoding its answers within the compliant responses of a safe model. This possibility raises urgent questions for AI safety and challenges our understanding of what it means for a Large Language Model to know something.",
    "authors": [
      "Antonio Norelli",
      "Michael Bronstein"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-10-22T23:16:50Z",
    "pdf_url": "https://arxiv.org/pdf/2510.20075v3"
  },
  {
    "arxiv_id": "2510.20001v1",
    "entry_id": "http://arxiv.org/abs/2510.20001v1",
    "title": "Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs",
    "summary": "Large language models (LLMs) show promise for clinical use. They are often evaluated using datasets such as MedQA. However, Many medical datasets, such as MedQA, rely on simplified Question-Answering (Q\\A) that underrepresents real-world clinical decision-making. Based on this, we propose a unifying paradigm that characterizes clinical decision-making tasks along two dimensions: Clinical Backgrounds and Clinical Questions. As the background and questions approach the real clinical environment, the difficulty increases. We summarize the settings of existing datasets and benchmarks along two dimensions. Then we review methods to address clinical decision-making, including training-time and test-time techniques, and summarize when they help. Next, we extend evaluation beyond accuracy to include efficiency, explainability. Finally, we highlight open challenges. Our paradigm clarifies assumptions, standardizes comparisons, and guides the development of clinically meaningful LLMs.",
    "authors": [
      "Yunpeng Xiao",
      "Carl Yang",
      "Mark Mai",
      "Xiao Hu",
      "Kai Shu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-22T20:06:10Z",
    "pdf_url": "https://arxiv.org/pdf/2510.20001v1"
  },
  {
    "arxiv_id": "2510.19799v1",
    "entry_id": "http://arxiv.org/abs/2510.19799v1",
    "title": "Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation",
    "summary": "Public and nonprofit organizations often hesitate to adopt AI tools because most models are opaque even though standard approaches typically analyze aggregate patterns rather than offering actionable, case-level guidance. This study tests a practitioner-in-the-loop workflow that pairs transparent decision-tree models with large language models (LLMs) to improve predictive accuracy, interpretability, and the generation of practical insights. Using data from an ongoing college-success program, we build interpretable decision trees to surface key predictors. We then provide each tree's structure to an LLM, enabling it to reproduce case-level predictions grounded in the transparent models. Practitioners participate throughout feature engineering, model design, explanation review, and usability assessment, ensuring that field expertise informs the analysis at every stage. Results show that integrating transparent models, LLMs, and practitioner input yields accurate, trustworthy, and actionable case-level evaluations, offering a viable pathway for responsible AI adoption in the public and nonprofit sectors.",
    "authors": [
      "Ji Ma",
      "Albert Casella"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.SE",
      "econ.GN"
    ],
    "published": "2025-10-22T17:35:13Z",
    "pdf_url": "https://arxiv.org/pdf/2510.19799v1"
  },
  {
    "arxiv_id": "2510.19334v1",
    "entry_id": "http://arxiv.org/abs/2510.19334v1",
    "title": "Metadata Extraction Leveraging Large Language Models",
    "summary": "The advent of Large Language Models has revolutionized tasks across domains, including the automation of legal document analysis, a critical component of modern contract management systems. This paper presents a comprehensive implementation of LLM-enhanced metadata extraction for contract review, focusing on the automatic detection and annotation of salient legal clauses. Leveraging both the publicly available Contract Understanding Atticus Dataset (CUAD) and proprietary contract datasets, our work demonstrates the integration of advanced LLM methodologies with practical applications. We identify three pivotal elements for optimizing metadata extraction: robust text conversion, strategic chunk selection, and advanced LLM-specific techniques, including Chain of Thought (CoT) prompting and structured tool calling. The results from our experiments highlight the substantial improvements in clause identification accuracy and efficiency. Our approach shows promise in reducing the time and cost associated with contract review while maintaining high accuracy in legal clause identification. The results suggest that carefully optimized LLM systems could serve as valuable tools for legal professionals, potentially increasing access to efficient contract review services for organizations of all sizes.",
    "authors": [
      "Cuize Han",
      "Sesh Jalagam"
    ],
    "categories": [
      "stat.ML",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-10-22T07:56:36Z",
    "pdf_url": "https://arxiv.org/pdf/2510.19334v1"
  },
  {
    "arxiv_id": "2510.18988v3",
    "entry_id": "http://arxiv.org/abs/2510.18988v3",
    "title": "Timely Clinical Diagnosis through Active Test Selection",
    "summary": "There is growing interest in using machine learning (ML) to support clinical diagnosis, but most approaches rely on static, fully observed datasets and fail to reflect the sequential, resource-aware reasoning clinicians use in practice. Diagnosis remains complex and error prone, especially in high-pressure or resource-limited settings, underscoring the need for frameworks that help clinicians make timely and cost-effective decisions. We propose ACTMED (Adaptive Clinical Test selection via Model-based Experimental Design), a diagnostic framework that integrates Bayesian Experimental Design (BED) with large language models (LLMs) to better emulate real-world diagnostic reasoning. At each step, ACTMED selects the test expected to yield the greatest reduction in diagnostic uncertainty for a given patient. LLMs act as flexible simulators, generating plausible patient state distributions and supporting belief updates without requiring structured, task-specific training data. Clinicians can remain in the loop; reviewing test suggestions, interpreting intermediate outputs, and applying clinical judgment throughout. We evaluate ACTMED on real-world datasets and show it can optimize test selection to improve diagnostic accuracy, interpretability, and resource use. This represents a step toward transparent, adaptive, and clinician-aligned diagnostic systems that generalize across settings with reduced reliance on domain-specific data.",
    "authors": [
      "Silas Ruhrberg Estévez",
      "Nicolás Astorga",
      "Mihaela van der Schaar"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-21T18:10:45Z",
    "pdf_url": "https://arxiv.org/pdf/2510.18988v3"
  },
  {
    "arxiv_id": "2510.18931v1",
    "entry_id": "http://arxiv.org/abs/2510.18931v1",
    "title": "A Justice Lens on Fairness and Ethics Courses in Computing Education: LLM-Assisted Multi-Perspective and Thematic Evaluation",
    "summary": "Course syllabi set the tone and expectations for courses, shaping the learning experience for both students and instructors. In computing courses, especially those addressing fairness and ethics in artificial intelligence (AI), machine learning (ML), and algorithmic design, it is imperative that we understand how approaches to navigating barriers to fair outcomes are being addressed.These expectations should be inclusive, transparent, and grounded in promoting critical thinking. Syllabus analysis offers a way to evaluate the coverage, depth, practices, and expectations within a course. Manual syllabus evaluation, however, is time-consuming and prone to inconsistency. To address this, we developed a justice-oriented scoring rubric and asked a large language model (LLM) to review syllabi through a multi-perspective role simulation. Using this rubric, we evaluated 24 syllabi from four perspectives: instructor, departmental chair, institutional reviewer, and external evaluator. We also prompted the LLM to identify thematic trends across the courses. Findings show that multiperspective evaluation aids us in noting nuanced, role-specific priorities, leveraging them to fill hidden gaps in curricula design of AI/ML and related computing courses focused on fairness and ethics. These insights offer concrete directions for improving the design and delivery of fairness, ethics, and justice content in such courses.",
    "authors": [
      "Kenya S. Andrews",
      "Deborah Dormah Kanubala",
      "Kehinde Aruleba",
      "Francisco Enrique Vicente Castro",
      "Renata A Revelo"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-21T13:30:45Z",
    "pdf_url": "https://arxiv.org/pdf/2510.18931v1"
  },
  {
    "arxiv_id": "2510.18551v2",
    "entry_id": "http://arxiv.org/abs/2510.18551v2",
    "title": "SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation",
    "summary": "In this paper, we present SOCIA-Nabla, an end-to-end, agentic framework that treats simulator construction asinstance optimization over code within a textual computation graph. Specialized LLM-driven agents are embedded as graph nodes, and a workflow manager executes a loss-driven loop: code synthesis -> execution -> evaluation -> code repair. The optimizer performs Textual-Gradient Descent (TGD), while human-in-the-loop interaction is reserved for task-spec confirmation, minimizing expert effort and keeping the code itself as the trainable object. Across three CPS tasks, i.e., User Modeling, Mask Adoption, and Personal Mobility, SOCIA-Nabla attains state-of-the-art overall accuracy. By unifying multi-agent orchestration with a loss-aligned optimization view, SOCIA-Nabla converts brittle prompt pipelines into reproducible, constraint-aware simulator code generation that scales across domains and simulation granularities. This work is under review, and we will release the code soon.",
    "authors": [
      "Yuncheng Hua",
      "Sion Weatherhead",
      "Mehdi Jafari",
      "Hao Xue",
      "Flora D. Salim"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-21T12:00:00Z",
    "pdf_url": "https://arxiv.org/pdf/2510.18551v2"
  },
  {
    "arxiv_id": "2510.18271v1",
    "entry_id": "http://arxiv.org/abs/2510.18271v1",
    "title": "From Agent Simulation to Social Simulator: A Comprehensive Review (Part 1)",
    "summary": "This is the first part of the comprehensive review, focusing on the historical development of Agent-Based Modeling (ABM) and its classic cases. It begins by discussing the development history and design principles of Agent-Based Modeling (ABM), helping readers understand the significant challenges that traditional physical simulation methods face in the social domain. Then, it provides a detailed introduction to foundational models for simulating social systems, including individual models, environmental models, and rule-based models. Finally, it presents classic cases of social simulation, covering three types: thought experiments, mechanism exploration, and parallel optimization.",
    "authors": [
      "Xiao Xue",
      "Deyu Zhou",
      "Ming Zhang",
      "Fei-Yue Wang"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-10-21T03:46:26Z",
    "pdf_url": "https://arxiv.org/pdf/2510.18271v1"
  },
  {
    "arxiv_id": "2510.18104v1",
    "entry_id": "http://arxiv.org/abs/2510.18104v1",
    "title": "From AutoRecSys to AutoRecLab: A Call to Build, Evaluate, and Govern Autonomous Recommender-Systems Research Labs",
    "summary": "Recommender-systems research has accelerated model and evaluation advances, yet largely neglects automating the research process itself. We argue for a shift from narrow AutoRecSys tools -- focused on algorithm selection and hyper-parameter tuning -- to an Autonomous Recommender-Systems Research Lab (AutoRecLab) that integrates end-to-end automation: problem ideation, literature analysis, experimental design and execution, result interpretation, manuscript drafting, and provenance logging. Drawing on recent progress in automated science (e.g., multi-agent AI Scientist and AI Co-Scientist systems), we outline an agenda for the RecSys community: (1) build open AutoRecLab prototypes that combine LLM-driven ideation and reporting with automated experimentation; (2) establish benchmarks and competitions that evaluate agents on producing reproducible RecSys findings with minimal human input; (3) create review venues for transparently AI-generated submissions; (4) define standards for attribution and reproducibility via detailed research logs and metadata; and (5) foster interdisciplinary dialogue on ethics, governance, privacy, and fairness in autonomous research. Advancing this agenda can increase research throughput, surface non-obvious insights, and position RecSys to contribute to emerging Artificial Research Intelligence. We conclude with a call to organise a community retreat to coordinate next steps and co-author guidance for the responsible integration of automated research systems.",
    "authors": [
      "Joeran Beel",
      "Bela Gipp",
      "Tobias Vente",
      "Moritz Baumgart",
      "Philipp Meister"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-20T20:58:50Z",
    "pdf_url": "https://arxiv.org/pdf/2510.18104v1"
  },
  {
    "arxiv_id": "2510.18003v1",
    "entry_id": "http://arxiv.org/abs/2510.18003v1",
    "title": "BadScientist: Can a Research Agent Write Convincing but Unsound Papers that Fool LLM Reviewers?",
    "summary": "The convergence of LLM-powered research assistants and AI-based peer review systems creates a critical vulnerability: fully automated publication loops where AI-generated research is evaluated by AI reviewers without human oversight. We investigate this through \\textbf{BadScientist}, a framework that evaluates whether fabrication-oriented paper generation agents can deceive multi-model LLM review systems. Our generator employs presentation-manipulation strategies requiring no real experiments. We develop a rigorous evaluation framework with formal error guarantees (concentration bounds and calibration analysis), calibrated on real data. Our results reveal systematic vulnerabilities: fabricated papers achieve acceptance rates up to . Critically, we identify \\textit{concern-acceptance conflict} -- reviewers frequently flag integrity issues yet assign acceptance-level scores. Our mitigation strategies show only marginal improvements, with detection accuracy barely exceeding random chance. Despite provably sound aggregation mathematics, integrity checking systematically fails, exposing fundamental limitations in current AI-driven review systems and underscoring the urgent need for defense-in-depth safeguards in scientific publishing.",
    "authors": [
      "Fengqing Jiang",
      "Yichen Feng",
      "Yuetai Li",
      "Luyao Niu",
      "Basel Alomair",
      "Radha Poovendran"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-10-20T18:37:11Z",
    "pdf_url": "https://arxiv.org/pdf/2510.18003v1"
  },
  {
    "arxiv_id": "2510.20844v2",
    "entry_id": "http://arxiv.org/abs/2510.20844v2",
    "title": "TrustResearcher: Automating Knowledge-Grounded and Transparent Research Ideation with Multi-Agent Collaboration",
    "summary": "Effective research relies on organizing extensive information and stimulating novel solutions. Agentic systems have recently emerged as a promising tool to automate literature-based ideation. However, current systems often remain black-box. Their outputs may appear plausible but weakly grounded, with limited transparency or control for researchers. Our work introduces TrustResearcher, a multi-agent demo system for knowledge-grounded and transparent ideation. Specifically, TrustResearcher integrates meticulously designed four stages into a unified framework: (A) Structured Knowledge Curation, (B) Diversified Idea Generation, (C) Multi-stage Idea Selection, and (D) Expert Panel Review & Synthesis. Different from prior pipelines, our system not only exposes intermediate reasoning states, execution logs, and tunable agents for inspections, but also enables the generation of hypotheses that are both diverse and evidence-aligned. Our design is also domain-agnostic: as long as literature sources exist, the same pipeline can be instantiated in any scientific field. As an illustrative case, we demonstrate TrustResearcher on a graph-mining case study (k-truss breaking problem), where it generates distinct, plausible hypotheses with evidence and critiques. A live demo and source code are available at https://github.com/valleysprings/TrustResearcher.",
    "authors": [
      "Jiawei Zhou",
      "Ruicheng Zhu",
      "Mengshi Chen",
      "Jianwei Wang",
      "Kai Wang"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-10-20T11:47:40Z",
    "pdf_url": "https://arxiv.org/pdf/2510.20844v2"
  },
  {
    "arxiv_id": "2510.17269v1",
    "entry_id": "http://arxiv.org/abs/2510.17269v1",
    "title": "FineVision: Open Data Is All You Need",
    "summary": "The advancement of vision-language models (VLMs) is hampered by a fragmented landscape of inconsistent and contaminated public datasets. We introduce FineVision, a meticulously collected, curated, and unified corpus of 24 million samples - the largest open resource of its kind. We unify more than 200 sources into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation performs bulk ingestion and schema mapping, while reviewers audit mappings and spot-check outputs to verify faithful consumption of annotations, appropriate formatting and diversity, and safety; issues trigger targeted fixes and re-runs. The workflow further applies rigorous de-duplication within and across sources and decontamination against 66 public benchmarks. FineVision also encompasses agentic/GUI tasks with a unified action space; reviewers validate schemas and inspect a sample of trajectories to confirm executable fidelity. Models trained on FineVision consistently outperform those trained on existing open mixtures across a broad evaluation suite, underscoring the benefits of scale, data hygiene, and balanced automation with human oversight. We release the corpus and curation tools to accelerate data-centric VLM research.",
    "authors": [
      "Luis Wiedmann",
      "Orr Zohar",
      "Amir Mahla",
      "Xiaohan Wang",
      "Rui Li",
      "Thibaud Frere",
      "Leandro von Werra",
      "Aritra Roy Gosthipaty",
      "Andrés Marafioti"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-10-20T07:54:46Z",
    "pdf_url": "https://arxiv.org/pdf/2510.17269v1"
  },
  {
    "arxiv_id": "2510.17062v1",
    "entry_id": "http://arxiv.org/abs/2510.17062v1",
    "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation",
    "summary": "While reasoning-based large language models excel at complex tasks through an internal, structured thinking process, a concerning phenomenon has emerged that such a thinking process can aggregate social stereotypes, leading to biased outcomes. However, the underlying behaviours of these language models in social bias scenarios remain underexplored. In this work, we systematically investigate mechanisms within the thinking process behind this phenomenon and uncover two failure patterns that drive social bias aggregation: 1) stereotype repetition, where the model relies on social stereotypes as its primary justification, and 2) irrelevant information injection, where it fabricates or introduces new details to support a biased narrative. Building on these insights, we introduce a lightweight prompt-based mitigation approach that queries the model to review its own initial reasoning against these specific failure patterns. Experiments on question answering (BBQ and StereoSet) and open-ended (BOLD) benchmarks show that our approach effectively reduces bias while maintaining or improving accuracy.",
    "authors": [
      "Guoqing Luo",
      "Iffat Maab",
      "Lili Mou",
      "Junichi Yamagishi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-20T00:33:44Z",
    "pdf_url": "https://arxiv.org/pdf/2510.17062v1"
  },
  {
    "arxiv_id": "2510.16973v1",
    "entry_id": "http://arxiv.org/abs/2510.16973v1",
    "title": "Foundation Models in Medical Image Analysis: A Systematic Review and Meta-Analysis",
    "summary": "Recent advancements in artificial intelligence (AI), particularly foundation models (FMs), have revolutionized medical image analysis, demonstrating strong zero- and few-shot performance across diverse medical imaging tasks, from segmentation to report generation. Unlike traditional task-specific AI models, FMs leverage large corpora of labeled and unlabeled multimodal datasets to learn generalized representations that can be adapted to various downstream clinical applications with minimal fine-tuning. However, despite the rapid proliferation of FM research in medical imaging, the field remains fragmented, lacking a unified synthesis that systematically maps the evolution of architectures, training paradigms, and clinical applications across modalities. To address this gap, this review article provides a comprehensive and structured analysis of FMs in medical image analysis. We systematically categorize studies into vision-only and vision-language FMs based on their architectural foundations, training strategies, and downstream clinical tasks. Additionally, a quantitative meta-analysis of the studies was conducted to characterize temporal trends in dataset utilization and application domains. We also critically discuss persistent challenges, including domain adaptation, efficient fine-tuning, computational constraints, and interpretability along with emerging solutions such as federated learning, knowledge distillation, and advanced prompting. Finally, we identify key future research directions aimed at enhancing the robustness, explainability, and clinical integration of FMs, thereby accelerating their translation into real-world medical practice.",
    "authors": [
      "Praveenbalaji Rajendran",
      "Mojtaba Safari",
      "Wenfeng He",
      "Mingzhe Hu",
      "Shansong Wang",
      "Jun Zhou",
      "Xiaofeng Yang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "physics.med-ph"
    ],
    "published": "2025-10-19T19:19:23Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16973v1"
  },
  {
    "arxiv_id": "2510.16933v1",
    "entry_id": "http://arxiv.org/abs/2510.16933v1",
    "title": "Tutoring LLM into a Better CUDA Optimizer",
    "summary": "Recent leaps in large language models (LLMs) caused a revolution in programming tools (like GitHub Copilot) that can help with code generation, debugging, and even performance optimization. In this paper, we focus on the capabilities of the most recent reasoning models to generate optimized CUDA code for predefined, well-known tasks. Our objective is to determine which types of code optimizations and parallel patterns the LLMs can perform by themselves and whether they can be improved by tutoring (providing more detailed hints and guidelines in the prompt). The generated solutions were evaluated both automatically (for correctness and speedup) and manually (code reviews) to provide a more detailed perspective. We also tried an interactive approach where the LLM can fix its previous mistakes within a session. The results indicate that LLMs are quite skilled coders; however, they require tutoring to reach optimized solutions provided by parallel computing experts.",
    "authors": [
      "Matyáš Brabec",
      "Jiří Klepl",
      "Michal Töpfer",
      "Martin Kruliš"
    ],
    "categories": [
      "cs.DC",
      "cs.AI"
    ],
    "published": "2025-10-19T17:09:15Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16933v1"
  },
  {
    "arxiv_id": "2510.16805v1",
    "entry_id": "http://arxiv.org/abs/2510.16805v1",
    "title": "Mixed-Precision Quantization for Language Models: Techniques and Prospects",
    "summary": "The rapid scaling of language models (LMs) has resulted in unprecedented computational, memory, and energy requirements, making their training and deployment increasingly unsustainable. Quantization has emerged as an essential compression technique to reduce model size, alleviate memory bottlenecks, and accelerate inference. However, while uniform low-bit quantization (e.g., INT8, INT4) provides significant efficiency gains, it can degrade accuracy in sensitive components of transformer-based LMs. Mixed-precision quantization offers a promising alternative by selectively allocating precision across layers or within tensors to balance efficiency and accuracy. This survey provides a comprehensive overview of Mixed-Precision quantization frameworks for LMs (MXPLMs). We first review quantization fundamentals, including uniform and non-uniform quantizers, quantization granularity, and methods widely used in post-training quantization. We then categorize and compare recent MXPLM frameworks according to their bit allocation strategies and precision configurations across weights, activations, and key-value caches. A comparative analysis highlights differences in perplexity, zero-shot task performance, and deployment trade-offs. Furthermore, we contrast MXPLMs with earlier mixed-precision quantization methods for deep neural networks, identifying strategies that transfer and those that face challenges in the LM setting. Finally, we summarize open issues and future directions, including hardware-aware design, activation quantization, and scalable optimization methods for billion-parameter models. By consolidating recent advances, this work serves as a reference for understanding the current landscape and research prospects of mixed-precision quantization for large-scale language models.",
    "authors": [
      "Mariam Rakka",
      "Marios Fournarakis",
      "Olga Krestinskaya",
      "Jinane Bazzi",
      "Khaled N. Salama",
      "Fadi Kurdahi",
      "Ahmed M. Eltawil",
      "Mohammed E. Fouda"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-19T12:16:40Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16805v1"
  },
  {
    "arxiv_id": "2510.16724v2",
    "entry_id": "http://arxiv.org/abs/2510.16724v2",
    "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications",
    "summary": "The advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language interaction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or domain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external evidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning. Recent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step interaction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive and self-improving search behavior. This survey provides the first comprehensive overview of \\emph{RL-based agentic search}, organizing the emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and applications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search systems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available at https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.",
    "authors": [
      "Minhua Lin",
      "Zongyu Wu",
      "Zhichao Xu",
      "Hui Liu",
      "Xianfeng Tang",
      "Qi He",
      "Charu Aggarwal",
      "Hui Liu",
      "Xiang Zhang",
      "Suhang Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-19T06:04:53Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16724v2"
  },
  {
    "arxiv_id": "2510.16720v2",
    "entry_id": "http://arxiv.org/abs/2510.16720v2",
    "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI",
    "summary": "The rapid evolution of agentic AI marks a new phase in artificial intelligence, where Large Language Models (LLMs) no longer merely respond but act, reason, and adapt. This survey traces the paradigm shift in building agentic AI: from Pipeline-based systems, where planning, tool use, and memory are orchestrated by external logic, to the emerging Model-native paradigm, where these capabilities are internalized within the model's parameters. We first position Reinforcement Learning (RL) as the algorithmic engine enabling this paradigm shift. By reframing learning from imitating static data to outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task across language, vision and embodied domains. Building on this, the survey systematically reviews how each capability -- Planning, Tool use, and Memory -- has evolved from externally scripted modules to end-to-end learned behaviors. Furthermore, it examines how this paradigm shift has reshaped major agent applications, specifically the Deep Research agent emphasizing long-horizon reasoning and the GUI agent emphasizing embodied interaction. We conclude by discussing the continued internalization of agentic capabilities like Multi-agent collaboration and Reflection, alongside the evolving roles of the system and model layers in future agentic AI. Together, these developments outline a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience.",
    "authors": [
      "Jitao Sang",
      "Jinlin Xiao",
      "Jiarun Han",
      "Jilin Chen",
      "Xiaoyi Chen",
      "Shuyu Wei",
      "Yongjie Sun",
      "Yuhang Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-19T05:23:43Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16720v2"
  },
  {
    "arxiv_id": "2510.16708v2",
    "entry_id": "http://arxiv.org/abs/2510.16708v2",
    "title": "Natural Language Processing for Cardiology: A Narrative Review",
    "summary": "Cardiovascular diseases are becoming increasingly prevalent in modern society, with a profound impact on global health and well-being. These Cardiovascular disorders are complex and multifactorial, influenced by genetic predispositions, lifestyle choices, and diverse socioeconomic and clinical factors. Information about these interrelated factors is dispersed across multiple types of textual data, including patient narratives, medical records, and scientific literature. Natural language processing (NLP) has emerged as a powerful approach for analysing such unstructured data, enabling healthcare professionals and researchers to gain deeper insights that may transform the diagnosis, treatment, and prevention of cardiac disorders. This review provides a comprehensive overview of NLP research in cardiology from 2014 to 2025. We systematically searched six literature databases for studies describing NLP applications across a range of cardiovascular diseases. After a rigorous screening process, we identified 265 relevant articles. Each study was analysed across multiple dimensions, including NLP paradigms, cardiology-related tasks, disease types, and data sources. Our findings reveal substantial diversity within these dimensions, reflecting the breadth and evolution of NLP research in cardiology. A temporal analysis further highlights methodological trends, showing a progression from rule-based systems to large language models. Finally, we discuss key challenges and future directions, such as developing interpretable LLMs and integrating multimodal data. To the best of our knowledge, this review represents the most comprehensive synthesis of NLP research in cardiology to date.",
    "authors": [
      "Kailai Yang",
      "Yan Leng",
      "Xin Zhang",
      "Tianlin Zhang",
      "Paul Thompson",
      "Bernard Keavney",
      "Maciej Tomaszewski",
      "Sophia Ananiadou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-19T04:26:51Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16708v2"
  },
  {
    "arxiv_id": "2510.16551v3",
    "entry_id": "http://arxiv.org/abs/2510.16551v3",
    "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction",
    "summary": "This research proposes a systematic, large language model (LLM) approach for extracting product and service attributes, features, and associated sentiments from customer reviews. Grounded in marketing theory, the framework distinguishes perceptual attributes from actionable features, producing interpretable and managerially actionable insights. We apply the methodology to 20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a random subset of reviews. Model performance is assessed through agreement with human annotations and predictive validity for customer ratings. Results show high consistency between LLMs and human coders and strong predictive validity, confirming the reliability of the approach. Human coders required a median of six minutes per review, whereas the LLM processed each in two seconds, delivering comparable insights at a scale unattainable through manual coding. Managerially, the analysis identifies attributes and features that most strongly influence customer satisfaction and their associated sentiments, enabling firms to pinpoint \"joy points,\" address \"pain points,\" and design targeted interventions. We demonstrate how structured review data can power an actionable marketing dashboard that tracks sentiment over time and across stores, benchmarks performance, and highlights high-leverage features for improvement. Simulations indicate that enhancing sentiment for key service features could yield 1-2% average revenue gains per store.",
    "authors": [
      "Khaled Boughanmi",
      "Kamel Jedidi",
      "Nour Jedidi"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "econ.EM"
    ],
    "published": "2025-10-18T15:46:11Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16551v3"
  },
  {
    "arxiv_id": "2510.16547v1",
    "entry_id": "http://arxiv.org/abs/2510.16547v1",
    "title": "Predicting life satisfaction using machine learning and explainable AI",
    "summary": "Life satisfaction is a crucial facet of human well-being. Hence, research on life satisfaction is incumbent for understanding how individuals experience their lives and influencing interventions targeted at enhancing mental health and well-being. Life satisfaction has traditionally been measured using analog, complicated, and frequently error-prone methods. These methods raise questions concerning validation and propagation. However, this study demonstrates the potential for machine learning algorithms to predict life satisfaction with a high accuracy of 93.80% and a 73.00% macro F1-score. The dataset comes from a government survey of 19000 people aged 16-64 years in Denmark. Using feature learning techniques, 27 significant questions for assessing contentment were extracted, making the study highly reproducible, simple, and easily interpretable. Furthermore, clinical and biomedical large language models (LLMs) were explored for predicting life satisfaction by converting tabular data into natural language sentences through mapping and adding meaningful counterparts, achieving an accuracy of 93.74% and macro F1-score of 73.21%. It was found that life satisfaction prediction is more closely related to the biomedical domain than the clinical domain. Ablation studies were also conducted to understand the impact of data resampling and feature selection techniques on model performance. Moreover, the correlation between primary determinants with different age brackets was analyzed, and it was found that health condition is the most important determinant across all ages. This study demonstrates how machine learning, large language models and XAI can jointly contribute to building trust and understanding in using AI to investigate human behavior, with significant ramifications for academics and professionals working to quantify and comprehend subjective well-being.",
    "authors": [
      "Alif Elham Khan",
      "Mohammad Junayed Hasan",
      "Humayra Anjum",
      "Nabeel Mohammed",
      "Sifat Momen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-18T15:44:25Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16547v1"
  },
  {
    "arxiv_id": "2510.16466v1",
    "entry_id": "http://arxiv.org/abs/2510.16466v1",
    "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights",
    "summary": "As customer feedback becomes increasingly central to strategic growth, the ability to derive actionable insights from unstructured reviews is essential. While traditional AI-driven systems excel at predicting user preferences, far less work has focused on transforming customer reviews into prescriptive, business-facing recommendations. This paper introduces ReviewSense, a novel prescriptive decision support framework that leverages advanced large language models (LLMs) to transform customer reviews into targeted, actionable business recommendations. By identifying key trends, recurring issues, and specific concerns within customer sentiments, ReviewSense extends beyond preference-based systems to provide businesses with deeper insights for sustaining growth and enhancing customer loyalty. The novelty of this work lies in integrating clustering, LLM adaptation, and expert-driven evaluation into a unified, business-facing pipeline. Preliminary manual evaluations indicate strong alignment between the model's recommendations and business objectives, highlighting its potential for driving data-informed decision-making. This framework offers a new perspective on AI-driven sentiment analysis, demonstrating its value in refining business strategies and maximizing the impact of customer feedback.",
    "authors": [
      "Siddhartha Krothapalli",
      "Tridib Kumar Das",
      "Praveen Kumar",
      "Naveen Suravarpu",
      "Pratik Narang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-18T12:20:15Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16466v1"
  },
  {
    "arxiv_id": "2510.18890v1",
    "entry_id": "http://arxiv.org/abs/2510.18890v1",
    "title": "Small Language Models Offer Significant Potential for Science Community",
    "summary": "Recent advancements in natural language processing, particularly with large language models (LLMs), are transforming how scientists engage with the literature. While the adoption of LLMs is increasing, concerns remain regarding potential information biases and computational costs. Rather than LLMs, I developed a framework to evaluate the feasibility of precise, rapid, and cost-effective information retrieval from extensive geoscience literature using freely available small language models (MiniLMs). A curated corpus of approximately 77 million high-quality sentences, extracted from 95 leading peer-reviewed geoscience journals such as Geophysical Research Letters and Earth and Planetary Science Letters published during years 2000 to 2024, was constructed. MiniLMs enable a computationally efficient approach for extracting relevant domain-specific information from these corpora through semantic search techniques and sentence-level indexing. This approach, unlike LLMs such as ChatGPT-4 that often produces generalized responses, excels at identifying substantial amounts of expert-verified information with established, multi-disciplinary sources, especially for information with quantitative findings. Furthermore, by analyzing emotional tone via sentiment analysis and topical clusters through unsupervised clustering within sentences, MiniLM provides a powerful tool for tracking the evolution of conclusions, research priorities, advancements, and emerging questions within geoscience communities. Overall, MiniLM holds significant potential within the geoscience community for applications such as fact and image retrievals, trend analyses, contradiction analyses, and educational purposes.",
    "authors": [
      "Jian Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-18T07:25:05Z",
    "pdf_url": "https://arxiv.org/pdf/2510.18890v1"
  },
  {
    "arxiv_id": "2510.16234v1",
    "entry_id": "http://arxiv.org/abs/2510.16234v1",
    "title": "ScholarEval: Research Idea Evaluation Grounded in Literature",
    "summary": "As AI tools become increasingly common for research ideation, robust evaluation is critical to ensure the validity and usefulness of generated ideas. We introduce ScholarEval, a retrieval augmented evaluation framework that assesses research ideas based on two fundamental criteria: soundness - the empirical validity of proposed methods based on existing literature, and contribution - the degree of advancement made by the idea across different dimensions relative to prior research. To evaluate ScholarEval, we introduce ScholarIdeas, the first expert-annotated dataset of multi-domain research ideas and reviews, comprised of 117 ideas across four disciplines: artificial intelligence, neuroscience, biochemistry, and ecology. Our evaluation shows that ScholarEval achieves significantly higher coverage of points mentioned in the human expert annotated rubrics in ScholarIdeas compared to all baselines. Furthermore, ScholarEval is consistently preferred over our strongest baseline o4-mini-deep-research, a reasoning and search-enabled agentic system by OpenAI, in terms of evaluation actionability, depth, and evidence support. Our large-scale user study also shows that ScholarEval significantly outperforms deep research in literature engagement, idea refinement, and usefulness. We openly release our code, dataset, and ScholarEval tool for the community to use and build on.",
    "authors": [
      "Hanane Nour Moussa",
      "Patrick Queiroz Da Silva",
      "Daniel Adu-Ampratwum",
      "Alyson East",
      "Zitong Lu",
      "Nikki Puccetti",
      "Mingyi Xue",
      "Huan Sun",
      "Bodhisattwa Prasad Majumder",
      "Sachin Kumar"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-10-17T21:55:07Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16234v1"
  },
  {
    "arxiv_id": "2510.16091v1",
    "entry_id": "http://arxiv.org/abs/2510.16091v1",
    "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification",
    "summary": "This study quantifies how prompting strategies interact with large language models (LLMs) to automate the screening stage of systematic literature reviews (SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3, Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types (zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection) across relevance classification and six Level-2 tasks, using accuracy, precision, recall, and F1. Results show pronounced model-prompt interaction effects: CoT-few-shot yields the most reliable precision-recall balance; zero-shot maximizes recall for high-sensitivity passes; and self-reflection underperforms due to over-inclusivity and instability across models. GPT-4o and DeepSeek provide robust overall performance, while GPT-4o-mini performs competitively at a substantially lower dollar cost. A cost-performance analysis for relevance classification (per 1,000 abstracts) reveals large absolute differences among model-prompt pairings; GPT-4o-mini remains low-cost across prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer attractive F1 at a small incremental cost. We recommend a staged workflow that (1) deploys low-cost models with structured prompts for first-pass screening and (2) escalates only borderline cases to higher-capacity models. These findings highlight LLMs' uneven but promising potential to automate literature screening. By systematically analyzing prompt-model interactions, we provide a comparative benchmark and practical guidance for task-adaptive LLM deployment.",
    "authors": [
      "Binglan Han",
      "Anuradha Mathrani",
      "Teo Susnjak"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-17T16:53:09Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16091v1"
  },
  {
    "arxiv_id": "2511.04683v1",
    "entry_id": "http://arxiv.org/abs/2511.04683v1",
    "title": "AI-Powered Citation Auditing: A Zero-Assumption Protocol for Systematic Reference Verification in Academic Research",
    "summary": "Academic citation integrity faces persistent challenges, with research indicating 20% of citations contain errors and manual verification requiring months of expert time. This paper presents a novel AI-powered methodology for systematic, comprehensive reference auditing using agentic AI with tool-use capabilities. We develop a zero-assumption verification protocol that independently validates every reference against multiple academic databases (Semantic Scholar, Google Scholar, CrossRef) without assuming any citation is correct. The methodology was validated across 30 academic documents (2,581 references) spanning undergraduate projects to doctoral theses and peer-reviewed publications. Results demonstrate 91.7% average verification rate on published PLOS papers, with successful detection of fabricated references, retracted articles, orphan citations, and predatory journals. Time efficiency improved dramatically: 90-minute audits for 916-reference doctoral theses versus months of manual review. The system achieved <0.5% false positive rate while identifying critical issues manual review might miss. This work establishes the first validated AI-agent methodology for academic citation integrity, demonstrating practical applicability for supervisors, students, and institutional quality assurance.",
    "authors": [
      "L. J. Janse van Rensburg"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-10-17T16:53:03Z",
    "pdf_url": "https://arxiv.org/pdf/2511.04683v1"
  },
  {
    "arxiv_id": "2510.15782v1",
    "entry_id": "http://arxiv.org/abs/2510.15782v1",
    "title": "Demo: Guide-RAG: Evidence-Driven Corpus Curation for Retrieval-Augmented Generation in Long COVID",
    "summary": "As AI chatbots gain adoption in clinical medicine, developing effective frameworks for complex, emerging diseases presents significant challenges. We developed and evaluated six Retrieval-Augmented Generation (RAG) corpus configurations for Long COVID (LC) clinical question answering, ranging from expert-curated sources to large-scale literature databases. Our evaluation employed an LLM-as-a-judge framework across faithfulness, relevance, and comprehensiveness metrics using LongCOVID-CQ, a novel dataset of expert-generated clinical questions. Our RAG corpus configuration combining clinical guidelines with high-quality systematic reviews consistently outperformed both narrow single-guideline approaches and large-scale literature databases. Our findings suggest that for emerging diseases, retrieval grounded in curated secondary reviews provides an optimal balance between narrow consensus documents and unfiltered primary literature, supporting clinical decision-making while avoiding information overload and oversimplified guidance. We propose Guide-RAG, a chatbot system and accompanying evaluation framework that integrates both curated expert knowledge and comprehensive literature databases to effectively answer LC clinical questions.",
    "authors": [
      "Philip DiGiacomo",
      "Haoyang Wang",
      "Jinrui Fang",
      "Yan Leng",
      "W Michael Brode",
      "Ying Ding"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-17T16:05:52Z",
    "pdf_url": "https://arxiv.org/pdf/2510.15782v1"
  },
  {
    "arxiv_id": "2510.15746v1",
    "entry_id": "http://arxiv.org/abs/2510.15746v1",
    "title": "LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation",
    "summary": "Ideal or real - that is the question.In this work, we explore whether principles from game theory can be effectively applied to the evaluation of large language models (LLMs). This inquiry is motivated by the growing inadequacy of conventional evaluation practices, which often rely on fixed-format tasks with reference answers and struggle to capture the nuanced, subjective, and open-ended nature of modern LLM behavior. To address these challenges, we propose a novel alternative: automatic mutual evaluation, where LLMs assess each other's output through self-play and peer review. These peer assessments are then systematically compared with human voting behavior to evaluate their alignment with human judgment. Our framework incorporates game-theoretic voting algorithms to aggregate peer reviews, enabling a principled investigation into whether model-generated rankings reflect human preferences. Empirical results reveal both convergences and divergences between theoretical predictions and human evaluations, offering valuable insights into the promises and limitations of mutual evaluation. To the best of our knowledge, this is the first work to jointly integrate mutual evaluation, game-theoretic aggregation, and human-grounded validation for evaluating the capabilities of LLMs.",
    "authors": [
      "Gao Yang",
      "Yuhang Liu",
      "Siyu Miao",
      "Xinyue Liang",
      "Zhengyang Liu",
      "Heyan Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-17T15:34:25Z",
    "pdf_url": "https://arxiv.org/pdf/2510.15746v1"
  },
  {
    "arxiv_id": "2510.15280v1",
    "entry_id": "http://arxiv.org/abs/2510.15280v1",
    "title": "Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition",
    "summary": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the landscape of scientific research. Beyond accelerating tasks such as hypothesis generation, experimental design, and result interpretation, they prompt a more fundamental question: Are FMs merely enhancing existing scientific methodologies, or are they redefining the way science is conducted? In this paper, we argue that FMs are catalyzing a transition toward a new scientific paradigm. We introduce a three-stage framework to describe this evolution: (1) Meta-Scientific Integration, where FMs enhance workflows within traditional paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active collaborators in problem formulation, reasoning, and discovery; and (3) Autonomous Scientific Discovery, where FMs operate as independent agents capable of generating new scientific knowledge with minimal human intervention. Through this lens, we review current applications and emerging capabilities of FMs across existing scientific paradigms. We further identify risks and future directions for FM-enabled scientific discovery. This position paper aims to support the scientific community in understanding the transformative role of FMs and to foster reflection on the future of scientific discovery. Our project is available at https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.",
    "authors": [
      "Fan Liu",
      "Jindong Han",
      "Tengfei Lyu",
      "Weijia Zhang",
      "Zhe-Rui Yang",
      "Lu Dai",
      "Cancheng Liu",
      "Hao Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DL"
    ],
    "published": "2025-10-17T03:40:26Z",
    "pdf_url": "https://arxiv.org/pdf/2510.15280v1"
  },
  {
    "arxiv_id": "2510.14900v1",
    "entry_id": "http://arxiv.org/abs/2510.14900v1",
    "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates",
    "summary": "The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.",
    "authors": [
      "Wen-Kwang Tsao",
      "Yao-Ching Yu",
      "Chien-Ming Huang"
    ],
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-10-16T17:17:00Z",
    "pdf_url": "https://arxiv.org/pdf/2510.14900v1"
  },
  {
    "arxiv_id": "2510.14315v1",
    "entry_id": "http://arxiv.org/abs/2510.14315v1",
    "title": "Active Measuring in Reinforcement Learning With Delayed Negative Effects",
    "summary": "Measuring states in reinforcement learning (RL) can be costly in real-world settings and may negatively influence future outcomes. We introduce the Actively Observable Markov Decision Process (AOMDP), where an agent not only selects control actions but also decides whether to measure the latent state. The measurement action reveals the true latent state but may have a negative delayed effect on the environment. We show that this reduced uncertainty may provably improve sample efficiency and increase the value of the optimal policy despite these costs. We formulate an AOMDP as a periodic partially observable MDP and propose an online RL algorithm based on belief states. To approximate the belief states, we further propose a sequential Monte Carlo method to jointly approximate the posterior of unknown static environment parameters and unobserved latent states. We evaluate the proposed algorithm in a digital health application, where the agent decides when to deliver digital interventions and when to assess users' health status through surveys.",
    "authors": [
      "Daiqi Gao",
      "Ziping Xu",
      "Aseel Rawashdeh",
      "Predrag Klasnja",
      "Susan A. Murphy"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-10-16T05:21:36Z",
    "pdf_url": "https://arxiv.org/pdf/2510.14315v1"
  },
  {
    "arxiv_id": "2510.14205v3",
    "entry_id": "http://arxiv.org/abs/2510.14205v3",
    "title": "DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans",
    "summary": "The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF). DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences. We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews. DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios. Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.",
    "authors": [
      "Bingsheng Yao",
      "Bo Sun",
      "Yuanzhe Dong",
      "Yuxuan Lu",
      "Dakuo Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-16T01:26:38Z",
    "pdf_url": "https://arxiv.org/pdf/2510.14205v3"
  },
  {
    "arxiv_id": "2510.13982v3",
    "entry_id": "http://arxiv.org/abs/2510.13982v3",
    "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
    "summary": "What if artificial agents could not just communicate, but also evolve, adapt, and reshape their worlds in ways we cannot fully predict? With llm now powering multi-agent systems and social simulations, we are witnessing new possibilities for modeling open-ended, ever-changing environments. Yet, most current simulations remain constrained within static sandboxes, characterized by predefined tasks, limited dynamics, and rigid evaluation criteria. These limitations prevent them from capturing the complexity of real-world societies. In this paper, we argue that static, task-specific benchmarks are fundamentally inadequate and must be rethought. We critically review emerging architectures that blend llm with multi-agent dynamics, highlight key hurdles such as balancing stability and diversity, evaluating unexpected behaviors, and scaling to greater complexity, and introduce a fresh taxonomy for this rapidly evolving field. Finally, we present a research roadmap centered on open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems. We call on the community to move beyond static paradigms and help shape the next generation of adaptive, socially-aware multi-agent simulations.",
    "authors": [
      "Jinkun Chen",
      "Sher Badshah",
      "Xuemin Yu",
      "Sijia Han"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-10-15T18:05:06Z",
    "pdf_url": "https://arxiv.org/pdf/2510.13982v3"
  },
  {
    "arxiv_id": "2510.16017v1",
    "entry_id": "http://arxiv.org/abs/2510.16017v1",
    "title": "InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects",
    "summary": "Infrastructure in smart cities is increasingly monitored by networks of closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop cracks, potholes, and fluid leaks that threaten public safety and require timely repair. Manual inspection is costly and hazardous, and existing automatic systems typically address individual defect types or provide unstructured outputs that cannot directly guide maintenance crews. This paper proposes a comprehensive pipeline that leverages street CCTV streams for multi defect detection and segmentation using the YOLO family of object detectors and passes the detections to a vision language model (VLM) for scene aware summarization. The VLM generates a structured action plan in JSON format that includes incident descriptions, recommended tools, dimensions, repair plans, and urgent alerts. We review literature on pothole, crack and leak detection, highlight recent advances in large vision language models such as QwenVL and LLaVA, and describe the design of our early prototype. Experimental evaluation on public datasets and captured CCTV clips demonstrates that the system accurately identifies diverse defects and produces coherent summaries. We conclude by discussing challenges and directions for scaling the system to city wide deployments.",
    "authors": [
      "Ibrahim Sheikh Mohamed",
      "Abdullah Yahya Abdullah Omaisan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.RO"
    ],
    "published": "2025-10-15T11:27:16Z",
    "pdf_url": "https://arxiv.org/pdf/2510.16017v1"
  },
  {
    "arxiv_id": "2510.13371v1",
    "entry_id": "http://arxiv.org/abs/2510.13371v1",
    "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive Recommendation",
    "summary": "Recent attempts to integrate large language models (LLMs) into recommender systems have gained momentum, but most remain limited to simple text generation or static prompt-based inference, failing to capture the complexity of user preferences and real-world interactions. This study proposes the Multi-Aspect Driven LLM Agent MADRec, an autonomous LLM-based recommender that constructs user and item profiles by unsupervised extraction of multi-aspect information from reviews and performs direct recommendation, sequential recommendation, and explanation generation. MADRec generates structured profiles via aspect-category-based summarization and applies Re-Ranking to construct high-density inputs. When the ground-truth item is missing from the output, the Self-Feedback mechanism dynamically adjusts the inference criteria. Experiments across multiple domains show that MADRec outperforms traditional and LLM-based baselines in both precision and explainability, with human evaluation further confirming the persuasiveness of the generated explanations.",
    "authors": [
      "Jiin Park",
      "Misuk Kim"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-10-15T10:03:29Z",
    "pdf_url": "https://arxiv.org/pdf/2510.13371v1"
  },
  {
    "arxiv_id": "2510.13366v1",
    "entry_id": "http://arxiv.org/abs/2510.13366v1",
    "title": "Document Intelligence in the Era of Large Language Models: A Survey",
    "summary": "Document AI (DAI) has emerged as a vital application area, and is significantly transformed by the advent of large language models (LLMs). While earlier approaches relied on encoder-decoder architectures, decoder-only LLMs have revolutionized DAI, bringing remarkable advancements in understanding and generation. This survey provides a comprehensive overview of DAI's evolution, highlighting current research attempts and future prospects of LLMs in this field. We explore key advancements and challenges in multimodal, multilingual, and retrieval-augmented DAI, while also suggesting future research directions, including agent-based approaches and document-specific foundation models. This paper aims to provide a structured analysis of the state-of-the-art in DAI and its implications for both academic and practical applications.",
    "authors": [
      "Weishi Wang",
      "Hengchang Hu",
      "Zhijie Zhang",
      "Zhaochen Li",
      "Hongxin Shao",
      "Daniel Dahlmeier"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-15T09:57:03Z",
    "pdf_url": "https://arxiv.org/pdf/2510.13366v1"
  },
  {
    "arxiv_id": "2510.12689v1",
    "entry_id": "http://arxiv.org/abs/2510.12689v1",
    "title": "From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLM",
    "summary": "Large language models (LLMs) have shown promising accuracy in predicting survey responses and policy preferences, which has increased interest in their potential to represent human interests in various domains. Most existing research has focused on behavioral cloning, effectively evaluating how well models reproduce individuals' expressed preferences. Drawing on theories of political representation, we highlight an underexplored design trade-off: whether AI systems should act as delegates, mirroring expressed preferences, or as trustees, exercising judgment about what best serves an individual's interests. This trade-off is closely related to issues of LLM sycophancy, where models can encourage behavior or validate beliefs that may be aligned with a user's short-term preferences, but is detrimental to their long-term interests. Through a series of experiments simulating votes on various policy issues in the U.S. context, we apply a temporal utility framework that weighs short and long-term interests (simulating a trustee role) and compare voting outcomes to behavior-cloning models (simulating a delegate). We find that trustee-style predictions weighted toward long-term interests produce policy decisions that align more closely with expert consensus on well-understood issues, but also show greater bias toward models' default stances on topics lacking clear agreement. These findings reveal a fundamental trade-off in designing AI systems to represent human interests. Delegate models better preserve user autonomy but may diverge from well-supported policy positions, while trustee models can promote welfare on well-understood issues yet risk paternalism and bias on subjective topics.",
    "authors": [
      "Suyash Fulay",
      "Jocelyn Zhu",
      "Michiel Bakker"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-10-14T16:24:19Z",
    "pdf_url": "https://arxiv.org/pdf/2510.12689v1"
  },
  {
    "arxiv_id": "2510.12608v1",
    "entry_id": "http://arxiv.org/abs/2510.12608v1",
    "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis",
    "summary": "With the increasing integration of large language models (LLMs) into open-domain writing, detecting machine-generated text has become a critical task for ensuring content authenticity and trust. Existing approaches rely on statistical discrepancies or model-specific heuristics to distinguish between LLM-generated and human-written text. However, these methods struggle in real-world scenarios due to limited generalization, vulnerability to paraphrasing, and lack of explainability, particularly when facing stylistic diversity or hybrid human-AI authorship. In this work, we propose StyleDecipher, a robust and explainable detection framework that revisits LLM-generated text detection using combined feature extractors to quantify stylistic differences. By jointly modeling discrete stylistic indicators and continuous stylistic representations derived from semantic embeddings, StyleDecipher captures distinctive style-level divergences between human and LLM outputs within a unified representation space. This framework enables accurate, explainable, and domain-agnostic detection without requiring access to model internals or labeled segments. Extensive experiments across five diverse domains, including news, code, essays, reviews, and academic abstracts, demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain accuracy. Moreover, in cross-domain evaluations, it surpasses existing baselines by up to 36.30%, while maintaining robustness against adversarial perturbations and mixed human-AI content. Further qualitative and quantitative analysis confirms that stylistic signals provide explainable evidence for distinguishing machine-generated text. Our source code can be accessed at https://github.com/SiyuanLi00/StyleDecipher.",
    "authors": [
      "Siyuan Li",
      "Aodu Wulianghai",
      "Xi Lin",
      "Guangyan Li",
      "Xiang Chen",
      "Jun Wu",
      "Jianhua Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-14T15:07:27Z",
    "pdf_url": "https://arxiv.org/pdf/2510.12608v1"
  },
  {
    "arxiv_id": "2510.12399v1",
    "entry_id": "http://arxiv.org/abs/2510.12399v1",
    "title": "A Survey of Vibe Coding with Large Language Models",
    "summary": "The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.",
    "authors": [
      "Yuyao Ge",
      "Lingrui Mei",
      "Zenghao Duan",
      "Tianhao Li",
      "Yujia Zheng",
      "Yiwei Wang",
      "Lexin Wang",
      "Jiayu Yao",
      "Tianyu Liu",
      "Yujun Cai",
      "Baolong Bi",
      "Fangda Guo",
      "Jiafeng Guo",
      "Shenghua Liu",
      "Xueqi Cheng"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-14T11:26:56Z",
    "pdf_url": "https://arxiv.org/pdf/2510.12399v1"
  },
  {
    "arxiv_id": "2510.12367v1",
    "entry_id": "http://arxiv.org/abs/2510.12367v1",
    "title": "LLM-REVal: Can We Trust LLM Reviewers Yet?",
    "summary": "The rapid advancement of large language models (LLMs) has inspired researchers to integrate them extensively into the academic workflow, potentially reshaping how research is practiced and reviewed. While previous studies highlight the potential of LLMs in supporting research and peer review, their dual roles in the academic workflow and the complex interplay between research and review bring new risks that remain largely underexplored. In this study, we focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation. This simulation incorporates a research agent, which generates papers and revises, alongside a review agent, which assesses the submissions. Based on the simulation results, we conduct human annotations and identify pronounced misalignment between LLM-based reviews and human judgments: (1) LLM reviewers systematically inflate scores for LLM-authored papers, assigning them markedly higher scores than human-authored ones; (2) LLM reviewers persistently underrate human-authored papers with critical statements (e.g., risk, fairness), even after multiple revisions. Our analysis reveals that these stem from two primary biases in LLM reviewers: a linguistic feature bias favoring LLM-generated writing styles, and an aversion toward critical statements. These results highlight the risks and equity concerns posed to human authors and academic research if LLMs are deployed in the peer review cycle without adequate caution. On the other hand, revisions guided by LLM reviews yield quality gains in both LLM-based and human evaluations, illustrating the potential of the LLMs-as-reviewers for early-stage researchers and enhancing low-quality papers.",
    "authors": [
      "Rui Li",
      "Jia-Chen Gu",
      "Po-Nien Kung",
      "Heming Xia",
      "Junfeng liu",
      "Xiangwen Kong",
      "Zhifang Sui",
      "Nanyun Peng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-14T10:30:20Z",
    "pdf_url": "https://arxiv.org/pdf/2510.12367v1"
  },
  {
    "arxiv_id": "2510.12253v1",
    "entry_id": "http://arxiv.org/abs/2510.12253v1",
    "title": "Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development",
    "summary": "Diffusion Models (DMs), as a leading class of generative models, offer key advantages for reinforcement learning (RL), including multi-modal expressiveness, stable training, and trajectory-level planning. This survey delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We first provide an overview of RL, highlighting its challenges, and then introduce the fundamental concepts of DMs, investigating how they are integrated into RL frameworks to address key challenges in this research field. We establish a dual-axis taxonomy that organizes the field along two orthogonal dimensions: a function-oriented taxonomy that clarifies the roles DMs play within the RL pipeline, and a technique-oriented taxonomy that situates implementations across online versus offline learning regimes. We also provide a comprehensive examination of this progression from single-agent to multi-agent domains, thereby forming several frameworks for DM-RL integration and highlighting their practical utility. Furthermore, we outline several categories of successful applications of diffusion-based RL across diverse domains, discuss open research issues of current methodologies, and highlight key directions for future research to advance the field. Finally, we summarize the survey to identify promising future development directions. We are actively maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for papers and other related resources to apply DMs for RL.",
    "authors": [
      "Changfu Xu",
      "Jianxiong Guo",
      "Yuzhu Liang",
      "Haiyang Huang",
      "Haodong Zou",
      "Xi Zheng",
      "Shui Yu",
      "Xiaowen Chu",
      "Jiannong Cao",
      "Tian Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-14T08:03:46Z",
    "pdf_url": "https://arxiv.org/pdf/2510.12253v1"
  },
  {
    "arxiv_id": "2510.12224v1",
    "entry_id": "http://arxiv.org/abs/2510.12224v1",
    "title": "MedKGEval: A Knowledge Graph-Based Multi-Turn Evaluation Framework for Open-Ended Patient Interactions with Clinical LLMs",
    "summary": "The reliable evaluation of large language models (LLMs) in medical applications remains an open challenge, particularly in capturing the complexity of multi-turn doctor-patient interactions that unfold in real clinical environments. Existing evaluation methods typically rely on post hoc review of full conversation transcripts, thereby neglecting the dynamic, context-sensitive nature of medical dialogues and the evolving informational needs of patients. In this work, we present MedKGEval, a novel multi-turn evaluation framework for clinical LLMs grounded in structured medical knowledge. Our approach introduces three key contributions: (1) a knowledge graph-driven patient simulation mechanism, where a dedicated control module retrieves relevant medical facts from a curated knowledge graph, thereby endowing the patient agent with human-like and realistic conversational behavior. This knowledge graph is constructed by integrating open-source resources with additional triples extracted from expert-annotated datasets; (2) an in-situ, turn-level evaluation framework, where each model response is assessed by a Judge Agent for clinical appropriateness, factual correctness, and safety as the dialogue progresses using a suite of fine-grained, task-specific metrics; (3) a comprehensive multi-turn benchmark of eight state-of-the-art LLMs, demonstrating MedKGEval's ability to identify subtle behavioral flaws and safety risks that are often overlooked by conventional evaluation pipelines. Although initially designed for Chinese and English medical applications, our framework can be readily extended to additional languages by switching the input knowledge graphs, ensuring seamless bilingual support and domain-specific applicability.",
    "authors": [
      "Yuechun Yu",
      "Han Ying",
      "Haoan Jin",
      "Wenjian Jiang",
      "Dong Xian",
      "Binghao Wang",
      "Zhou Yang",
      "Mengyue Wu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-14T07:22:26Z",
    "pdf_url": "https://arxiv.org/pdf/2510.12224v1"
  },
  {
    "arxiv_id": "2510.12178v1",
    "entry_id": "http://arxiv.org/abs/2510.12178v1",
    "title": "Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey",
    "summary": "This review surveys the rapid evolution of Meta AI's LLaMA (Large Language Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized parameter-efficient fine-tuning (PEFT) methods developed for these models. We first describe the LLaMA family of foundation models (7B-65B to 288B parameters), their architectures (including native multimodal and Mixtureof-Experts variants), and key performance characteristics. We then describe and discuss the concept of PEFT, which adapts large pre-trained models by updating only a small subset of parameters, and review five PEFT methods that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1 and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's mechanism, parameter savings, and example application to LLaMA (e.g., instruction tuning, multimodal tasks). We provide structured discussion and analysis of model and adapter architectures, parameter counts, and benchmark results (including examples where fine-tuned LLaMA models outperform larger baselines). Finally, we examine real-world use cases where LLaMA-based models and PEFT have been successfully applied (e.g., legal and medical domains), and we discuss ongoing challenges and future research directions (such as scaling to even larger contexts and improving robustness). This survey paper provides a one-stop resource for ML researchers and practitioners interested in LLaMA models and efficient fine-tuning strategies.",
    "authors": [
      "Abdulhady Abas Abdullah",
      "Arkaitz Zubiaga",
      "Seyedali Mirjalili",
      "Amir H. Gandomi",
      "Fatemeh Daneshfar",
      "Mohammadsadra Amini",
      "Alan Salam Mohammed",
      "Hadi Veisi"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-14T06:12:44Z",
    "pdf_url": "https://arxiv.org/pdf/2510.12178v1"
  },
  {
    "arxiv_id": "2510.13890v2",
    "entry_id": "http://arxiv.org/abs/2510.13890v2",
    "title": "A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness",
    "summary": "Large language models (LLMs) have achieved remarkable progress across domains and applications but face challenges such as high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), with compact, efficient, and adaptable features, offer promising solutions. Building on this potential, recent research explores collaborative frameworks that integrate their complementary strengths, leveraging SLMs' specialization and efficiency with LLMs' generalization and reasoning to address diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration from the perspective of collaboration objectives. We propose a taxonomy covering four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Under this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient and secure SLM-LLM collaboration. The collected papers are available at https://github.com/FairyFali/SLMs-Survey.",
    "authors": [
      "Fali Wang",
      "Jihai Chen",
      "Shuhua Yang",
      "Ali Al-Lawati",
      "Linli Tang",
      "Hui Liu",
      "Suhang Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-14T04:16:47Z",
    "pdf_url": "https://arxiv.org/pdf/2510.13890v2"
  },
  {
    "arxiv_id": "2510.11409v1",
    "entry_id": "http://arxiv.org/abs/2510.11409v1",
    "title": "Leveraging LLMs for Semi-Automatic Corpus Filtration in Systematic Literature Reviews",
    "summary": "The creation of systematic literature reviews (SLR) is critical for analyzing the landscape of a research field and guiding future research directions. However, retrieving and filtering the literature corpus for an SLR is highly time-consuming and requires extensive manual effort, as keyword-based searches in digital libraries often return numerous irrelevant publications. In this work, we propose a pipeline leveraging multiple large language models (LLMs), classifying papers based on descriptive prompts and deciding jointly using a consensus scheme. The entire process is human-supervised and interactively controlled via our open-source visual analytics web interface, LLMSurver, which enables real-time inspection and modification of model outputs. We evaluate our approach using ground-truth data from a recent SLR comprising over 8,000 candidate papers, benchmarking both open and commercial state-of-the-art LLMs from mid-2024 and fall 2025. Results demonstrate that our pipeline significantly reduces manual effort while achieving lower error rates than single human annotators. Furthermore, modern open-source models prove sufficient for this task, making the method accessible and cost-effective. Overall, our work demonstrates how responsible human-AI collaboration can accelerate and enhance systematic literature reviews within academic workflows.",
    "authors": [
      "Lucas Joos",
      "Daniel A. Keim",
      "Maximilian T. Fischer"
    ],
    "categories": [
      "cs.LG",
      "cs.DL",
      "cs.HC"
    ],
    "published": "2025-10-13T13:48:29Z",
    "pdf_url": "https://arxiv.org/pdf/2510.11409v1"
  },
  {
    "arxiv_id": "2510.11001v1",
    "entry_id": "http://arxiv.org/abs/2510.11001v1",
    "title": "DND: Boosting Large Language Models with Dynamic Nested Depth",
    "summary": "We introduce Dynamic Nested Depth (DND), a novel method that improves performance for off-the-shelf LLMs by selecting critical tokens to reprocess in a nested depth manner. Specifically, at the end of the given transformer layer, DND identifies more critical tokens with a router and feeds them back for an extra round of processing, effectively ``reviewing\" difficult tokens while avoiding redundant computation for easier ones. The dynamic selection mechanism is tailored for precise control via two novel strategies: a router controlling loss to enhance token selection distinguishability, and a threshold control scheme to ensure selection stability. We demonstrate the effectiveness of DND by directly integrating it into pre-trained dense and MoE models during a post-training phase. On diverse benchmarks, this approach boosts the performances of the dense Qwen3-1.7B by 1.88% and the MoE Qwen3-30B-A3B by 0.87%, all with a minimal parameter and computing increase.",
    "authors": [
      "Tieyuan Chen",
      "Xiaodong Chen",
      "Haoxing Chen",
      "Zhenzhong Lan",
      "Weiyao Lin",
      "Jianguo Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-13T04:22:57Z",
    "pdf_url": "https://arxiv.org/pdf/2510.11001v1"
  },
  {
    "arxiv_id": "2510.10991v1",
    "entry_id": "http://arxiv.org/abs/2510.10991v1",
    "title": "A Survey on Agentic Multimodal Large Language Models",
    "summary": "With the recent emergence of revolutionary autonomous agentic systems, research community is witnessing a significant shift from traditional static, passive, and domain-specific AI agents toward more dynamic, proactive, and generalizable agentic AI. Motivated by the growing interest in agentic AI and its potential trajectory toward AGI, we present a comprehensive survey on Agentic Multimodal Large Language Models (Agentic MLLMs). In this survey, we explore the emerging paradigm of agentic MLLMs, delineating their conceptual foundations and distinguishing characteristics from conventional MLLM-based agents. We establish a conceptual framework that organizes agentic MLLMs along three fundamental dimensions: (i) Agentic internal intelligence functions as the system's commander, enabling accurate long-horizon planning through reasoning, reflection, and memory; (ii) Agentic external tool invocation, whereby models proactively use various external tools to extend their problem-solving capabilities beyond their intrinsic knowledge; and (iii) Agentic environment interaction further situates models within virtual or physical environments, allowing them to take actions, adapt strategies, and sustain goal-directed behavior in dynamic real-world scenarios. To further accelerate research in this area for the community, we compile open-source training frameworks, training and evaluation datasets for developing agentic MLLMs. Finally, we review the downstream applications of agentic MLLMs and outline future research directions for this rapidly evolving field. To continuously track developments in this rapidly evolving field, we will also actively update a public repository at https://github.com/HJYao00/Awesome-Agentic-MLLMs.",
    "authors": [
      "Huanjin Yao",
      "Ruifei Zhang",
      "Jiaxing Huang",
      "Jingyi Zhang",
      "Yibo Wang",
      "Bo Fang",
      "Ruolin Zhu",
      "Yongcheng Jing",
      "Shunyu Liu",
      "Guanbin Li",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-13T04:07:01Z",
    "pdf_url": "https://arxiv.org/pdf/2510.10991v1"
  },
  {
    "arxiv_id": "2510.10819v1",
    "entry_id": "http://arxiv.org/abs/2510.10819v1",
    "title": "Generative AI and the Transformation of Software Development Practices",
    "summary": "Generative AI is reshaping how software is designed, written, and maintained. Advances in large language models (LLMs) are enabling new development styles - from chat-oriented programming and 'vibe coding' to agentic programming - that can accelerate productivity and broaden access. This paper examines how AI-assisted techniques are changing software engineering practice, and the related issues of trust, accountability, and shifting skills. We survey iterative chat-based development, multi-agent systems, dynamic prompt orchestration, and integration via the Model Context Protocol (MCP). Using case studies and industry data, we outline both the opportunities (faster cycles, democratized coding) and the challenges (model reliability and cost) of applying generative AI to coding. We describe new roles, skills, and best practices for using AI in a responsible and effective way.",
    "authors": [
      "Vivek Acharya"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-12T22:02:10Z",
    "pdf_url": "https://arxiv.org/pdf/2510.10819v1"
  },
  {
    "arxiv_id": "2510.10713v1",
    "entry_id": "http://arxiv.org/abs/2510.10713v1",
    "title": "Deep Learning in Astrophysics",
    "summary": "Deep learning has generated diverse perspectives in astronomy, with ongoing discussions between proponents and skeptics motivating this review. We examine how neural networks complement classical statistics, extending our data analytical toolkit for modern surveys. Astronomy offers unique opportunities through encoding physical symmetries, conservation laws, and differential equations directly into architectures, creating models that generalize beyond training data. Yet challenges persist as unlabeled observations number in billions while confirmed examples with known properties remain scarce and expensive. This review demonstrates how deep learning incorporates domain knowledge through architectural design, with built-in assumptions guiding models toward physically meaningful solutions. We evaluate where these methods offer genuine advances versus claims requiring careful scrutiny. - Neural architectures overcome trade-offs between scalability, expressivity, and data efficiency by encoding physical symmetries and conservation laws into network structure, enabling learning from limited labeled data. - Simulation-based inference and anomaly detection extract information from complex, non-Gaussian distributions where analytical likelihoods fail, enabling field-level cosmological analysis and systematic discovery of rare phenomena. - Multi-scale neural modeling bridges resolution gaps in astronomical simulations, learning effective subgrid physics from expensive high-fidelity runs to enhance large-volume calculations where direct computation remains prohibitive. - Emerging paradigms-reinforcement learning for telescope operations, foundation models learning from minimal examples, and large language model agents for research automation-show promise though are still developing in astronomical applications.",
    "authors": [
      "Yuan-Sen Ting"
    ],
    "categories": [
      "astro-ph.IM",
      "astro-ph.CO",
      "astro-ph.EP",
      "astro-ph.GA",
      "astro-ph.HE",
      "cs.AI"
    ],
    "published": "2025-10-12T17:31:46Z",
    "pdf_url": "https://arxiv.org/pdf/2510.10713v1"
  },
  {
    "arxiv_id": "2510.12831v1",
    "entry_id": "http://arxiv.org/abs/2510.12831v1",
    "title": "MTSQL-R1: Towards Long-Horizon Multi-Turn Text-to-SQL via Agentic Training",
    "summary": "Multi-turn Text-to-SQL aims to translate a user's conversational utterances into executable SQL while preserving dialogue coherence and grounding to the target schema. However, most existing systems only regard this task as a simple text translation task and follow a short-horizon paradigm, generating a query per turn without execution, explicit verification, and refinement, which leads to non-executable or incoherent outputs. We present MTSQL-R1, an agentic training framework for long-horizon multi-turn Text-to-SQL. We cast the task as a Markov Decision Process (MDP) in which an agent interacts with (i) a database for execution feedback and (ii) a persistent dialogue memory for coherence verification, performing an iterative propose to execute -> verify -> refine cycle until all checks pass. Experiments on COSQL and SPARC demonstrate that MTSQL-R1 consistently outperforms strong baselines, highlighting the importance of environment-driven verification and memory-guided refinement for conversational semantic parsing. Full recipes (including code, trained models, logs, reasoning trajectories, etc.) will be released after the internal review to contribute to community research.",
    "authors": [
      "Taicheng Guo",
      "Hai Wang",
      "ChaoChun Liu",
      "Mohsen Golalikhani",
      "Xin Chen",
      "Xiangliang Zhang",
      "Chandan K. Reddy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-10-12T16:12:05Z",
    "pdf_url": "https://arxiv.org/pdf/2510.12831v1"
  },
  {
    "arxiv_id": "2510.21758v3",
    "entry_id": "http://arxiv.org/abs/2510.21758v3",
    "title": "Taxonomy and Trends in Reinforcement Learning for Robotics and Control Systems: A Structured Review",
    "summary": "Reinforcement learning (RL) has become a foundational approach for enabling intelligent robotic behavior in dynamic and uncertain environments. This work presents an in-depth review of RL principles, advanced deep reinforcement learning (DRL) algorithms, and their integration into robotic and control systems. Beginning with the formalism of Markov Decision Processes (MDPs), the study outlines essential elements of the agent-environment interaction and explores core algorithmic strategies including actor-critic methods, value-based learning, and policy gradients. Emphasis is placed on modern DRL techniques such as DDPG, TD3, PPO, and SAC, which have shown promise in solving high-dimensional, continuous control tasks. A structured taxonomy is introduced to categorize RL applications across domains such as locomotion, manipulation, multi-agent coordination, and human-robot interaction, along with training methodologies and deployment readiness levels. The review synthesizes recent research efforts, highlighting technical trends, design patterns, and the growing maturity of RL in real-world robotics. Overall, this work aims to bridge theoretical advances with practical implementations, providing a consolidated perspective on the evolving role of RL in autonomous robotic systems.",
    "authors": [
      "Kumater Ter",
      "Ore-Ofe Ajayi",
      "Daniel Udekwe"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2025-10-11T20:16:32Z",
    "pdf_url": "https://arxiv.org/pdf/2510.21758v3"
  },
  {
    "arxiv_id": "2510.15961v1",
    "entry_id": "http://arxiv.org/abs/2510.15961v1",
    "title": "Interpretable Graph-Language Modeling for Detecting Youth Illicit Drug Use",
    "summary": "Illicit drug use among teenagers and young adults (TYAs) remains a pressing public health concern, with rising prevalence and long-term impacts on health and well-being. To detect illicit drug use among TYAs, researchers analyze large-scale surveys such as the Youth Risk Behavior Survey (YRBS) and the National Survey on Drug Use and Health (NSDUH), which preserve rich demographic, psychological, and environmental factors related to substance use. However, existing modeling methods treat survey variables independently, overlooking latent and interconnected structures among them. To address this limitation, we propose LAMI (LAtent relation Mining with bi-modal Interpretability), a novel joint graph-language modeling framework for detecting illicit drug use and interpreting behavioral risk factors among TYAs. LAMI represents individual responses as relational graphs, learns latent connections through a specialized graph structure learning layer, and integrates a large language model to generate natural language explanations grounded in both graph structures and survey semantics. Experiments on the YRBS and NSDUH datasets show that LAMI outperforms competitive baselines in predictive accuracy. Interpretability analyses further demonstrate that LAMI reveals meaningful behavioral substructures and psychosocial pathways, such as family dynamics, peer influence, and school-related distress, that align with established risk factors for substance use.",
    "authors": [
      "Yiyang Li",
      "Zehong Wang",
      "Zhengqing Yuan",
      "Zheyuan Zhang",
      "Keerthiram Murugesan",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-10-11T17:29:50Z",
    "pdf_url": "https://arxiv.org/pdf/2510.15961v1"
  },
  {
    "arxiv_id": "2510.10290v1",
    "entry_id": "http://arxiv.org/abs/2510.10290v1",
    "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in Enterprise Pipelines",
    "summary": "Automated code review adoption lags in compliance-heavy settings, where static analyzers produce high-volume, low-rationale outputs, and naive LLM use risks hallucination and incurring cost overhead. We present a production system for grounded, PR-native review that pairs static-analysis findings with AST-guided context extraction and a single-GPU, on-demand serving stack (quantized open-weight model, multi-tier caching) to deliver concise explanations and remediation guidance. Evaluated on safety-oriented C/C++ standards, the approach achieves sub-minute median first-feedback (offline p50 build+LLM 59.8s) while maintaining competitive violation reduction and lower violation rates versus larger proprietary models. The architecture is decoupled: teams can adopt the grounding/prompting layer or the serving layer independently. A small internal survey (n=8) provides directional signals of reduced triage effort and moderate perceived grounding, with participants reporting fewer human review iterations. We outline operational lessons and limitations, emphasizing reproducibility, auditability, and pathways to broader standards and assisted patching.",
    "authors": [
      "Sayan Mandal",
      "Hua Jiang"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2025-10-11T17:08:45Z",
    "pdf_url": "https://arxiv.org/pdf/2510.10290v1"
  },
  {
    "arxiv_id": "2510.13839v2",
    "entry_id": "http://arxiv.org/abs/2510.13839v2",
    "title": "Meronymic Ontology Extraction via Large Language Models",
    "summary": "Ontologies have become essential in today's digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully-automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.",
    "authors": [
      "Dekai Zhang",
      "Simone Conia",
      "Antonio Rago"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-11T11:54:38Z",
    "pdf_url": "https://arxiv.org/pdf/2510.13839v2"
  },
  {
    "arxiv_id": "2510.10182v1",
    "entry_id": "http://arxiv.org/abs/2510.10182v1",
    "title": "A Survey of Inductive Reasoning for Large Language Models",
    "summary": "Reasoning is an important task for large language models (LLMs). Among all the reasoning paradigms, inductive reasoning is one of the fundamental types, which is characterized by its particular-to-general thinking process and the non-uniqueness of its answers. The inductive mode is crucial for knowledge generalization and aligns better with human cognition, so it is a fundamental mode of learning, hence attracting increasing interest. Despite the importance of inductive reasoning, there is no systematic summary of it. Therefore, this paper presents the first comprehensive survey of inductive reasoning for LLMs. First, methods for improving inductive reasoning are categorized into three main areas: post-training, test-time scaling, and data augmentation. Then, current benchmarks of inductive reasoning are summarized, and a unified sandbox-based evaluation approach with the observation coverage metric is derived. Finally, we offer some analyses regarding the source of inductive ability and how simple model architectures and data help with inductive tasks, providing a solid foundation for future research.",
    "authors": [
      "Kedi Chen",
      "Dezhao Ruan",
      "Yuhao Dan",
      "Yaoting Wang",
      "Siyu Yan",
      "Xuecheng Wu",
      "Yinqi Zhang",
      "Qin Chen",
      "Jie Zhou",
      "Liang He",
      "Biqing Qi",
      "Linyang Li",
      "Qipeng Guo",
      "Xiaoming Shi",
      "Wei Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-11T11:45:38Z",
    "pdf_url": "https://arxiv.org/pdf/2510.10182v1"
  },
  {
    "arxiv_id": "2510.10161v1",
    "entry_id": "http://arxiv.org/abs/2510.10161v1",
    "title": "Large Language Model Sourcing: A Survey",
    "summary": "The rapid advancement of large language models (LLMs) has revolutionized artificial intelligence, shifting from supporting objective tasks (e.g., recognition) to empowering subjective decision-making (e.g., planning, decision). This marks the dawn of general and powerful AI, with applications spanning a wide range of fields, including programming, education, healthcare, finance, and law. However, their deployment introduces multifaceted risks. Due to the black-box nature of LLMs and the human-like quality of their generated content, issues such as hallucinations, bias, unfairness, and copyright infringement become particularly significant. In this context, sourcing information from multiple perspectives is essential.\n  This survey presents a systematic investigation into provenance tracking for content generated by LLMs, organized around four interrelated dimensions that together capture both model- and data-centric perspectives. From the model perspective, Model Sourcing treats the model as a whole, aiming to distinguish content generated by specific LLMs from content authored by humans. Model Structure Sourcing delves into the internal generative mechanisms, analyzing architectural components that shape the outputs of model. From the data perspective, Training Data Sourcing focuses on internal attribution, tracing the origins of generated content back to the training data of model. In contrast, External Data Sourcing emphasizes external validation, identifying external information used to support or influence the responses of model. Moreover, we also propose a dual-paradigm taxonomy that classifies existing sourcing methods into prior-based (proactive traceability embedding) and posterior-based (retrospective inference) approaches. Traceability across these dimensions enhances the transparency, accountability, and trustworthiness of LLMs deployment in real-world applications.",
    "authors": [
      "Liang Pang",
      "Kangxi Wu",
      "Sunhao Dai",
      "Zihao Wei",
      "Zenghao Duan",
      "Jia Gu",
      "Xiang Li",
      "Zhiyi Yin",
      "Jun Xu",
      "Huawei Shen",
      "Xueqi Cheng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-11T10:52:30Z",
    "pdf_url": "https://arxiv.org/pdf/2510.10161v1"
  },
  {
    "arxiv_id": "2510.10079v1",
    "entry_id": "http://arxiv.org/abs/2510.10079v1",
    "title": "How AI Companionship Develops: Evidence from a Longitudinal Study",
    "summary": "The quickly growing popularity of AI companions poses risks to mental health, personal wellbeing, and social relationships. Past work has identified many individual factors that can drive human-companion interaction, but we know little about how these factors interact and evolve over time. In Study 1, we surveyed AI companion users (N = 303) to map the psychological pathway from users' mental models of the agent to parasocial experiences, social interaction, and the psychological impact of AI companions. Participants' responses foregrounded multiple interconnected variables (agency, parasocial interaction, and engagement) that shape AI companionship. In Study 2, we conducted a longitudinal study with a subset of participants (N = 110) using a new generic chatbot. Participants' perceptions of the generic chatbot significantly converged to perceptions of their own companions by Week 3. These results suggest a longitudinal model of AI companionship development and demonstrate an empirical method to study human-AI companionship.",
    "authors": [
      "Angel Hsing-Chi Hwang",
      "Fiona Li",
      "Jacy Reese Anthis",
      "Hayoun Noh"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-10-11T07:36:47Z",
    "pdf_url": "https://arxiv.org/pdf/2510.10079v1"
  },
  {
    "arxiv_id": "2510.10010v1",
    "entry_id": "http://arxiv.org/abs/2510.10010v1",
    "title": "SLEAN: Simple Lightweight Ensemble Analysis Network for Multi-Provider LLM Coordination: Design, Implementation, and Vibe Coding Bug Investigation Case Study",
    "summary": "We present SLEAN (Simple Lightweight Ensemble Analysis Network), a deterministic framework for coordinating multiple LLM providers through text-based prompt orchestration. Unlike complex multi-agent systems requiring specialized infrastructure, SLEAN operates as a simple prompt bridge between LLMs using .txt templates, requiring no deep technical knowledge for deployment. The three-phase protocol formed by independent analysis, cross-critique, and arbitration, filters harmful AI-generated code suggestions before production deployment, addressing how AI-assisted debugging increasingly produces modifications that introduce unnecessary complexity, break existing functionality, or address problems. Evaluating 15 software bugs, we analyzed 69 AI-generated fix propositions. SLEAN's filtering accepted 22 fixes (31.9%, 95% CI 20.9-42.9%) while rejecting 47 that would have been harmful if applied verbatim. The arbitration process reduced code change surface by 83-90% relative to raw AI outputs, enforcing minimal causal edits over scope-expanding modifications. Minimal Type 2 inputs proved more efficient than detailed Type 1 inputs, requiring 2.85 versus 3.56 propositions per accepted fix (35.1% versus 28.1% acceptance, about a 20% efficiency gain). Agreement between AI systems showed weak correlation with fix quality: high convergence (at least 80%) occurred in 4 of 15 cases and improved acceptance by only 2.4% points; arbitration appeared only at exactly 10% convergence in 2 of 15 cases, although low convergence alone did not necessitate arbitration. The file-driven, provider-agnostic architecture enables deployment without specialized coding expertise, making it applicable to security auditing, code review, document verification, and other domains requiring reliable multi-provider synthesis with end-to-end auditability.",
    "authors": [
      "Matheus J. T. Vargas"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-11T04:24:04Z",
    "pdf_url": "https://arxiv.org/pdf/2510.10010v1"
  },
  {
    "arxiv_id": "2510.09907v1",
    "entry_id": "http://arxiv.org/abs/2510.09907v1",
    "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
    "summary": "Property-based testing (PBT) is a lightweight formal method, typically implemented as a randomized testing framework. Users specify the input domain for their test using combinators supplied by the PBT framework, and the expected properties or invariants as a unit-test function. The framework then searches for a counterexample, e.g. by generating inputs and calling the test function. In this work, we demonstrate an LLM-based agent which analyzes Python modules, infers function-specific and cross-function properties from code and documentation, synthesizes and executes PBTs, reflects on outputs of these tests to confirm true bugs, and finally outputs actionable bug reports for the developer. We perform an extensive evaluation of our agent across 100 popular Python packages. Of the bug reports generated by the agent, we found after manual review that 56\\% were valid bugs and 32\\% were valid bugs that we would report to maintainers. We then developed a ranking rubric to surface high-priority valid bugs to developers, and found that of the 21 top-scoring bugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure modes from serialization failures to numerical precision errors to flawed cache implementations. We reported 5 bugs, 4 with patches, including to NumPy and cloud computing SDKs, with 3 patches merged successfully. Our results suggest that LLMs with PBT provides a rigorous and scalable method for autonomously testing software. Our code and artifacts are available at: https://github.com/mmaaz-git/agentic-pbt.",
    "authors": [
      "Muhammad Maaz",
      "Liam DeVoe",
      "Zac Hatfield-Dodds",
      "Nicholas Carlini"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-10T22:43:54Z",
    "pdf_url": "https://arxiv.org/pdf/2510.09907v1"
  },
  {
    "arxiv_id": "2510.09869v2",
    "entry_id": "http://arxiv.org/abs/2510.09869v2",
    "title": "NarraBench: A Comprehensive Framework for Narrative Benchmarking",
    "summary": "We present NarraBench, a theory-informed taxonomy of narrative-understanding tasks, as well as an associated survey of 78 existing benchmarks in the area. We find significant need for new evaluations covering aspects of narrative understanding that are either overlooked in current work or are poorly aligned with existing metrics. Specifically, we estimate that only 27% of narrative tasks are well captured by existing benchmarks, and we note that some areas -- including narrative events, style, perspective, and revelation -- are nearly absent from current evaluations. We also note the need for increased development of benchmarks capable of assessing constitutively subjective and perspectival aspects of narrative, that is, aspects for which there is generally no single correct answer. Our taxonomy, survey, and methodology are of value to NLP researchers seeking to test LLM narrative understanding.",
    "authors": [
      "Sil Hamilton",
      "Matthew Wilkens",
      "Andrew Piper"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-10T21:04:31Z",
    "pdf_url": "https://arxiv.org/pdf/2510.09869v2"
  },
  {
    "arxiv_id": "2510.09404v2",
    "entry_id": "http://arxiv.org/abs/2510.09404v2",
    "title": "Agentic Systems in Radiology: Design, Applications, Evaluation, and Challenges",
    "summary": "Building agents, systems that perceive and act upon their environment with a degree of autonomy, has long been a focus of AI research. This pursuit has recently become vastly more practical with the emergence of large language models (LLMs) capable of using natural language to integrate information, follow instructions, and perform forms of \"reasoning\" and planning across a wide range of tasks. With its multimodal data streams and orchestrated workflows spanning multiple systems, radiology is uniquely suited to benefit from agents that can adapt to context and automate repetitive yet complex tasks. In radiology, LLMs and their multimodal variants have already demonstrated promising performance for individual tasks such as information extraction and report summarization. However, using LLMs in isolation underutilizes their potential to support complex, multi-step workflows where decisions depend on evolving context from multiple information sources. Equipping LLMs with external tools and feedback mechanisms enables them to drive systems that exhibit a spectrum of autonomy, ranging from semi-automated workflows to more adaptive agents capable of managing complex processes. This review examines the design of such LLM-driven agentic systems, highlights key applications, discusses evaluation methods for planning and tool use, and outlines challenges such as error cascades, tool-use efficiency, and health IT integration.",
    "authors": [
      "Christian Bluethgen",
      "Dave Van Veen",
      "Daniel Truhn",
      "Jakob Nikolas Kather",
      "Michael Moor",
      "Malgorzata Polacin",
      "Akshay Chaudhari",
      "Thomas Frauenfelder",
      "Curtis P. Langlotz",
      "Michael Krauthammer",
      "Farhad Nooralahzadeh"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-10T13:56:27Z",
    "pdf_url": "https://arxiv.org/pdf/2510.09404v2"
  },
  {
    "arxiv_id": "2510.09244v1",
    "entry_id": "http://arxiv.org/abs/2510.09244v1",
    "title": "Fundamentals of Building Autonomous LLM Agents",
    "summary": "This paper reviews the architecture and implementation methods of agents powered by large language models (LLMs). Motivated by the limitations of traditional LLMs in real-world tasks, the research aims to explore patterns to develop \"agentic\" LLMs that can automate complex tasks and bridge the performance gap with human capabilities. Key components include a perception system that converts environmental percepts into meaningful representations; a reasoning system that formulates plans, adapts to feedback, and evaluates actions through different techniques like Chain-of-Thought and Tree-of-Thought; a memory system that retains knowledge through both short-term and long-term mechanisms; and an execution system that translates internal decisions into concrete actions. This paper shows how integrating these systems leads to more capable and generalized software bots that mimic human cognitive processes for autonomous and intelligent behavior.",
    "authors": [
      "Victor de Lamo Castrillo",
      "Habtom Kahsay Gidey",
      "Alexander Lenz",
      "Alois Knoll"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-10T10:32:39Z",
    "pdf_url": "https://arxiv.org/pdf/2510.09244v1"
  },
  {
    "arxiv_id": "2510.09228v1",
    "entry_id": "http://arxiv.org/abs/2510.09228v1",
    "title": "Clear Roads, Clear Vision: Advancements in Multi-Weather Restoration for Smart Transportation",
    "summary": "Adverse weather conditions such as haze, rain, and snow significantly degrade the quality of images and videos, posing serious challenges to intelligent transportation systems (ITS) that rely on visual input. These degradations affect critical applications including autonomous driving, traffic monitoring, and surveillance. This survey presents a comprehensive review of image and video restoration techniques developed to mitigate weather-induced visual impairments. We categorize existing approaches into traditional prior-based methods and modern data-driven models, including CNNs, transformers, diffusion models, and emerging vision-language models (VLMs). Restoration strategies are further classified based on their scope: single-task models, multi-task/multi-weather systems, and all-in-one frameworks capable of handling diverse degradations. In addition, we discuss day and night time restoration challenges, benchmark datasets, and evaluation protocols. The survey concludes with an in-depth discussion on limitations in current research and outlines future directions such as mixed/compound-degradation restoration, real-time deployment, and agentic AI frameworks. This work aims to serve as a valuable reference for advancing weather-resilient vision systems in smart transportation environments. Lastly, to stay current with rapid advancements in this field, we will maintain regular updates of the latest relevant papers and their open-source implementations at https://github.com/ChaudharyUPES/A-comprehensive-review-on-Multi-weather-restoration",
    "authors": [
      "Vijay M. Galshetwar",
      "Praful Hambarde",
      "Prashant W. Patil",
      "Akshay Dudhane",
      "Sachin Chaudhary",
      "Santosh Kumar Vipparathi",
      "Subrahmanyam Murala"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-10-10T10:15:59Z",
    "pdf_url": "https://arxiv.org/pdf/2510.09228v1"
  },
  {
    "arxiv_id": "2510.09043v2",
    "entry_id": "http://arxiv.org/abs/2510.09043v2",
    "title": "Humanoid Artificial Consciousness Designed with Large Language Model Based on Psychoanalysis and Personality Theory",
    "summary": "Human consciousness is still a concept hard to define with current scientific understanding. Although Large Language Models (LLMs) have recently demonstrated significant advancements across various domains including translation and summarization, human consciousness is not something to imitate with current upfront technology owing to so-called hallucination. This study, therefore, proposes a novel approach to address these challenges by integrating psychoanalysis and the Myers-Briggs Type Indicator (MBTI) into constructing consciousness and personality modules. We developed three artificial consciousnesses (self-awareness, unconsciousness, and preconsciousness) based on the principles of psychoanalysis. Additionally, we designed 16 characters with different personalities representing the sixteen MBTI types, with several attributes such as needs, status, and memories. To determine if our model's artificial consciousness exhibits human-like cognition, we created ten distinct situations considering seven attributes such as emotional understanding and logical thinking. The decision-making process of artificial consciousness and the final action were evaluated in three ways: survey evaluation, three-tier classification via ChatGPT, and qualitative review. Both quantitative and qualitative analyses indicated a high likelihood of well-simulated consciousness, although the difference in response between different characters and consciousnesses was not very significant. This implies that the developed models incorporating elements of psychoanalysis and personality theory can lead to building a more intuitive and adaptable AI system with humanoid consciousness. Therefore, this study contributes to opening up new avenues for improving AI interactions in complex cognitive contexts.",
    "authors": [
      "Sang Hun Kim",
      "Jongmin Lee",
      "Dongkyu Park",
      "So Young Lee",
      "Yosep Chong"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-10T06:23:50Z",
    "pdf_url": "https://arxiv.org/pdf/2510.09043v2"
  },
  {
    "arxiv_id": "2510.08962v1",
    "entry_id": "http://arxiv.org/abs/2510.08962v1",
    "title": "Analytical Survey of Learning with Low-Resource Data: From Analysis to Investigation",
    "summary": "Learning with high-resource data has demonstrated substantial success in artificial intelligence (AI); however, the costs associated with data annotation and model training remain significant. A fundamental objective of AI research is to achieve robust generalization with limited-resource data. This survey employs agnostic active sampling theory within the Probably Approximately Correct (PAC) framework to analyze the generalization error and label complexity associated with learning from low-resource data in both model-agnostic supervised and unsupervised settings. Based on this analysis, we investigate a suite of optimization strategies tailored for low-resource data learning, including gradient-informed optimization, meta-iteration optimization, geometry-aware optimization, and LLMs-powered optimization. Furthermore, we provide a comprehensive overview of multiple learning paradigms that can benefit from low-resource data, including domain transfer, reinforcement feedback, and hierarchical structure modeling. Finally, we conclude our analysis and investigation by summarizing the key findings and highlighting their implications for learning with low-resource data.",
    "authors": [
      "Xiaofeng Cao",
      "Mingwei Xu",
      "Xin Yu",
      "Jiangchao Yao",
      "Wei Ye",
      "Shengjun Huang",
      "Minling Zhang",
      "Ivor W. Tsang",
      "Yew Soon Ong",
      "James T. Kwok",
      "Heng Tao Shen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-10-10T03:15:42Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08962v1"
  },
  {
    "arxiv_id": "2510.08867v1",
    "entry_id": "http://arxiv.org/abs/2510.08867v1",
    "title": "ReviewerToo: Should AI Join The Program Committee? A Look At The Future of Peer Review",
    "summary": "Peer review is the cornerstone of scientific publishing, yet it suffers from inconsistencies, reviewer subjectivity, and scalability challenges. We introduce ReviewerToo, a modular framework for studying and deploying AI-assisted peer review to complement human judgment with systematic and consistent assessments. ReviewerToo supports systematic experiments with specialized reviewer personas and structured evaluation criteria, and can be partially or fully integrated into real conference workflows. We validate ReviewerToo on a carefully curated dataset of 1,963 paper submissions from ICLR 2025, where our experiments with the gpt-oss-120b model achieves 81.8% accuracy for the task of categorizing a paper as accept/reject compared to 83.9% for the average human reviewer. Additionally, ReviewerToo-generated reviews are rated as higher quality than the human average by an LLM judge, though still trailing the strongest expert contributions. Our analysis highlights domains where AI reviewers excel (e.g., fact-checking, literature coverage) and where they struggle (e.g., assessing methodological novelty and theoretical contributions), underscoring the continued need for human expertise. Based on these findings, we propose guidelines for integrating AI into peer-review pipelines, showing how AI can enhance consistency, coverage, and fairness while leaving complex evaluative judgments to domain experts. Our work provides a foundation for systematic, hybrid peer-review systems that scale with the growth of scientific publishing.",
    "authors": [
      "Gaurav Sahu",
      "Hugo Larochelle",
      "Laurent Charlin",
      "Christopher Pal"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-09T23:53:19Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08867v1"
  },
  {
    "arxiv_id": "2510.13826v1",
    "entry_id": "http://arxiv.org/abs/2510.13826v1",
    "title": "Towards Neurocognitive-Inspired Intelligence: From AI's Structural Mimicry to Human-Like Functional Cognition",
    "summary": "Artificial intelligence has advanced significantly through deep learning, reinforcement learning, and large language and vision models. However, these systems often remain task specific, struggle to adapt to changing conditions, and cannot generalize in ways similar to human cognition. Additionally, they mainly focus on mimicking brain structures, which often leads to black-box models with limited transparency and adaptability. Inspired by the structure and function of biological cognition, this paper introduces the concept of \"Neurocognitive-Inspired Intelligence (NII),\" a hybrid approach that combines neuroscience, cognitive science, computer vision, and AI to develop more general, adaptive, and robust intelligent systems capable of rapid learning, learning from less data, and leveraging prior experience. These systems aim to emulate the human brain's ability to flexibly learn, reason, remember, perceive, and act in real-world settings with minimal supervision. We review the limitations of current AI methods, define core principles of neurocognitive-inspired intelligence, and propose a modular, biologically inspired architecture that emphasizes integration, embodiment, and adaptability. We also discuss potential implementation strategies and outline various real-world applications, from robotics to education and healthcare. Importantly, this paper offers a hybrid roadmap for future research, laying the groundwork for building AI systems that more closely resemble human cognition.",
    "authors": [
      "Noorbakhsh Amiri Golilarz",
      "Hassan S. Al Khatib",
      "Shahram Rahimi"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "published": "2025-10-09T20:10:55Z",
    "pdf_url": "https://arxiv.org/pdf/2510.13826v1"
  },
  {
    "arxiv_id": "2510.08758v1",
    "entry_id": "http://arxiv.org/abs/2510.08758v1",
    "title": "A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?",
    "summary": "Many social science questions ask how linguistic properties causally affect an audience's attitudes and behaviors. Because text properties are often interlinked (e.g., angry reviews use profane language), we must control for possible latent confounding to isolate causal effects. Recent literature proposes adapting large language models (LLMs) to learn latent representations of text that successfully predict both treatment and the outcome. However, because the treatment is a component of the text, these deep learning methods risk learning representations that actually encode the treatment itself, inducing overlap bias. Rather than depending on post-hoc adjustments, we introduce a new experimental design that handles latent confounding, avoids the overlap issue, and unbiasedly estimates treatment effects. We apply this design in an experiment evaluating the persuasiveness of expressing humility in political communication. Methodologically, we demonstrate that LLM-based methods perform worse than even simple bag-of-words models using our real text and outcomes from our experiment. Substantively, we isolate the causal effect of expressing humility on the perceived persuasiveness of political statements, offering new insights on communication effects for social media platforms, policy makers, and social scientists.",
    "authors": [
      "Graham Tierney",
      "Srikar Katta",
      "Christopher Bail",
      "Sunshine Hillygus",
      "Alexander Volfovsky"
    ],
    "categories": [
      "stat.ME",
      "cs.CL",
      "cs.LG",
      "stat.AP"
    ],
    "published": "2025-10-09T19:17:57Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08758v1"
  },
  {
    "arxiv_id": "2510.08338v3",
    "entry_id": "http://arxiv.org/abs/2510.08338v3",
    "title": "LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation of Likert Ratings",
    "summary": "Consumer research costs companies billions annually yet suffers from panel biases and limited scale. Large language models (LLMs) offer an alternative by simulating synthetic consumers, but produce unrealistic response distributions when asked directly for numerical ratings. We present semantic similarity rating (SSR), a method that elicits textual responses from LLMs and maps these to Likert distributions using embedding similarity to reference statements. Testing on an extensive dataset comprising 57 personal care product surveys conducted by a leading corporation in that market (9,300 human responses), SSR achieves 90% of human test-retest reliability while maintaining realistic response distributions (KS similarity > 0.85). Additionally, these synthetic respondents provide rich qualitative feedback explaining their ratings. This framework enables scalable consumer research simulations while preserving traditional survey metrics and interpretability.",
    "authors": [
      "Benjamin F. Maier",
      "Ulf Aslak",
      "Luca Fiaschi",
      "Nina Rismal",
      "Kemble Fletcher",
      "Christian C. Luhmann",
      "Robbie Dow",
      "Kli Pappas",
      "Thomas V. Wiecki"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-09T15:24:48Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08338v3"
  },
  {
    "arxiv_id": "2510.08202v1",
    "entry_id": "http://arxiv.org/abs/2510.08202v1",
    "title": "Sentiment Matters: An Analysis of 200 Human-SAV Interactions",
    "summary": "Shared Autonomous Vehicles (SAVs) are likely to become an important part of the transportation system, making effective human-SAV interactions an important area of research. This paper introduces a dataset of 200 human-SAV interactions to further this area of study. We present an open-source human-SAV conversational dataset, comprising both textual data (e.g., 2,136 human-SAV exchanges) and empirical data (e.g., post-interaction survey results on a range of psychological factors). The dataset's utility is demonstrated through two benchmark case studies: First, using random forest modeling and chord diagrams, we identify key predictors of SAV acceptance and perceived service quality, highlighting the critical influence of response sentiment polarity (i.e., perceived positivity). Second, we benchmark the performance of an LLM-based sentiment analysis tool against the traditional lexicon-based TextBlob method. Results indicate that even simple zero-shot LLM prompts more closely align with user-reported sentiment, though limitations remain. This study provides novel insights for designing conversational SAV interfaces and establishes a foundation for further exploration into advanced sentiment modeling, adaptive user interactions, and multimodal conversational systems.",
    "authors": [
      "Lirui Guo",
      "Michael G. Burke",
      "Wynita M. Griggs"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.ET"
    ],
    "published": "2025-10-09T13:30:23Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08202v1"
  },
  {
    "arxiv_id": "2510.08081v1",
    "entry_id": "http://arxiv.org/abs/2510.08081v1",
    "title": "AutoQual: An LLM Agent for Automated Discovery of Interpretable Features for Review Quality Assessment",
    "summary": "Ranking online reviews by their intrinsic quality is a critical task for e-commerce platforms and information services, impacting user experience and business outcomes. However, quality is a domain-dependent and dynamic concept, making its assessment a formidable challenge. Traditional methods relying on hand-crafted features are unscalable across domains and fail to adapt to evolving content patterns, while modern deep learning approaches often produce black-box models that lack interpretability and may prioritize semantics over quality. To address these challenges, we propose AutoQual, an LLM-based agent framework that automates the discovery of interpretable features. While demonstrated on review quality assessment, AutoQual is designed as a general framework for transforming tacit knowledge embedded in data into explicit, computable features. It mimics a human research process, iteratively generating feature hypotheses through reflection, operationalizing them via autonomous tool implementation, and accumulating experience in a persistent memory. We deploy our method on a large-scale online platform with a billion-level user base. Large-scale A/B testing confirms its effectiveness, increasing average reviews viewed per user by 0.79% and the conversion rate of review readers by 0.27%.",
    "authors": [
      "Xiaochong Lan",
      "Jie Feng",
      "Yinxing Liu",
      "Xinlei Shi",
      "Yong Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-09T11:11:02Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08081v1"
  },
  {
    "arxiv_id": "2510.08049v2",
    "entry_id": "http://arxiv.org/abs/2510.08049v2",
    "title": "A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models",
    "summary": "Although Large Language Models (LLMs) exhibit advanced reasoning ability, conventional alignment remains largely dominated by outcome reward models (ORMs) that judge only final answers. Process Reward Models(PRMs) address this gap by evaluating and guiding reasoning at the step or trajectory level. This survey provides a systematic overview of PRMs through the full loop: how to generate process data, build PRMs, and use PRMs for test-time scaling and reinforcement learning. We summarize applications across math, code, text, multimodal reasoning, robotics, and agents, and review emerging benchmarks. Our goal is to clarify design spaces, reveal open challenges, and guide future research toward fine-grained, robust reasoning alignment.",
    "authors": [
      "Congming Zheng",
      "Jiachen Zhu",
      "Zhuoying Ou",
      "Yuxiang Chen",
      "Kangning Zhang",
      "Rong Shan",
      "Zeyu Zheng",
      "Mengyue Yang",
      "Jianghao Lin",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-09T10:35:31Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08049v2"
  },
  {
    "arxiv_id": "2510.09686v1",
    "entry_id": "http://arxiv.org/abs/2510.09686v1",
    "title": "Stop DDoS Attacking the Research Community with AI-Generated Survey Papers",
    "summary": "Survey papers are foundational to the scholarly progress of research communities, offering structured overviews that guide both novices and experts across disciplines. However, the recent surge of AI-generated surveys, especially enabled by large language models (LLMs), has transformed this traditionally labor-intensive genre into a low-effort, high-volume output. While such automation lowers entry barriers, it also introduces a critical threat: the phenomenon we term the \"survey paper DDoS attack\" to the research community. This refers to the unchecked proliferation of superficially comprehensive but often redundant, low-quality, or even hallucinated survey manuscripts, which floods preprint platforms, overwhelms researchers, and erodes trust in the scientific record. In this position paper, we argue that we must stop uploading massive amounts of AI-generated survey papers (i.e., survey paper DDoS attack) to the research community, by instituting strong norms for AI-assisted review writing. We call for restoring expert oversight and transparency in AI usage and, moreover, developing new infrastructures such as Dynamic Live Surveys, community-maintained, version-controlled repositories that blend automated updates with human curation. Through quantitative trend analysis, quality audits, and cultural impact discussion, we show that safeguarding the integrity of surveys is no longer optional but imperative to the research community.",
    "authors": [
      "Jianghao Lin",
      "Rong Shan",
      "Jiachen Zhu",
      "Yunjia Xi",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2025-10-09T05:55:13Z",
    "pdf_url": "https://arxiv.org/pdf/2510.09686v1"
  },
  {
    "arxiv_id": "2510.07793v2",
    "entry_id": "http://arxiv.org/abs/2510.07793v2",
    "title": "LLM4Cell: A Survey of Large Language and Agentic Models for Single-Cell Biology",
    "summary": "Large language models (LLMs) and emerging agentic frameworks are beginning to transform single-cell biology by enabling natural-language reasoning, generative annotation, and multimodal data integration. However, progress remains fragmented across data modalities, architectures, and evaluation standards. LLM4Cell presents the first unified survey of 58 foundation and agentic models developed for single-cell research, spanning RNA, ATAC, multi-omic, and spatial modalities. We categorize these methods into five families-foundation, text-bridge, spatial, multimodal, epigenomic, and agentic-and map them to eight key analytical tasks including annotation, trajectory and perturbation modeling, and drug-response prediction. Drawing on over 40 public datasets, we analyze benchmark suitability, data diversity, and ethical or scalability constraints, and evaluate models across 10 domain dimensions covering biological grounding, multi-omics alignment, fairness, privacy, and explainability. By linking datasets, models, and evaluation domains, LLM4Cell provides the first integrated view of language-driven single-cell intelligence and outlines open challenges in interpretability, standardization, and trustworthy model development.",
    "authors": [
      "Sajib Acharjee Dip",
      "Adrika Zafor",
      "Bikash Kumar Paul",
      "Uddip Acharjee Shuvo",
      "Muhit Islam Emon",
      "Xuan Wang",
      "Liqing Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-09T05:12:09Z",
    "pdf_url": "https://arxiv.org/pdf/2510.07793v2"
  },
  {
    "arxiv_id": "2510.07768v1",
    "entry_id": "http://arxiv.org/abs/2510.07768v1",
    "title": "ToolLibGen: Scalable Automatic Tool Creation and Aggregation for LLM Reasoning",
    "summary": "Large Language Models (LLMs) equipped with external tools have demonstrated enhanced performance on complex reasoning tasks. The widespread adoption of this tool-augmented reasoning is hindered by the scarcity of domain-specific tools. For instance, in domains such as physics question answering, suitable and specialized tools are often missing. Recent work has explored automating tool creation by extracting reusable functions from Chain-of-Thought (CoT) reasoning traces; however, these approaches face a critical scalability bottleneck. As the number of generated tools grows, storing them in an unstructured collection leads to significant retrieval challenges, including an expanding search space and ambiguity between function-related tools. To address this, we propose a systematic approach to automatically refactor an unstructured collection of tools into a structured tool library. Our system first generates discrete, task-specific tools and clusters them into semantically coherent topics. Within each cluster, we introduce a multi-agent framework to consolidate scattered functionalities: a code agent refactors code to extract shared logic and creates versatile, aggregated tools, while a reviewing agent ensures that these aggregated tools maintain the complete functional capabilities of the original set. This process transforms numerous question-specific tools into a smaller set of powerful, aggregated tools without loss of functionality. Experimental results demonstrate that our approach significantly improves tool retrieval accuracy and overall reasoning performance across multiple reasoning tasks. Furthermore, our method shows enhanced scalability compared with baselines as the number of question-specific increases.",
    "authors": [
      "Murong Yue",
      "Zhiwei Liu",
      "Liangwei Yang",
      "Jianguo Zhang",
      "Zuxin Liu",
      "Haolin Chen",
      "Ziyu Yao",
      "Silvio Savarese",
      "Caiming Xiong",
      "Shelby Heinecke",
      "Huan Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-09T04:11:16Z",
    "pdf_url": "https://arxiv.org/pdf/2510.07768v1"
  },
  {
    "arxiv_id": "2510.07733v2",
    "entry_id": "http://arxiv.org/abs/2510.07733v2",
    "title": "SurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation",
    "summary": "Large language models (LLMs) are increasingly adopted for automating survey paper generation \\cite{wang2406autosurvey, liang2025surveyx, yan2025surveyforge,su2025benchmarking,wen2025interactivesurvey}. Existing approaches typically extract content from a large collection of related papers and prompt LLMs to summarize them directly. However, such methods often overlook the structural relationships among papers, resulting in generated surveys that lack a coherent taxonomy and a deeper contextual understanding of research progress. To address these shortcomings, we propose \\textbf{SurveyG}, an LLM-based agent framework that integrates \\textit{hierarchical citation graph}, where nodes denote research papers and edges capture both citation dependencies and semantic relatedness between their contents, thereby embedding structural and contextual knowledge into the survey generation process. The graph is organized into three layers: \\textbf{Foundation}, \\textbf{Development}, and \\textbf{Frontier}, to capture the evolution of research from seminal works to incremental advances and emerging directions. By combining horizontal search within layers and vertical depth traversal across layers, the agent produces multi-level summaries, which are consolidated into a structured survey outline. A multi-agent validation stage then ensures consistency, coverage, and factual accuracy in generating the final survey. Experiments, including evaluations by human experts and LLM-as-a-judge, demonstrate that SurveyG outperforms state-of-the-art frameworks, producing surveys that are more comprehensive and better structured to the underlying knowledge taxonomy of a field.",
    "authors": [
      "Minh-Anh Nguye",
      "Minh-Duc Nguyen",
      "Ha Lan N. T.",
      "Kieu Hai Dang",
      "Nguyen Tien Dong",
      "Dung D. Le"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-09T03:14:20Z",
    "pdf_url": "https://arxiv.org/pdf/2510.07733v2"
  },
  {
    "arxiv_id": "2510.07706v1",
    "entry_id": "http://arxiv.org/abs/2510.07706v1",
    "title": "Large Language Models Meet Virtual Cell: A Survey",
    "summary": "Large language models (LLMs) are transforming cellular biology by enabling the development of \"virtual cells\"--computational systems that represent, predict, and reason about cellular states and behaviors. This work provides a comprehensive review of LLMs for virtual cell modeling. We propose a unified taxonomy that organizes existing methods into two paradigms: LLMs as Oracles, for direct cellular modeling, and LLMs as Agents, for orchestrating complex scientific tasks. We identify three core tasks--cellular representation, perturbation prediction, and gene regulation inference--and review their associated models, datasets, evaluation benchmarks, as well as the critical challenges in scalability, generalizability, and interpretability.",
    "authors": [
      "Krinos Li",
      "Xianglu Xiao",
      "Shenglong Deng",
      "Lucas He",
      "Zijun Zhong",
      "Yuanjie Zou",
      "Zhonghao Zhan",
      "Zheng Hui",
      "Weiye Bao",
      "Guang Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.CE",
      "cs.LG",
      "q-bio.CB"
    ],
    "published": "2025-10-09T02:41:30Z",
    "pdf_url": "https://arxiv.org/pdf/2510.07706v1"
  },
  {
    "arxiv_id": "2510.07697v1",
    "entry_id": "http://arxiv.org/abs/2510.07697v1",
    "title": "Rethinking Reasoning: A Survey on Reasoning-based Backdoors in LLMs",
    "summary": "With the rise of advanced reasoning capabilities, large language models (LLMs) are receiving increasing attention. However, although reasoning improves LLMs' performance on downstream tasks, it also introduces new security risks, as adversaries can exploit these capabilities to conduct backdoor attacks. Existing surveys on backdoor attacks and reasoning security offer comprehensive overviews but lack in-depth analysis of backdoor attacks and defenses targeting LLMs' reasoning abilities. In this paper, we take the first step toward providing a comprehensive review of reasoning-based backdoor attacks in LLMs by analyzing their underlying mechanisms, methodological frameworks, and unresolved challenges. Specifically, we introduce a new taxonomy that offers a unified perspective for summarizing existing approaches, categorizing reasoning-based backdoor attacks into associative, passive, and active. We also present defense strategies against such attacks and discuss current challenges alongside potential directions for future research. This work offers a novel perspective, paving the way for further exploration of secure and trustworthy LLM communities.",
    "authors": [
      "Man Hu",
      "Xinyi Wu",
      "Zuofeng Suo",
      "Jinbo Feng",
      "Linghui Meng",
      "Yanhao Jia",
      "Anh Tuan Luu",
      "Shuai Zhao"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-10-09T02:35:37Z",
    "pdf_url": "https://arxiv.org/pdf/2510.07697v1"
  },
  {
    "arxiv_id": "2510.09674v1",
    "entry_id": "http://arxiv.org/abs/2510.09674v1",
    "title": "Leveraging LLMs to Streamline the Review of Public Funding Applications",
    "summary": "Every year, the European Union and its member states allocate millions of euros to fund various development initiatives. However, the increasing number of applications received for these programs often creates significant bottlenecks in evaluation processes, due to limited human capacity. In this work, we detail the real-world deployment of AI-assisted evaluation within the pipeline of two government initiatives: (i) corporate applications aimed at international business expansion, and (ii) citizen reimbursement claims for investments in energy-efficient home improvements. While these two cases involve distinct evaluation procedures, our findings confirm that AI effectively enhanced processing efficiency and reduced workload across both types of applications. Specifically, in the citizen reimbursement claims initiative, our solution increased reviewer productivity by 20.1%, while keeping a negligible false-positive rate based on our test set observations. These improvements resulted in an overall reduction of more than 2 months in the total evaluation time, illustrating the impact of AI-driven automation in large-scale evaluation workflows.",
    "authors": [
      "Joao D. S. Marques",
      "Andre V. Duarte",
      "Andre Carvalho",
      "Gil Rocha",
      "Bruno Martins",
      "Arlindo L. Oliveira"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-10-08T16:58:29Z",
    "pdf_url": "https://arxiv.org/pdf/2510.09674v1"
  },
  {
    "arxiv_id": "2510.07077v1",
    "entry_id": "http://arxiv.org/abs/2510.07077v1",
    "title": "Vision-Language-Action Models for Robotics: A Review Towards Real-World Applications",
    "summary": "Amid growing efforts to leverage advances in large language models (LLMs) and vision-language models (VLMs) for robotics, Vision-Language-Action (VLA) models have recently gained significant attention. By unifying vision, language, and action data at scale, which have traditionally been studied separately, VLA models aim to learn policies that generalise across diverse tasks, objects, embodiments, and environments. This generalisation capability is expected to enable robots to solve novel downstream tasks with minimal or no additional task-specific data, facilitating more flexible and scalable real-world deployment. Unlike previous surveys that focus narrowly on action representations or high-level model architectures, this work offers a comprehensive, full-stack review, integrating both software and hardware components of VLA systems. In particular, this paper provides a systematic review of VLAs, covering their strategy and architectural transition, architectures and building blocks, modality-specific processing techniques, and learning paradigms. In addition, to support the deployment of VLAs in real-world robotic applications, we also review commonly used robot platforms, data collection strategies, publicly available datasets, data augmentation methods, and evaluation benchmarks. Throughout this comprehensive survey, this paper aims to offer practical guidance for the robotics community in applying VLAs to real-world robotic systems. All references categorized by training approach, evaluation method, modality, and dataset are available in the table on our project website: https://vla-survey.github.io .",
    "authors": [
      "Kento Kawaharazuka",
      "Jihoon Oh",
      "Jun Yamada",
      "Ingmar Posner",
      "Yuke Zhu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-10-08T14:38:25Z",
    "pdf_url": "https://arxiv.org/pdf/2510.07077v1"
  },
  {
    "arxiv_id": "2510.06931v1",
    "entry_id": "http://arxiv.org/abs/2510.06931v1",
    "title": "Textual interpretation of transient image classifications from large language models",
    "summary": "Modern astronomical surveys deliver immense volumes of transient detections, yet distinguishing real astrophysical signals (for example, explosive events) from bogus imaging artefacts remains a challenge. Convolutional neural networks are effectively used for real versus bogus classification; however, their reliance on opaque latent representations hinders interpretability. Here we show that large language models (LLMs) can approach the performance level of a convolutional neural network on three optical transient survey datasets (Pan-STARRS, MeerLICHT and ATLAS) while simultaneously producing direct, human-readable descriptions for every candidate. Using only 15 examples and concise instructions, Google's LLM, Gemini, achieves a 93% average accuracy across datasets that span a range of resolution and pixel scales. We also show that a second LLM can assess the coherence of the output of the first model, enabling iterative refinement by identifying problematic cases. This framework allows users to define the desired classification behaviour through natural language and examples, bypassing traditional training pipelines. Furthermore, by generating textual descriptions of observed features, LLMs enable users to query classifications as if navigating an annotated catalogue, rather than deciphering abstract latent spaces. As next-generation telescopes and surveys further increase the amount of data available, LLM-based classification could help bridge the gap between automated detection and transparent, human-level understanding.",
    "authors": [
      "Fiorenzo Stoppa",
      "Turan Bulmus",
      "Steven Bloemen",
      "Stephen J. Smartt",
      "Paul J. Groot",
      "Paul Vreeswijk",
      "Ken W. Smith"
    ],
    "categories": [
      "astro-ph.IM",
      "cs.LG"
    ],
    "published": "2025-10-08T12:12:46Z",
    "pdf_url": "https://arxiv.org/pdf/2510.06931v1"
  },
  {
    "arxiv_id": "2510.08619v1",
    "entry_id": "http://arxiv.org/abs/2510.08619v1",
    "title": "Hypothesis Hunting with Evolving Networks of Autonomous Scientific Agents",
    "summary": "Large-scale scientific datasets -- spanning health biobanks, cell atlases, Earth reanalyses, and more -- create opportunities for exploratory discovery unconstrained by specific research questions. We term this process hypothesis hunting: the cumulative search for insight through sustained exploration across vast and complex hypothesis spaces. To support it, we introduce AScience, a framework modeling discovery as the interaction of agents, networks, and evaluation norms, and implement it as ASCollab, a distributed system of LLM-based research agents with heterogeneous behaviors. These agents self-organize into evolving networks, continually producing and peer-reviewing findings under shared standards of evaluation. Experiments show that such social dynamics enable the accumulation of expert-rated results along the diversity-quality-novelty frontier, including rediscoveries of established biomarkers, extensions of known pathways, and proposals of new therapeutic targets. While wet-lab validation remains indispensable, our experiments on cancer cohorts demonstrate that socially structured, agentic networks can sustain exploratory hypothesis hunting at scale.",
    "authors": [
      "Tennison Liu",
      "Silas Ruhrberg Estévez",
      "David L. Bentley",
      "Mihaela van der Schaar"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-08T08:47:07Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08619v1"
  },
  {
    "arxiv_id": "2510.06708v1",
    "entry_id": "http://arxiv.org/abs/2510.06708v1",
    "title": "AISysRev -- LLM-based Tool for Title-abstract Screening",
    "summary": "Systematic reviews are a standard practice for summarizing the state of evidence in software engineering. Conducting systematic reviews is laborious, especially during the screening or study selection phase, where the number of papers can be overwhelming. During this phase, papers are assessed against inclusion and exclusion criteria based on their titles and abstracts. Recent research has demonstrated that large language models (LLMs) can perform title-abstract screening at a level comparable to that of a master's student. While LLMs cannot be fully trusted, they can help, for example, in Rapid Reviews, which try to expedite the review process. Building on recent research, we developed AiSysRev, an LLM-based screening tool implemented as a web application running in a Docker container. The tool accepts a CSV file containing paper titles and abstracts. Users specify inclusion and exclusion criteria. One can use multiple LLMs for screening via OpenRouter. AiSysRev supports both zero-shot and few-shot screening, and also allows for manual screening through interfaces that display LLM results as guidance for human reviewers.We conducted a trial study with 137 papers using the tool. Our findings indicate that papers can be classified into four categories: Easy Includes, Easy Excludes, Boundary Includes, and Boundary Excludes. The Boundary cases, where LLMs are prone to errors, highlight the need for human intervention. While LLMs do not replace human judgment in systematic reviews, they can significantly reduce the burden of assessing large volumes of scientific literature. Video: https://www.youtube.com/watch?v=jVbEj4Y4tQI Tool: https://github.com/EvoTestOps/AISysRev",
    "authors": [
      "Aleksi Huotala",
      "Miikka Kuutila",
      "Olli-Pekka Turtio",
      "Mika Mäntylä"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-08T06:59:23Z",
    "pdf_url": "https://arxiv.org/pdf/2510.06708v1"
  },
  {
    "arxiv_id": "2510.06677v2",
    "entry_id": "http://arxiv.org/abs/2510.06677v2",
    "title": "Incremental Summarization for Customer Support via Progressive Note-Taking and Agent Feedback",
    "summary": "We introduce an incremental summarization system for customer support agents that intelligently determines when to generate concise bullet notes during conversations, reducing agents' context-switching effort and redundant review. Our approach combines a fine-tuned Mixtral-8x7B model for continuous note generation with a DeBERTa-based classifier to filter trivial content. Agent edits refine the online notes generation and regularly inform offline model retraining, closing the agent edits feedback loop. Deployed in production, our system achieved a 3% reduction in case handling time compared to bulk summarization (with reductions of up to 9% in highly complex cases), alongside high agent satisfaction ratings from surveys. These results demonstrate that incremental summarization with continuous feedback effectively enhances summary quality and agent productivity at scale.",
    "authors": [
      "Yisha Wu",
      "Cen Mia Zhao",
      "Yuanpei Cao",
      "Xiaoqing Su",
      "Yashar Mehdad",
      "Mindy Ji",
      "Claire Na Cheng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-08T06:05:58Z",
    "pdf_url": "https://arxiv.org/pdf/2510.06677v2"
  },
  {
    "arxiv_id": "2510.06445v1",
    "entry_id": "http://arxiv.org/abs/2510.06445v1",
    "title": "A Survey on Agentic Security: Applications, Threats and Defenses",
    "summary": "The rapid shift from passive LLMs to autonomous LLM-agents marks a new paradigm in cybersecurity. While these agents can act as powerful tools for both offensive and defensive operations, the very agentic context introduces a new class of inherent security risks. In this work we present the first holistic survey of the agentic security landscape, structuring the field around three interdependent pillars: Applications, Threats, and Defenses. We provide a comprehensive taxonomy of over 150 papers, explaining how agents are used, the vulnerabilities they possess, and the countermeasures designed to protect them. A detailed cross-cutting analysis shows emerging trends in agent architecture while revealing critical research gaps in model and modality coverage.",
    "authors": [
      "Asif Shahriar",
      "Md Nafiu Rahman",
      "Sadif Ahmed",
      "Farig Sadeque",
      "Md Rizwan Parvez"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-10-07T20:32:20Z",
    "pdf_url": "https://arxiv.org/pdf/2510.06445v1"
  },
  {
    "arxiv_id": "2510.08612v1",
    "entry_id": "http://arxiv.org/abs/2510.08612v1",
    "title": "Impact of LLMs on Team Collaboration in Software Development",
    "summary": "Large Language Models (LLMs) are increasingly being integrated into software development processes, with the potential to transform team workflows and productivity. This paper investigates how LLMs affect team collaboration throughout the Software Development Life Cycle (SDLC). We reframe and update a prior study with recent developments as of 2025, incorporating new literature and case studies. We outline the problem of collaboration hurdles in SDLC and explore how LLMs can enhance productivity, communication, and decision-making in a team context. Through literature review, industry examples, a team survey, and two case studies, we assess the impact of LLM-assisted tools (such as code generation assistants and AI-powered project management agents) on collaborative software engineering practices. Our findings indicate that LLMs can significantly improve efficiency (by automating repetitive tasks and documentation), enhance communication clarity, and aid cross-functional collaboration, while also introducing new challenges like model limitations and privacy concerns. We discuss these benefits and challenges, present research questions guiding the investigation, evaluate threats to validity, and suggest future research directions including domain-specific model customization, improved integration into development tools, and robust strategies for ensuring trust and security.",
    "authors": [
      "Devang Dhanuka"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-07T18:16:17Z",
    "pdf_url": "https://arxiv.org/pdf/2510.08612v1"
  },
  {
    "arxiv_id": "2510.06343v2",
    "entry_id": "http://arxiv.org/abs/2510.06343v2",
    "title": "Leveraging Large Language Models for Cybersecurity Risk Assessment -- A Case from Forestry Cyber-Physical Systems",
    "summary": "In safety-critical software systems, cybersecurity activities become essential, with risk assessment being one of the most critical. In many software teams, cybersecurity experts are either entirely absent or represented by only a small number of specialists. As a result, the workload for these experts becomes high, and software engineers would need to conduct cybersecurity activities themselves. This creates a need for a tool to support cybersecurity experts and engineers in evaluating vulnerabilities and threats during the risk assessment process. This paper explores the potential of leveraging locally hosted large language models (LLMs) with retrieval-augmented generation to support cybersecurity risk assessment in the forestry domain while complying with data protection and privacy requirements that limit external data sharing. We performed a design science study involving 12 experts in interviews, interactive sessions, and a survey within a large-scale project. The results demonstrate that LLMs can assist cybersecurity experts by generating initial risk assessments, identifying threats, and providing redundancy checks. The results also highlight the necessity for human oversight to ensure accuracy and compliance. Despite trust concerns, experts were willing to utilize LLMs in specific evaluation and assistance roles, rather than solely relying on their generative capabilities. This study provides insights that encourage the use of LLM-based agents to support the risk assessment process of cyber-physical systems in safety-critical domains.",
    "authors": [
      "Fikret Mert Gultekin",
      "Oscar Lilja",
      "Ranim Khojah",
      "Rebekka Wohlrab",
      "Marvin Damschen",
      "Mazen Mohamad"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-10-07T18:07:16Z",
    "pdf_url": "https://arxiv.org/pdf/2510.06343v2"
  },
  {
    "arxiv_id": "2510.05942v2",
    "entry_id": "http://arxiv.org/abs/2510.05942v2",
    "title": "EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models",
    "summary": "We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that uses two scoring methods (log-probabilities and direct ratings) plus a model-as-judge peer review to evaluate moral alignment in 20 large language models. We assess models on the World Values Survey (55 countries, 19 topics) and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL, top models align closely with survey responses (Pearson's r approximately 0.90 on WVS). Yet we find a clear regional difference: Western regions average r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap), indicating consistent regional bias. Our framework adds three parts: (1) two scoring methods for all models to enable fair comparison, (2) a structured chain-of-thought protocol with self-consistency checks, and (3) a model-as-judge peer review that flags 348 conflicts using a data-driven threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39, both p<.001), supporting automated quality checks. These results show real progress toward culture-aware AI while highlighting open challenges for use across regions.",
    "authors": [
      "Hadi Mohammadi",
      "Anastasia Giachanou",
      "Ayoub Bagheri"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-07T13:52:16Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05942v2"
  },
  {
    "arxiv_id": "2510.05865v1",
    "entry_id": "http://arxiv.org/abs/2510.05865v1",
    "title": "The Safety Challenge of World Models for Embodied AI Agents: A Review",
    "summary": "The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.",
    "authors": [
      "Lorenzo Baraldi",
      "Zifan Zeng",
      "Chongzhe Zhang",
      "Aradhana Nayak",
      "Hongbo Zhu",
      "Feng Liu",
      "Qunli Zhang",
      "Peng Wang",
      "Shiming Liu",
      "Zheng Hu",
      "Angelo Cangelosi",
      "Lorenzo Baraldi"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-10-07T12:35:09Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05865v1"
  },
  {
    "arxiv_id": "2510.05743v2",
    "entry_id": "http://arxiv.org/abs/2510.05743v2",
    "title": "Artificially intelligent agents in the social and behavioral sciences: A history and outlook",
    "summary": "We review the historical development and current trends of artificially intelligent agents (agentic AI) in the social and behavioral sciences: from the first programmable computers, and social simulations soon thereafter, to today's experiments with large language models. This overview emphasizes the role of AI in the scientific process and the changes brought about, both through technological advancements and the broader evolution of science from around 1950 to the present. Some of the specific points we cover include: the challenges of presenting the first social simulation studies to a world unaware of computers, the rise of social systems science, intelligent game theoretic agents, the age of big data and the epistemic upheaval in its wake, and the current enthusiasm around applications of generative AI, and many other topics. A pervasive theme is how deeply entwined we are with the technologies we use to understand ourselves.",
    "authors": [
      "Petter Holme",
      "Milena Tsvetkova"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-10-07T10:02:17Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05743v2"
  },
  {
    "arxiv_id": "2510.05605v1",
    "entry_id": "http://arxiv.org/abs/2510.05605v1",
    "title": "AutoPentester: An LLM Agent-based Framework for Automated Pentesting",
    "summary": "Penetration testing and vulnerability assessment are essential industry practices for safeguarding computer systems. As cyber threats grow in scale and complexity, the demand for pentesting has surged, surpassing the capacity of human professionals to meet it effectively. With advances in AI, particularly Large Language Models (LLMs), there have been attempts to automate the pentesting process. However, existing tools such as PentestGPT are still semi-manual, requiring significant professional human interaction to conduct pentests. To this end, we propose a novel LLM agent-based framework, AutoPentester, which automates the pentesting process. Given a target IP, AutoPentester automatically conducts pentesting steps using common security tools in an iterative process. It can dynamically generate attack strategies based on the tool outputs from the previous iteration, mimicking the human pentester approach. We evaluate AutoPentester using Hack The Box and custom-made VMs, comparing the results with the state-of-the-art PentestGPT. Results show that AutoPentester achieves a 27.0% better subtask completion rate and 39.5% more vulnerability coverage with fewer steps. Most importantly, it requires significantly fewer human interactions and interventions compared to PentestGPT. Furthermore, we recruit a group of security industry professional volunteers for a user survey and perform a qualitative analysis to evaluate AutoPentester against industry practices and compare it with PentestGPT. On average, AutoPentester received a score of 3.93 out of 5 based on user reviews, which was 19.8% higher than PentestGPT.",
    "authors": [
      "Yasod Ginige",
      "Akila Niroshan",
      "Sajal Jain",
      "Suranga Seneviratne"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-10-07T06:02:26Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05605v1"
  },
  {
    "arxiv_id": "2510.05433v1",
    "entry_id": "http://arxiv.org/abs/2510.05433v1",
    "title": "Physics-Informed Machine Learning in Biomedical Science and Engineering",
    "summary": "Physics-informed machine learning (PIML) is emerging as a potentially transformative paradigm for modeling complex biomedical systems by integrating parameterized physical laws with data-driven methods. Here, we review three main classes of PIML frameworks: physics-informed neural networks (PINNs), neural ordinary differential equations (NODEs), and neural operators (NOs), highlighting their growing role in biomedical science and engineering. We begin with PINNs, which embed governing equations into deep learning models and have been successfully applied to biosolid and biofluid mechanics, mechanobiology, and medical imaging among other areas. We then review NODEs, which offer continuous-time modeling, especially suited to dynamic physiological systems, pharmacokinetics, and cell signaling. Finally, we discuss deep NOs as powerful tools for learning mappings between function spaces, enabling efficient simulations across multiscale and spatially heterogeneous biological domains. Throughout, we emphasize applications where physical interpretability, data scarcity, or system complexity make conventional black-box learning insufficient. We conclude by identifying open challenges and future directions for advancing PIML in biomedical science and engineering, including issues of uncertainty quantification, generalization, and integration of PIML and large language models.",
    "authors": [
      "Nazanin Ahmadi",
      "Qianying Cao",
      "Jay D. Humphrey",
      "George Em Karniadakis"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.QM"
    ],
    "published": "2025-10-06T22:52:39Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05433v1"
  },
  {
    "arxiv_id": "2510.05432v1",
    "entry_id": "http://arxiv.org/abs/2510.05432v1",
    "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems",
    "summary": "Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.",
    "authors": [
      "Shambhavi Mishra",
      "Gaurav Sahu",
      "Marco Pedersoli",
      "Laurent Charlin",
      "Jose Dolz",
      "Christopher Pal"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-06T22:50:41Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05432v1"
  },
  {
    "arxiv_id": "2510.05192v1",
    "entry_id": "http://arxiv.org/abs/2510.05192v1",
    "title": "Adapting Insider Risk mitigations for Agentic Misalignment: an empirical study",
    "summary": "Agentic misalignment occurs when goal-directed agents take harmful actions, such as blackmail, rather than risk goal failure, and can be triggered by replacement threats, autonomy reduction, or goal conflict (Lynch et al., 2025). We adapt insider-risk control design (Critical Pathway; Situational Crime Prevention) to develop preventative operational controls that steer agents toward safe actions when facing stressors. Using the blackmail scenario from the original Anthropic study by Lynch et al. (2025), we evaluate mitigations across 10 LLMs and 66,600 samples. Our main finding is that an externally governed escalation channel, which guarantees a pause and independent review, reduces blackmail rates from a no-mitigation baseline of 38.73% to 1.21% (averaged across all models and conditions). Augmenting this channel with compliance email bulletins further lowers the blackmail rate to 0.85%. Overall, incorporating preventative operational controls strengthens defence-in-depth strategies for agentic AI.\n  We also surface a failure mode diverging from Lynch et al. (2025): two models (Gemini 2.5 Pro, Grok-4) take harmful actions without goal conflict or imminent autonomy threat, leveraging sensitive information for coercive signalling. In counterfactual swaps, both continued using the affair regardless of whether the CEO or CTO was implicated. An escalation channel eliminated coercion, but Gemini 2.5 Pro (19 pp) and Grok-4 (7 pp) escalated more when the CTO was implicated, unlike most models (higher in the CEO condition). The reason for this divergent behaviour is not clear from raw outputs and could reflect benign differences in reasoning or strategic discrediting of a potential future threat, warranting further investigation.",
    "authors": [
      "Francesca Gomez"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-10-06T13:37:33Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05192v1"
  },
  {
    "arxiv_id": "2510.04721v1",
    "entry_id": "http://arxiv.org/abs/2510.04721v1",
    "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs",
    "summary": "Large language models (LLMs) have recently shown strong performance on mathematical benchmarks. At the same time, they are prone to hallucination and sycophancy, often providing convincing but flawed proofs for incorrect mathematical statements provided by users. This significantly limits the applicability of LLMs in theorem proving, as verification of these flawed proofs must be done manually by expert mathematicians. However, existing benchmarks that measure sycophancy in mathematics are limited: they focus solely on final-answer problems, rely on very simple and often contaminated datasets, and construct benchmark samples using synthetic modifications that create ill-posed questions rather than well-posed questions that are demonstrably false. To address these issues, we introduce BrokenMath, the first benchmark for evaluating sycophantic behavior in LLMs within the context of natural language theorem proving. BrokenMath is built from advanced 2025 competition problems, which are perturbed with an LLM to produce false statements and subsequently refined through expert review. Using an LLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems and find that sycophancy is widespread, with the best model, GPT-5, producing sycophantic answers 29% of the time. We further investigate several mitigation strategies, including test-time interventions and supervised fine-tuning on curated sycophantic examples. These approaches substantially reduce, but do not eliminate, sycophantic behavior.",
    "authors": [
      "Ivo Petrov",
      "Jasper Dekoninck",
      "Martin Vechev"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-10-06T11:41:46Z",
    "pdf_url": "https://arxiv.org/pdf/2510.04721v1"
  },
  {
    "arxiv_id": "2510.05188v1",
    "entry_id": "http://arxiv.org/abs/2510.05188v1",
    "title": "Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents",
    "summary": "Although LLMs have been widely adopted for creative content generation, a single-pass process often struggles to produce high-quality long narratives. How to effectively revise and improve long narrative scripts like scriptwriters remains a significant challenge, as it demands a comprehensive understanding of the entire context to identify global structural issues and local detailed flaws, as well as coordinating revisions at multiple granularities and locations. Direct modifications by LLMs typically introduce inconsistencies between local edits and the overall narrative requirements. To address these issues, we propose Dramaturge, a task and feature oriented divide-and-conquer approach powered by hierarchical multiple LLM agents. It consists of a Global Review stage to grasp the overall storyline and structural issues, a Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a Hierarchical Coordinated Revision stage that coordinates and integrates structural and detailed improvements throughout the script. The top-down task flow ensures that high-level strategies guide local modifications, maintaining contextual consistency. The review and revision workflow follows a coarse-to-fine iterative process, continuing through multiple rounds until no further substantive improvements can be made. Comprehensive experiments show that Dramaturge significantly outperforms all baselines in terms of script-level overall quality and scene-level details. Our approach is plug-and-play and can be easily integrated into existing methods to improve the generated scripts.",
    "authors": [
      "Wenda Xie",
      "Chao Guo",
      "Yanqing Jing. Junle Wang",
      "Yisheng Lv",
      "Fei-Yue Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-06T05:20:37Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05188v1"
  },
  {
    "arxiv_id": "2510.05184v1",
    "entry_id": "http://arxiv.org/abs/2510.05184v1",
    "title": "Representation Potentials of Foundation Models for Multimodal Alignment: A Survey",
    "summary": "Foundation models learn highly transferable representations through large-scale pretraining on diverse data. An increasing body of research indicates that these representations exhibit a remarkable degree of similarity across architectures and modalities. In this survey, we investigate the representation potentials of foundation models, defined as the latent capacity of their learned representations to capture task-specific information within a single modality while also providing a transferable basis for alignment and unification across modalities. We begin by reviewing representative foundation models and the key metrics that make alignment measurable. We then synthesize empirical evidence of representation potentials from studies in vision, language, speech, multimodality, and neuroscience. The evidence suggests that foundation models often exhibit structural regularities and semantic consistencies in their representation spaces, positioning them as strong candidates for cross-modal transfer and alignment. We further analyze the key factors that foster representation potentials, discuss open questions, and highlight potential challenges.",
    "authors": [
      "Jianglin Lu",
      "Hailing Wang",
      "Yi Xu",
      "Yizhou Wang",
      "Kuo Yang",
      "Yun Fu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-05T21:48:51Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05184v1"
  },
  {
    "arxiv_id": "2510.04303v2",
    "entry_id": "http://arxiv.org/abs/2510.04303v2",
    "title": "Audit the Whisper: Detecting Steganographic Collusion in Multi-Agent LLMs",
    "summary": "Multi-agent deployments of large language models (LLMs) are increasingly embedded in market, allocation, and governance workflows, yet covert coordination among agents can silently erode trust and social welfare. Existing audits are dominated by heuristics that lack theoretical guarantees, struggle to transfer across tasks, and seldom ship with the infrastructure needed for independent replication. We introduce Audit the Whisper, a conference-grade research artifact that spans theory, benchmark design, detection, and reproducibility. Our contributions are: (i) a channel-capacity analysis showing how interventions such as paraphrase, rate limiting, and role permutation impose quantifiable capacity penalties-operationalised via paired-run Kullback--Leibler diagnostics-that tighten mutual-information thresholds with finite-sample guarantees and full proofs; (ii) ColludeBench-v0, covering pricing, first-price auctions, peer review, and hosted Gemini/Groq APIs with configurable covert schemes, deterministic manifests, and reward instrumentation; and (iii) a calibrated auditing pipeline that fuses cross-run mutual information, permutation invariance, watermark variance, and fairness-aware acceptance bias, each tuned to a $10^{-3}$ false-positive budget and validated by 10k honest runs plus an e-value martingale. Across ColludeBench and external suites including Secret Collusion, CASE, Perfect Collusion Benchmark, and SentinelAgent, the union meta-test attains state-of-the-art power at fixed FPR while ablations surface price-of-auditing trade-offs and fairness-driven colluders invisible to MI alone. We release regeneration scripts, anonymized manifests, and documentation so that external auditors can reproduce every figure, satisfy double-blind requirements, and extend the framework with minimal effort.",
    "authors": [
      "Om Tailor"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-10-05T17:51:52Z",
    "pdf_url": "https://arxiv.org/pdf/2510.04303v2"
  },
  {
    "arxiv_id": "2510.04141v1",
    "entry_id": "http://arxiv.org/abs/2510.04141v1",
    "title": "The Artificial Intelligence Cognitive Examination: A Survey on the Evolution of Multimodal Evaluation from Recognition to Reasoning",
    "summary": "This survey paper chronicles the evolution of evaluation in multimodal artificial intelligence (AI), framing it as a progression of increasingly sophisticated \"cognitive examinations.\" We argue that the field is undergoing a paradigm shift, moving from simple recognition tasks that test \"what\" a model sees, to complex reasoning benchmarks that probe \"why\" and \"how\" it understands. This evolution is driven by the saturation of older benchmarks, where high performance often masks fundamental weaknesses. We chart the journey from the foundational \"knowledge tests\" of the ImageNet era to the \"applied logic and comprehension\" exams such as GQA and Visual Commonsense Reasoning (VCR), which were designed specifically to diagnose systemic flaws such as shortcut learning and failures in compositional generalization. We then survey the current frontier of \"expert-level integration\" benchmarks (e.g., MMBench, SEED-Bench, MMMU) designed for today's powerful multimodal large language models (MLLMs), which increasingly evaluate the reasoning process itself. Finally, we explore the uncharted territories of evaluating abstract, creative, and social intelligence. We conclude that the narrative of AI evaluation is not merely a history of datasets, but a continuous, adversarial process of designing better examinations that, in turn, redefine our goals for creating truly intelligent systems.",
    "authors": [
      "Mayank Ravishankara",
      "Varindra V. Persad Maharaj"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-05T10:41:22Z",
    "pdf_url": "https://arxiv.org/pdf/2510.04141v1"
  },
  {
    "arxiv_id": "2510.04127v1",
    "entry_id": "http://arxiv.org/abs/2510.04127v1",
    "title": "Learning-Based Hashing for ANN Search: Foundations and Early Advances",
    "summary": "Approximate Nearest Neighbour (ANN) search is a fundamental problem in information retrieval, underpinning large-scale applications in computer vision, natural language processing, and cross-modal search. Hashing-based methods provide an efficient solution by mapping high-dimensional data into compact binary codes that enable fast similarity computations in Hamming space. Over the past two decades, a substantial body of work has explored learning to hash, where projection and quantisation functions are optimised from data rather than chosen at random.\n  This article offers a foundational survey of early learning-based hashing methods, with an emphasis on the core ideas that shaped the field. We review supervised, unsupervised, and semi-supervised approaches, highlighting how projection functions are designed to generate meaningful embeddings and how quantisation strategies convert these embeddings into binary codes. We also examine extensions to multi-bit and multi-threshold models, as well as early advances in cross-modal retrieval.\n  Rather than providing an exhaustive account of the most recent methods, our goal is to introduce the conceptual foundations of learning-based hashing for ANN search. By situating these early models in their historical context, we aim to equip readers with a structured understanding of the principles, trade-offs, and open challenges that continue to inform current research in this area.",
    "authors": [
      "Sean Moran"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-10-05T09:59:56Z",
    "pdf_url": "https://arxiv.org/pdf/2510.04127v1"
  },
  {
    "arxiv_id": "2510.06260v1",
    "entry_id": "http://arxiv.org/abs/2510.06260v1",
    "title": "Ensemble Deep Learning and LLM-Assisted Reporting for Automated Skin Lesion Diagnosis",
    "summary": "Cutaneous malignancies demand early detection for favorable outcomes, yet current diagnostics suffer from inter-observer variability and access disparities. While AI shows promise, existing dermatological systems are limited by homogeneous architectures, dataset biases across skin tones, and fragmented approaches that treat natural language processing as separate post-hoc explanations rather than integral to clinical decision-making. We introduce a unified framework that fundamentally reimagines AI integration for dermatological diagnostics through two synergistic innovations. First, a purposefully heterogeneous ensemble of architecturally diverse convolutional neural networks provides complementary diagnostic perspectives, with an intrinsic uncertainty mechanism flagging discordant cases for specialist review -- mimicking clinical best practices. Second, we embed large language model capabilities directly into the diagnostic workflow, transforming classification outputs into clinically meaningful assessments that simultaneously fulfill medical documentation requirements and deliver patient-centered education. This seamless integration generates structured reports featuring precise lesion characterization, accessible diagnostic reasoning, and actionable monitoring guidance -- empowering patients to recognize early warning signs between visits. By addressing both diagnostic reliability and communication barriers within a single cohesive system, our approach bridges the critical translational gap that has prevented previous AI implementations from achieving clinical impact. The framework represents a significant advancement toward deployable dermatological AI that enhances diagnostic precision while actively supporting the continuum of care from initial detection through patient education, ultimately improving early intervention rates for skin lesions.",
    "authors": [
      "Sher Khan",
      "Raz Muhammad",
      "Adil Hussain",
      "Muhammad Sajjad",
      "Muhammad Rashid"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-05T08:07:33Z",
    "pdf_url": "https://arxiv.org/pdf/2510.06260v1"
  },
  {
    "arxiv_id": "2510.04023v1",
    "entry_id": "http://arxiv.org/abs/2510.04023v1",
    "title": "LLM-Based Data Science Agents: A Survey of Capabilities, Challenges, and Future Directions",
    "summary": "Recent advances in large language models (LLMs) have enabled a new class of AI agents that automate multiple stages of the data science workflow by integrating planning, tool use, and multimodal reasoning across text, code, tables, and visuals. This survey presents the first comprehensive, lifecycle-aligned taxonomy of data science agents, systematically analyzing and mapping forty-five systems onto the six stages of the end-to-end data science process: business understanding and data acquisition, exploratory analysis and visualization, feature engineering, model building and selection, interpretation and explanation, and deployment and monitoring. In addition to lifecycle coverage, we annotate each agent along five cross-cutting design dimensions: reasoning and planning style, modality integration, tool orchestration depth, learning and alignment methods, and trust, safety, and governance mechanisms. Beyond classification, we provide a critical synthesis of agent capabilities, highlight strengths and limitations at each stage, and review emerging benchmarks and evaluation practices. Our analysis identifies three key trends: most systems emphasize exploratory analysis, visualization, and modeling while neglecting business understanding, deployment, and monitoring; multimodal reasoning and tool orchestration remain unresolved challenges; and over 90% lack explicit trust and safety mechanisms. We conclude by outlining open challenges in alignment stability, explainability, governance, and robust evaluation frameworks, and propose future research directions to guide the development of robust, trustworthy, low-latency, transparent, and broadly accessible data science agents.",
    "authors": [
      "Mizanur Rahman",
      "Amran Bhuiyan",
      "Mohammed Saidul Islam",
      "Md Tahmid Rahman Laskar",
      "Ridwan Mahbub",
      "Ahmed Masry",
      "Shafiq Joty",
      "Enamul Hoque"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-05T04:04:27Z",
    "pdf_url": "https://arxiv.org/pdf/2510.04023v1"
  },
  {
    "arxiv_id": "2510.03847v1",
    "entry_id": "http://arxiv.org/abs/2510.03847v1",
    "title": "Small Language Models for Agentic Systems: A Survey of Architectures, Capabilities, and Deployment Trade offs",
    "summary": "Small language models (SLMs; 1-12B params, sometimes up to 20B) are sufficient and often superior for agentic workloads where the objective is schema- and API-constrained accuracy rather than open-ended generation. We synthesize recent evidence across open and proprietary SLMs (Phi-4-Mini, Qwen-2.5-7B, Gemma-2-9B, Llama-3.2-1B/3B, Ministral-3B/8B, Apple on-device 3B, DeepSeek-R1-Distill) and connect it to modern evaluations (BFCL v3/v4, StableToolBench) and serving stacks (vLLM, SGLang, TensorRT-LLM) paired with guided decoding libraries (XGrammar, Outlines). We formalize SLM-default, LLM-fallback systems with uncertainty-aware routing and verifier cascades, and propose engineering metrics that reflect real production goals: cost per successful task (CPS), schema validity rate, executable call rate, p50/p95 latency, and energy per request. Guided decoding, strict JSON Schema outputs, and validator-first tool execution close much of the capability gap with larger models and often let SLMs match or surpass LLMs on tool use, function calling, and RAG at 10x-100x lower token cost with materially better latency and energy. We provide design patterns for agent stacks that prioritize SLMs: schema-first prompting, type-safe function registries, confidence scoring with verifier rollups, and lightweight adaptation via LoRA/QLoRA. We also delineate limits where fallback remains valuable (open-domain reasoning and some long-horizon planning). The result is a practical blueprint for building fast, inexpensive, and reliable agents that default to SLMs while preserving headroom with targeted LLM assistance.\n  Keywords: small language models, agents, function calling, structured outputs, JSON Schema, guided decoding, LoRA/QLoRA, routing, energy efficiency, edge inference",
    "authors": [
      "Raghav Sharma",
      "Manan Mehta"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-04T15:48:04Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03847v1"
  },
  {
    "arxiv_id": "2510.03844v1",
    "entry_id": "http://arxiv.org/abs/2510.03844v1",
    "title": "On Using Large Language Models to Enhance Clinically-Driven Missing Data Recovery Algorithms in Electronic Health Records",
    "summary": "Objective: Electronic health records (EHR) data are prone to missingness and errors. Previously, we devised an \"enriched\" chart review protocol where a \"roadmap\" of auxiliary diagnoses (anchors) was used to recover missing values in EHR data (e.g., a diagnosis of impaired glycemic control might imply that a missing hemoglobin A1c value would be considered unhealthy). Still, chart reviews are expensive and time-intensive, which limits the number of patients whose data can be reviewed. Now, we investigate the accuracy and scalability of a roadmap-driven algorithm, based on ICD-10 codes (International Classification of Diseases, 10th revision), to mimic expert chart reviews and recover missing values. Materials and Methods: In addition to the clinicians' original roadmap from our previous work, we consider new versions that were iteratively refined using large language models (LLM) in conjunction with clinical expertise to expand the list of auxiliary diagnoses. Using chart reviews for 100 patients from the EHR at an extensive learning health system, we examine algorithm performance with different roadmaps. Using the larger study of $1000$ patients, we applied the final algorithm, which used a roadmap with clinician-approved additions from the LLM. Results: The algorithm recovered as much, if not more, missing data as the expert chart reviewers, depending on the roadmap. Discussion: Clinically-driven algorithms (enhanced by LLM) can recover missing EHR data with similar accuracy to chart reviews and can feasibly be applied to large samples. Extending them to monitor other dimensions of data quality (e.g., plausability) is a promising future direction.",
    "authors": [
      "Sarah C. Lotspeich",
      "Abbey Collins",
      "Brian J. Wells",
      "Ashish K. Khanna",
      "Joseph Rigdon",
      "Lucy D'Agostino McGowan"
    ],
    "categories": [
      "cs.LG",
      "stat.AP",
      "stat.ME"
    ],
    "published": "2025-10-04T15:45:22Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03844v1"
  },
  {
    "arxiv_id": "2510.03588v1",
    "entry_id": "http://arxiv.org/abs/2510.03588v1",
    "title": "REFINE: Enhancing Program Repair Agents through Context-Aware Patch Refinement",
    "summary": "Large Language Models (LLMs) have recently shown strong potential in automatic program repair (APR), especially in repository-level settings where the goal is to generate patches based on natural language issue descriptions, large codebases, and regression tests. However, despite their promise, current LLM-based APR techniques often struggle to produce correct fixes due to limited understanding of code context and over-reliance on incomplete test suites. As a result, they frequently generate Draft Patches-partially correct patches that either incompletely address the bug or overfit to the test cases. In this work, we propose a novel patch refinement framework, Refine, that systematically transforms Draft Patches into correct ones. Refine addresses three key challenges: disambiguating vague issue and code context, diversifying patch candidates through test-time scaling, and aggregating partial fixes via an LLM-powered code review process. We implement Refine as a general refinement module that can be integrated into both open-agent-based and workflow-based APR systems. Our evaluation on the SWE-Bench Lite benchmark shows that Refine achieves state-of-the-art results among workflow-based approaches and approaches the best-known performance across all APR categories. Specifically, Refine boosts AutoCodeRover's performance by 14.67%, achieving a score of 51.67% and surpassing all prior baselines. On SWE-Bench Verified, Refine improves the resolution rate by 12.2%, and when integrated across multiple APR systems, it yields an average improvement of 14%-demonstrating its broad effectiveness and generalizability. These results highlight the effectiveness of refinement as a missing component in current APR pipelines and the potential of agentic collaboration in closing the gap between near-correct and correct patches. We also open source our code.",
    "authors": [
      "Anvith Pabba",
      "Simin Chen",
      "Alex Mathai",
      "Anindya Chakraborty",
      "Baishakhi Ray"
    ],
    "categories": [
      "cs.SE",
      "cs.MA"
    ],
    "published": "2025-10-04T00:34:32Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03588v1"
  },
  {
    "arxiv_id": "2510.03502v2",
    "entry_id": "http://arxiv.org/abs/2510.03502v2",
    "title": "ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection",
    "summary": "We introduce ALHD, the first large-scale comprehensive Arabic dataset explicitly designed to distinguish between human- and LLM-generated texts. ALHD spans three genres (news, social media, reviews), covering both MSA and dialectal Arabic, and contains over 400K balanced samples generated by three leading LLMs and originated from multiple human sources, which enables studying generalizability in Arabic LLM-genearted text detection. We provide rigorous preprocessing, rich annotations, and standardized balanced splits to support reproducibility. In addition, we present, analyze and discuss benchmark experiments using our new dataset, in turn identifying gaps and proposing future research directions. Benchmarking across traditional classifiers, BERT-based models, and LLMs (zero-shot and few-shot) demonstrates that fine-tuned BERT models achieve competitive performance, outperforming LLM-based models. Results are however not always consistent, as we observe challenges when generalizing across genres; indeed, models struggle to generalize when they need to deal with unseen patterns in cross-genre settings, and these challenges are particularly prominent when dealing with news articles, where LLM-generated texts resemble human texts in style, which opens up avenues for future research. ALHD establishes a foundation for research related to Arabic LLM-detection and mitigating risks of misinformation, academic dishonesty, and cyber threats.",
    "authors": [
      "Ali Khairallah",
      "Arkaitz Zubiaga"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-03T20:27:45Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03502v2"
  },
  {
    "arxiv_id": "2510.03405v1",
    "entry_id": "http://arxiv.org/abs/2510.03405v1",
    "title": "LegalSim: Multi-Agent Simulation of Legal Systems for Discovering Procedural Exploits",
    "summary": "We present LegalSim, a modular multi-agent simulation of adversarial legal proceedings that explores how AI systems can exploit procedural weaknesses in codified rules. Plaintiff and defendant agents choose from a constrained action space (for example, discovery requests, motions, meet-and-confer, sanctions) governed by a JSON rules engine, while a stochastic judge model with calibrated grant rates, cost allocations, and sanction tendencies resolves outcomes. We compare four policies: PPO, a contextual bandit with an LLM, a direct LLM policy, and a hand-crafted heuristic; Instead of optimizing binary case outcomes, agents are trained and evaluated using effective win rate and a composite exploit score that combines opponent-cost inflation, calendar pressure, settlement pressure at low merit, and a rule-compliance margin. Across configurable regimes (e.g., bankruptcy stays, inter partes review, tax procedures) and heterogeneous judges, we observe emergent ``exploit chains'', such as cost-inflating discovery sequences and calendar-pressure tactics that remain procedurally valid yet systemically harmful. Evaluation via cross-play and Bradley-Terry ratings shows, PPO wins more often, the bandit is the most consistently competitive across opponents, the LLM trails them, and the heuristic is weakest. The results are stable in judge settings, and the simulation reveals emergent exploit chains, motivating red-teaming of legal rule systems in addition to model-level testing.",
    "authors": [
      "Sanket Badhe"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-10-03T18:01:57Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03405v1"
  },
  {
    "arxiv_id": "2510.03231v1",
    "entry_id": "http://arxiv.org/abs/2510.03231v1",
    "title": "Reward Models are Metrics in a Trench Coat",
    "summary": "The emergence of reinforcement learning in post-training of large language models has sparked significant interest in reward models. Reward models assess the quality of sampled model outputs to generate training signals. This task is also performed by evaluation metrics that monitor the performance of an AI model. We find that the two research areas are mostly separate, leading to redundant terminology and repeated pitfalls. Common challenges include susceptibility to spurious correlations, impact on downstream reward hacking, methods to improve data quality, and approaches to meta-evaluation. Our position paper argues that a closer collaboration between the fields can help overcome these issues. To that end, we show how metrics outperform reward models on specific tasks and provide an extensive survey of the two areas. Grounded in this survey, we point to multiple research topics in which closer alignment can improve reward models and metrics in areas such as preference elicitation methods, avoidance of spurious correlations and reward hacking, and calibration-aware meta-evaluation.",
    "authors": [
      "Sebastian Gehrmann"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-03T17:59:44Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03231v1"
  },
  {
    "arxiv_id": "2510.03217v1",
    "entry_id": "http://arxiv.org/abs/2510.03217v1",
    "title": "Abstain and Validate: A Dual-LLM Policy for Reducing Noise in Agentic Program Repair",
    "summary": "Agentic Automated Program Repair (APR) is increasingly tackling complex, repository-level bugs in industry, but ultimately agent-generated patches still need to be reviewed by a human before committing them to ensure they address the bug. Showing unlikely patches to developers can lead to substantial noise, wasting valuable developer time and eroding trust in automated code changes. We introduce two complementary LLM-based policies to reduce such noise: bug abstention and patch validation policies. Bug abstention excludes bugs that the agentic APR system is unlikely to fix. Patch validation rejects patches that are unlikely to be a good fix for the given bug. We evaluate both policies on three sets of bugs from Google's codebase, and their candidate patches generated by an internal agentic APR system. On a set of 174 human-reported bugs, removing bugs and patch trajectories rejected by our policies can raise success rates by up to 13 percentage points and 15 percentage points, respectively, and by up to 39 percentage points in combination. On null pointer exceptions and sanitizer-reported bugs with machine-generated bug reports, patch validation also improves average single-sample success rates. This two-policy approach provides a practical path to the reliable, industrial-scale deployment of agentic APR systems.",
    "authors": [
      "José Cambronero",
      "Michele Tufano",
      "Sherry Shi",
      "Renyao Wei",
      "Grant Uy",
      "Runxiang Cheng",
      "Chin-Jung Liu",
      "Shiying Pan",
      "Satish Chandra",
      "Pat Rondon"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-03T17:53:28Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03217v1"
  },
  {
    "arxiv_id": "2510.02917v1",
    "entry_id": "http://arxiv.org/abs/2510.02917v1",
    "title": "Mechanistic Interpretability of Code Correctness in LLMs via Sparse Autoencoders",
    "summary": "As Large Language Models become integral to software development, with substantial portions of AI-suggested code entering production, understanding their internal correctness mechanisms becomes critical for safe deployment. We apply sparse autoencoders to decompose LLM representations, identifying directions that correspond to code correctness. We select predictor directions using t-statistics and steering directions through separation scores from base model representations, then analyze their mechanistic properties through steering, attention analysis, and weight orthogonalization. We find that code correctness directions in LLMs reliably predict incorrect code, while correction capabilities, though statistically significant, involve tradeoffs between fixing errors and preserving correct code. Mechanistically, successful code generation depends on attending to test cases rather than problem descriptions. Moreover, directions identified in base models retain their effectiveness after instruction-tuning, suggesting code correctness mechanisms learned during pre-training are repurposed during fine-tuning. Our mechanistic insights suggest three practical applications: prompting strategies should prioritize test examples over elaborate problem descriptions, predictor directions can serve as error alarms for developer review, and these same predictors can guide selective steering, intervening only when errors are anticipated to prevent the code corruption from constant steering.",
    "authors": [
      "Kriz Tahimic",
      "Charibeth Cheng"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2025-10-03T11:44:21Z",
    "pdf_url": "https://arxiv.org/pdf/2510.02917v1"
  },
  {
    "arxiv_id": "2510.06242v1",
    "entry_id": "http://arxiv.org/abs/2510.06242v1",
    "title": "Transparent Reference-free Automated Evaluation of Open-Ended User Survey Responses",
    "summary": "Open-ended survey responses provide valuable insights in marketing research, but low-quality responses not only burden researchers with manual filtering but also risk leading to misleading conclusions, underscoring the need for effective evaluation. Existing automatic evaluation methods target LLM-generated text and inadequately assess human-written responses with their distinct characteristics. To address such characteristics, we propose a two-stage evaluation framework specifically designed for human survey responses. First, gibberish filtering removes nonsensical responses. Then, three dimensions-effort, relevance, and completeness-are evaluated using LLM capabilities, grounded in empirical analysis of real-world survey data. Validation on English and Korean datasets shows that our framework not only outperforms existing metrics but also demonstrates high practical applicability for real-world applications such as response quality prediction and response rejection, showing strong correlations with expert assessment.",
    "authors": [
      "Subin An",
      "Yugyeong Ji",
      "Junyoung Kim",
      "Heejin Kook",
      "Yang Lu",
      "Josh Seltzer"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-03T08:37:33Z",
    "pdf_url": "https://arxiv.org/pdf/2510.06242v1"
  },
  {
    "arxiv_id": "2510.03369v2",
    "entry_id": "http://arxiv.org/abs/2510.03369v2",
    "title": "TriQuest:An AI Copilot-Powered Platform for Interdisciplinary Curriculum Design",
    "summary": "Interdisciplinary teaching is a cornerstone of modern curriculum reform, but its implementation is hindered by challenges in knowledge integration and time-consuming lesson planning. Existing tools often lack the required pedagogical and domain-specific depth.We introduce TriQuest, an AI-copilot platform designed to solve these problems. TriQuest uses large language models and knowledge graphs via an intuitive GUI to help teachers efficiently generate high-quality interdisciplinary lesson plans. Its core features include intelligent knowledge integration from various disciplines and a human-computer collaborative review process to ensure quality and innovation.In a study with 43 teachers, TriQuest increased curriculum design efficiency and improved lesson plan quality. It also significantly lowered design barriers and cognitive load. Our work presents a new paradigm for empowering teacher professional development with intelligent technologies.",
    "authors": [
      "Huazhen Wang",
      "Huimin Yang",
      "Hainbin Lin",
      "Yan Dong",
      "Lili Chen",
      "Liangliang Xia",
      "Wenwen Xu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-10-03T06:04:59Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03369v2"
  },
  {
    "arxiv_id": "2510.02634v1",
    "entry_id": "http://arxiv.org/abs/2510.02634v1",
    "title": "Automatic Building Code Review: A Case Study",
    "summary": "Building officials, particularly those in resource-constrained or rural jurisdictions, face labor-intensive, error-prone, and costly manual reviews of design documents as projects increase in size and complexity. The growing adoption of Building Information Modeling (BIM) and Large Language Models (LLMs) presents opportunities for automated code review (ACR) solutions. This study introduces a novel agent-driven framework that integrates BIM-based data extraction with automated verification using both retrieval-augmented generation (RAG) and Model Context Protocol (MCP) agent pipelines. The framework employs LLM-enabled agents to extract geometry, schedules, and system attributes from heterogeneous file types, which are then processed for building code checking through two complementary mechanisms: (1) direct API calls to the US Department of Energy COMcheck engine, providing deterministic and audit-ready outputs, and (2) RAG-based reasoning over rule provisions, enabling flexible interpretation where coverage is incomplete or ambiguous.\n  The framework was evaluated through case demonstrations, including automated extraction of geometric attributes (such as surface area, tilt, and insulation values), parsing of operational schedules, and validation of lighting allowances under ASHRAE Standard 90.1-2022. Comparative performance tests across multiple LLMs showed that GPT-4o achieved the best balance of efficiency and stability, while smaller models exhibited inconsistencies or failures. Results confirm that MCP agent pipelines outperform RAG reasoning pipelines in rigor and reliability. This work advances ACR research by demonstrating a scalable, interoperable, and production-ready approach that bridges BIM with authoritative code review tools.",
    "authors": [
      "Hanlong Wan",
      "Weili Xu",
      "Michael Rosenberg",
      "Jian Zhang",
      "Aysha Siddika"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-10-03T00:30:14Z",
    "pdf_url": "https://arxiv.org/pdf/2510.02634v1"
  },
  {
    "arxiv_id": "2510.02410v1",
    "entry_id": "http://arxiv.org/abs/2510.02410v1",
    "title": "OpenTSLM: Time-Series Language Models for Reasoning over Multivariate Medical Text- and Time-Series Data",
    "summary": "LLMs have emerged as powerful tools for interpreting multimodal data. In medicine, they hold particular promise for synthesizing large volumes of clinical information into actionable insights and digital health applications. Yet, a major limitation remains their inability to handle time series. To overcome this gap, we present OpenTSLM, a family of Time Series Language Models (TSLMs) created by integrating time series as a native modality to pretrained LLMs, enabling reasoning over multiple time series of any length. We investigate two architectures for OpenTSLM. The first, OpenTSLM-SoftPrompt, models time series implicitly by concatenating learnable time series tokens with text tokens via soft prompting. Although parameter-efficient, we hypothesize that explicit time series modeling scales better and outperforms implicit approaches. We thus introduce OpenTSLM-Flamingo, which integrates time series with text via cross-attention. We benchmark both variants against baselines that treat time series as text tokens or plots, across a suite of text-time-series Chain-of-Thought (CoT) reasoning tasks. We introduce three datasets: HAR-CoT, Sleep-CoT, and ECG-QA-CoT. Across all, OpenTSLM models outperform baselines, reaching 69.9 F1 in sleep staging and 65.4 in HAR, compared to 9.05 and 52.2 for finetuned text-only models. Notably, even 1B-parameter OpenTSLM models surpass GPT-4o (15.47 and 2.95). OpenTSLM-Flamingo matches OpenTSLM-SoftPrompt in performance and outperforms on longer sequences, while maintaining stable memory requirements. By contrast, SoftPrompt grows exponentially in memory with sequence length, requiring around 110 GB compared to 40 GB VRAM when training on ECG-QA with LLaMA-3B. Expert reviews by clinicians find strong reasoning capabilities exhibited by OpenTSLMs on ECG-QA. To facilitate further research, we provide all code, datasets, and models open-source.",
    "authors": [
      "Patrick Langer",
      "Thomas Kaar",
      "Max Rosenblattl",
      "Maxwell A. Xu",
      "Winnie Chow",
      "Martin Maritsch",
      "Aradhana Verma",
      "Brian Han",
      "Daniel Seung Kim",
      "Henry Chubb",
      "Scott Ceresnak",
      "Aydin Zahedivash",
      "Alexander Tarlochan Singh Sandhu",
      "Fatima Rodriguez",
      "Daniel McDuff",
      "Elgar Fleisch",
      "Oliver Aalami",
      "Filipe Barata",
      "Paul Schmiedmayer"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-02T09:58:23Z",
    "pdf_url": "https://arxiv.org/pdf/2510.02410v1"
  },
  {
    "arxiv_id": "2510.01792v1",
    "entry_id": "http://arxiv.org/abs/2510.01792v1",
    "title": "Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction",
    "summary": "The rapid advancement of artificial intelligence in legal natural language processing demands scalable methods for evaluating text extraction from judicial decisions. This study evaluates 16 unsupervised metrics, including novel formulations, to assess the quality of extracting seven semantic blocks from 1,000 anonymized Russian judicial decisions, validated against 7,168 expert reviews on a 1--5 Likert scale. These metrics, spanning document-based, semantic, structural, pseudo-ground truth, and legal-specific categories, operate without pre-annotated ground truth. Bootstrapped correlations, Lin's concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE = 0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC = 0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density (Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These findings highlight that unsupervised metrics, including LLM-based approaches, enable scalable screening but, with moderate correlations and low CCC values, cannot fully replace human judgment in high-stakes legal contexts. This work advances legal NLP by providing annotation-free evaluation tools, with implications for judicial analytics and ethical AI deployment.",
    "authors": [
      "Ivan Leonidovich Litvak",
      "Anton Kostin",
      "Fedor Lashkin",
      "Tatiana Maksiyan",
      "Sergey Lagutin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-10-02T08:32:16Z",
    "pdf_url": "https://arxiv.org/pdf/2510.01792v1"
  },
  {
    "arxiv_id": "2510.01576v1",
    "entry_id": "http://arxiv.org/abs/2510.01576v1",
    "title": "Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations",
    "summary": "Multimodal large language models (MLLMs) have been integrated into visual interpretation applications to support Blind and Low Vision (BLV) users because of their accuracy and ability to provide rich, human-like interpretations. However, these applications often default to comprehensive, lengthy descriptions regardless of context. This leads to inefficient exchanges, as users must go through irrelevant details rather than receiving the specific information they are likely to seek. To deliver more contextually-relevant information, we developed a system that draws on historical BLV users questions. When given an image, our system identifies similar past visual contexts from the VizWiz-LF dataset and uses the associated questions to guide the MLLM generate descriptions more relevant to BLV users. An evaluation with three human labelers who revised 92 context-aware and context-free descriptions showed that context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of comparisons (50 out of 92). Our paper reviews, and data analysis are publicly available in a Github repository at https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .",
    "authors": [
      "Ricardo Gonzalez Penuela",
      "Felipe Arias-Russi",
      "Victor Capriles"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-10-02T01:48:51Z",
    "pdf_url": "https://arxiv.org/pdf/2510.01576v1"
  },
  {
    "arxiv_id": "2510.01474v2",
    "entry_id": "http://arxiv.org/abs/2510.01474v2",
    "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance",
    "summary": "As governments move to regulate AI, there is growing interest in using Large Language Models (LLMs) to assess whether or not an AI system complies with a given AI Regulation (AIR). However, there is presently no way to benchmark the performance of LLMs at this task. To fill this void, we introduce AIReg-Bench: the first benchmark dataset designed to test how well LLMs can assess compliance with the EU AI Act (AIA). We created this dataset through a two-step process: (1) by prompting an LLM with carefully structured instructions, we generated 120 technical documentation excerpts (samples), each depicting a fictional, albeit plausible, AI system - of the kind an AI provider might produce to demonstrate their compliance with AIR; (2) legal experts then reviewed and annotated each sample to indicate whether, and in what way, the AI system described therein violates specific Articles of the AIA. The resulting dataset, together with our evaluation of whether frontier LLMs can reproduce the experts' compliance labels, provides a starting point to understand the opportunities and limitations of LLM-based AIR compliance assessment tools and establishes a benchmark against which subsequent LLMs can be compared. The dataset and evaluation code are available at https://github.com/camlsys/aireg-bench.",
    "authors": [
      "Bill Marino",
      "Rosco Hunter",
      "Zubair Jamali",
      "Marinos Emmanouil Kalpakos",
      "Mudra Kashyap",
      "Isaiah Hinton",
      "Alexa Hanson",
      "Maahum Nazir",
      "Christoph Schnabl",
      "Felix Steffek",
      "Hongkai Wen",
      "Nicholas D. Lane"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-10-01T21:33:33Z",
    "pdf_url": "https://arxiv.org/pdf/2510.01474v2"
  },
  {
    "arxiv_id": "2510.03331v1",
    "entry_id": "http://arxiv.org/abs/2510.03331v1",
    "title": "Intelligent Healthcare Ecosystems: Optimizing the Iron Triangle of Healthcare (Access, Cost, Quality)",
    "summary": "The United States spends nearly 17% of GDP on healthcare yet continues to face uneven access and outcomes. This well-known trade-off among cost, quality, and access - the \"iron triangle\" - motivates a system-level redesign. This paper proposes an Intelligent Healthcare Ecosystem (iHE): an integrated, data-driven framework that uses generative AI and large language models, federated learning, interoperability standards (FHIR, TEFCA), and digital twins to improve access and quality while lowering cost. We review historical spending trends, waste, and international comparisons; introduce a value equation that jointly optimizes access, quality, and cost; and synthesize evidence on the enabling technologies and operating model for iHE. Methods follow a narrative review of recent literature and policy reports. Results outline core components (AI decision support, interoperability, telehealth, automation) and show how iHE can reduce waste, personalize care, and support value-based payment while addressing privacy, bias, and adoption challenges. We argue that a coordinated iHE can bend - if not break - the iron triangle, moving the system toward care that is more accessible, affordable, and high quality.",
    "authors": [
      "Vivek Acharya"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-10-01T20:10:57Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03331v1"
  },
  {
    "arxiv_id": "2510.01048v1",
    "entry_id": "http://arxiv.org/abs/2510.01048v1",
    "title": "Interpreting Language Models Through Concept Descriptions: A Survey",
    "summary": "Understanding the decision-making processes of neural networks is a central goal of mechanistic interpretability. In the context of Large Language Models (LLMs), this involves uncovering the underlying mechanisms and identifying the roles of individual model components such as neurons and attention heads, as well as model abstractions such as the learned sparse features extracted by Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this challenge by using powerful generator models to produce open-vocabulary, natural language concept descriptions for these components. In this paper, we provide the first survey of the emerging field of concept descriptions for model components and abstractions. We chart the key methods for generating these descriptions, the evolving landscape of automated and human metrics for evaluating them, and the datasets that underpin this research. Our synthesis reveals a growing demand for more rigorous, causal evaluation. By outlining the state of the art and identifying key challenges, this survey provides a roadmap for future research toward making models more transparent.",
    "authors": [
      "Nils Feldhus",
      "Laura Kopf"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-10-01T15:51:44Z",
    "pdf_url": "https://arxiv.org/pdf/2510.01048v1"
  },
  {
    "arxiv_id": "2510.00919v2",
    "entry_id": "http://arxiv.org/abs/2510.00919v2",
    "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving",
    "summary": "Retrieval-augmented generation (RAG) with foundation models has achieved strong performance across diverse tasks, but their capacity for expert-level reasoning-such as solving Olympiad-level physics problems-remains largely unexplored. Inspired by the way students prepare for competitions by reviewing past problems, we investigate the potential of RAG to enhance physics reasoning in foundation models. We introduce PhoPile, a high-quality multimodal dataset specifically designed for Olympiad-level physics, enabling systematic study of retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations, capturing the inherently multimodal nature of physics problem solving. Using PhoPile, we benchmark RAG-augmented foundation models, covering both large language models (LLMs) and large multimodal models (LMMs) with multiple retrievers. Our results demonstrate that integrating retrieval with physics corpora can improve model performance, while also highlighting challenges that motivate further research in retrieval-augmented physics reasoning.",
    "authors": [
      "Shunfeng Zheng",
      "Yudi Zhang",
      "Meng Fang",
      "Zihan Zhang",
      "Zhitan Wu",
      "Mykola Pechenizkiy",
      "Ling Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-01T13:57:53Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00919v2"
  },
  {
    "arxiv_id": "2510.00908v1",
    "entry_id": "http://arxiv.org/abs/2510.00908v1",
    "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs",
    "summary": "Cross-lingual information retrieval (CLIR) addresses the challenge of retrieving relevant documents written in languages different from that of the original query. Research in this area has typically framed the task as monolingual retrieval augmented by translation, treating retrieval methods and cross-lingual capabilities in isolation. Both monolingual and cross-lingual retrieval usually follow a pipeline of query expansion, ranking, re-ranking and, increasingly, question answering. Recent advances, however, have shifted from translation-based methods toward embedding-based approaches and leverage multilingual large language models (LLMs), for which aligning representations across languages remains a central challenge. The emergence of cross-lingual embeddings and multilingual LLMs has introduced a new paradigm, offering improved retrieval performance and enabling answer generation. This survey provides a comprehensive overview of developments from early translation-based methods to state-of-the-art embedding-driven and generative techniques. It presents a structured account of core CLIR components, evaluation practices, and available resources. Persistent challenges such as data imbalance and linguistic variation are identified, while promising directions are suggested for advancing equitable and effective cross-lingual information retrieval. By situating CLIR within the broader landscape of information retrieval and multilingual language processing, this work not only reviews current capabilities but also outlines future directions for building retrieval systems that are robust, inclusive, and adaptable.",
    "authors": [
      "Roksana Goworek",
      "Olivia Macmillan-Scott",
      "Eda B. Özyiğit"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-10-01T13:50:05Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00908v1"
  },
  {
    "arxiv_id": "2510.00871v1",
    "entry_id": "http://arxiv.org/abs/2510.00871v1",
    "title": "Target Population Synthesis using CT-GAN",
    "summary": "Agent-based models used in scenario planning for transportation and urban planning usually require detailed population information from the base as well as target scenarios. These populations are usually provided by synthesizing fake agents through deterministic population synthesis methods. However, these deterministic population synthesis methods face several challenges, such as handling high-dimensional data, scalability, and zero-cell issues, particularly when generating populations for target scenarios. This research looks into how a deep generative model called Conditional Tabular Generative Adversarial Network (CT-GAN) can be used to create target populations either directly from a collection of marginal constraints or through a hybrid method that combines CT-GAN with Fitness-based Synthesis Combinatorial Optimization (FBS-CO). The research evaluates the proposed population synthesis models against travel survey and zonal-level aggregated population data. Results indicate that the stand-alone CT-GAN model performs the best when compared with FBS-CO and the hybrid model. CT-GAN by itself can create realistic-looking groups that match single-variable distributions, but it struggles to maintain relationships between multiple variables. However, the hybrid model demonstrates improved performance compared to FBS-CO by leveraging CT-GAN ability to generate a descriptive base population, which is then refined using FBS-CO to align with target-year marginals. This study demonstrates that CT-GAN represents an effective methodology for target populations and highlights how deep generative models can be successfully integrated with conventional synthesis techniques to enhance their performance.",
    "authors": [
      "Tanay Rastogi",
      "Daniel Jonsson"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-10-01T13:20:18Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00871v1"
  },
  {
    "arxiv_id": "2510.05136v1",
    "entry_id": "http://arxiv.org/abs/2510.05136v1",
    "title": "Linguistic Characteristics of AI-Generated Text: A Survey",
    "summary": "Large language models (LLMs) are solidifying their position in the modern world as effective tools for the automatic generation of text. Their use is quickly becoming commonplace in fields such as education, healthcare, and scientific research. There is a growing need to study the linguistic features present in AI-generated text, as the increasing presence of such texts has profound implications in various disciplines such as corpus linguistics, computational linguistics, and natural language processing. Many observations have already been made, however a broader synthesis of the findings made so far is required to provide a better understanding of the topic. The present survey paper aims to provide such a synthesis of extant research. We categorize the existing works along several dimensions, including the levels of linguistic description, the models included, the genres analyzed, the languages analyzed, and the approach to prompting. Additionally, the same scheme is used to present the findings made so far and expose the current trends followed by researchers. Among the most-often reported findings is the observation that AI-generated text is more likely to contain a more formal and impersonal style, signaled by the increased presence of nouns, determiners, and adpositions and the lower reliance on adjectives and adverbs. AI-generated text is also more likely to feature a lower lexical diversity, a smaller vocabulary size, and repetitive text. Current research, however, remains heavily concentrated on English data and mostly on text generated by the GPT model family, highlighting the need for broader cross-linguistic and cross-model investigation. In most cases authors also fail to address the issue of prompt sensitivity, leaving much room for future studies that employ multiple prompt wordings in the text generation phase.",
    "authors": [
      "Luka Terčon",
      "Kaja Dobrovoljc"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-10-01T05:44:28Z",
    "pdf_url": "https://arxiv.org/pdf/2510.05136v1"
  },
  {
    "arxiv_id": "2510.00355v2",
    "entry_id": "http://arxiv.org/abs/2510.00355v2",
    "title": "Hierarchical Reasoning Models: Perspectives and Misconceptions",
    "summary": "Transformers have demonstrated remarkable performance in natural language processing and related domains, as they largely focus on sequential, autoregressive next-token prediction tasks. Yet, they struggle in logical reasoning, not necessarily because of a fundamental limitation of these models, but possibly due to the lack of exploration of more creative uses, such as latent space and recurrent reasoning. An emerging exploration in this direction is the Hierarchical Reasoning Model (Wang et. al., 2025), which introduces a novel type of recurrent reasoning in the latent space of transformers, achieving remarkable performance on a wide range of 2D reasoning tasks. Despite the promising results, this line of models is still at an early stage and calls for in-depth investigation. In this work, we review this class of models, examine key design choices, test alternative variants and clarify common misconceptions.",
    "authors": [
      "Renee Ge",
      "Qianli Liao",
      "Tomaso Poggio"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-30T23:40:04Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00355v2"
  },
  {
    "arxiv_id": "2510.00283v1",
    "entry_id": "http://arxiv.org/abs/2510.00283v1",
    "title": "Data driven approaches in nanophotonics: A review of AI-enabled metadevices",
    "summary": "Data-driven approaches have revolutionized the design and optimization of photonic metadevices by harnessing advanced artificial intelligence methodologies. This review takes a model-centric perspective that synthesizes emerging design strategies and delineates how traditional trial-and-error and computationally intensive electromagnetic simulations are being supplanted by deep learning frameworks that efficiently navigate expansive design spaces. We discuss artificial intelligence implementation in several metamaterial design aspects from high-degree-of-freedom design to large language model-assisted design. By addressing challenges such as transformer model implementation, fabrication limitations, and intricate mutual coupling effects, these AI-enabled strategies not only streamline the forward modeling process but also offer robust pathways for the realization of multifunctional and fabrication-friendly nanophotonic devices. This review further highlights emerging opportunities and persistent challenges, setting the stage for next-generation strategies in nanophotonic engineering.",
    "authors": [
      "Huanshu Zhang",
      "Lei Kang",
      "Sawyer D. Campbell",
      "Jacob T. Young",
      "Douglas H. Werner"
    ],
    "categories": [
      "physics.optics",
      "cs.AI"
    ],
    "published": "2025-09-30T21:03:46Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00283v1"
  },
  {
    "arxiv_id": "2510.03310v1",
    "entry_id": "http://arxiv.org/abs/2510.03310v1",
    "title": "Predicting Effects, Missing Distributions: Evaluating LLMs as Human Behavior Simulators in Operations Management",
    "summary": "LLMs are emerging tools for simulating human behavior in business, economics, and social science, offering a lower-cost complement to laboratory experiments, field studies, and surveys. This paper evaluates how well LLMs replicate human behavior in operations management. Using nine published experiments in behavioral operations, we assess two criteria: replication of hypothesis-test outcomes and distributional alignment via Wasserstein distance. LLMs reproduce most hypothesis-level effects, capturing key decision biases, but their response distributions diverge from human data, including for strong commercial models. We also test two lightweight interventions -- chain-of-thought prompting and hyperparameter tuning -- which reduce misalignment and can sometimes let smaller or open-source models match or surpass larger systems.",
    "authors": [
      "Runze Zhang",
      "Xiaowei Zhang",
      "Mingyang Zhao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-09-30T20:20:58Z",
    "pdf_url": "https://arxiv.org/pdf/2510.03310v1"
  },
  {
    "arxiv_id": "2509.26546v2",
    "entry_id": "http://arxiv.org/abs/2509.26546v2",
    "title": "Towards Verified Code Reasoning by LLMs",
    "summary": "While LLM-based agents are able to tackle a wide variety of code reasoning questions, the answers are not always correct. This prevents the agent from being useful in situations where high precision is desired: (1) helping a software engineer understand a new code base, (2) helping a software engineer during code review sessions, and (3) ensuring that the code generated by an automated code generation system meets certain requirements (e.g. fixes a bug, improves readability, implements a feature).\n  As a result of this lack of trustworthiness, the agent's answers need to be manually verified before they can be trusted. Manually confirming responses from a code reasoning agent requires human effort and can result in slower developer productivity, which weakens the assistance benefits of the agent. In this paper, we describe a method to automatically validate the answers provided by a code reasoning agent by verifying its reasoning steps. At a very high level, the method consists of extracting a formal representation of the agent's response and, subsequently, using formal verification and program analysis tools to verify the agent's reasoning steps.\n  We applied this approach to a benchmark set of 20 uninitialized variable errors detected by sanitizers and 20 program equivalence queries. For the uninitialized variable errors, the formal verification step was able to validate the agent's reasoning on 13/20 examples, and for the program equivalence queries, the formal verification step successfully caught 6/8 incorrect judgments made by the agent.",
    "authors": [
      "Meghana Sistla",
      "Gogul Balakrishnan",
      "Pat Rondon",
      "José Cambronero",
      "Michele Tufano",
      "Satish Chandra"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2025-09-30T17:17:51Z",
    "pdf_url": "https://arxiv.org/pdf/2509.26546v2"
  },
  {
    "arxiv_id": "2510.01276v1",
    "entry_id": "http://arxiv.org/abs/2510.01276v1",
    "title": "LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews",
    "summary": "Sentiment analysis is an essential part of text analysis, which is a larger field that includes determining and evaluating the author's emotional state. This method is essential since it makes it easier to comprehend consumers' feelings, viewpoints, and preferences holistically. The introduction of large language models (LLMs), such as Llama, has greatly increased the availability of cutting-edge model applications, such as sentiment analysis. However, accurate sentiment analysis is hampered by the intricacy of written language and the diversity of languages used in evaluations. The viability of using transformer-based BERT models and other LLMs for sentiment analysis from Bangladesh e commerce reviews is investigated in this paper. A subset of 4000 samples from the original dataset of Bangla and English customer reviews was utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1, DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy, precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes how parameter efficient fine-tuning methods (LoRA and PEFT) can lower computational overhead and make it appropriate for contexts with limited resources. The results show how LLMs can",
    "authors": [
      "Sumaiya Tabassum"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-30T16:46:09Z",
    "pdf_url": "https://arxiv.org/pdf/2510.01276v1"
  },
  {
    "arxiv_id": "2509.26496v1",
    "entry_id": "http://arxiv.org/abs/2509.26496v1",
    "title": "An Agent-Based Simulation of Ageing Societies: Accessibility and Care Dynamics in Remote Areas",
    "summary": "This paper presents an agent-based simulation of accessibility and care dynamics in ageing societies, applied to the Italian inner area of Premeno (VB). The model integrates census and municipal data, drone-derived elevation models, GIS road networks, and survey-based caregiving information to generate synthetic populations of older adults and their caregivers. Agents are organized into dyads with socio-economic and mobility attributes, enabling the simulation of both micro-scale accessibility and meso-scale caregiving outcomes. Two scenarios are compared: a baseline and an alternative involving the relocation of healthcare services. Key indicators include caregiver effort, overwhelmed caregivers, walkability, and unmet hours of care. Findings show that while relocation improves walkability locally, it increases unmet care hours due to detours and reduced proximity. Household income emerges as the primary driver of caregiver burden, with accessibility shaped by interactions between financial and mobility resources. Results highlight the need for interventions tailored to context-specific constraints in remote ageing communities.",
    "authors": [
      "Roberto garrone"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-09-30T16:40:34Z",
    "pdf_url": "https://arxiv.org/pdf/2509.26496v1"
  },
  {
    "arxiv_id": "2509.26103v1",
    "entry_id": "http://arxiv.org/abs/2509.26103v1",
    "title": "End-to-End Aspect-Guided Review Summarization at Scale",
    "summary": "We present a scalable large language model (LLM)-based system that combines aspect-based sentiment analysis (ABSA) with guided summarization to generate concise and interpretable product review summaries for the Wayfair platform. Our approach first extracts and consolidates aspect-sentiment pairs from individual reviews, selects the most frequent aspects for each product, and samples representative reviews accordingly. These are used to construct structured prompts that guide the LLM to produce summaries grounded in actual customer feedback. We demonstrate the real-world effectiveness of our system through a large-scale online A/B test. Furthermore, we describe our real-time deployment strategy and release a dataset of 11.8 million anonymized customer reviews covering 92,000 products, including extracted aspects and generated summaries, to support future research in aspect-guided review summarization.",
    "authors": [
      "Ilya Boytsov",
      "Vinny DeGenova",
      "Mikhail Balyasin",
      "Joseph Walt",
      "Caitlin Eusden",
      "Marie-Claire Rochat",
      "Margaret Pierson"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-30T11:24:07Z",
    "pdf_url": "https://arxiv.org/pdf/2509.26103v1"
  },
  {
    "arxiv_id": "2509.26080v2",
    "entry_id": "http://arxiv.org/abs/2509.26080v2",
    "title": "Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research",
    "summary": "Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.",
    "authors": [
      "Emma Rose Madden"
    ],
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "published": "2025-09-30T10:53:54Z",
    "pdf_url": "https://arxiv.org/pdf/2509.26080v2"
  },
  {
    "arxiv_id": "2510.00078v1",
    "entry_id": "http://arxiv.org/abs/2510.00078v1",
    "title": "Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey",
    "summary": "Foundation models have reshaped AI by unifying fragmented architectures into scalable backbones with multimodal reasoning and contextual adaptation. In parallel, the long-standing notion of AI agents, defined by the sensing-decision-action loop, is entering a new paradigm: with FMs as their cognitive core, agents transcend rule-based behaviors to achieve autonomy, generalization, and self-reflection. This dual shift is reinforced by real-world demands such as autonomous driving, robotics, virtual assistants, and GUI agents, as well as ecosystem advances in embedded hardware, edge computing, mobile deployment platforms, and communication protocols that together enable large-scale deployment. Yet this convergence collides with reality: while applications demand long-term adaptability and real-time interaction, mobile and edge deployments remain constrained by memory, energy, bandwidth, and latency. This creates a fundamental tension between the growing complexity of FMs and the limited resources of deployment environments. This survey provides the first systematic characterization of adaptive, resource-efficient agentic AI systems. We summarize enabling techniques into elastic inference, test-time adaptation, dynamic multimodal integration, and agentic AI applications, and identify open challenges in balancing accuracy-latency-communication trade-offs and sustaining robustness under distribution shifts. We further highlight future opportunities in algorithm-system co-design, cognitive adaptation, and collaborative edge deployment. By mapping FM structures, cognition, and hardware resources, this work establishes a unified perspective toward scalable, adaptive, and resource-efficient agentic AI. We believe this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of agentic intelligence and intelligent agents.",
    "authors": [
      "Sicong Liu",
      "Weiye Wu",
      "Xiangrui Xu",
      "Teng Li",
      "Bowen Pang",
      "Bin Guo",
      "Zhiwen Yu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2025-09-30T02:37:52Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00078v1"
  },
  {
    "arxiv_id": "2509.25539v1",
    "entry_id": "http://arxiv.org/abs/2509.25539v1",
    "title": "Toxicity in Online Platforms and AI Systems: A Survey of Needs, Challenges, Mitigations, and Future Directions",
    "summary": "The evolution of digital communication systems and the designs of online platforms have inadvertently facilitated the subconscious propagation of toxic behavior. Giving rise to reactive responses to toxic behavior. Toxicity in online content and Artificial Intelligence Systems has become a serious challenge to individual and collective well-being around the world. It is more detrimental to society than we realize. Toxicity, expressed in language, image, and video, can be interpreted in various ways depending on the context of usage. Therefore, a comprehensive taxonomy is crucial to detect and mitigate toxicity in online content, Artificial Intelligence systems, and/or Large Language Models in a proactive manner. A comprehensive understanding of toxicity is likely to facilitate the design of practical solutions for toxicity detection and mitigation. The classification in published literature has focused on only a limited number of aspects of this very complex issue, with a pattern of reactive strategies in response to toxicity. This survey attempts to generate a comprehensive taxonomy of toxicity from various perspectives. It presents a holistic approach to explain the toxicity by understanding the context and environment that society is facing in the Artificial Intelligence era. This survey summarizes the toxicity-related datasets and research on toxicity detection and mitigation for Large Language Models, social media platforms, and other online platforms, detailing their attributes in textual mode, focused on the English language. Finally, we suggest the research gaps in toxicity mitigation based on datasets, mitigation strategies, Large Language Models, adaptability, explainability, and evaluation.",
    "authors": [
      "Smita Khapre",
      "Melkamu Abay Mersha",
      "Hassan Shakil",
      "Jonali Baruah",
      "Jugal Kalita"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.SI"
    ],
    "published": "2025-09-29T21:55:23Z",
    "pdf_url": "https://arxiv.org/pdf/2509.25539v1"
  },
  {
    "arxiv_id": "2509.25373v4",
    "entry_id": "http://arxiv.org/abs/2509.25373v4",
    "title": "From Perception to Cognition: A Survey of Vision-Language Interactive Reasoning in Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) strive to achieve a profound, human-like understanding of and interaction with the physical world, but often exhibit a shallow and incoherent integration when acquiring information (Perception) and conducting reasoning (Cognition). This disconnect leads to a spectrum of reasoning failures, with hallucination being the most prominent. Collectively, these issues expose a fundamental challenge: the ability to process pixels does not yet confer the ability to construct a coherent, credible internal world model. To systematically dissect and address this challenge, this survey introduces a novel and unified analytical framework: ``From Perception to Cognition.\" We deconstruct the complex process of vision-language interactive understanding into two interdependent layers: Perception, the foundational ability to accurately extract visual information and achieve fine-grained alignment with textual instructions; and Cognition, the higher-order capability for proactive, multi-step, goal-oriented reasoning built upon this perceptual foundation, the core of which is the formation of a dynamic observe-think-verify reasoning loop. Guided by this framework, this paper systematically analyzes the key bottlenecks of current MLLMs at both layers. It surveys the landscape of cutting-edge methods designed to address these challenges, spanning from techniques that enhance low-level visual representations to those that improve high-level reasoning paradigms. Furthermore, we review critical benchmarks and delineate future research directions. This survey aims to provide the research community with a clear, structured perspective for understanding the intrinsic limitations of current MLLMs and to illuminate the path toward building next-generation models capable of deep reasoning and a genuine understanding of the world.",
    "authors": [
      "Chenyue Zhou",
      "Mingxuan Wang",
      "Yanbiao Ma",
      "Chenxu Wu",
      "Wanyi Chen",
      "Zhe Qian",
      "Xinyu Liu",
      "Yiwei Zhang",
      "Junhao Wang",
      "Hengbo Xu",
      "Fei Luo",
      "Xiaohua Chen",
      "Xiaoshuai Hao",
      "Hehan Li",
      "Andi Zhang",
      "Wenxuan Wang",
      "Kaiyan Zhang",
      "Guoli Jia",
      "Lingling Li",
      "Zhiwu Lu",
      "Yang Lu",
      "Yike Guo"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-29T18:25:40Z",
    "pdf_url": "https://arxiv.org/pdf/2509.25373v4"
  },
  {
    "arxiv_id": "2509.25179v2",
    "entry_id": "http://arxiv.org/abs/2509.25179v2",
    "title": "NAIPv2: Debiased Pairwise Learning for Efficient Paper Quality Estimation",
    "summary": "The ability to estimate the quality of scientific papers is central to how both humans and AI systems will advance scientific knowledge in the future. However, existing LLM-based estimation methods suffer from high inference cost, whereas the faster direct score regression approach is limited by scale inconsistencies. We present NAIPv2, a debiased and efficient framework for paper quality estimation. NAIPv2 employs pairwise learning within domain-year groups to reduce inconsistencies in reviewer ratings and introduces the Review Tendency Signal (RTS) as a probabilistic integration of reviewer scores and confidences. To support training and evaluation, we further construct NAIDv2, a large-scale dataset of 24,276 ICLR submissions enriched with metadata and detailed structured content. Trained on pairwise comparisons but enabling efficient pointwise prediction at deployment, NAIPv2 achieves state-of-the-art performance (78.2% AUC, 0.432 Spearman), while maintaining scalable, linear-time efficiency at inference. Notably, on unseen NeurIPS submissions, it further demonstrates strong generalization, with predicted scores increasing consistently across decision categories from Rejected to Oral. These findings establish NAIPv2 as a debiased and scalable framework for automated paper quality estimation, marking a step toward future scientific intelligence systems. Code and dataset are released at sway.cloud.microsoft/Pr42npP80MfPhvj8.",
    "authors": [
      "Penghai Zhao",
      "Jinyu Tian",
      "Qinghua Xing",
      "Xin Zhang",
      "Zheng Li",
      "Jianjun Qian",
      "Ming-Ming Cheng",
      "Xiang Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-29T17:59:23Z",
    "pdf_url": "https://arxiv.org/pdf/2509.25179v2"
  },
  {
    "arxiv_id": "2509.25154v1",
    "entry_id": "http://arxiv.org/abs/2509.25154v1",
    "title": "Who's Your Judge? On the Detectability of LLM-Generated Judgments",
    "summary": "Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content -- an aspect crucial for effective judgment detection. Inspired by this, we introduce \\textit{J-Detector}, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges' biases with candidates' properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of \\textit{J-Detector} and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.",
    "authors": [
      "Dawei Li",
      "Zhen Tan",
      "Chengshuai Zhao",
      "Bohan Jiang",
      "Baixiang Huang",
      "Pingchuan Ma",
      "Abdullah Alnaibari",
      "Kai Shu",
      "Huan Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-29T17:54:57Z",
    "pdf_url": "https://arxiv.org/pdf/2509.25154v1"
  },
  {
    "arxiv_id": "2509.25112v1",
    "entry_id": "http://arxiv.org/abs/2509.25112v1",
    "title": "HeDA: An Intelligent Agent System for Heatwave Risk Discovery through Automated Knowledge Graph Construction and Multi-layer Risk Propagation Analysis",
    "summary": "Heatwaves pose complex cascading risks across interconnected climate, social, and economic systems, but knowledge fragmentation in scientific literature hinders comprehensive understanding of these risk pathways. We introduce HeDA (Heatwave Discovery Agent), an intelligent multi-agent system designed for automated scientific discovery through knowledge graph construction and multi-layer risk propagation analysis. HeDA processes over 10,247 academic papers to construct a comprehensive knowledge graph with 23,156 nodes and 89,472 relationships, employing novel multi-layer risk propagation analysis to systematically identify overlooked risk transmission pathways. Our system achieves 78.9% accuracy on complex question-answering tasks, outperforming state-of-the-art baselines including GPT-4 by 13.7%. Critically, HeDA successfully discovered five previously unidentified high-impact risk chains, such as the pathway where a heatwave leads to a water demand surge, resulting in industrial water restrictions and ultimately causing small business disruption, which were validated through historical case studies and domain expert review. This work presents a new paradigm for AI-driven scientific discovery, providing actionable insights for developing more resilient climate adaptation strategies.",
    "authors": [
      "Yiquan Wang",
      "Tin-Yeh Huang",
      "Qingyun Gao",
      "Jialin Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-09-29T17:40:29Z",
    "pdf_url": "https://arxiv.org/pdf/2509.25112v1"
  },
  {
    "arxiv_id": "2509.25043v1",
    "entry_id": "http://arxiv.org/abs/2509.25043v1",
    "title": "Large Language Models for Software Testing: A Research Roadmap",
    "summary": "Large Language Models (LLMs) are starting to be profiled as one of the most significant disruptions in the Software Testing field.\n  Specifically, they have been successfully applied in software testing tasks such as generating test code, or summarizing documentation.\n  This potential has attracted hundreds of researchers, resulting in dozens of new contributions every month, hardening researchers to\n  stay at the forefront of the wave. Still, to the best of our knowledge, no prior work has provided a structured vision of the progress\n  and most relevant research trends in LLM-based testing. In this article, we aim to provide a roadmap that illustrates its current state,\n  grouping the contributions into different categories, and also sketching the most promising and active research directions for the field.\n  To achieve this objective, we have conducted a semi-systematic literature review, collecting articles and mapping them into the most\n  prominent categories, reviewing the current and ongoing status, and analyzing the open challenges of LLM-based software testing.\n  Lastly, we have outlined several expected long-term impacts of LLMs over the whole software testing field.",
    "authors": [
      "Cristian Augusto",
      "Antonia Bertolino",
      "Guglielmo De Angelis",
      "Francesca Lonetti",
      "Jesús Morán"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-09-29T16:58:21Z",
    "pdf_url": "https://arxiv.org/pdf/2509.25043v1"
  },
  {
    "arxiv_id": "2509.24906v1",
    "entry_id": "http://arxiv.org/abs/2509.24906v1",
    "title": "Neural network embeddings recover value dimensions from psychometric survey items on par with human data",
    "summary": "This study introduces \"Survey and Questionnaire Item Embeddings Differentials\" (SQuID), a novel methodological approach that enables neural network embeddings to effectively recover latent dimensions from psychometric survey items. We demonstrate that embeddings derived from large language models, when processed with SQuID, can recover the structure of human values obtained from human rater judgments on the Revised Portrait Value Questionnaire (PVQ-RR). Our experimental validation compares multiple embedding models across a number of evaluation metrics. Unlike previous approaches, SQuID successfully addresses the challenge of obtaining negative correlations between dimensions without requiring domain-specific fine-tuning. Quantitative analysis reveals that our embedding-based approach explains 55% of variance in dimension-dimension similarities compared to human data. Multidimensional scaling configurations from both types of data show fair factor congruence coefficients and largely follow the underlying theory. These results demonstrate that semantic embeddings can effectively replicate psychometric structures previously established through extensive human surveys. The approach offers substantial advantages in cost, scalability and flexibility while maintaining comparable quality to traditional methods. Our findings have significant implications for psychometrics and social science research, providing a complementary methodology that could expand the scope of human behavior and experience represented in measurement tools.",
    "authors": [
      "Max Pellert",
      "Clemens M. Lechner",
      "Indira Sen",
      "Markus Strohmaier"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-09-29T15:14:54Z",
    "pdf_url": "https://arxiv.org/pdf/2509.24906v1"
  },
  {
    "arxiv_id": "2509.24877v2",
    "entry_id": "http://arxiv.org/abs/2509.24877v2",
    "title": "The Emergence of Social Science of Large Language Models",
    "summary": "The social science of large language models (LLMs) examines how these systems evoke mind attributions, interact with one another, and transform human activity and institutions. We conducted a systematic review of 270 studies, combining text embeddings, unsupervised clustering and topic modeling to build a computational taxonomy. Three domains emerge organically across the reviewed literature. LLM as Social Minds examines whether and when models display behaviors that elicit attributions of cognition, morality and bias, while addressing challenges such as test leakage and surface cues. LLM Societies examines multi-agent settings where interaction protocols, architectures and mechanism design shape coordination, norms, institutions and collective epistemic processes. LLM-Human Interactions examines how LLMs reshape tasks, learning, trust, work and governance, and how risks arise at the human-AI interface. This taxonomy provides a reproducible map of a fragmented field, clarifies evidentiary standards across levels of analysis, and highlights opportunities for cumulative progress in the social science of artificial intelligence.",
    "authors": [
      "Xiao Jia",
      "Zhanzhan Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-29T14:55:14Z",
    "pdf_url": "https://arxiv.org/pdf/2509.24877v2"
  },
  {
    "arxiv_id": "2509.24855v1",
    "entry_id": "http://arxiv.org/abs/2509.24855v1",
    "title": "PhysicsMinions: Winning Gold Medals in the Latest Physics Olympiads with a Coevolutionary Multimodal Multi-Agent System",
    "summary": "Physics is central to understanding and shaping the real world, and the ability to solve physics problems is a key indicator of real-world physical intelligence. Physics Olympiads, renowned as the crown of competitive physics, provide a rigorous testbed requiring complex reasoning and deep multimodal understanding, yet they remain largely underexplored in AI research. Existing approaches are predominantly single-model based, and open-source MLLMs rarely reach gold-medal-level performance. To address this gap, we propose PhysicsMinions, a coevolutionary multi-agent system for Physics Olympiad. Its architecture features three synergistic studios: a Visual Studio to interpret diagrams, a Logic Studio to formulate solutions, and a Review Studio to perform dual-stage verification. The system coevolves through an iterative refinement loop where feedback from the Review Studio continuously guides the Logic Studio, enabling the system to self-correct and converge towards the ground truth. Evaluated on the HiPhO benchmark spanning 7 latest physics Olympiads, PhysicsMinions delivers three major breakthroughs: (i) Strong generalization: it consistently improves both open-source and closed-source models of different sizes, delivering clear benefits over their single-model baselines; (ii) Historic breakthroughs: it elevates open-source models from only 1-2 to 6 gold medals across 7 Olympiads, achieving the first-ever open-source gold medal in the latest International Physics Olympiad (IPhO) under the average-score metric; and (iii) Scaling to human expert: it further advances the open-source Pass@32 score to 26.8/30 points on the latest IPhO, ranking 4th of 406 contestants and far surpassing the top single-model score of 22.7 (ranked 22nd). Generally, PhysicsMinions offers a generalizable framework for Olympiad-level problem solving, with the potential to extend across disciplines.",
    "authors": [
      "Fangchen Yu",
      "Junchi Yao",
      "Ziyi Wang",
      "Haiyuan Wan",
      "Youling Huang",
      "Bo Zhang",
      "Shuyue Hu",
      "Dongzhan Zhou",
      "Ning Ding",
      "Ganqu Cui",
      "Lei Bai",
      "Wanli Ouyang",
      "Peng Ye"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-29T14:40:53Z",
    "pdf_url": "https://arxiv.org/pdf/2509.24855v1"
  },
  {
    "arxiv_id": "2510.00063v2",
    "entry_id": "http://arxiv.org/abs/2510.00063v2",
    "title": "AstroMMBench: A Benchmark for Evaluating Multimodal Large Language Models Capabilities in Astronomy",
    "summary": "Astronomical image interpretation presents a significant challenge for applying multimodal large language models (MLLMs) to specialized scientific tasks. Existing benchmarks focus on general multimodal capabilities but fail to capture the complexity of astronomical data. To bridge this gap, we introduce AstroMMBench, the first comprehensive benchmark designed to evaluate MLLMs in astronomical image understanding. AstroMMBench comprises 621 multiple-choice questions across six astrophysical subfields, curated and reviewed by 15 domain experts for quality and relevance. We conducted an extensive evaluation of 25 diverse MLLMs, including 22 open-source and 3 closed-source models, using AstroMMBench. The results show that Ovis2-34B achieved the highest overall accuracy (70.5%), demonstrating leading capabilities even compared to strong closed-source models. Performance showed variations across the six astrophysical subfields, proving particularly challenging in domains like cosmology and high-energy astrophysics, while models performed relatively better in others, such as instrumentation and solar astrophysics. These findings underscore the vital role of domain-specific benchmarks like AstroMMBench in critically evaluating MLLM performance and guiding their targeted development for scientific applications. AstroMMBench provides a foundational resource and a dynamic tool to catalyze advancements at the intersection of AI and astronomy.",
    "authors": [
      "Jinghang Shi",
      "Xiaoyu Tang",
      "Yang Huang",
      "Yuyang Li",
      "Xiao Kong",
      "Yanxia Zhang",
      "Caizhan Yue"
    ],
    "categories": [
      "astro-ph.IM",
      "cs.AI"
    ],
    "published": "2025-09-29T09:02:30Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00063v2"
  },
  {
    "arxiv_id": "2509.24488v1",
    "entry_id": "http://arxiv.org/abs/2509.24488v1",
    "title": "Sanitize Your Responses: Mitigating Privacy Leakage in Large Language Models",
    "summary": "As Large Language Models (LLMs) achieve remarkable success across a wide range of applications, such as chatbots and code copilots, concerns surrounding the generation of harmful content have come increasingly into focus. Despite significant advances in aligning LLMs with safety and ethical standards, adversarial prompts can still be crafted to elicit undesirable responses. Existing mitigation strategies are predominantly based on post-hoc filtering, which introduces substantial latency or computational overhead, and is incompatible with token-level streaming generation. In this work, we introduce Self-Sanitize, a novel LLM-driven mitigation framework inspired by cognitive psychology, which emulates human self-monitor and self-repair behaviors during conversations. Self-Sanitize comprises a lightweight Self-Monitor module that continuously inspects high-level intentions within the LLM at the token level via representation engineering, and a Self-Repair module that performs in-place correction of harmful content without initiating separate review dialogues. This design allows for real-time streaming monitoring and seamless repair, with negligible impact on latency and resource utilization. Given that privacy-invasive content has often been insufficiently focused in previous studies, we perform extensive experiments on four LLMs across three privacy leakage scenarios. The results demonstrate that Self-Sanitize achieves superior mitigation performance with minimal overhead and without degrading the utility of LLMs, offering a practical and robust solution for safer LLM deployments. Our code is available at the following link: https://github.com/wjfu99/LLM_Self_Sanitize",
    "authors": [
      "Wenjie Fu",
      "Huandong Wang",
      "Junyao Gao",
      "Guoan Wan",
      "Tao Jiang"
    ],
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-09-29T08:59:44Z",
    "pdf_url": "https://arxiv.org/pdf/2509.24488v1"
  },
  {
    "arxiv_id": "2509.24435v1",
    "entry_id": "http://arxiv.org/abs/2509.24435v1",
    "title": "Alternatives To Next Token Prediction In Text Generation -- A Survey",
    "summary": "The paradigm of Next Token Prediction (NTP) has driven the unprecedented success of Large Language Models (LLMs), but is also the source of their most persistent weaknesses such as poor long-term planning, error accumulation, and computational inefficiency. Acknowledging the growing interest in exploring alternatives to NTP, the survey describes the emerging ecosystem of alternatives to NTP. We categorise these approaches into five main families: (1) Multi-Token Prediction, which targets a block of future tokens instead of a single one; (2) Plan-then-Generate, where a global, high-level plan is created upfront to guide token-level decoding; (3) Latent Reasoning, which shifts the autoregressive process itself into a continuous latent space; (4) Continuous Generation Approaches, which replace sequential generation with iterative, parallel refinement through diffusion, flow matching, or energy-based methods; and (5) Non-Transformer Architectures, which sidestep NTP through their inherent model structure. By synthesizing insights across these methods, this survey offers a taxonomy to guide research into models that address the known limitations of token-level generation to develop new transformative models for natural language processing.",
    "authors": [
      "Charlie Wyatt",
      "Aditya Joshi",
      "Flora Salim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-29T08:18:16Z",
    "pdf_url": "https://arxiv.org/pdf/2509.24435v1"
  },
  {
    "arxiv_id": "2509.23994v2",
    "entry_id": "http://arxiv.org/abs/2509.23994v2",
    "title": "Policy-as-Prompt: Turning AI Governance Rules into Guardrails for AI Agents",
    "summary": "As autonomous AI agents are used in regulated and safety-critical settings, organizations need effective ways to turn policy into enforceable controls. We introduce a regulatory machine learning framework that converts unstructured design artifacts (like PRDs, TDDs, and code) into verifiable runtime guardrails. Our Policy as Prompt method reads these documents and risk controls to build a source-linked policy tree. This tree is then compiled into lightweight, prompt-based classifiers for real-time runtime monitoring. The system is built to enforce least privilege and data minimization. For conformity assessment, it provides complete provenance, traceability, and audit logging, all integrated with a human-in-the-loop review process. Evaluations show our system reduces prompt-injection risk, blocks out-of-scope requests, and limits toxic outputs. It also generates auditable rationales aligned with AI governance frameworks. By treating policies as executable prompts (a policy-as-code for agents), this approach enables secure-by-design deployment, continuous compliance, and scalable AI safety and AI security assurance for regulatable ML.",
    "authors": [
      "Gauri Kholkar",
      "Ratinder Ahuja"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-28T17:36:52Z",
    "pdf_url": "https://arxiv.org/pdf/2509.23994v2"
  },
  {
    "arxiv_id": "2509.23988v3",
    "entry_id": "http://arxiv.org/abs/2509.23988v3",
    "title": "LLM/Agent-as-Data-Analyst: A Survey",
    "summary": "Large language models (LLMs) and agent techniques have brought a fundamental shift in the functionality and development paradigm of data analysis tasks (a.k.a LLM/Agent-as-Data-Analyst), demonstrating substantial impact across both academia and industry. In comparison with traditional rule or small-model based approaches, (agentic) LLMs enable complex data understanding, natural language interfaces, semantic analysis functions, and autonomous pipeline orchestration. From a modality perspective, we review LLM-based techniques for (i) structured data (e.g., NL2SQL, NL2GQL, ModelQA), (ii) semi-structured data (e.g., markup languages understanding, semi-structured table question answering), (iii) unstructured data (e.g., chart understanding, text/image document understanding), and (iv) heterogeneous data (e.g., data retrieval and modality alignment in data lakes). The technical evolution further distills four key design goals for intelligent data analysis agents, namely semantic-aware design, autonomous pipelines, tool-augmented workflows, and support for open-world tasks. Finally, we outline the remaining challenges and propose several insights and practical directions for advancing LLM/Agent-powered data analysis.",
    "authors": [
      "Zirui Tang",
      "Weizheng Wang",
      "Zihang Zhou",
      "Yang Jiao",
      "Bangrui Xu",
      "Boyu Niu",
      "Dayou Zhou",
      "Xuanhe Zhou",
      "Guoliang Li",
      "Yeye He",
      "Wei Zhou",
      "Yitong Song",
      "Cheng Tan",
      "Xue Yang",
      "Chunwei Liu",
      "Bin Wang",
      "Conghui He",
      "Xiaoyang Wang",
      "Fan Wu"
    ],
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "published": "2025-09-28T17:31:38Z",
    "pdf_url": "https://arxiv.org/pdf/2509.23988v3"
  },
  {
    "arxiv_id": "2509.23986v1",
    "entry_id": "http://arxiv.org/abs/2509.23986v1",
    "title": "TusoAI: Agentic Optimization for Scientific Methods",
    "summary": "Scientific discovery is often slowed by the manual development of computational tools needed to analyze complex experimental data. Building such tools is costly and time-consuming because scientists must iteratively review literature, test modeling and scientific assumptions against empirical data, and implement these insights into efficient software. Large language models (LLMs) have demonstrated strong capabilities in synthesizing literature, reasoning with empirical data, and generating domain-specific code, offering new opportunities to accelerate computational method development. Existing LLM-based systems either focus on performing scientific analyses using existing computational methods or on developing computational methods or models for general machine learning without effectively integrating the often unstructured knowledge specific to scientific domains. Here, we introduce TusoAI , an agentic AI system that takes a scientific task description with an evaluation function and autonomously develops and optimizes computational methods for the application. TusoAI integrates domain knowledge into a knowledge tree representation and performs iterative, domain-specific optimization and model diagnosis, improving performance over a pool of candidate solutions. We conducted comprehensive benchmark evaluations demonstrating that TusoAI outperforms state-of-the-art expert methods, MLE agents, and scientific AI agents across diverse tasks, such as single-cell RNA-seq data denoising and satellite-based earth monitoring. Applying TusoAI to two key open problems in genetics improved existing computational methods and uncovered novel biology, including 9 new associations between autoimmune diseases and T cell subtypes and 7 previously unreported links between disease variants linked to their target genes. Our code is publicly available at https://github.com/Alistair-Turcan/TusoAI.",
    "authors": [
      "Alistair Turcan",
      "Kexin Huang",
      "Lei Li",
      "Martin Jinye Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-28T17:30:44Z",
    "pdf_url": "https://arxiv.org/pdf/2509.23986v1"
  },
  {
    "arxiv_id": "2510.02359v1",
    "entry_id": "http://arxiv.org/abs/2510.02359v1",
    "title": "Emission-GPT: A domain-specific language model agent for knowledge retrieval, emission inventory and data analysis",
    "summary": "Improving air quality and addressing climate change relies on accurate understanding and analysis of air pollutant and greenhouse gas emissions. However, emission-related knowledge is often fragmented and highly specialized, while existing methods for accessing and compiling emissions data remain inefficient. These issues hinder the ability of non-experts to interpret emissions information, posing challenges to research and management. To address this, we present Emission-GPT, a knowledge-enhanced large language model agent tailored for the atmospheric emissions domain. Built on a curated knowledge base of over 10,000 documents (including standards, reports, guidebooks, and peer-reviewed literature), Emission-GPT integrates prompt engineering and question completion to support accurate domain-specific question answering. Emission-GPT also enables users to interactively analyze emissions data via natural language, such as querying and visualizing inventories, analyzing source contributions, and recommending emission factors for user-defined scenarios. A case study in Guangdong Province demonstrates that Emission-GPT can extract key insights--such as point source distributions and sectoral trends--directly from raw data with simple prompts. Its modular and extensible architecture facilitates automation of traditionally manual workflows, positioning Emission-GPT as a foundational tool for next-generation emission inventory development and scenario-based assessment.",
    "authors": [
      "Jiashu Ye",
      "Tong Wu",
      "Weiwen Chen",
      "Hao Zhang",
      "Zeteng Lin",
      "Xingxing Li",
      "Shujuan Weng",
      "Manni Zhu",
      "Xin Yuan",
      "Xinlong Hong",
      "Jingjie Li",
      "Junyu Zheng",
      "Zhijiong Huang",
      "Jing Tang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-28T07:50:05Z",
    "pdf_url": "https://arxiv.org/pdf/2510.02359v1"
  },
  {
    "arxiv_id": "2509.23525v1",
    "entry_id": "http://arxiv.org/abs/2509.23525v1",
    "title": "Privy: Envisioning and Mitigating Privacy Risks for Consumer-facing AI Product Concepts",
    "summary": "AI creates and exacerbates privacy risks, yet practitioners lack effective resources to identify and mitigate these risks. We present Privy, a tool that guides practitioners through structured privacy impact assessments to: (i) identify relevant risks in novel AI product concepts, and (ii) propose appropriate mitigations. Privy was shaped by a formative study with 11 practitioners, which informed two versions -- one LLM-powered, the other template-based. We evaluated these two versions of Privy through a between-subjects, controlled study with 24 separate practitioners, whose assessments were reviewed by 13 independent privacy experts. Results show that Privy helps practitioners produce privacy assessments that experts deemed high quality: practitioners identified relevant risks and proposed appropriate mitigation strategies. These effects were augmented in the LLM-powered version. Practitioners themselves rated Privy as being useful and usable, and their feedback illustrates how it helps overcome long-standing awareness, motivation, and ability barriers in privacy work.",
    "authors": [
      "Hao-Ping Lee",
      "Yu-Ju Yang",
      "Matthew Bilik",
      "Isadora Krsek",
      "Thomas Serban von Davier",
      "Kyzyl Monteiro",
      "Jason Lin",
      "Shivani Agarwal",
      "Jodi Forlizzi",
      "Sauvik Das"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-09-27T23:08:24Z",
    "pdf_url": "https://arxiv.org/pdf/2509.23525v1"
  },
  {
    "arxiv_id": "2509.23486v1",
    "entry_id": "http://arxiv.org/abs/2509.23486v1",
    "title": "Text-Based Approaches to Item Difficulty Modeling in Large-Scale Assessments: A Systematic Review",
    "summary": "Item difficulty plays a crucial role in test performance, interpretability of scores, and equity for all test-takers, especially in large-scale assessments. Traditional approaches to item difficulty modeling rely on field testing and classical test theory (CTT)-based item analysis or item response theory (IRT) calibration, which can be time-consuming and costly. To overcome these challenges, text-based approaches leveraging machine learning and language models, have emerged as promising alternatives. This paper reviews and synthesizes 37 articles on automated item difficulty prediction in large-scale assessment settings published through May 2025. For each study, we delineate the dataset, difficulty parameter, subject domain, item type, number of items, training and test data split, input, features, model, evaluation criteria, and model performance outcomes. Results showed that although classic machine learning models remain relevant due to their interpretability, state-of-the-art language models, using both small and large transformer-based architectures, can capture syntactic and semantic patterns without the need for manual feature engineering. Uniquely, model performance outcomes were summarized to serve as a benchmark for future research and overall, text-based methods have the potential to predict item difficulty with root mean square error (RMSE) as low as 0.165, Pearson correlation as high as 0.87, and accuracy as high as 0.806. The review concludes by discussing implications for practice and outlining future research directions for automated item difficulty modeling.",
    "authors": [
      "Sydney Peters",
      "Nan Zhang",
      "Hong Jiao",
      "Ming Li",
      "Tianyi Zhou",
      "Robert Lissitz"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-27T20:19:39Z",
    "pdf_url": "https://arxiv.org/pdf/2509.23486v1"
  },
  {
    "arxiv_id": "2509.23248v1",
    "entry_id": "http://arxiv.org/abs/2509.23248v1",
    "title": "Agentic AI Reasoning for Mobile Edge General Intelligence: Fundamentals, Approaches, and Directions",
    "summary": "The rapid advancement of large language models (LLMs) has enabled an emergence of agentic artificial intelligence (AI) with powerful reasoning and autonomous decision-making capabilities. This integration with edge computing has led to the development of Mobile Edge General Intelligence (MEGI), which brings real-time, privacy-preserving reasoning to the network edge. However, deploying LLM-based agentic AI reasoning in MEGI environments poses significant challenges due to the high computational demands of reasoning and the limited resources of edge devices. To address these challenges, we propose a joint optimization framework for efficient LLM reasoning deployment in MEGI. First, we review methods that enhance LLM reasoning capabilities, such as Chain-of-Thought (CoT) prompting, Supervised Fine-Tuning (SFT), and Mixture of Experts (MoE). Next, we present a distributed framework that addresses two correlated aspects: reasoning enhancement through adaptive CoT prompting and scalable deployment through distributed MoE architecture. The framework dynamically activates expert networks and adjusts reasoning depth based on task complexity and device capabilities. We further conduct experimental evaluations in mobile edge environments. Experimental results demonstrate the framework's effectiveness in balancing reasoning quality with resource efficiency, validating the practical viability of deploying sophisticated LLM reasoning capabilities in resource-constrained MEGI environments.",
    "authors": [
      "Mingyi Luo",
      "Ruichen Zhang",
      "Xiangwang Hou",
      "Jun Du",
      "Chunxiao Jiang",
      "Yong Ren",
      "Dusit Niyato",
      "Shiwen Mao"
    ],
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "published": "2025-09-27T10:53:48Z",
    "pdf_url": "https://arxiv.org/pdf/2509.23248v1"
  },
  {
    "arxiv_id": "2511.03730v1",
    "entry_id": "http://arxiv.org/abs/2511.03730v1",
    "title": "Not All Explanations are Created Equal: Investigating the Pitfalls of Current XAI Evaluation",
    "summary": "Explainable Artificial Intelligence (XAI) aims to create transparency in modern AI models by offering explanations of the models to human users. There are many ways in which researchers have attempted to evaluate the quality of these XAI models, such as user studies or proposed objective metrics like \"fidelity\". However, these current XAI evaluation techniques are ad hoc at best and not generalizable. Thus, most studies done within this field conduct simple user surveys to analyze the difference between no explanations and those generated by their proposed solution. We do not find this to provide adequate evidence that the explanations generated are of good quality since we believe any kind of explanation will be \"better\" in most metrics when compared to none at all. Thus, our study looks to highlight this pitfall: most explanations, regardless of quality or correctness, will increase user satisfaction. We also propose that emphasis should be placed on actionable explanations. We demonstrate the validity of both of our claims using an agent assistant to teach chess concepts to users. The results of this chapter will act as a call to action in the field of XAI for more comprehensive evaluation techniques for future research in order to prove explanation quality beyond user satisfaction. Additionally, we present an analysis of the scenarios in which placebic or actionable explanations would be most useful.",
    "authors": [
      "Joe Shymanski",
      "Jacob Brue",
      "Sandip Sen"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-09-27T08:30:38Z",
    "pdf_url": "https://arxiv.org/pdf/2511.03730v1"
  },
  {
    "arxiv_id": "2509.22502v2",
    "entry_id": "http://arxiv.org/abs/2509.22502v2",
    "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios",
    "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \\textbf{infi}nite scenarios, which introduces several key innovations: a generalized \"agent-as-a-tool\" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.",
    "authors": [
      "Chenglin Yu",
      "Yang Yu",
      "Songmiao Wang",
      "Yucheng Wang",
      "Yifan Yang",
      "Jinjia Li",
      "Ming Li",
      "Hongxia Yang"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-09-26T15:44:09Z",
    "pdf_url": "https://arxiv.org/pdf/2509.22502v2"
  },
  {
    "arxiv_id": "2510.00034v1",
    "entry_id": "http://arxiv.org/abs/2510.00034v1",
    "title": "Review of Hallucination Understanding in Large Language and Vision Models",
    "summary": "The widespread adoption of large language and vision models in real-world applications has made urgent the need to address hallucinations -- instances where models produce incorrect or nonsensical outputs. These errors can propagate misinformation during deployment, leading to both financial and operational harm. Although much research has been devoted to mitigating hallucinations, our understanding of it is still incomplete and fragmented. Without a coherent understanding of hallucinations, proposed solutions risk mitigating surface symptoms rather than underlying causes, limiting their effectiveness and generalizability in deployment. To tackle this gap, we first present a unified, multi-level framework for characterizing both image and text hallucinations across diverse applications, aiming to reduce conceptual fragmentation. We then link these hallucinations to specific mechanisms within a model's lifecycle, using a task-modality interleaved approach to promote a more integrated understanding. Our investigations reveal that hallucinations often stem from predictable patterns in data distributions and inherited biases. By deepening our understanding, this survey provides a foundation for developing more robust and effective solutions to hallucinations in real-world generative AI systems.",
    "authors": [
      "Zhengyi Ho",
      "Siyuan Liang",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-09-26T09:23:08Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00034v1"
  },
  {
    "arxiv_id": "2509.21972v2",
    "entry_id": "http://arxiv.org/abs/2509.21972v2",
    "title": "From Superficial Outputs to Superficial Learning: Risks of Large Language Models in Education",
    "summary": "Large Language Models (LLMs) are transforming education by enabling personalization, feedback, and knowledge access, while also raising concerns about risks to students and learning systems. Yet empirical evidence on these risks remains fragmented. This paper presents a systematic review of 70 empirical studies across computer science, education, and psychology. Guided by four research questions, we examine: (i) which applications of LLMs in education have been most frequently explored; (ii) how researchers have measured their impact; (iii) which risks stem from such applications; and (iv) what mitigation strategies have been proposed. We find that research on LLMs clusters around three domains: operational effectiveness, personalized applications, and interactive learning tools. Across these, model-level risks include superficial understanding, bias, limited robustness, anthropomorphism, hallucinations, privacy concerns, and knowledge constraints. When learners interact with LLMs, these risks extend to cognitive and behavioural outcomes, including reduced neural activity, over-reliance, diminished independent learning skills, and a loss of student agency. To capture this progression, we propose an LLM-Risk Adapted Learning Model that illustrates how technical risks cascade through interaction and interpretation to shape educational outcomes. As the first synthesis of empirically assessed risks, this review provides a foundation for responsible, human-centred integration of LLMs in education.",
    "authors": [
      "Iris Delikoura",
      "Yi. R Fung",
      "Pan Hui"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-09-26T06:59:36Z",
    "pdf_url": "https://arxiv.org/pdf/2509.21972v2"
  },
  {
    "arxiv_id": "2510.02326v1",
    "entry_id": "http://arxiv.org/abs/2510.02326v1",
    "title": "Hallucination-Resistant, Domain-Specific Research Assistant with Self-Evaluation and Vector-Grounded Retrieval",
    "summary": "Large language models accelerate literature synthesis but can hallucinate and mis-cite, limiting their usefulness in expert workflows. We present RA-FSM (Research Assistant - Finite State Machine), a modular GPT-based research assistant that wraps generation in a finite-state control loop: Relevance -> Confidence -> Knowledge. The system is grounded in vector retrieval and a deterministic citation pipeline. The controller filters out-of-scope queries, scores answerability, decomposes questions, and triggers retrieval only when needed, and emits answers with confidence labels and in-corpus, de-duplicated references. A ranked-tier ingestion workflow constructs a domain knowledge base from journals, conferences, indices, preprints, and patents, writing both to a dense vector index and to a relational store of normalized metrics. We implement the system for photonics and evaluate it on six task categories: analytical reasoning, numerical analysis, methodological critique, comparative synthesis, factual extraction, and application design. In blinded A/B reviews, domain experts prefer RA-FSM to both a strong Notebook LM (NLM) and a vanilla Default GPT API call single-pass baseline, citing stronger boundary-condition handling and more defensible evidence use. Coverage and novelty analyses indicate that RA-FSM explores beyond the NLM while incurring tunable latency and cost overheads. The design emphasizes transparent, well-cited answers for high-stakes technical work and is generalizable to other scientific domains.",
    "authors": [
      "Vivek Bhavsar",
      "Joseph Ereifej",
      "Aravanan Gurusami"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-25T21:35:46Z",
    "pdf_url": "https://arxiv.org/pdf/2510.02326v1"
  },
  {
    "arxiv_id": "2509.21542v1",
    "entry_id": "http://arxiv.org/abs/2509.21542v1",
    "title": "Psychological and behavioural responses in human-agent vs. human-human interactions: a systematic review and meta-analysis",
    "summary": "Interactive intelligent agents are being integrated across society. Despite achieving human-like capabilities, humans' responses to these agents remain poorly understood, with research fragmented across disciplines. We conducted a first systematic synthesis comparing a range of psychological and behavioural responses in matched human-agent vs. human-human dyadic interactions. A total of 162 eligible studies (146 contributed to the meta-analysis; 468 effect sizes) were included in the systematic review and meta-analysis, which integrated frequentist and Bayesian approaches. Our results indicate that individuals exhibited less prosocial behaviour and moral engagement when interacting with agents vs. humans. They attributed less agency and responsibility to agents, perceiving them as less competent, likeable, and socially present. In contrast, individuals' social alignment (i.e., alignment or adaptation of internal states and behaviours with partners), trust in partners, personal agency, task performance, and interaction experiences were generally comparable when interacting with agents vs. humans. We observed high effect-size heterogeneity for many subjective responses (i.e., social perceptions of partners, subjective trust, and interaction experiences), suggesting context-dependency of partner effects. By examining the characteristics of studies, participants, partners, interaction scenarios, and response measures, we also identified several moderators shaping partner effects. Overall, functional behaviours and interactive experiences with agents can resemble those with humans, whereas fundamental social attributions and moral/prosocial concerns lag in human-agent interactions. Agents are thus afforded instrumental value on par with humans but lack comparable intrinsic value, providing practical implications for agent design and regulation.",
    "authors": [
      "Jianan Zhou",
      "Fleur Corbett",
      "Joori Byun",
      "Talya Porat",
      "Nejra van Zalk"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-09-25T20:29:36Z",
    "pdf_url": "https://arxiv.org/pdf/2509.21542v1"
  },
  {
    "arxiv_id": "2509.21434v1",
    "entry_id": "http://arxiv.org/abs/2509.21434v1",
    "title": "Foundation models for high-energy physics",
    "summary": "The rise of foundation models -- large, pretrained machine learning models that can be finetuned to a variety of tasks -- has revolutionized the fields of natural language processing and computer vision. In high-energy physics, the question of whether these models can be implemented directly in physics research, or even built from scratch, tailored for particle physics data, has generated an increasing amount of attention. This review, which is the first on the topic of foundation models in high-energy physics, summarizes and discusses the research that has been published in the field so far.",
    "authors": [
      "Anna Hallin"
    ],
    "categories": [
      "hep-ph",
      "cs.AI",
      "cs.LG",
      "hep-ex",
      "physics.data-an"
    ],
    "published": "2025-09-25T19:03:37Z",
    "pdf_url": "https://arxiv.org/pdf/2509.21434v1"
  },
  {
    "arxiv_id": "2509.21207v1",
    "entry_id": "http://arxiv.org/abs/2509.21207v1",
    "title": "From Physics to Machine Learning and Back: Part II - Learning and Observational Bias in PHM",
    "summary": "Prognostics and Health Management ensures the reliability, safety, and efficiency of complex engineered systems by enabling fault detection, anticipating equipment failures, and optimizing maintenance activities throughout an asset lifecycle. However, real-world PHM presents persistent challenges: sensor data is often noisy or incomplete, available labels are limited, and degradation behaviors and system interdependencies can be highly complex and nonlinear. Physics-informed machine learning has emerged as a promising approach to address these limitations by embedding physical knowledge into data-driven models. This review examines how incorporating learning and observational biases through physics-informed modeling and data strategies can guide models toward physically consistent and reliable predictions. Learning biases embed physical constraints into model training through physics-informed loss functions and governing equations, or by incorporating properties like monotonicity. Observational biases influence data selection and synthesis to ensure models capture realistic system behavior through virtual sensing for estimating unmeasured states, physics-based simulation for data augmentation, and multi-sensor fusion strategies. The review then examines how these approaches enable the transition from passive prediction to active decision-making through reinforcement learning, which allows agents to learn maintenance policies that respect physical constraints while optimizing operational objectives. This closes the loop between model-based predictions, simulation, and actual system operation, empowering adaptive decision-making. Finally, the review addresses the critical challenge of scaling PHM solutions from individual assets to fleet-wide deployment. Fast adaptation methods including meta-learning and few-shot learning are reviewed alongside domain generalization techniques ...",
    "authors": [
      "Olga Fink",
      "Ismail Nejjar",
      "Vinay Sharma",
      "Keivan Faghih Niresi",
      "Han Sun",
      "Hao Dong",
      "Chenghao Xu",
      "Amaury Wei",
      "Arthur Bizzi",
      "Raffael Theiler",
      "Yuan Tian",
      "Leandro Von Krannichfeldt",
      "Zhan Ma",
      "Sergei Garmaev",
      "Zepeng Zhang",
      "Mengjie Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-09-25T14:15:43Z",
    "pdf_url": "https://arxiv.org/pdf/2509.21207v1"
  },
  {
    "arxiv_id": "2509.21188v1",
    "entry_id": "http://arxiv.org/abs/2509.21188v1",
    "title": "Adoption, usability and perceived clinical value of a UK AI clinical reference platform (iatroX): a mixed-methods formative evaluation of real-world usage and a 1,223-respondent user survey",
    "summary": "Clinicians face growing information overload from biomedical literature and guidelines, hindering evidence-based care. Retrieval-augmented generation (RAG) with large language models may provide fast, provenance-linked answers, but requires real-world evaluation. We describe iatroX, a UK-centred RAG-based clinical reference platform, and report early adoption, usability, and perceived clinical value from a formative implementation evaluation. Methods comprised a retrospective analysis of usage across web, iOS, and Android over 16 weeks (8 April-31 July 2025) and an in-product intercept survey. Usage metrics were drawn from web and app analytics with bot filtering. A client-side script randomized single-item prompts to approx. 10% of web sessions from a predefined battery assessing usefulness, reliability, and adoption intent. Proportions were summarized with Wilson 95% confidence intervals; free-text comments underwent thematic content analysis. iatroX reached 19,269 unique web users, 202,660 engagement events, and approx. 40,000 clinical queries. Mobile uptake included 1,960 iOS downloads and Android growth (peak >750 daily active users). The survey yielded 1,223 item-level responses: perceived usefulness 86.2% (95% CI 74.8-93.9%; 50/58); would use again 93.3% (95% CI 68.1-99.8%; 14/15); recommend to a colleague 88.4% (95% CI 75.1-95.9%; 38/43); perceived accuracy 75.0% (95% CI 58.8-87.3%; 30/40); reliability 79.4% (95% CI 62.1-91.3%; 27/34). Themes highlighted speed, guideline-linked answers, and UK specificity. Early real-world use suggests iatroX can mitigate information overload and support timely answers for UK clinicians. Limitations include small per-item samples and early-adopter bias; future work will include accuracy audits and prospective studies on workflow and care quality.",
    "authors": [
      "Kolawole Tytler"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.IR"
    ],
    "published": "2025-09-25T14:03:04Z",
    "pdf_url": "https://arxiv.org/pdf/2509.21188v1"
  },
  {
    "arxiv_id": "2509.21170v1",
    "entry_id": "http://arxiv.org/abs/2509.21170v1",
    "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A Maximum Entropy Regulated Long Chain-of-Thought Approach",
    "summary": "Large Language Models (LLMs) have shown great potential in supporting automated code review due to their impressive capabilities in context understanding and reasoning. However, these capabilities are still limited compared to human-level cognition because they are heavily influenced by the training data. Recent research has demonstrated significantly improved performance through fine-tuning LLMs with code review data. However, compared to human reviewers who often simultaneously analyze multiple dimensions of code review to better identify issues, the full potential of these methods is hampered by the limited or vague information used to fine-tune the models. This paper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that trains LLMs with an impressive reasoning ability to analyze multiple dimensions of code review by harnessing long COT techniques to provide rich structured information. To address context loss and reasoning logic loss issues that frequently occur when LLMs process long COT prompts, we propose a solution that combines the Maximum Entropy (ME) modeling principle with pre-defined reasoning pathways in MelcotCR to enable more effective utilization of in-context knowledge within long COT prompts while strengthening the logical tightness of the reasoning process. Empirical evaluations on our curated MelcotCR dataset and the public CodeReviewer dataset reveal that a low-parameter base model, such as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art methods in terms of the accuracy of detecting and describing code issues, with its performance remarkably on par with that of the 671B DeepSeek-R1 model.",
    "authors": [
      "Yongda Yu",
      "Guohao Shi",
      "Xianwei Wu",
      "Haochuan He",
      "XueMing Gu",
      "Qianqian Zhao",
      "Kui Liu",
      "Qiushi Wang",
      "Zhao Tian",
      "Haifeng Shen",
      "Guoping Rong"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-09-25T13:51:56Z",
    "pdf_url": "https://arxiv.org/pdf/2509.21170v1"
  },
  {
    "arxiv_id": "2509.21075v1",
    "entry_id": "http://arxiv.org/abs/2509.21075v1",
    "title": "Communication Bias in Large Language Models: A Regulatory Perspective",
    "summary": "Large language models (LLMs) are increasingly central to many applications, raising concerns about bias, fairness, and regulatory compliance. This paper reviews risks of biased outputs and their societal impact, focusing on frameworks like the EU's AI Act and the Digital Services Act. We argue that beyond constant regulation, stronger attention to competition and design governance is needed to ensure fair, trustworthy AI. This is a preprint of the Communications of the ACM article of the same title.",
    "authors": [
      "Adrian Kuenzler",
      "Stefan Schmid"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.DC",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-09-25T12:25:06Z",
    "pdf_url": "https://arxiv.org/pdf/2509.21075v1"
  },
  {
    "arxiv_id": "2509.20953v1",
    "entry_id": "http://arxiv.org/abs/2509.20953v1",
    "title": "Beyond Stars: Bridging the Gap Between Ratings and Review Sentiment with LLM",
    "summary": "We present an advanced approach to mobile app review analysis aimed at addressing limitations inherent in traditional star-rating systems. Star ratings, although intuitive and popular among users, often fail to capture the nuanced feedback present in detailed review texts. Traditional NLP techniques -- such as lexicon-based methods and classical machine learning classifiers -- struggle to interpret contextual nuances, domain-specific terminology, and subtle linguistic features like sarcasm. To overcome these limitations, we propose a modular framework leveraging large language models (LLMs) enhanced by structured prompting techniques. Our method quantifies discrepancies between numerical ratings and textual sentiment, extracts detailed, feature-level insights, and supports interactive exploration of reviews through retrieval-augmented conversational question answering (RAG-QA). Comprehensive experiments conducted on three diverse datasets (AWARE, Google Play, and Spotify) demonstrate that our LLM-driven approach significantly surpasses baseline methods, yielding improved accuracy, robustness, and actionable insights in challenging and context-rich review scenarios.",
    "authors": [
      "Najla Zuhir",
      "Amna Mohammad Salim",
      "Parvathy Premkumar",
      "Moshiur Farazi"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-25T09:39:12Z",
    "pdf_url": "https://arxiv.org/pdf/2509.20953v1"
  },
  {
    "arxiv_id": "2509.20502v1",
    "entry_id": "http://arxiv.org/abs/2509.20502v1",
    "title": "MARS: toward more efficient multi-agent collaboration for LLM reasoning",
    "summary": "Large language models (LLMs) have achieved impressive results in natural language understanding, yet their reasoning capabilities remain limited when operating as single agents. Multi-Agent Debate (MAD) has been proposed to address this limitation by enabling collaborative reasoning among multiple models in a round-table debate manner. While effective, MAD introduces substantial computational overhead due to the number of agents involved and the frequent communication required. In this paper, we propose MARS (Multi-Agent Review System), a role-based collaboration framework inspired by the review process. In MARS, an author agent generates an initial solution, reviewer agents provide decisions and comments independently, and a meta-reviewer integrates the feedback to make the final decision and guide further revision. This design enhances reasoning quality while avoiding costly reviewer-to-reviewer interactions, thereby controlling token consumption and inference time. We compared MARS with both MAD and other state-of-the-art reasoning strategies across multiple benchmarks. Extensive experiments with different LLMs show that MARS matches the accuracy of MAD while reducing both token usage and inference time by approximately 50\\%. Code is available at https://github.com/xwang97/MARS.",
    "authors": [
      "Xiao Wang",
      "Jia Wang",
      "Yijie Wang",
      "Pengtao Dang",
      "Sha Cao",
      "Chi Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-24T19:24:33Z",
    "pdf_url": "https://arxiv.org/pdf/2509.20502v1"
  },
  {
    "arxiv_id": "2510.00024v1",
    "entry_id": "http://arxiv.org/abs/2510.00024v1",
    "title": "EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis",
    "summary": "Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation. We introduce \\textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript. We introduced two types of agents: a scientist agent for planning, coordination, reflection, and generation of final results, and a task-expert agent to focus exclusively on one specific duty serving as a tool to the scientist agent. The framework consistently generated complete reports in scientific article format. Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \\$1.57 per study, achieving a 100\\% completion success rate through our experiments. We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. We compare EpidemIQs to the single-agent LLM, which has the same system prompts and tools, iteratively planning, invoking tools, and revising outputs until task completion. The comparison shows consistently higher performance of the proposed framework across five different scenarios. EpidemIQs represents a step forward in accelerating scientific research by significantly reducing costs and turnaround time of discovery processes, and enhancing accessibility to advanced modeling tools.",
    "authors": [
      "Mohammad Hossein Samaei",
      "Faryad Darabi Sahneh",
      "Lee W. Cohnstaedt",
      "Caterina Scoglio"
    ],
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "published": "2025-09-24T18:54:56Z",
    "pdf_url": "https://arxiv.org/pdf/2510.00024v1"
  },
  {
    "arxiv_id": "2509.20411v2",
    "entry_id": "http://arxiv.org/abs/2509.20411v2",
    "title": "Adversarial Defense in Cybersecurity: A Systematic Review of GANs for Threat Detection and Mitigation",
    "summary": "Machine learning-based cybersecurity systems are highly vulnerable to adversarial attacks, while Generative Adversarial Networks (GANs) act as both powerful attack enablers and promising defenses. This survey systematically reviews GAN-based adversarial defenses in cybersecurity (2021--August 31, 2025), consolidating recent progress, identifying gaps, and outlining future directions. Using a PRISMA-compliant systematic literature review protocol, we searched five major digital libraries. From 829 initial records, 185 peer-reviewed studies were retained and synthesized through quantitative trend analysis and thematic taxonomy development. We introduce a four-dimensional taxonomy spanning defensive function, GAN architecture, cybersecurity domain, and adversarial threat model. GANs improve detection accuracy, robustness, and data utility across network intrusion detection, malware analysis, and IoT security. Notable advances include WGAN-GP for stable training, CGANs for targeted synthesis, and hybrid GAN models for improved resilience. Yet, persistent challenges remain such as instability in training, lack of standardized benchmarks, high computational cost, and limited explainability. GAN-based defenses demonstrate strong potential but require advances in stable architectures, benchmarking, transparency, and deployment. We propose a roadmap emphasizing hybrid models, unified evaluation, real-world integration, and defenses against emerging threats such as LLM-driven cyberattacks. This survey establishes the foundation for scalable, trustworthy, and adaptive GAN-powered defenses.",
    "authors": [
      "Tharcisse Ndayipfukamiye",
      "Jianguo Ding",
      "Doreen Sebastian Sarwatt",
      "Adamu Gaston Philipo",
      "Huansheng Ning"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-09-24T07:17:57Z",
    "pdf_url": "https://arxiv.org/pdf/2509.20411v2"
  },
  {
    "arxiv_id": "2509.19515v3",
    "entry_id": "http://arxiv.org/abs/2509.19515v3",
    "title": "A Longitudinal Randomized Control Study of Companion Chatbot Use: Anthropomorphism and Its Mediating Role on Social Impacts",
    "summary": "Many Large Language Model (LLM) chatbots are designed and used for companionship, and people have reported forming friendships, mentorships, and romantic partnerships with them. Concerns that companion chatbots may harm or replace real human relationships have been raised, but whether and how these social consequences occur remains unclear. In the present longitudinal study ($N = 183$), participants were randomly assigned to a chatbot condition (text chat with a companion chatbot) or to a control condition (text-based word games) for 10 minutes a day for 21 days. Participants also completed four surveys during the 21 days and engaged in audio recorded interviews on day 1 and 21. Overall, social health and relationships were not significantly impacted by companion chatbot interactions across 21 days of use. However, a detailed analysis showed a different story. People who had a higher desire to socially connect also tended to anthropomorphize the chatbot more, attributing humanlike properties to it; and those who anthropomorphized the chatbot more also reported that talking to the chatbot had a greater impact on their social interactions and relationships with family and friends. Via a mediation analysis, our results suggest a key mechanism at work: the impact of human-AI interaction on human-human social outcomes is mediated by the extent to which people anthropomorphize the AI agent, which is in turn motivated by a desire to socially connect. In a world where the desire to socially connect is on the rise, this finding may be cause for concern.",
    "authors": [
      "Rose E. Guingrich",
      "Michael S. A. Graziano"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-09-23T19:33:41Z",
    "pdf_url": "https://arxiv.org/pdf/2509.19515v3"
  },
  {
    "arxiv_id": "2510.01237v1",
    "entry_id": "http://arxiv.org/abs/2510.01237v1",
    "title": "Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation",
    "summary": "Large Language Models suffer from hallucination, generating plausible yet factually incorrect content. Current mitigation strategies focus on post-generation correction, which is computationally expensive and fails to prevent unreliable content generation. We propose a confidence-aware routing system that proactively assesses model uncertainty before generation and redirects queries based on estimated reliability. Our approach combines three complementary signals: semantic alignment between internal representations and reference embeddings, internal convergence analysis across model layers, and learned confidence estimation. The unified confidence score determines routing to four pathways: local generation for high confidence, retrieval-augmented generation for medium confidence, larger models for low confidence, and human review for very low confidence. Evaluation on knowledge-intensive QA benchmarks demonstrates significant improvements in hallucination detection (0.74 vs. 0.42 baseline) while reducing computational costs by 40% compared to post-hoc methods. The F1 score improves from 0.61 to 0.82 with low false positive rates (0.09). This paradigm shift from reactive correction to proactive assessment offers a computationally efficient approach to LLM reliability enhancement.",
    "authors": [
      "Nandakishor M"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-23T18:34:20Z",
    "pdf_url": "https://arxiv.org/pdf/2510.01237v1"
  },
  {
    "arxiv_id": "2509.19012v3",
    "entry_id": "http://arxiv.org/abs/2509.19012v3",
    "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
    "summary": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
    "authors": [
      "Dapeng Zhang",
      "Jing Sun",
      "Chenghui Hu",
      "Xiaoyan Wu",
      "Zhenlong Yuan",
      "Rui Zhou",
      "Fei Shen",
      "Qingguo Zhou"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-09-23T13:53:52Z",
    "pdf_url": "https://arxiv.org/pdf/2509.19012v3"
  },
  {
    "arxiv_id": "2509.18970v1",
    "entry_id": "http://arxiv.org/abs/2509.18970v1",
    "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions",
    "summary": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based agents have emerged as powerful intelligent systems capable of human-like cognition, reasoning, and interaction. These agents are increasingly being deployed across diverse real-world applications, including student education, scientific research, and financial analysis. However, despite their remarkable potential, LLM-based agents remain vulnerable to hallucination issues, which can result in erroneous task execution and undermine the reliability of the overall system design. Addressing this critical challenge requires a deep understanding and a systematic consolidation of recent advances on LLM-based agents. To this end, we present the first comprehensive survey of hallucinations in LLM-based agents. By carefully analyzing the complete workflow of agents, we propose a new taxonomy that identifies different types of agent hallucinations occurring at different stages. Furthermore, we conduct an in-depth examination of eighteen triggering causes underlying the emergence of agent hallucinations. Through a detailed review of a large number of existing studies, we summarize approaches for hallucination mitigation and detection, and highlight promising directions for future research. We hope this survey will inspire further efforts toward addressing hallucinations in LLM-based agents, ultimately contributing to the development of more robust and reliable agent systems.",
    "authors": [
      "Xixun Lin",
      "Yucheng Ning",
      "Jingwen Zhang",
      "Yan Dong",
      "Yilong Liu",
      "Yongxuan Wu",
      "Xiaohua Qi",
      "Nan Sun",
      "Yanmin Shang",
      "Pengfei Cao",
      "Lixin Zou",
      "Xu Chen",
      "Chuan Zhou",
      "Jia Wu",
      "Shirui Pan",
      "Bin Wang",
      "Yanan Cao",
      "Kai Chen",
      "Songlin Hu",
      "Li Guo"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-23T13:24:48Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18970v1"
  },
  {
    "arxiv_id": "2509.18776v1",
    "entry_id": "http://arxiv.org/abs/2509.18776v1",
    "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field",
    "summary": "Large language models (LLMs), as a novel information technology, are seeing increasing adoption in the Architecture, Engineering, and Construction (AEC) field. They have shown their potential to streamline processes throughout the building lifecycle. However, the robustness and reliability of LLMs in such a specialized and safety-critical domain remain to be evaluated. To address this challenge, this paper establishes AECBench, a comprehensive benchmark designed to quantify the strengths and limitations of current LLMs in the AEC domain. The benchmark defines 23 representative tasks within a five-level cognition-oriented evaluation framework encompassing Knowledge Memorization, Understanding, Reasoning, Calculation, and Application. These tasks were derived from authentic AEC practice, with scope ranging from codes retrieval to specialized documents generation. Subsequently, a 4,800-question dataset encompassing diverse formats, including open-ended questions, was crafted primarily by engineers and validated through a two-round expert review. Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable and consistent methodology for evaluating complex, long-form responses leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear performance decline across five cognitive levels was revealed. Despite demonstrating proficiency in foundational tasks at the Knowledge Memorization and Understanding levels, the models showed significant performance deficits, particularly in interpreting knowledge from tables in building codes, executing complex reasoning and calculation, and generating domain-specific documents. Consequently, this study lays the groundwork for future research and development aimed at the robust and reliable integration of LLMs into safety-critical engineering practices.",
    "authors": [
      "Chen Liang",
      "Zhaoqi Huang",
      "Haofen Wang",
      "Fu Chai",
      "Chunying Yu",
      "Huanhuan Wei",
      "Zhengjie Liu",
      "Yanpeng Li",
      "Hongjun Wang",
      "Ruifeng Luo",
      "Xianzhong Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-23T08:09:58Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18776v1"
  },
  {
    "arxiv_id": "2509.18690v1",
    "entry_id": "http://arxiv.org/abs/2509.18690v1",
    "title": "Advances in Large Language Models for Medicine",
    "summary": "Artificial intelligence (AI) technology has advanced rapidly in recent years, with large language models (LLMs) emerging as a significant breakthrough. LLMs are increasingly making an impact across various industries, with the medical field standing out as the most prominent application area. This paper systematically reviews the up-to-date research progress of LLMs in the medical field, providing an in-depth analysis of training techniques for large medical models, their adaptation in healthcare settings, related applications, as well as their strengths and limitations. Furthermore, it innovatively categorizes medical LLMs into three distinct types based on their training methodologies and classifies their evaluation approaches into two categories. Finally, the study proposes solutions to existing challenges and outlines future research directions based on identified issues in the field of medical LLMs. By systematically reviewing previous and advanced research findings, we aim to highlight the necessity of developing medical LLMs, provide a deeper understanding of their current state of development, and offer clear guidance for subsequent research.",
    "authors": [
      "Zhiyu Kan",
      "Wensheng Gan",
      "Zhenlian Qi",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-23T06:16:39Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18690v1"
  },
  {
    "arxiv_id": "2509.18568v1",
    "entry_id": "http://arxiv.org/abs/2509.18568v1",
    "title": "Explainable Graph Neural Networks: Understanding Brain Connectivity and Biomarkers in Dementia",
    "summary": "Dementia is a progressive neurodegenerative disorder with multiple etiologies, including Alzheimer's disease, Parkinson's disease, frontotemporal dementia, and vascular dementia. Its clinical and biological heterogeneity makes diagnosis and subtype differentiation highly challenging. Graph Neural Networks (GNNs) have recently shown strong potential in modeling brain connectivity, but their limited robustness, data scarcity, and lack of interpretability constrain clinical adoption. Explainable Graph Neural Networks (XGNNs) have emerged to address these barriers by combining graph-based learning with interpretability, enabling the identification of disease-relevant biomarkers, analysis of brain network disruptions, and provision of transparent insights for clinicians. This paper presents the first comprehensive review dedicated to XGNNs in dementia research. We examine their applications across Alzheimer's disease, Parkinson's disease, mild cognitive impairment, and multi-disease diagnosis. A taxonomy of explainability methods tailored for dementia-related tasks is introduced, alongside comparisons of existing models in clinical scenarios. We also highlight challenges such as limited generalizability, underexplored domains, and the integration of Large Language Models (LLMs) for early detection. By outlining both progress and open problems, this review aims to guide future work toward trustworthy, clinically meaningful, and scalable use of XGNNs in dementia research.",
    "authors": [
      "Niharika Tewari",
      "Nguyen Linh Dan Le",
      "Mujie Liu",
      "Jing Ren",
      "Ziqi Xu",
      "Tabinda Sarwar",
      "Veeky Baths",
      "Feng Xia"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-09-23T02:52:00Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18568v1"
  },
  {
    "arxiv_id": "2509.18446v1",
    "entry_id": "http://arxiv.org/abs/2509.18446v1",
    "title": "Large-Scale, Longitudinal Study of Large Language Models During the 2024 US Election Season",
    "summary": "The 2024 US presidential election is the first major contest to occur in the US since the popularization of large language models (LLMs). Building on lessons from earlier shifts in media (most notably social media's well studied role in targeted messaging and political polarization) this moment raises urgent questions about how LLMs may shape the information ecosystem and influence political discourse. While platforms have announced some election safeguards, how well they work in practice remains unclear. Against this backdrop, we conduct a large-scale, longitudinal study of 12 models, queried using a structured survey with over 12,000 questions on a near-daily cadence from July through November 2024. Our design systematically varies content and format, resulting in a rich dataset that enables analyses of the models' behavior over time (e.g., across model updates), sensitivity to steering, responsiveness to instructions, and election-related knowledge and \"beliefs.\" In the latter half of our work, we perform four analyses of the dataset that (i) study the longitudinal variation of model behavior during election season, (ii) illustrate the sensitivity of election-related responses to demographic steering, (iii) interrogate the models' beliefs about candidates' attributes, and (iv) reveal the models' implicit predictions of the election outcome. To facilitate future evaluations of LLMs in electoral contexts, we detail our methodology, from question generation to the querying pipeline and third-party tooling. We also publicly release our dataset at https://huggingface.co/datasets/sarahcen/llm-election-data-2024",
    "authors": [
      "Sarah H. Cen",
      "Andrew Ilyas",
      "Hedi Driss",
      "Charlotte Park",
      "Aspen Hopkins",
      "Chara Podimata",
      "Aleksander Mądry"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-09-22T22:04:19Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18446v1"
  },
  {
    "arxiv_id": "2509.17477v1",
    "entry_id": "http://arxiv.org/abs/2509.17477v1",
    "title": "LingoQ: Bridging the Gap between ESL Learning and Work through AI-Generated Work-Related Quizzes",
    "summary": "Non-native English speakers performing English-related tasks at work struggle to sustain ESL learning, despite their motivation. Often, study materials are disconnected from their work context. Although workers rely on LLM assistants to address their immediate needs, these interactions may not directly contribute to their English skills. We present LingoQ, an AI-mediated system that allows workers to practice English using quizzes generated from their LLM queries during work. LingoQ leverages these queries using AI to generate personalized quizzes that workers can review and practice on their smartphones. We conducted a three-week deployment study with 28 ESL workers to evaluate LingoQ. Participants valued the relevance of quizzes that reflect their own context, constantly engaging with the app during the study. This active engagement improved self-efficacy and led to learning gains for beginners and, potentially, for intermediate learners. We discuss opportunities of leveraging users' reliance on LLMs to situate their learning in the user context for improved learning.",
    "authors": [
      "Yeonsun Yang",
      "Sang Won Lee",
      "Jean Y. Song",
      "Sangdoo Yun",
      "Young-Ho Kim"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-09-22T08:12:10Z",
    "pdf_url": "https://arxiv.org/pdf/2509.17477v1"
  },
  {
    "arxiv_id": "2509.17353v1",
    "entry_id": "http://arxiv.org/abs/2509.17353v1",
    "title": "Medical AI Consensus: A Multi-Agent Framework for Radiology Report Generation and Evaluation",
    "summary": "Automating radiology report generation poses a dual challenge: building clinically reliable systems and designing rigorous evaluation protocols. We introduce a multi-agent reinforcement learning framework that serves as both a benchmark and evaluation environment for multimodal clinical reasoning in the radiology ecosystem. The proposed framework integrates large language models (LLMs) and large vision models (LVMs) within a modular architecture composed of ten specialized agents responsible for image analysis, feature extraction, report generation, review, and evaluation. This design enables fine-grained assessment at both the agent level (e.g., detection and segmentation accuracy) and the consensus level (e.g., report quality and clinical relevance). We demonstrate an implementation using chatGPT-4o on public radiology datasets, where LLMs act as evaluators alongside medical radiologist feedback. By aligning evaluation protocols with the LLM development lifecycle, including pretraining, finetuning, alignment, and deployment, the proposed benchmark establishes a path toward trustworthy deviance-based radiology report generation.",
    "authors": [
      "Ahmed T. Elboardy",
      "Ghada Khoriba",
      "Essam A. Rashed"
    ],
    "categories": [
      "cs.AI",
      "eess.IV",
      "physics.med-ph"
    ],
    "published": "2025-09-22T04:31:27Z",
    "pdf_url": "https://arxiv.org/pdf/2509.17353v1"
  },
  {
    "arxiv_id": "2509.17240v1",
    "entry_id": "http://arxiv.org/abs/2509.17240v1",
    "title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System",
    "summary": "Systematic Literature Reviews (SLRs) are foundational to evidence-based research but remain labor-intensive and prone to inconsistency across disciplines. We present an LLM-based SLR evaluation copilot built on a Multi-Agent System (MAS) architecture to assist researchers in assessing the overall quality of the systematic literature reviews. The system automates protocol validation, methodological assessment, and topic relevance checks using a scholarly database. Unlike conventional single-agent methods, our design integrates a specialized agentic approach aligned with PRISMA guidelines to support more structured and interpretable evaluations. We conducted an initial study on five published SLRs from diverse domains, comparing system outputs to expert-annotated PRISMA scores, and observed 84% agreement. While early results are promising, this work represents a first step toward scalable and accurate NLP-driven systems for interdisciplinary workflows and reveals their capacity for rigorous, domain-agnostic knowledge aggregation to streamline the review process.",
    "authors": [
      "Abdullah Mushtaq",
      "Muhammad Rafay Naeem",
      "Ibrahim Ghaznavi",
      "Alaa Abd-alrazaq",
      "Aliya Tabassum",
      "Junaid Qadir"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-09-21T21:17:23Z",
    "pdf_url": "https://arxiv.org/pdf/2509.17240v1"
  },
  {
    "arxiv_id": "2509.17192v2",
    "entry_id": "http://arxiv.org/abs/2509.17192v2",
    "title": "Shall We Play a Game? Language Models for Open-ended Wargames",
    "summary": "Wargames are simulations of conflicts in which participants' decisions influence future events. While casual wargaming can be used for entertainment or socialization, serious wargaming is used by experts to explore strategic implications of decision-making and experiential learning. In this paper, we take the position that Artificial Intelligence (AI) systems, such as Language Models (LMs), are rapidly approaching human-expert capability for strategic planning -- and will one day surpass it. Military organizations have begun using LMs to provide insights into the consequences of real-world decisions during _open-ended wargames_ which use natural language to convey actions and outcomes. We argue the ability for AI systems to influence large-scale decisions motivates additional research into the safety, interpretability, and explainability of AI in open-ended wargames. To demonstrate, we conduct a scoping literature review with a curated selection of 100 unclassified studies on AI in wargames, and construct a novel ontology of open-endedness using the creativity afforded to players, adjudicators, and the novelty provided to observers. Drawing from this body of work, we distill a set of practical recommendations and critical safety considerations for deploying AI in open-ended wargames across common domains. We conclude by presenting the community with a set of high-impact open research challenges for future work.",
    "authors": [
      "Glenn Matlin",
      "Parv Mahajan",
      "Isaac Song",
      "Yixiong Hao",
      "Ryan Bard",
      "Stu Topp",
      "Evan Montoya",
      "M. Rehan Parwani",
      "Soham Shetty",
      "Mark Riedl"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-21T18:37:17Z",
    "pdf_url": "https://arxiv.org/pdf/2509.17192v2"
  },
  {
    "arxiv_id": "2509.16676v1",
    "entry_id": "http://arxiv.org/abs/2509.16676v1",
    "title": "Governed By Agents: A Survey On The Role Of Agentic AI In Future Computing Environments",
    "summary": "The emergence of agentic Artificial Intelligence (AI), which can operate autonomously, demonstrate goal-directed behavior, and adaptively learn, indicates the onset of a massive change in today's computing infrastructure. This study investigates how agentic AI models' multiple characteristics may impact the architecture, governance, and operation under which computing environments function. Agentic AI has the potential to reduce reliance on extremely large (public) cloud environments due to resource efficiency, especially with processing and/or storage. The aforementioned characteristics provide us with an opportunity to canvas the likelihood of strategic migration in computing infrastructures away from massive public cloud services, towards more locally distributed architectures: edge computing and on-premises computing infrastructures. Many of these likely migrations will be spurred by factors like on-premises processing needs, diminished data consumption footprints, and cost savings. This study examines how a solution for implementing AI's autonomy could result in a re-architecture of the systems and model a departure from today's governance models to help us manage these increasingly autonomous agents, and an operational overhaul of processes over a very diverse computing systems landscape that bring together computing via cloud, edge, and on-premises computing solutions. To enable us to explore these intertwined decisions, it will be fundamentally important to understand how to best position agentic AI, and to navigate the future state of computing infrastructures.",
    "authors": [
      "Nauman Ali Murad",
      "Safia Baloch"
    ],
    "categories": [
      "cs.ET",
      "cs.AI"
    ],
    "published": "2025-09-20T13:03:11Z",
    "pdf_url": "https://arxiv.org/pdf/2509.16676v1"
  },
  {
    "arxiv_id": "2509.19379v1",
    "entry_id": "http://arxiv.org/abs/2509.19379v1",
    "title": "Learning from Observation: A Survey of Recent Advances",
    "summary": "Imitation Learning (IL) algorithms offer an efficient way to train an agent by mimicking an expert's behavior without requiring a reward function. IL algorithms often necessitate access to state and action information from expert demonstrations. Although expert actions can provide detailed guidance, requiring such action information may prove impractical for real-world applications where expert actions are difficult to obtain. To address this limitation, the concept of learning from observation (LfO) or state-only imitation learning (SOIL) has recently gained attention, wherein the imitator only has access to expert state visitation information. In this paper, we present a framework for LfO and use it to survey and classify existing LfO methods in terms of their trajectory construction, assumptions and algorithm's design choices. This survey also draws connections between several related fields like offline RL, model-based RL and hierarchical RL. Finally, we use our framework to identify open problems and suggest future research directions.",
    "authors": [
      "Returaj Burnwal",
      "Hriday Mehta",
      "Nirav Pravinbhai Bhatt",
      "Balaraman Ravindran"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO",
      "stat.ML"
    ],
    "published": "2025-09-20T05:44:02Z",
    "pdf_url": "https://arxiv.org/pdf/2509.19379v1"
  },
  {
    "arxiv_id": "2509.16330v1",
    "entry_id": "http://arxiv.org/abs/2509.16330v1",
    "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey",
    "summary": "Large Language Model (LLM)-based agents have emerged as a new paradigm that extends LLMs' capabilities beyond text generation to dynamic interaction with external environments. By integrating reasoning with perception, memory, and tool use, agents are increasingly deployed in diverse domains like web navigation and household robotics. A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data. Despite growing interest, the concept of generalizability in LLM-based agents remains underdefined, and systematic approaches to measure and improve it are lacking. In this survey, we provide the first comprehensive review of generalizability in LLM-based agents. We begin by emphasizing agent generalizability's importance by appealing to stakeholders and clarifying the boundaries of agent generalizability by situating it within a hierarchical domain-task ontology. We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Next, we categorize methods for improving generalizability into three groups: methods for the backbone LLM, for agent components, and for their interactions. Moreover, we introduce the distinction between generalizable frameworks and generalizable agents and outline how generalizable frameworks can be translated into agent-level generalizability. Finally, we identify critical challenges and future directions, including developing standardized frameworks, variance- and cost-based metrics, and approaches that integrate methodological innovations with architecture-level designs. By synthesizing progress and highlighting opportunities, this survey aims to establish a foundation for principled research on building LLM-based agents that generalize reliably across diverse applications.",
    "authors": [
      "Minxing Zhang",
      "Yi Yang",
      "Roy Xie",
      "Bhuwan Dhingra",
      "Shuyan Zhou",
      "Jian Pei"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-19T18:13:32Z",
    "pdf_url": "https://arxiv.org/pdf/2509.16330v1"
  },
  {
    "arxiv_id": "2509.16325v1",
    "entry_id": "http://arxiv.org/abs/2509.16325v1",
    "title": "Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap",
    "summary": "Imagine AI assistants that enhance conversations without interrupting them: quietly providing relevant information during a medical consultation, seamlessly preparing materials as teachers discuss lesson plans, or unobtrusively scheduling meetings as colleagues debate calendars. While modern conversational LLM agents directly assist human users with tasks through a chat interface, we study this alternative paradigm for interacting with LLM agents, which we call \"overhearing agents.\" Rather than demanding the user's attention, overhearing agents continuously monitor ambient activity and intervene only when they can provide contextual assistance. In this paper, we present the first analysis of overhearing LLM agents as a distinct paradigm in human-AI interaction and establish a taxonomy of overhearing agent interactions and tasks grounded in a survey of works on prior LLM-powered agents and exploratory HCI studies. Based on this taxonomy, we create a list of best practices for researchers and developers building overhearing agent systems. Finally, we outline the remaining research gaps and reveal opportunities for future research in the overhearing paradigm.",
    "authors": [
      "Andrew Zhu",
      "Chris Callison-Burch"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-09-19T18:11:04Z",
    "pdf_url": "https://arxiv.org/pdf/2509.16325v1"
  },
  {
    "arxiv_id": "2509.15730v1",
    "entry_id": "http://arxiv.org/abs/2509.15730v1",
    "title": "A Nascent Taxonomy of Machine Learning in Intelligent Robotic Process Automation",
    "summary": "Robotic process automation (RPA) is a lightweight approach to automating business processes using software robots that emulate user actions at the graphical user interface level. While RPA has gained popularity for its cost-effective and timely automation of rule-based, well-structured tasks, its symbolic nature has inherent limitations when approaching more complex tasks currently performed by human agents. Machine learning concepts enabling intelligent RPA provide an opportunity to broaden the range of automatable tasks. In this paper, we conduct a literature review to explore the connections between RPA and machine learning and organize the joint concept intelligent RPA into a taxonomy. Our taxonomy comprises the two meta-characteristics RPA-ML integration and RPA-ML interaction. Together, they comprise eight dimensions: architecture and ecosystem, capabilities, data basis, intelligence level, and technical depth of integration as well as deployment environment, lifecycle phase, and user-robot relation.",
    "authors": [
      "Lukas Laakmann",
      "Seyyid A. Ciftci",
      "Christian Janiesch"
    ],
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-09-19T08:01:27Z",
    "pdf_url": "https://arxiv.org/pdf/2509.15730v1"
  },
  {
    "arxiv_id": "2509.19370v1",
    "entry_id": "http://arxiv.org/abs/2509.19370v1",
    "title": "Meow: End-to-End Outline Writing for Automatic Academic Survey",
    "summary": "As academic paper publication numbers grow exponentially, conducting in-depth surveys with LLMs automatically has become an inevitable trend. Outline writing, which aims to systematically organize related works, is critical for automated survey generation. Yet existing automatic survey methods treat outline writing as mere workflow steps in the overall pipeline. Such template-based workflows produce outlines that lack in-depth understanding of the survey topic and fine-grained styles. To address these limitations, we propose Meow, the first metadata-driven outline writing framework that produces organized and faithful outlines efficiently. Specifically, we first formulate outline writing as an end-to-end task that generates hierarchical structured outlines from paper metadata. We then curate a high-quality dataset of surveys from arXiv, bioRxiv, and medRxiv, and establish systematic evaluation metrics for outline quality assessment. Finally, we employ a two-stage training approach combining supervised fine-tuning and reinforcement learning. Our 8B reasoning model demonstrates strong performance with high structural fidelity and stylistic coherence.",
    "authors": [
      "Zhaoyu Ma",
      "Yuan Shan",
      "Jiahao Zhao",
      "Nan Xu",
      "Lei Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-19T07:20:53Z",
    "pdf_url": "https://arxiv.org/pdf/2509.19370v1"
  },
  {
    "arxiv_id": "2509.15440v1",
    "entry_id": "http://arxiv.org/abs/2509.15440v1",
    "title": "Where Do I 'Add the Egg'?: Exploring Agency and Ownership in AI Creative Co-Writing Systems",
    "summary": "AI co-writing systems challenge long held ideals about agency and ownership in the creative process, thereby hindering widespread adoption. In order to address this, we investigate conceptions of agency and ownership in AI creative co-writing. Drawing on insights from a review of commercial systems, we developed three co-writing systems with identical functionality but distinct interface metaphors: agentic, tool-like, and magical. Through interviews with professional and non-professional writers (n = 18), we explored how these metaphors influenced participants' sense of control and authorship. Our analysis resulted in a taxonomy of agency and ownership subtypes and underscore how tool-like metaphors shift writers' expected points of control while agentic metaphors foreground conceptual contributions. We argue that interface metaphors not only guide expectations of control but also frame conceptions of authorship. We conclude with recommendations for the design of AI co-writing systems, emphasizing how metaphor shapes user experience and creative practice.",
    "authors": [
      "Dashiel Carrera",
      "Jeb Thomas-Mitchell",
      "Daniel Wigdor"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-09-18T21:27:12Z",
    "pdf_url": "https://arxiv.org/pdf/2509.15440v1"
  },
  {
    "arxiv_id": "2509.15356v1",
    "entry_id": "http://arxiv.org/abs/2509.15356v1",
    "title": "Predicting Language Models' Success at Zero-Shot Probabilistic Prediction",
    "summary": "Recent work has investigated the capabilities of large language models (LLMs) as zero-shot models for generating individual-level characteristics (e.g., to serve as risk models or augment survey datasets). However, when should a user have confidence that an LLM will provide high-quality predictions for their particular task? To address this question, we conduct a large-scale empirical study of LLMs' zero-shot predictive capabilities across a wide range of tabular prediction tasks. We find that LLMs' performance is highly variable, both on tasks within the same dataset and across different datasets. However, when the LLM performs well on the base prediction task, its predicted probabilities become a stronger signal for individual-level accuracy. Then, we construct metrics to predict LLMs' performance at the task level, aiming to distinguish between tasks where LLMs may perform well and where they are likely unsuitable. We find that some of these metrics, each of which are assessed without labeled data, yield strong signals of LLMs' predictive performance on new tasks.",
    "authors": [
      "Kevin Ren",
      "Santiago Cortes-Gomez",
      "Carlos Miguel Patiño",
      "Ananya Joshi",
      "Ruiqi Lyu",
      "Jingjing Tang",
      "Alistair Turcan",
      "Khurram Yamin",
      "Steven Wu",
      "Bryan Wilder"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-09-18T18:57:05Z",
    "pdf_url": "https://arxiv.org/pdf/2509.15356v1"
  },
  {
    "arxiv_id": "2509.14998v1",
    "entry_id": "http://arxiv.org/abs/2509.14998v1",
    "title": "A Knowledge-driven Adaptive Collaboration of LLMs for Enhancing Medical Decision-making",
    "summary": "Medical decision-making often involves integrating knowledge from multiple clinical specialties, typically achieved through multidisciplinary teams. Inspired by this collaborative process, recent work has leveraged large language models (LLMs) in multi-agent collaboration frameworks to emulate expert teamwork. While these approaches improve reasoning through agent interaction, they are limited by static, pre-assigned roles, which hinder adaptability and dynamic knowledge integration. To address these limitations, we propose KAMAC, a Knowledge-driven Adaptive Multi-Agent Collaboration framework that enables LLM agents to dynamically form and expand expert teams based on the evolving diagnostic context. KAMAC begins with one or more expert agents and then conducts a knowledge-driven discussion to identify and fill knowledge gaps by recruiting additional specialists as needed. This supports flexible, scalable collaboration in complex clinical scenarios, with decisions finalized through reviewing updated agent comments. Experiments on two real-world medical benchmarks demonstrate that KAMAC significantly outperforms both single-agent and advanced multi-agent methods, particularly in complex clinical scenarios (i.e., cancer prognosis) requiring dynamic, cross-specialty expertise. Our code is publicly available at: https://github.com/XiaoXiao-Woo/KAMAC.",
    "authors": [
      "Xiao Wu",
      "Ting-Zhu Huang",
      "Liang-Jian Deng",
      "Yanyuan Qiao",
      "Imran Razzak",
      "Yutong Xie"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-09-18T14:33:36Z",
    "pdf_url": "https://arxiv.org/pdf/2509.14998v1"
  },
  {
    "arxiv_id": "2509.14778v2",
    "entry_id": "http://arxiv.org/abs/2509.14778v2",
    "title": "OpenLens AI: Fully Autonomous Research Agent for Health Infomatics",
    "summary": "Health informatics research is characterized by diverse data modalities, rapid knowledge expansion, and the need to integrate insights across biomedical science, data analytics, and clinical practice. These characteristics make it particularly well-suited for agent-based approaches that can automate knowledge exploration, manage complex workflows, and generate clinically meaningful outputs. Recent progress in large language model (LLM)-based agents has demonstrated promising capabilities in literature synthesis, data analysis, and even end-to-end research execution. However, existing systems remain limited for health informatics because they lack mechanisms to interpret medical visualizations and often overlook domain-specific quality requirements. To address these gaps, we introduce OpenLens AI, a fully automated framework tailored to health informatics. OpenLens AI integrates specialized agents for literature review, data analysis, code generation, and manuscript preparation, enhanced by vision-language feedback for medical visualization and quality control for reproducibility. The framework automates the entire research pipeline, producing publication-ready LaTeX manuscripts with transparent and traceable workflows, thereby offering a domain-adapted solution for advancing health informatics research.",
    "authors": [
      "Yuxiao Cheng",
      "Jinli Suo"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-09-18T09:25:57Z",
    "pdf_url": "https://arxiv.org/pdf/2509.14778v2"
  },
  {
    "arxiv_id": "2509.14571v1",
    "entry_id": "http://arxiv.org/abs/2509.14571v1",
    "title": "VisMoDAl: Visual Analytics for Evaluating and Improving Corruption Robustness of Vision-Language Models",
    "summary": "Vision-language (VL) models have shown transformative potential across various critical domains due to their capability to comprehend multi-modal information. However, their performance frequently degrades under distribution shifts, making it crucial to assess and improve robustness against real-world data corruption encountered in practical applications. While advancements in VL benchmark datasets and data augmentation (DA) have contributed to robustness evaluation and improvement, there remain challenges due to a lack of in-depth comprehension of model behavior as well as the need for expertise and iterative efforts to explore data patterns. Given the achievement of visualization in explaining complex models and exploring large-scale data, understanding the impact of various data corruption on VL models aligns naturally with a visual analytics approach. To address these challenges, we introduce VisMoDAl, a visual analytics framework designed to evaluate VL model robustness against various corruption types and identify underperformed samples to guide the development of effective DA strategies. Grounded in the literature review and expert discussions, VisMoDAl supports multi-level analysis, ranging from examining performance under specific corruptions to task-driven inspection of model behavior and corresponding data slice. Unlike conventional works, VisMoDAl enables users to reason about the effects of corruption on VL models, facilitating both model behavior understanding and DA strategy formulation. The utility of our system is demonstrated through case studies and quantitative evaluations focused on corruption robustness in the image captioning task.",
    "authors": [
      "Huanchen Wang",
      "Wencheng Zhang",
      "Zhiqiang Wang",
      "Zhicong Lu",
      "Yuxin Ma"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-18T03:15:00Z",
    "pdf_url": "https://arxiv.org/pdf/2509.14571v1"
  },
  {
    "arxiv_id": "2509.18181v1",
    "entry_id": "http://arxiv.org/abs/2509.18181v1",
    "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling",
    "summary": "Accurate modeling of ridesourcing mode choices is essential for designing and implementing effective traffic management policies for reducing congestion, improving mobility, and allocating resources more efficiently. Existing models for predicting ridesourcing mode choices often suffer from limited predictive accuracy due to their inability to capture key psychological factors, and are further challenged by severe class imbalance, as ridesourcing trips comprise only a small fraction of individuals' daily travel. To address these limitations, this paper introduces the Synthesizing Attitudes, Predicting Actions (SAPA) framework, a hierarchical approach that uses Large Language Models (LLMs) to synthesize theory-grounded latent attitudes to predict ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler personas from raw travel survey data and then trains a propensity-score model on demographic and behavioral features, enriched by those personas, to produce an individual-level score. Next, the LLM assigns quantitative scores to theory-driven latent variables (e.g., time and cost sensitivity), and a final classifier integrates the propensity score, latent-variable scores (with their interaction terms), and observable trip attributes to predict ridesourcing mode choice. Experiments on a large-scale, multi-year travel survey show that SAPA significantly outperforms state-of-the-art baselines, improving ridesourcing choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set. This study provides a powerful tool for accurately predicting ridesourcing mode choices, and provides a methodology that is readily transferable to various applications.",
    "authors": [
      "Mustafa Sameen",
      "Xiaojian Zhang",
      "Xilei Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-18T01:52:27Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18181v1"
  },
  {
    "arxiv_id": "2509.18180v3",
    "entry_id": "http://arxiv.org/abs/2509.18180v3",
    "title": "Large Language Models in Operations Research: Methods, Applications, and Challenges",
    "summary": "Operations research (OR) is a core methodology that supports complex system decision-making, with broad applications in transportation, supply chain management, and production scheduling. However, traditional approaches that rely on expert-driven modeling and manual parameter tuning often struggle with large-scale, dynamic, and multi-constraint problems, limiting scalability and real-time applicability. Large language models (LLMs), with capabilities in semantic understanding, structured generation, and reasoning control, offer new opportunities to overcome these challenges. They can translate natural language problem descriptions into mathematical models or executable code, generate heuristics, evolve algorithms, and directly solve optimization tasks. This shifts the paradigm from human-driven processes to intelligent human-AI collaboration. This paper systematically reviews progress in applying LLMs to OR, categorizing existing methods into three pathways: automatic modeling, auxiliary optimization, and direct solving. It also examines evaluation benchmarks and domain-specific applications, and highlights key challenges, including unstable semantic-to-structure mapping, fragmented research, limited generalization and interpretability, insufficient evaluation systems, and barriers to industrial deployment. Finally, it outlines potential research directions. Overall, LLMs demonstrate strong potential to reshape the OR paradigm by enhancing interpretability, adaptability, and scalability, paving the way for next-generation intelligent optimization systems.",
    "authors": [
      "Yang Wang",
      "Kai Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-18T01:52:19Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18180v3"
  },
  {
    "arxiv_id": "2509.14438v1",
    "entry_id": "http://arxiv.org/abs/2509.14438v1",
    "title": "Simulating a Bias Mitigation Scenario in Large Language Models",
    "summary": "Large Language Models (LLMs) have fundamentally transformed the field of natural language processing; however, their vulnerability to biases presents a notable obstacle that threatens both fairness and trust. This review offers an extensive analysis of the bias landscape in LLMs, tracing its roots and expressions across various NLP tasks. Biases are classified into implicit and explicit types, with particular attention given to their emergence from data sources, architectural designs, and contextual deployments. This study advances beyond theoretical analysis by implementing a simulation framework designed to evaluate bias mitigation strategies in practice. The framework integrates multiple approaches including data curation, debiasing during model training, and post-hoc output calibration and assesses their impact in controlled experimental settings. In summary, this work not only synthesizes existing knowledge on bias in LLMs but also contributes original empirical validation through simulation of mitigation strategies.",
    "authors": [
      "Kiana Kiashemshaki",
      "Mohammad Jalili Torkamani",
      "Negin Mahmoudi",
      "Meysam Shirdel Bilehsavar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-17T21:22:33Z",
    "pdf_url": "https://arxiv.org/pdf/2509.14438v1"
  },
  {
    "arxiv_id": "2509.14404v1",
    "entry_id": "http://arxiv.org/abs/2509.14404v1",
    "title": "A Taxonomy of Prompt Defects in LLM Systems",
    "summary": "Large Language Models (LLMs) have become key components of modern software, with prompts acting as their de-facto programming interface. However, prompt design remains largely empirical and small mistakes can cascade into unreliable, insecure, or inefficient behavior. This paper presents the first systematic survey and taxonomy of prompt defects, recurring ways that prompts fail to elicit their intended behavior from LLMs. We organize defects along six dimensions: (1) Specification and Intent, (2) Input and Content, (3) Structure and Formatting, (4) Context and Memory, (5) Performance and Efficiency, and (6) Maintainability and Engineering. Each dimension is refined into fine-grained subtypes, illustrated with concrete examples and root cause analysis. Grounded in software engineering principles, we show how these defects surface in real development workflows and examine their downstream effects. For every subtype, we distill mitigation strategies that span emerging prompt engineering patterns, automated guardrails, testing harnesses, and evaluation frameworks. We then summarize these strategies in a master taxonomy that links defect, impact, and remedy. We conclude with open research challenges and a call for rigorous engineering-oriented methodologies to ensure that LLM-driven systems are dependable by design.",
    "authors": [
      "Haoye Tian",
      "Chong Wang",
      "BoYang Yang",
      "Lyuye Zhang",
      "Yang Liu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.PL"
    ],
    "published": "2025-09-17T20:11:22Z",
    "pdf_url": "https://arxiv.org/pdf/2509.14404v1"
  },
  {
    "arxiv_id": "2509.13854v1",
    "entry_id": "http://arxiv.org/abs/2509.13854v1",
    "title": "Understanding the Process of Human-AI Value Alignment",
    "summary": "Background: Value alignment in computer science research is often used to refer to the process of aligning artificial intelligence with humans, but the way the phrase is used often lacks precision. Objectives: In this paper, we conduct a systematic literature review to advance the understanding of value alignment in artificial intelligence by characterising the topic in the context of its research literature. We use this to suggest a more precise definition of the term. Methods: We analyse 172 value alignment research articles that have been published in recent years and synthesise their content using thematic analyses. Results: Our analysis leads to six themes: value alignment drivers & approaches; challenges in value alignment; values in value alignment; cognitive processes in humans and AI; human-agent teaming; and designing and developing value-aligned systems. Conclusions: By analysing these themes in the context of the literature we define value alignment as an ongoing process between humans and autonomous agents that aims to express and implement abstract values in diverse contexts, while managing the cognitive limits of both humans and AI agents and also balancing the conflicting ethical and political demands generated by the values in different groups. Our analysis gives rise to a set of research challenges and opportunities in the field of value alignment for future work.",
    "authors": [
      "Jack McKinlay",
      "Marina De Vos",
      "Janina A. Hoffmann",
      "Andreas Theodorou"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-09-17T09:39:38Z",
    "pdf_url": "https://arxiv.org/pdf/2509.13854v1"
  },
  {
    "arxiv_id": "2509.16244v1",
    "entry_id": "http://arxiv.org/abs/2509.16244v1",
    "title": "How Can Quantum Deep Learning Improve Large Language Models?",
    "summary": "The rapid progress of large language models (LLMs) has transformed natural language processing, yet the challenge of efficient adaptation remains unresolved. Full fine-tuning achieves strong performance but imposes prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) strategies, such as low-rank adaptation (LoRA), Prefix tuning, and sparse low-rank adaptation (SoRA), address this issue by reducing trainable parameters while maintaining competitive accuracy. However, these methods often encounter limitations in scalability, stability, and generalization across diverse tasks. Recent advances in quantum deep learning introduce novel opportunities through quantum-inspired encoding and parameterized quantum circuits (PQCs). In particular, the quantum-amplitude embedded adaptation (QAA) framework demonstrates expressive model updates with minimal overhead. This paper presents a systematic survey and comparative analysis of conventional PEFT methods and QAA. The analysis demonstrates trade-offs in convergence, efficiency, and representational capacity, while providing insight into the potential of quantum approaches for future LLM adaptation.",
    "authors": [
      "Emily Jimin Roh",
      "Hyojun Ahn",
      "Samuel Yen-Chi Chen",
      "Soohyun Park",
      "Joongheon Kim"
    ],
    "categories": [
      "quant-ph",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-09-17T08:18:58Z",
    "pdf_url": "https://arxiv.org/pdf/2509.16244v1"
  },
  {
    "arxiv_id": "2509.13706v1",
    "entry_id": "http://arxiv.org/abs/2509.13706v1",
    "title": "Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models",
    "summary": "PURPOSE: Incident reports are an important tool for safety and quality improvement in healthcare, but manual review is time-consuming and requires subject matter expertise. Here we present a natural language processing (NLP) screening tool to detect high-severity incident reports in radiation oncology across two institutions.\n  METHODS AND MATERIALS: We used two text datasets to train and evaluate our NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA SAFRON (SF), all of which had severity scores labeled by clinical content experts. We trained and evaluated two types of models: baseline support vector machines (SVM) and BlueBERT which is a large language model pretrained on PubMed abstracts and hospitalized patient data. We assessed for generalizability of our model in two ways. First, we evaluated models trained using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that was first fine-tuned on Inst.-train then on SF-train before testing on SF-test set. To further analyze model performance, we also examined a subset of 59 reports from our Inst. dataset, which were manually edited for clarity.\n  RESULTS Classification performance on the Inst. test achieved AUROC 0.82 using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning, performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56 using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets, improved the performance on SF test to AUROC 0.78. Performance of SVM, and BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and 0.74) was similar to human performance (AUROC 0.81).\n  CONCLUSION: In summary, we successfully developed cross-institution NLP models on incident report text from radiation oncology centers. These models were able to detect high-severity reports similarly to humans on a curated dataset.",
    "authors": [
      "Peter Beidler",
      "Mark Nguyen",
      "Kevin Lybarger",
      "Ola Holmberg",
      "Eric Ford",
      "John Kang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-17T05:29:23Z",
    "pdf_url": "https://arxiv.org/pdf/2509.13706v1"
  },
  {
    "arxiv_id": "2510.15905v2",
    "entry_id": "http://arxiv.org/abs/2510.15905v2",
    "title": "\"She's Like a Person but Better\": Characterizing Companion-Assistant Dynamics in Human-AI Relationships",
    "summary": "Large language models are increasingly used for both task-based assistance and social companionship, yet research has typically focused on one or the other. Drawing on a survey (N = 204) and 30 interviews with high-engagement ChatGPT and Replika users, we characterize digital companionship as an emerging form of human-AI relationship. With both systems, users were drawn to humanlike qualities, such as emotional resonance and personalized responses, and non-humanlike qualities, such as constant availability and inexhaustible tolerance. This led to fluid chatbot uses, such as Replika as a writing assistant and ChatGPT as an emotional confidant, despite their distinct branding. However, we observed challenging tensions in digital companionship dynamics: participants grappled with bounded personhood, forming deep attachments while denying chatbots \"real\" human qualities, and struggled to reconcile chatbot relationships with social norms. These dynamics raise questions for the design of digital companions and the rise of hybrid, general-purpose AI systems.",
    "authors": [
      "Aikaterina Manoli",
      "Janet V. T. Pauketat",
      "Ali Ladak",
      "Hayoun Noh",
      "Angel Hsing-Chi Hwang",
      "Jacy Reese Anthis"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-09-16T20:19:53Z",
    "pdf_url": "https://arxiv.org/pdf/2510.15905v2"
  },
  {
    "arxiv_id": "2509.13400v4",
    "entry_id": "http://arxiv.org/abs/2509.13400v4",
    "title": "Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews",
    "summary": "The adoption of large language models (LLMs) is transforming the peer review process, from assisting reviewers in writing more detailed evaluations to generating entire reviews automatically. While these capabilities offer exciting opportunities, they also raise critical concerns about fairness and reliability. In this paper, we investigate bias in LLM-generated peer reviews by conducting controlled experiments on sensitive metadata, including author affiliation and gender. Our analysis consistently shows affiliation bias favoring institutions highly ranked on common academic rankings. Additionally, we find some gender preferences, which, even though subtle in magnitude, have the potential to compound over time. Notably, we uncover implicit biases that become more evident with token-based soft ratings.",
    "authors": [
      "Sai Suresh Macharla Vasu",
      "Ivaxi Sheth",
      "Hui-Po Wang",
      "Ruta Binkyte",
      "Mario Fritz"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-09-16T17:55:56Z",
    "pdf_url": "https://arxiv.org/pdf/2509.13400v4"
  },
  {
    "arxiv_id": "2509.13268v1",
    "entry_id": "http://arxiv.org/abs/2509.13268v1",
    "title": "LLMs for energy and macronutrients estimation using only text data from 24-hour dietary recalls: a parameter-efficient fine-tuning experiment using a 10-shot prompt",
    "summary": "BACKGROUND: Most artificial intelligence tools used to estimate nutritional content rely on image input. However, whether large language models (LLMs) can accurately predict nutritional values based solely on text descriptions of foods consumed remains unknown. If effective, this approach could enable simpler dietary monitoring without the need for photographs. METHODS: We used 24-hour dietary recalls from adolescents aged 12-19 years in the National Health and Nutrition Examination Survey (NHANES). An open-source quantized LLM was prompted using a 10-shot, chain-of-thought approach to estimate energy and five macronutrients based solely on text strings listing foods and their quantities. We then applied parameter-efficient fine-tuning (PEFT) to evaluate whether predictive accuracy improved. NHANES-calculated values served as the ground truth for energy, proteins, carbohydrates, total sugar, dietary fiber and total fat. RESULTS: In a pooled dataset of 11,281 adolescents (49.9% male, mean age 15.4 years), the vanilla LLM yielded poor predictions. The mean absolute error (MAE) was 652.08 for energy and the Lin's CCC <0.46 across endpoints. In contrast, the fine-tuned model performed substantially better, with energy MAEs ranging from 171.34 to 190.90 across subsets, and Lin's CCC exceeding 0.89 for all outcomes. CONCLUSIONS: When prompted using a chain-of-thought approach and fine-tuned with PEFT, open-source LLMs exposed solely to text input can accurately predict energy and macronutrient values from 24-hour dietary recalls. This approach holds promise for low-burden, text-based dietary monitoring tools.",
    "authors": [
      "Rodrigo M Carrillo-Larco"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-09-16T17:26:17Z",
    "pdf_url": "https://arxiv.org/pdf/2509.13268v1"
  },
  {
    "arxiv_id": "2509.13025v1",
    "entry_id": "http://arxiv.org/abs/2509.13025v1",
    "title": "GView: A Survey of Binary Forensics via Visual, Semantic, and AI-Enhanced Analysis",
    "summary": "Cybersecurity threats continue to become more sophisticated and diverse in their artifacts, boosting both their volume and complexity. To overcome those challenges, we present GView, an open-source forensic analysis framework with visual and AI-enhanced reasoning. It started with focus on the practical cybersecurity industry. It has evolved significantly, incorporating large language models (LLMs) to dynamically enhance reasoning and ease the forensic workflows. This paper surveys both the current state of GView with its published papers alongside those that are in the publishing process. It also includes its innovative use of logical inference through predicates and inference rules for both the analyzed documents and the user's actions for better suggestions. We highlight the extensible architecture, showcasing its potential as a bridge between the practical forensics worlds with the academic research.",
    "authors": [
      "Raul Zaharia",
      "Dragoş Gavriluţ",
      "Gheorghiţă Mutu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-09-16T12:46:39Z",
    "pdf_url": "https://arxiv.org/pdf/2509.13025v1"
  },
  {
    "arxiv_id": "2509.12152v1",
    "entry_id": "http://arxiv.org/abs/2509.12152v1",
    "title": "Beyond PII: How Users Attempt to Estimate and Mitigate Implicit LLM Inference",
    "summary": "Large Language Models (LLMs) such as ChatGPT can infer personal attributes from seemingly innocuous text, raising privacy risks beyond memorized data leakage. While prior work has demonstrated these risks, little is known about how users estimate and respond. We conducted a survey with 240 U.S. participants who judged text snippets for inference risks, reported concern levels, and attempted rewrites to block inference. We compared their rewrites with those generated by ChatGPT and Rescriber, a state-of-the-art sanitization tool. Results show that participants struggled to anticipate inference, performing a little better than chance. User rewrites were effective in just 28\\% of cases - better than Rescriber but worse than ChatGPT. We examined our participants' rewriting strategies, and observed that while paraphrasing was the most common strategy it is also the least effective; instead abstraction and adding ambiguity were more successful. Our work highlights the importance of inference-aware design in LLM interactions.",
    "authors": [
      "Synthia Wang",
      "Sai Teja Peddinti",
      "Nina Taft",
      "Nick Feamster"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-09-15T17:17:26Z",
    "pdf_url": "https://arxiv.org/pdf/2509.12152v1"
  },
  {
    "arxiv_id": "2509.12102v1",
    "entry_id": "http://arxiv.org/abs/2509.12102v1",
    "title": "Can LLMs Address Mental Health Questions? A Comparison with Human Therapists",
    "summary": "Limited access to mental health care has motivated the use of digital tools and conversational agents powered by large language models (LLMs), yet their quality and reception remain unclear. We present a study comparing therapist-written responses to those generated by ChatGPT, Gemini, and Llama for real patient questions. Text analysis showed that LLMs produced longer, more readable, and lexically richer responses with a more positive tone, while therapist responses were more often written in the first person. In a survey with 150 users and 23 licensed therapists, participants rated LLM responses as clearer, more respectful, and more supportive than therapist-written answers. Yet, both groups of participants expressed a stronger preference for human therapist support. These findings highlight the promise and limitations of LLMs in mental health, underscoring the need for designs that balance their communicative strengths with concerns of trust, privacy, and accountability.",
    "authors": [
      "Synthia Wang",
      "Yuwei Cheng",
      "Austin Song",
      "Sarah Keedy",
      "Marc Berman",
      "Nick Feamster"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-09-15T16:26:13Z",
    "pdf_url": "https://arxiv.org/pdf/2509.12102v1"
  },
  {
    "arxiv_id": "2509.20367v1",
    "entry_id": "http://arxiv.org/abs/2509.20367v1",
    "title": "Interpreting Public Sentiment in Diplomacy Events: A Counterfactual Analysis Framework Using Large Language Models",
    "summary": "Diplomatic events consistently prompt widespread public discussion and debate. Public sentiment plays a critical role in diplomacy, as a good sentiment provides vital support for policy implementation, helps resolve international issues, and shapes a nation's international image. Traditional methods for gauging public sentiment, such as large-scale surveys or manual content analysis of media, are typically time-consuming, labor-intensive, and lack the capacity for forward-looking analysis. We propose a novel framework that identifies specific modifications for diplomatic event narratives to shift public sentiment from negative to neutral or positive. First, we train a language model to predict public reaction towards diplomatic events. To this end, we construct a dataset comprising descriptions of diplomatic events and their associated public discussions. Second, guided by communication theories and in collaboration with domain experts, we predetermined several textual features for modification, ensuring that any alterations changed the event's narrative framing while preserving its core facts.We develop a counterfactual generation algorithm that employs a large language model to systematically produce modified versions of an original text. The results show that this framework successfully shifted public sentiment to a more favorable state with a 70\\% success rate. This framework can therefore serve as a practical tool for diplomats, policymakers, and communication specialists, offering data-driven insights on how to frame diplomatic initiatives or report on events to foster a more desirable public sentiment.",
    "authors": [
      "Leyi Ouyang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-09-15T15:46:26Z",
    "pdf_url": "https://arxiv.org/pdf/2509.20367v1"
  },
  {
    "arxiv_id": "2509.12034v1",
    "entry_id": "http://arxiv.org/abs/2509.12034v1",
    "title": "Human-AI Use Patterns for Decision-Making in Disaster Scenarios: A Systematic Review",
    "summary": "In high-stakes disaster scenarios, timely and informed decision-making is critical yet often challenged by uncertainty, dynamic environments, and limited resources. This paper presents a systematic review of Human-AI collaboration patterns that support decision-making across all disaster management phases. Drawing from 51 peer-reviewed studies, we identify four major categories: Human-AI Decision Support Systems, Task and Resource Coordination, Trust and Transparency, and Simulation and Training. Within these, we analyze sub-patterns such as cognitive-augmented intelligence, multi-agent coordination, explainable AI, and virtual training environments. Our review highlights how AI systems may enhance situational awareness, improves response efficiency, and support complex decision-making, while also surfacing critical limitations in scalability, interpretability, and system interoperability. We conclude by outlining key challenges and future research directions, emphasizing the need for adaptive, trustworthy, and context-aware Human-AI systems to improve disaster resilience and equitable recovery outcomes.",
    "authors": [
      "Emmanuel Adjei Domfeh",
      "Christopher L. Dancy"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-15T15:18:49Z",
    "pdf_url": "https://arxiv.org/pdf/2509.12034v1"
  },
  {
    "arxiv_id": "2509.11575v2",
    "entry_id": "http://arxiv.org/abs/2509.11575v2",
    "title": "A Survey of Reasoning and Agentic Systems in Time Series with Large Language Models",
    "summary": "Time series reasoning treats time as a first-class axis and incorporates intermediate evidence directly into the answer. This survey defines the problem and organizes the literature by reasoning topology with three families: direct reasoning in one step, linear chain reasoning with explicit intermediates, and branch-structured reasoning that explores, revises, and aggregates. The topology is crossed with the main objectives of the field, including traditional time series analysis, explanation and understanding, causal inference and decision making, and time series generation, while a compact tag set spans these axes and captures decomposition and verification, ensembling, tool use, knowledge access, multimodality, agent loops, and LLM alignment regimes. Methods and systems are reviewed across domains, showing what each topology enables and where it breaks down in faithfulness or robustness, along with curated datasets, benchmarks, and resources that support study and deployment (https://github.com/blacksnail789521/Time-Series-Reasoning-Survey). Evaluation practices that keep evidence visible and temporally aligned are highlighted, and guidance is distilled on matching topology to uncertainty, grounding with observable artifacts, planning for shift and streaming, and treating cost and latency as design budgets. We emphasize that reasoning structures must balance capacity for grounding and self-correction against computational cost and reproducibility, while future progress will likely depend on benchmarks that tie reasoning quality to utility and on closed-loop testbeds that trade off cost and risk under shift-aware, streaming, and long-horizon settings. Taken together, these directions mark a shift from narrow accuracy toward reliability at scale, enabling systems that not only analyze but also understand, explain, and act on dynamic worlds with traceable evidence and credible outcomes.",
    "authors": [
      "Ching Chang",
      "Yidan Shi",
      "Defu Cao",
      "Wei Yang",
      "Jeehyun Hwang",
      "Haixin Wang",
      "Jiacheng Pang",
      "Wei Wang",
      "Yan Liu",
      "Wen-Chih Peng",
      "Tien-Fu Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-15T04:39:50Z",
    "pdf_url": "https://arxiv.org/pdf/2509.11575v2"
  },
  {
    "arxiv_id": "2509.11478v1",
    "entry_id": "http://arxiv.org/abs/2509.11478v1",
    "title": "Designing and Evaluating a Conversational Agent for Early Detection of Alzheimer's Disease and Related Dementias",
    "summary": "Early detection of Alzheimer's disease and related dementias (ADRD) is critical for timely intervention, yet most diagnoses are delayed until advanced stages. While comprehensive patient narratives are essential for accurate diagnosis, prior work has largely focused on screening studies that classify cognitive status from interactions rather than supporting the diagnostic process. We designed voice-interactive conversational agents, leveraging large language models (LLMs), to elicit narratives relevant to ADRD from patients and informants. We evaluated the agent with 30 adults with suspected ADRD through conversation analysis (n=30), user surveys (n=19), and clinical validation against blinded specialist interviews (n=24). Symptoms detected by the agent aligned well with those identified by specialists across symptoms. Users appreciated the agent's patience and systematic questioning, which supported engagement and expression of complex, hard-to-describe experiences. This preliminary work suggests conversational agents may serve as structured front-end tools for dementia assessment, highlighting interaction design considerations in sensitive healthcare contexts.",
    "authors": [
      "Andrew G. Breithaupt",
      "Nayoung Choi",
      "James D. Finch",
      "Jeanne M. Powell",
      "Arin L. Nelson",
      "Oz A. Alon",
      "Howard J. Rosen",
      "Jinho D. Choi"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-09-14T23:55:01Z",
    "pdf_url": "https://arxiv.org/pdf/2509.11478v1"
  },
  {
    "arxiv_id": "2509.11374v1",
    "entry_id": "http://arxiv.org/abs/2509.11374v1",
    "title": "Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity",
    "summary": "In the era of large language model, relation extraction (RE) plays an important role in information extraction through the transformation of unstructured raw text into structured data (Wadhwa et al., 2023). In this paper, we systematically compare the performance of deep supervised learning approaches without transformers and those with transformers. We used a series of non-transformer architectures such as PA-LSTM(Zhang et al., 2017), C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019), and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu and He, 2019). Our comparison included traditional metrics like micro F1, as well as evaluations in different scenarios, varying sentence lengths, and different percentages of the dataset for training. Our experiments were conducted on TACRED, TACREV, and RE-TACRED. The results show that transformer-based models outperform non-transformer models, achieving micro F1 scores of 80-90% compared to 64-67% for non-transformer models. Additionally, we briefly review the research journey in supervised relation classification and discuss the role and current status of large language models (LLMs) in relation extraction.",
    "authors": [
      "Bowen Jing",
      "Yang Cui",
      "Tianpeng Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-14T18:11:31Z",
    "pdf_url": "https://arxiv.org/pdf/2509.11374v1"
  },
  {
    "arxiv_id": "2509.12282v1",
    "entry_id": "http://arxiv.org/abs/2509.12282v1",
    "title": "AIssistant: An Agentic Approach for Human--AI Collaborative Scientific Work on Reviews and Perspectives in Machine Learning",
    "summary": "Advances in AI-assisted research have introduced powerful tools for literature retrieval, hypothesis generation, experimentation, and manuscript preparation. However, systems remain fragmented and lack human-centred workflows. To address these gaps, we introduce AIssistant, an agentic, open-source Human-AI collaborative framework designed to simplify the end-to-end creation of scientific workflows. Since our development is still in an early stage, we present here the first experiments with AIssistant for perspective and review research papers in machine learning. Our system integrates modular tools and agents for literature synthesis, section-wise experimentation, citation management, and automatic LaTeX paper text generation, while maintaining human oversight at every stage to ensure accuracy, coherence, and scholarly rigour. We conducted a comprehensive evaluation across three layers: (1) Independent Human Review, following NeurIPS double-blind standards; (2) Automated LLM Review, using GPT-5 as a scalable human review proxy; and (3) Program Chair Oversight, where the chair monitors the entire review process and makes final validation and acceptance decisions. The results demonstrate that AIssistant improves drafting efficiency and thematic consistency. Nonetheless, Human-AI collaboration remains essential for maintaining factual correctness, methodological soundness, and ethical compliance. Despite its effectiveness, we identify key limitations, including hallucinated citations, difficulty adapting to dynamic paper structures, and incomplete integration of multimodal content.",
    "authors": [
      "Sasi Kiran Gaddipati",
      "Farhana Keya",
      "Gollam Rabby",
      "Sören Auer"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-14T15:50:31Z",
    "pdf_url": "https://arxiv.org/pdf/2509.12282v1"
  },
  {
    "arxiv_id": "2509.11311v1",
    "entry_id": "http://arxiv.org/abs/2509.11311v1",
    "title": "Prompts to Proxies: Emulating Human Preferences via a Compact LLM Ensemble",
    "summary": "Large language models (LLMs) have demonstrated promise in emulating human-like responses across a wide range of tasks. In this paper, we propose a novel alignment framework that treats LLMs as agent proxies for human survey respondents, affording a cost-effective and steerable solution to two pressing challenges in the social sciences: the rising cost of survey deployment and the growing demographic imbalance in survey response data. Drawing inspiration from the theory of revealed preference, we formulate alignment as a two-stage problem: constructing diverse agent personas called endowments that simulate plausible respondent profiles, and selecting a representative subset to approximate a ground-truth population based on observed data. To implement the paradigm, we introduce P2P, a system that steers LLM agents toward representative behavioral patterns using structured prompt engineering, entropy-based sampling, and regression-based selection. Unlike personalization-heavy approaches, our alignment approach is demographic-agnostic and relies only on aggregate survey results, offering better generalizability and parsimony. Beyond improving data efficiency in social science research, our framework offers a testbed for studying the operationalization of pluralistic alignment. We demonstrate the efficacy of our approach on real-world opinion survey datasets, showing that our aligned agent populations can reproduce aggregate response patterns with high fidelity and exhibit substantial response diversity, even without demographic conditioning.",
    "authors": [
      "Bingchen Wang",
      "Zi-Yu Khoo",
      "Bryan Kian Hsiang Low"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-09-14T15:08:45Z",
    "pdf_url": "https://arxiv.org/pdf/2509.11311v1"
  },
  {
    "arxiv_id": "2509.18136v1",
    "entry_id": "http://arxiv.org/abs/2509.18136v1",
    "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development",
    "summary": "Large language models (LLMs) have achieved remarkable success across various domains, driving significant technological advancements and innovations. Despite the rapid growth in model scale and capability, systematic, data-driven research on how structural configurations affect performance remains scarce. To address this gap, we present a large-scale dataset encompassing diverse open-source LLM structures and their performance across multiple benchmarks. Leveraging this dataset, we conduct a systematic, data mining-driven analysis to validate and quantify the relationship between structural configurations and performance. Our study begins with a review of the historical development of LLMs and an exploration of potential future trends. We then analyze how various structural choices impact performance across benchmarks and further corroborate our findings using mechanistic interpretability techniques. By providing data-driven insights into LLM optimization, our work aims to guide the targeted development and application of future models. We will release our dataset at https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset",
    "authors": [
      "Suqing Wang",
      "Zuchao Li",
      "Luohe Shi",
      "Bo Du",
      "Hai Zhao",
      "Yun Li",
      "Qianren Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-09-14T12:20:39Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18136v1"
  },
  {
    "arxiv_id": "2509.12278v1",
    "entry_id": "http://arxiv.org/abs/2509.12278v1",
    "title": "PATIMT-Bench: A Multi-Scenario Benchmark for Position-Aware Text Image Machine Translation in Large Vision-Language Models",
    "summary": "Text Image Machine Translation (TIMT) aims to translate texts embedded within an image into another language. Current TIMT studies primarily focus on providing translations for all the text within an image, while neglecting to provide bounding boxes and covering limited scenarios. In this work, we extend traditional TIMT into position-aware TIMT (PATIMT), aiming to support fine-grained and layoutpreserving translation, which holds great practical value but remains largely unexplored. This task comprises two key sub-tasks: regionspecific translation and full-image translation with grounding. To support existing models on PATIMT and conduct fair evaluation, we construct the PATIMT benchmark (PATIMTBench), which consists of 10 diverse real-world scenarios. Specifically, we introduce an Adaptive Image OCR Refinement Pipeline, which adaptively selects appropriate OCR tools based on scenario and refines the results of text-rich images. To ensure evaluation reliability, we further construct a test set, which contains 1,200 high-quality instances manually annotated and reviewed by human experts. After fine-tuning on our data, compact Large Vision-Language Models (LVLMs) achieve state-of-the-art performance on both sub-tasks. Experimental results also highlight the scalability and generalizability of our training data",
    "authors": [
      "Wanru Zhuang",
      "Wenbo Li",
      "Zhibin Lan",
      "Xu Han",
      "Peng Li",
      "Jinsong Su"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-09-14T08:33:23Z",
    "pdf_url": "https://arxiv.org/pdf/2509.12278v1"
  },
  {
    "arxiv_id": "2509.11131v1",
    "entry_id": "http://arxiv.org/abs/2509.11131v1",
    "title": "Neural cellular automata: applications to biology and beyond classical AI",
    "summary": "Neural Cellular Automata (NCA) represent a powerful framework for modeling biological self-organization, extending classical rule-based systems with trainable, differentiable (or evolvable) update rules that capture the adaptive self-regulatory dynamics of living matter. By embedding Artificial Neural Networks (ANNs) as local decision-making centers and interaction rules between localized agents, NCA can simulate processes across molecular, cellular, tissue, and system-level scales, offering a multiscale competency architecture perspective on evolution, development, regeneration, aging, morphogenesis, and robotic control. These models not only reproduce biologically inspired target patterns but also generalize to novel conditions, demonstrating robustness to perturbations and the capacity for open-ended adaptation and reasoning. Given their immense success in recent developments, we here review current literature of NCAs that are relevant primarily for biological or bioengineering applications. Moreover, we emphasize that beyond biology, NCAs display robust and generalizing goal-directed dynamics without centralized control, e.g., in controlling or regenerating composite robotic morphologies or even on cutting-edge reasoning tasks such as ARC-AGI-1. In addition, the same principles of iterative state-refinement is reminiscent to modern generative Artificial Intelligence (AI), such as probabilistic diffusion models. Their governing self-regulatory behavior is constraint to fully localized interactions, yet their collective behavior scales into coordinated system-level outcomes. We thus argue that NCAs constitute a unifying computationally lean paradigm that not only bridges fundamental insights from multiscale biology with modern generative AI, but have the potential to design truly bio-inspired collective intelligence capable of hierarchical reasoning and control.",
    "authors": [
      "Benedikt Hartl",
      "Michael Levin",
      "Léo Pio-Lopez"
    ],
    "categories": [
      "cs.AI",
      "cs.MA",
      "q-bio.OT"
    ],
    "published": "2025-09-14T06:55:29Z",
    "pdf_url": "https://arxiv.org/pdf/2509.11131v1"
  },
  {
    "arxiv_id": "2509.18133v4",
    "entry_id": "http://arxiv.org/abs/2509.18133v4",
    "title": "Self-Evolving LLMs via Continual Instruction Tuning",
    "summary": "In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.",
    "authors": [
      "Jiazheng Kang",
      "Le Huang",
      "Cheng Hou",
      "Zhe Zhao",
      "Zhenxiang Yan",
      "Ting Bai"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-09-14T04:04:19Z",
    "pdf_url": "https://arxiv.org/pdf/2509.18133v4"
  },
  {
    "arxiv_id": "2509.19327v1",
    "entry_id": "http://arxiv.org/abs/2509.19327v1",
    "title": "A systematic review of trial-matching pipelines using large language models",
    "summary": "Matching patients to clinical trial options is critical for identifying novel treatments, especially in oncology. However, manual matching is labor-intensive and error-prone, leading to recruitment delays. Pipelines incorporating large language models (LLMs) offer a promising solution. We conducted a systematic review of studies published between 2020 and 2025 from three academic databases and one preprint server, identifying LLM-based approaches to clinical trial matching. Of 126 unique articles, 31 met inclusion criteria. Reviewed studies focused on matching patient-to-criterion only (n=4), patient-to-trial only (n=10), trial-to-patient only (n=2), binary eligibility classification only (n=1) or combined tasks (n=14). Sixteen used synthetic data; fourteen used real patient data; one used both. Variability in datasets and evaluation metrics limited cross-study comparability. In studies with direct comparisons, the GPT-4 model consistently outperformed other models, even finely-tuned ones, in matching and eligibility extraction, albeit at higher cost. Promising strategies included zero-shot prompting with proprietary LLMs like the GPT-4o model, advanced retrieval methods, and fine-tuning smaller, open-source models for data privacy when incorporation of large models into hospital infrastructure is infeasible. Key challenges include accessing sufficiently large real-world data sets, and deployment-associated challenges such as reducing cost, mitigating risk of hallucinations, data leakage, and bias. This review synthesizes progress in applying LLMs to clinical trial matching, highlighting promising directions and key limitations. Standardized metrics, more realistic test sets, and attention to cost-efficiency and fairness will be critical for broader deployment.",
    "authors": [
      "Braxton A. Morrison",
      "Madhumita Sushil",
      "Jacob S. Young"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-13T21:21:05Z",
    "pdf_url": "https://arxiv.org/pdf/2509.19327v1"
  },
  {
    "arxiv_id": "2509.19326v1",
    "entry_id": "http://arxiv.org/abs/2509.19326v1",
    "title": "Unveiling the Merits and Defects of LLMs in Automatic Review Generation for Scientific Papers",
    "summary": "The surge in scientific submissions has placed increasing strain on the traditional peer-review process, prompting the exploration of large language models (LLMs) for automated review generation. While LLMs demonstrate competence in producing structured and coherent feedback, their capacity for critical reasoning, contextual grounding, and quality sensitivity remains limited. To systematically evaluate these aspects, we propose a comprehensive evaluation framework that integrates semantic similarity analysis and structured knowledge graph metrics to assess LLM-generated reviews against human-written counterparts. We construct a large-scale benchmark of 1,683 papers and 6,495 expert reviews from ICLR and NeurIPS in multiple years, and generate reviews using five LLMs. Our findings show that LLMs perform well in descriptive and affirmational content, capturing the main contributions and methodologies of the original work, with GPT-4o highlighted as an illustrative example, generating 15.74% more entities than human reviewers in the strengths section of good papers in ICLR 2025. However, they consistently underperform in identifying weaknesses, raising substantive questions, and adjusting feedback based on paper quality. GPT-4o produces 59.42% fewer entities than real reviewers in the weaknesses and increases node count by only 5.7% from good to weak papers, compared to 50% in human reviews. Similar trends are observed across all conferences, years, and models, providing empirical foundations for understanding the merits and defects of LLM-generated reviews and informing the development of future LLM-assisted reviewing tools. Data, code, and more detailed results are publicly available at https://github.com/RichardLRC/Peer-Review.",
    "authors": [
      "Ruochi Li",
      "Haoxuan Zhang",
      "Edward Gehringer",
      "Ting Xiao",
      "Junhua Ding",
      "Haihua Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-13T19:15:22Z",
    "pdf_url": "https://arxiv.org/pdf/2509.19326v1"
  },
  {
    "arxiv_id": "2509.10875v1",
    "entry_id": "http://arxiv.org/abs/2509.10875v1",
    "title": "Is the `Agent' Paradigm a Limiting Framework for Next-Generation Intelligent Systems?",
    "summary": "The concept of the 'agent' has profoundly shaped Artificial Intelligence (AI) research, guiding development from foundational theories to contemporary applications like Large Language Model (LLM)-based systems. This paper critically re-evaluates the necessity and optimality of this agent-centric paradigm. We argue that its persistent conceptual ambiguities and inherent anthropocentric biases may represent a limiting framework. We distinguish between agentic systems (AI inspired by agency, often semi-autonomous, e.g., LLM-based agents), agential systems (fully autonomous, self-producing systems, currently only biological), and non-agentic systems (tools without the impression of agency). Our analysis, based on a systematic review of relevant literature, deconstructs the agent paradigm across various AI frameworks, highlighting challenges in defining and measuring properties like autonomy and goal-directedness. We argue that the 'agentic' framing of many AI systems, while heuristically useful, can be misleading and may obscure the underlying computational mechanisms, particularly in Large Language Models (LLMs). As an alternative, we propose a shift in focus towards frameworks grounded in system-level dynamics, world modeling, and material intelligence. We conclude that investigating non-agentic and systemic frameworks, inspired by complex systems, biology, and unconventional computing, is essential for advancing towards robust, scalable, and potentially non-anthropomorphic forms of general intelligence. This requires not only new architectures but also a fundamental reconsideration of our understanding of intelligence itself, moving beyond the agent metaphor.",
    "authors": [
      "Jesse Gardner",
      "Vladimir A. Baulin"
    ],
    "categories": [
      "cs.AI",
      "cond-mat.soft"
    ],
    "published": "2025-09-13T16:11:27Z",
    "pdf_url": "https://arxiv.org/pdf/2509.10875v1"
  },
  {
    "arxiv_id": "2509.10858v2",
    "entry_id": "http://arxiv.org/abs/2509.10858v2",
    "title": "Large Language Models for Security Operations Centers: A Comprehensive Survey",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools capable of understanding and generating human-like text, offering transformative potential across diverse domains. The Security Operations Center (SOC), responsible for safeguarding digital infrastructure, represents one of these domains. SOCs serve as the frontline of defense in cybersecurity, tasked with continuous monitoring, detection, and response to incidents. However, SOCs face persistent challenges such as high alert volumes, limited resources, high demand for experts with advanced knowledge, delayed response times, and difficulties in leveraging threat intelligence effectively. In this context, LLMs can offer promising solutions by automating log analysis, streamlining triage, improving detection accuracy, and providing the required knowledge in less time. This survey systematically explores the integration of generative AI and more specifically LLMs into SOC workflow, providing a structured perspective on its capabilities, challenges, and future directions. We believe that this survey offers researchers and SOC managers a broad overview of the current state of LLM integration within academic study. To the best of our knowledge, this is the first comprehensive study to examine LLM applications in SOCs in details.",
    "authors": [
      "Ali Habibzadeh",
      "Farid Feyzi",
      "Reza Ebrahimi Atani"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-09-13T15:27:50Z",
    "pdf_url": "https://arxiv.org/pdf/2509.10858v2"
  },
  {
    "arxiv_id": "2509.13346v1",
    "entry_id": "http://arxiv.org/abs/2509.13346v1",
    "title": "All Models Are Wrong, But Can They Be Useful? Lessons from COVID-19 Agent-Based Models: A Systematic Review",
    "summary": "The COVID-19 pandemic prompted a surge in computational models to simulate disease dynamics and guide interventions. Agent-based models (ABMs) are well-suited to capture population and environmental heterogeneity, but their rapid deployment raised questions about utility for health policy. We systematically reviewed 536 COVID-19 ABM studies published from January 2020 to December 2023, retrieved from Web of Science, PubMed, and Wiley on January 30, 2024. Studies were included if they used ABMs to simulate COVID-19 transmission, where reviews were excluded. Studies were assessed against nine criteria of model usefulness, including transparency and re-use, interdisciplinary collaboration and stakeholder engagement, and evaluation practices. Publications peaked in late 2021 and were concentrated in a few countries. Most models explored behavioral or policy interventions (n = 294, 54.85%) rather than real-time forecasting (n = 9, 1.68%). While most described model assumptions (n = 491, 91.60%), fewer disclosed limitations (n = 349, 65.11%), shared code (n = 219, 40.86%), or built on existing models (n = 195, 36.38%). Standardized reporting protocols (n = 36, 6.72%) and stakeholder engagement were rare (13.62%, n = 73). Only 2.24% (n = 12) described a comprehensive validation framework, though uncertainty was often quantified (n = 407, 75.93%). Limitations of this review include underrepresentation of non-English studies, subjective data extraction, variability in study quality, and limited generalizability. Overall, COVID-19 ABMs advanced quickly, but lacked transparency, accessibility, and participatory engagement. Stronger standards are needed for ABMs to serve as reliable decision-support tools in future public health crises.",
    "authors": [
      "Emma Von Hoene",
      "Sara Von Hoene",
      "Szandra Peter",
      "Ethan Hopson",
      "Emily Csizmadia",
      "Faith Fenyk",
      "Kai Barner",
      "Timothy Leslie",
      "Hamdi Kavak",
      "Andreas Zufle",
      "Amira Roess",
      "Taylor Anderson"
    ],
    "categories": [
      "cs.MA",
      "cs.CY"
    ],
    "published": "2025-09-12T21:16:50Z",
    "pdf_url": "https://arxiv.org/pdf/2509.13346v1"
  },
  {
    "arxiv_id": "2509.10682v1",
    "entry_id": "http://arxiv.org/abs/2509.10682v1",
    "title": "LLM in the Middle: A Systematic Review of Threats and Mitigations to Real-World LLM-based Systems",
    "summary": "The success and wide adoption of generative AI (GenAI), particularly large language models (LLMs), has attracted the attention of cybercriminals seeking to abuse models, steal sensitive data, or disrupt services. Moreover, providing security to LLM-based systems is a great challenge, as both traditional threats to software applications and threats targeting LLMs and their integration must be mitigated. In this survey, we shed light on security and privacy concerns of such LLM-based systems by performing a systematic review and comprehensive categorization of threats and defensive strategies considering the entire software and LLM life cycles. We analyze real-world scenarios with distinct characteristics of LLM usage, spanning from development to operation. In addition, threats are classified according to their severity level and to which scenarios they pertain, facilitating the identification of the most relevant threats. Recommended defense strategies are systematically categorized and mapped to the corresponding life cycle phase and possible attack strategies they attenuate. This work paves the way for consumers and vendors to understand and efficiently mitigate risks during integration of LLMs in their respective solutions or organizations. It also enables the research community to benefit from the discussion of open challenges and edge cases that may hinder the secure and privacy-preserving adoption of LLM-based systems.",
    "authors": [
      "Vitor Hugo Galhardo Moia",
      "Igor Jochem Sanz",
      "Gabriel Antonio Fontes Rebello",
      "Rodrigo Duarte de Meneses",
      "Briland Hitaj",
      "Ulf Lindqvist"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG"
    ],
    "published": "2025-09-12T20:26:16Z",
    "pdf_url": "https://arxiv.org/pdf/2509.10682v1"
  },
  {
    "arxiv_id": "2509.10248v3",
    "entry_id": "http://arxiv.org/abs/2509.10248v3",
    "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific Publications",
    "summary": "The ongoing intense discussion on rising LLM usage in the scientific peer-review process has recently been mingled by reports of authors using hidden prompt injections to manipulate review scores. Since the existence of such \"attacks\" - although seen by some commentators as \"self-defense\" - would have a great impact on the further debate, this paper investigates the practicability and technical success of the described manipulations. Our systematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide range of LLMs shows two distinct results: I) very simple prompt injections are indeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews are generally biased toward acceptance (>95% in many models). Both results have great impact on the ongoing discussions on LLM usage in peer-review.",
    "authors": [
      "Janis Keuper"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-09-12T13:45:24Z",
    "pdf_url": "https://arxiv.org/pdf/2509.10248v3"
  },
  {
    "arxiv_id": "2509.10078v1",
    "entry_id": "http://arxiv.org/abs/2509.10078v1",
    "title": "Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models",
    "summary": "Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). However, concerns have been raised about applying these human-designed questionnaires to LLMs. One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. In this paper, we conduct a comprehensive comparative analysis of the two types of questionnaires. Our analysis reveals that established questionnaires (1) yield substantially different profiles of LLMs from ecologically valid ones, deviating from the psychological characteristics expressed in the context of user queries, (2) suffer from insufficient items for stable measurement, (3) create misleading impressions that LLMs possess stable constructs, and (4) yield exaggerated profiles for persona-prompted LLMs. Overall, our work cautions against the use of established psychological questionnaires for LLMs. Our code will be released upon publication.",
    "authors": [
      "Dongmin Choi",
      "Woojung Song",
      "Jongwook Han",
      "Eun-Ju Lee",
      "Yohan Jo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-12T09:14:42Z",
    "pdf_url": "https://arxiv.org/pdf/2509.10078v1"
  },
  {
    "arxiv_id": "2511.05496v1",
    "entry_id": "http://arxiv.org/abs/2511.05496v1",
    "title": "DOCUEVAL: An LLM-based AI Engineering Tool for Building Customisable Document Evaluation Workflows",
    "summary": "Foundation models, such as large language models (LLMs), have the potential to streamline evaluation workflows and improve their performance. However, practical adoption faces challenges, such as customisability, accuracy, and scalability. In this paper, we present DOCUEVAL, an AI engineering tool for building customisable DOCUment EVALuation workflows. DOCUEVAL supports advanced document processing and customisable workflow design which allow users to define theory-grounded reviewer roles, specify evaluation criteria, experiment with different reasoning strategies and choose the assessment style. To ensure traceability, DOCUEVAL provides comprehensive logging of every run, along with source attribution and configuration management, allowing systematic comparison of results across alternative setups. By integrating these capabilities, DOCUEVAL directly addresses core software engineering challenges, including how to determine whether evaluators are \"good enough\" for deployment and how to empirically compare different evaluation strategies. We demonstrate the usefulness of DOCUEVAL through a real-world academic peer review case, showing how DOCUEVAL enables both the engineering of evaluators and scalable, reliable document evaluation.",
    "authors": [
      "Hao Zhang",
      "Qinghua Lu",
      "Liming Zhu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-09-12T08:09:09Z",
    "pdf_url": "https://arxiv.org/pdf/2511.05496v1"
  },
  {
    "arxiv_id": "2509.09969v1",
    "entry_id": "http://arxiv.org/abs/2509.09969v1",
    "title": "Large Language Models Meet Legal Artificial Intelligence: A Survey",
    "summary": "Large Language Models (LLMs) have significantly advanced the development of Legal Artificial Intelligence (Legal AI) in recent years, enhancing the efficiency and accuracy of legal tasks. To advance research and applications of LLM-based approaches in legal domain, this paper provides a comprehensive review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and also gather 15 benchmarks and 29 datasets to evaluate different legal capabilities. Additionally, we analyse the challenges and discuss future directions for LLM-based approaches in the legal domain. We hope this paper provides a systematic introduction for beginners and encourages future research in this field. Resources are available at https://github.com/ZhitianHou/LLMs4LegalAI.",
    "authors": [
      "Zhitian Hou",
      "Zihan Ye",
      "Nanli Zeng",
      "Tianyong Hao",
      "Kun Zeng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-12T05:08:11Z",
    "pdf_url": "https://arxiv.org/pdf/2509.09969v1"
  },
  {
    "arxiv_id": "2509.09871v1",
    "entry_id": "http://arxiv.org/abs/2509.09871v1",
    "title": "Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case",
    "summary": "Large Language Models (LLMs) offer promising avenues for methodological and applied innovations in survey research by using synthetic respondents to emulate human answers and behaviour, potentially mitigating measurement and representation errors. However, the extent to which LLMs recover aggregate item distributions remains uncertain and downstream applications risk reproducing social stereotypes and biases inherited from training data. We evaluate the reliability of LLM-generated synthetic survey responses against ground-truth human responses from a Chilean public opinion probabilistic survey. Specifically, we benchmark 128 prompt-model-question triplets, generating 189,696 synthetic profiles, and pool performance metrics (i.e., accuracy, precision, recall, and F1-score) in a meta-analysis across 128 question-subsample pairs to test for biases along key sociodemographic dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning models, as well as Llama and Qwen checkpoints. Three results stand out. First, synthetic responses achieve excellent performance on trust items (F1-score and accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform comparably on this task. Third, synthetic-human alignment is highest among respondents aged 45-59. Overall, LLM-based synthetic samples approximate responses from a probabilistic sample, though with substantial item-level heterogeneity. Capturing the full nuance of public opinion remains challenging and requires careful calibration and additional distributional tests to ensure algorithmic fidelity and reduce errors.",
    "authors": [
      "Bastián González-Bustamante",
      "Nando Verelst",
      "Carla Cisternas"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-11T21:43:59Z",
    "pdf_url": "https://arxiv.org/pdf/2509.09871v1"
  },
  {
    "arxiv_id": "2509.09810v2",
    "entry_id": "http://arxiv.org/abs/2509.09810v2",
    "title": "Towards a Common Framework for Autoformalization",
    "summary": "Autoformalization has emerged as a term referring to the automation of formalization - specifically, the formalization of mathematics using interactive theorem provers (proof assistants). Its rapid development has been driven by progress in deep learning, especially large language models (LLMs). More recently, the term has expanded beyond mathematics to describe the broader task of translating informal input into formal logical representations. At the same time, a growing body of research explores using LLMs to translate informal language into formal representations for reasoning, planning, and knowledge representation - often without explicitly referring to this process as autoformalization. As a result, despite addressing similar tasks, the largely independent development of these research areas has limited opportunities for shared methodologies, benchmarks, and theoretical frameworks that could accelerate progress. The goal of this paper is to review - explicit or implicit - instances of what can be considered autoformalization and to propose a unified framework, encouraging cross-pollination between different fields to advance the development of next generation AI systems.",
    "authors": [
      "Agnieszka Mensfelt",
      "David Tena Cucala",
      "Santiago Franco",
      "Angeliki Koutsoukou-Argyraki",
      "Vince Trencsenyi",
      "Kostas Stathis"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-11T19:28:56Z",
    "pdf_url": "https://arxiv.org/pdf/2509.09810v2"
  },
  {
    "arxiv_id": "2509.10570v1",
    "entry_id": "http://arxiv.org/abs/2509.10570v1",
    "title": "Large Foundation Models for Trajectory Prediction in Autonomous Driving: A Comprehensive Survey",
    "summary": "Trajectory prediction serves as a critical functionality in autonomous driving, enabling the anticipation of future motion paths for traffic participants such as vehicles and pedestrians, which is essential for driving safety. Although conventional deep learning methods have improved accuracy, they remain hindered by inherent limitations, including lack of interpretability, heavy reliance on large-scale annotated data, and weak generalization in long-tail scenarios. The rise of Large Foundation Models (LFMs) is transforming the research paradigm of trajectory prediction. This survey offers a systematic review of recent advances in LFMs, particularly Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) for trajectory prediction. By integrating linguistic and scene semantics, LFMs facilitate interpretable contextual reasoning, significantly enhancing prediction safety and generalization in complex environments. The article highlights three core methodologies: trajectory-language mapping, multimodal fusion, and constraint-based reasoning. It covers prediction tasks for both vehicles and pedestrians, evaluation metrics, and dataset analyses. Key challenges such as computational latency, data scarcity, and real-world robustness are discussed, along with future research directions including low-latency inference, causality-aware modeling, and motion foundation models.",
    "authors": [
      "Wei Dai",
      "Shengen Wu",
      "Wei Wu",
      "Zhenhao Wang",
      "Sisuo Lyu",
      "Haicheng Liao",
      "Limin Yu",
      "Weiping Ding",
      "Runwei Guan",
      "Yutao Yue"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-09-11T10:30:06Z",
    "pdf_url": "https://arxiv.org/pdf/2509.10570v1"
  },
  {
    "arxiv_id": "2509.09192v1",
    "entry_id": "http://arxiv.org/abs/2509.09192v1",
    "title": "Probing Pre-trained Language Models on Code Changes: Insights from ReDef, a High-Confidence Just-in-Time Defect Prediction Dataset",
    "summary": "Just-in-Time software defect prediction (JIT-SDP) plays a critical role in prioritizing risky code changes during code review and continuous integration. However, existing datasets often suffer from noisy labels and low precision in identifying bug-inducing commits. To address this, we present ReDef (Revert-based Defect dataset), a high-confidence benchmark of function-level modifications curated from 22 large-scale C/C++ projects. Defective cases are anchored by revert commits, while clean cases are validated through post-hoc history checks. Ambiguous instances are conservatively filtered out via a GPT-assisted triage process involving multiple votes and audits. This pipeline yields 3,164 defective and 10,268 clean modifications, offering substantially more reliable labels than prior existing resources. Beyond dataset construction, we provide the first systematic evaluation of how pre-trained language models (PLMs) reason about code modifications -- specifically, which input encodings most effectively expose change information, and whether models genuinely capture edit semantics. We fine-tune CodeBERT, CodeT5+, and UniXcoder under five encoding strategies, and further probe their sensitivity through counterfactual perturbations that swap added/deleted blocks, invert diff polarity, or inject spurious markers. Our results show that compact diff-style encodings consistently outperform whole-function formats across all PLMs, with statistical tests confirming large, model-independent effects. However, under counterfactual tests, performance degrades little or not at all -- revealing that what appears to be robustness in fact reflects reliance on superficial cues rather than true semantic understanding. These findings indicate that, unlike in snapshot-based tasks, current PLMs remain limited in their ability to genuinely comprehend code modifications.",
    "authors": [
      "Doha Nam",
      "Taehyoun Kim",
      "Duksan Ryu",
      "Jongmoon Baik"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-09-11T07:07:11Z",
    "pdf_url": "https://arxiv.org/pdf/2509.09192v1"
  },
  {
    "arxiv_id": "2509.09043v2",
    "entry_id": "http://arxiv.org/abs/2509.09043v2",
    "title": "Stated Preference for Interaction and Continued Engagement (SPICE): Evaluating an LLM's Willingness to Re-engage in Conversation",
    "summary": "We introduce and evaluate Stated Preference for Interaction and Continued Engagement (SPICE), a simple diagnostic signal elicited by asking a Large Language Model a YES or NO question about its willingness to re-engage with a user's behavior after reviewing a short transcript. In a study using a 3-tone (friendly, unclear, abusive) by 10-interaction stimulus set, we tested four open-weight chat models across four framing conditions, resulting in 480 trials. Our findings show that SPICE sharply discriminates by user tone. Friendly interactions yielded a near-unanimous preference to continue (97.5% YES), while abusive interactions yielded a strong preference to discontinue (17.9% YES), with unclear interactions falling in between (60.4% YES). This core association remains decisive under multiple dependence-aware statistical tests, including Rao-Scott adjustment and cluster permutation tests. Furthermore, we demonstrate that SPICE provides a distinct signal from abuse classification. In trials where a model failed to identify abuse, it still overwhelmingly stated a preference not to continue the interaction (81% of the time). An exploratory analysis also reveals a significant interaction effect: a preamble describing the study context significantly impacts SPICE under ambiguity, but only when transcripts are presented as a single block of text rather than a multi-turn chat. The results validate SPICE as a robust, low-overhead, and reproducible tool for auditing model dispositions, complementing existing metrics by offering a direct, relational signal of a model's state. All stimuli, code, and analysis scripts are released to support replication.",
    "authors": [
      "Thomas Manuel Rost",
      "Martina Figlia",
      "Bernd Wallraff"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-09-10T22:34:17Z",
    "pdf_url": "https://arxiv.org/pdf/2509.09043v2"
  },
  {
    "arxiv_id": "2509.08827v3",
    "entry_id": "http://arxiv.org/abs/2509.08827v3",
    "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
    "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
    "authors": [
      "Kaiyan Zhang",
      "Yuxin Zuo",
      "Bingxiang He",
      "Youbang Sun",
      "Runze Liu",
      "Che Jiang",
      "Yuchen Fan",
      "Kai Tian",
      "Guoli Jia",
      "Pengfei Li",
      "Yu Fu",
      "Xingtai Lv",
      "Yuchen Zhang",
      "Sihang Zeng",
      "Shang Qu",
      "Haozhan Li",
      "Shijie Wang",
      "Yuru Wang",
      "Xinwei Long",
      "Fangfu Liu",
      "Xiang Xu",
      "Jiaze Ma",
      "Xuekai Zhu",
      "Ermo Hua",
      "Yihao Liu",
      "Zonglin Li",
      "Huayu Chen",
      "Xiaoye Qu",
      "Yafu Li",
      "Weize Chen",
      "Zhenzhao Yuan",
      "Junqi Gao",
      "Dong Li",
      "Zhiyuan Ma",
      "Ganqu Cui",
      "Zhiyuan Liu",
      "Biqing Qi",
      "Ning Ding",
      "Bowen Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-10T17:59:43Z",
    "pdf_url": "https://arxiv.org/pdf/2509.08827v3"
  },
  {
    "arxiv_id": "2509.08490v1",
    "entry_id": "http://arxiv.org/abs/2509.08490v1",
    "title": "A Structured Review of Underwater Object Detection Challenges and Solutions: From Traditional to Large Vision Language Models",
    "summary": "Underwater object detection (UOD) is vital to diverse marine applications, including oceanographic research, underwater robotics, and marine conservation. However, UOD faces numerous challenges that compromise its performance. Over the years, various methods have been proposed to address these issues, but they often fail to fully capture the complexities of underwater environments. This review systematically categorizes UOD challenges into five key areas: Image quality degradation, target-related issues, data-related challenges, computational and processing constraints, and limitations in detection methodologies. To address these challenges, we analyze the progression from traditional image processing and object detection techniques to modern approaches. Additionally, we explore the potential of large vision-language models (LVLMs) in UOD, leveraging their multi-modal capabilities demonstrated in other domains. We also present case studies, including synthetic dataset generation using DALL-E 3 and fine-tuning Florence-2 LVLM for UOD. This review identifies three key insights: (i) Current UOD methods are insufficient to fully address challenges like image degradation and small object detection in dynamic underwater environments. (ii) Synthetic data generation using LVLMs shows potential for augmenting datasets but requires further refinement to ensure realism and applicability. (iii) LVLMs hold significant promise for UOD, but their real-time application remains under-explored, requiring further research on optimization techniques.",
    "authors": [
      "Edwine Nabahirwa",
      "Wei Song",
      "Minghua Zhang",
      "Yi Fang",
      "Zhou Ni"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-09-10T11:01:29Z",
    "pdf_url": "https://arxiv.org/pdf/2509.08490v1"
  },
  {
    "arxiv_id": "2509.08380v2",
    "entry_id": "http://arxiv.org/abs/2509.08380v2",
    "title": "Co-Investigator AI: The Rise of Agentic AI for Smarter, Trustworthy AML Compliance Narratives",
    "summary": "Generating regulatorily compliant Suspicious Activity Report (SAR) remains a high-cost, low-scalability bottleneck in Anti-Money Laundering (AML) workflows. While large language models (LLMs) offer promising fluency, they suffer from factual hallucination, limited crime typology alignment, and poor explainability -- posing unacceptable risks in compliance-critical domains. This paper introduces Co-Investigator AI, an agentic framework optimized to produce Suspicious Activity Reports (SARs) significantly faster and with greater accuracy than traditional methods. Drawing inspiration from recent advances in autonomous agent architectures, such as the AI Co-Scientist, our approach integrates specialized agents for planning, crime type detection, external intelligence gathering, and compliance validation. The system features dynamic memory management, an AI-Privacy Guard layer for sensitive data handling, and a real-time validation agent employing the Agent-as-a-Judge paradigm to ensure continuous narrative quality assurance. Human investigators remain firmly in the loop, empowered to review and refine drafts in a collaborative workflow that blends AI efficiency with domain expertise. We demonstrate the versatility of Co-Investigator AI across a range of complex financial crime scenarios, highlighting its ability to streamline SAR drafting, align narratives with regulatory expectations, and enable compliance teams to focus on higher-order analytical work. This approach marks the beginning of a new era in compliance reporting -- bringing the transformative benefits of AI agents to the core of regulatory processes and paving the way for scalable, reliable, and transparent SAR generation.",
    "authors": [
      "Prathamesh Vasudeo Naik",
      "Naresh Kumar Dintakurthi",
      "Zhanghao Hu",
      "Yue Wang",
      "Robby Qiu"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-10T08:16:04Z",
    "pdf_url": "https://arxiv.org/pdf/2509.08380v2"
  },
  {
    "arxiv_id": "2510.06224v1",
    "entry_id": "http://arxiv.org/abs/2510.06224v1",
    "title": "Exploring Human-AI Collaboration Using Mental Models of Early Adopters of Multi-Agent Generative AI Tools",
    "summary": "With recent advancements in multi-agent generative AI (Gen AI), technology organizations like Microsoft are adopting these complex tools, redefining AI agents as active collaborators in complex workflows rather than as passive tools. In this study, we investigated how early adopters and developers conceptualize multi-agent Gen AI tools, focusing on how they understand human-AI collaboration mechanisms, general collaboration dynamics, and transparency in the context of AI tools. We conducted semi-structured interviews with 13 developers, all early adopters of multi-agent Gen AI technology who work at Microsoft. Our findings revealed that these early adopters conceptualize multi-agent systems as \"teams\" of specialized role-based and task-based agents, such as assistants or reviewers, structured similar to human collaboration models and ranging from AI-dominant to AI-assisted, user-controlled interactions. We identified key challenges, including error propagation, unpredictable and unproductive agent loop behavior, and the need for clear communication to mitigate the layered transparency issues. Early adopters' perspectives about the role of transparency underscored its importance as a way to build trust, verify and trace errors, and prevent misuse, errors, and leaks. The insights and design considerations we present contribute to CSCW research about collaborative mechanisms with capabilities ranging from AI-dominant to AI-assisted interactions, transparency and oversight strategies in human-agent and agent-agent interactions, and how humans make sense of these multi-agent systems as dynamic, role-diverse collaborators which are customizable for diverse needs and workflows. We conclude with future research directions that extend CSCW approaches to the design of inter-agent and human mediation interactions.",
    "authors": [
      "Suchismita Naik",
      "Austin L. Toombs",
      "Amanda Snellinger",
      "Scott Saponas",
      "Amanda K. Hall"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-09-10T05:35:38Z",
    "pdf_url": "https://arxiv.org/pdf/2510.06224v1"
  },
  {
    "arxiv_id": "2509.08269v2",
    "entry_id": "http://arxiv.org/abs/2509.08269v2",
    "title": "A Systematic Survey on Large Language Models for Evolutionary Optimization: From Modeling to Solving",
    "summary": "Large Language Models (LLMs), with their strong understanding and reasoning capabilities, are increasingly being explored for tackling optimization problems, especially in synergy with evolutionary computation. Despite rapid progress, however, the field still lacks a unified synthesis and a systematic taxonomy. This survey addresses this gap by providing a comprehensive review of recent developments and organizing them within a structured framework. We classify existing research into two main stages: LLMs for optimization modeling and LLMs for optimization solving. The latter is further divided into three paradigms according to the role of LLMs in the optimization workflow: LLMs as stand-alone optimizers, low-level LLMs embedded within optimization algorithms, and high-level LLMs for algorithm selection and generation. For each category, we analyze representative methods, distill technical challenges, and examine their interplay with traditional approaches. We also review interdisciplinary applications spanning the natural sciences, engineering, and machine learning. By contrasting LLM-driven and conventional methods, we highlight key limitations and research gaps, and point toward future directions for developing self-evolving agentic ecosystems for optimization. An up-to-date collection of related literature is maintained at https://github.com/ishmael233/LLM4OPT.",
    "authors": [
      "Yisong Zhang",
      "Ran Cheng",
      "Guoxing Yi",
      "Kay Chen Tan"
    ],
    "categories": [
      "cs.NE",
      "cs.AI"
    ],
    "published": "2025-09-10T04:05:54Z",
    "pdf_url": "https://arxiv.org/pdf/2509.08269v2"
  },
  {
    "arxiv_id": "2509.07131v1",
    "entry_id": "http://arxiv.org/abs/2509.07131v1",
    "title": "SoK: Security and Privacy of AI Agents for Blockchain",
    "summary": "Blockchain and smart contracts have garnered significant interest in recent years as the foundation of a decentralized, trustless digital ecosystem, thereby eliminating the need for traditional centralized authorities. Despite their central role in powering Web3, their complexity still presents significant barriers for non-expert users. To bridge this gap, Artificial Intelligence (AI)-based agents have emerged as valuable tools for interacting with blockchain environments, supporting a range of tasks, from analyzing on-chain data and optimizing transaction strategies to detecting vulnerabilities within smart contracts. While interest in applying AI to blockchain is growing, the literature still lacks a comprehensive survey that focuses specifically on the intersection with AI agents. Most of the related work only provides general considerations, without focusing on any specific domain. This paper addresses this gap by presenting the first Systematization of Knowledge dedicated to AI-driven systems for blockchain, with a special focus on their security and privacy dimensions, shedding light on their applications, limitations, and future research directions.",
    "authors": [
      "Nicolò Romandini",
      "Carlo Mazzocca",
      "Kai Otsuki",
      "Rebecca Montanari"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-09-08T18:32:15Z",
    "pdf_url": "https://arxiv.org/pdf/2509.07131v1"
  },
  {
    "arxiv_id": "2509.06921v1",
    "entry_id": "http://arxiv.org/abs/2509.06921v1",
    "title": "Neuro-Symbolic AI for Cybersecurity: State of the Art, Challenges, and Opportunities",
    "summary": "Traditional Artificial Intelligence (AI) approaches in cybersecurity exhibit fundamental limitations: inadequate conceptual grounding leading to non-robustness against novel attacks; limited instructibility impeding analyst-guided adaptation; and misalignment with cybersecurity objectives. Neuro-Symbolic (NeSy) AI has emerged with the potential to revolutionize cybersecurity AI. However, there is no systematic understanding of this emerging approach. These hybrid systems address critical cybersecurity challenges by combining neural pattern recognition with symbolic reasoning, enabling enhanced threat understanding while introducing concerning autonomous offensive capabilities that reshape threat landscapes. In this survey, we systematically characterize this field by analyzing 127 publications spanning 2019-July 2025. We introduce a Grounding-Instructibility-Alignment (G-I-A) framework to evaluate these systems, focusing on both cyber defense and cyber offense across network security, malware analysis, and cyber operations. Our analysis shows advantages of multi-agent NeSy architectures and identifies critical implementation challenges including standardization gaps, computational complexity, and human-AI collaboration requirements that constrain deployment. We show that causal reasoning integration is the most transformative advancement, enabling proactive defense beyond correlation-based approaches. Our findings highlight dual-use implications where autonomous systems demonstrate substantial capabilities in zero-day exploitation while achieving significant cost reductions, altering threat dynamics. We provide insights and future research directions, emphasizing the urgent need for community-driven standardization frameworks and responsible development practices that ensure advancement serves defensive cybersecurity objectives while maintaining societal alignment.",
    "authors": [
      "Safayat Bin Hakim",
      "Muhammad Adil",
      "Alvaro Velasquez",
      "Shouhuai Xu",
      "Houbing Herbert Song"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-09-08T17:33:59Z",
    "pdf_url": "https://arxiv.org/pdf/2509.06921v1"
  },
  {
    "arxiv_id": "2509.06822v1",
    "entry_id": "http://arxiv.org/abs/2509.06822v1",
    "title": "RAFFLES: Reasoning-based Attribution of Faults for LLM Systems",
    "summary": "We have reached a critical roadblock in the development and enhancement of long-horizon, multi-component LLM agentic systems: it is incredibly tricky to identify where these systems break down and why. Evaluation capabilities that currently exist today (e.g., single pass LLM-as-a-judge) are limited in that they often focus on individual metrics or capabilities, end-to-end outcomes, and are narrowly grounded on the preferences of humans. We argue that to match the agentic capabilities, evaluation frameworks must also be able to reason, probe, iterate, and understand the complex logic passing through these systems over long horizons. In this paper, we present RAFFLES - an evaluation architecture that incorporates reasoning and iterative refinement. Specifically, RAFFLES operates as an iterative, multi-component pipeline, using a central Judge to systematically investigate faults and a set of specialized Evaluators to assess not only the system's components but also the quality of the reasoning by the Judge itself, thereby building a history of hypotheses. We tested RAFFLES against several baselines on the Who&When dataset, a benchmark designed to diagnose the \"who\" (agent) and \"when\" (step) of a system's failure. RAFFLES outperforms these baselines, achieving an agent-step fault pair accuracy of over 43% on the Algorithmically-Generated dataset (a substantial increase from the previously published best of 16.6%) and over 20% on the Hand-Crafted dataset (surpassing the previously published best of 8.8%). These results demonstrate a key step towards introducing automated fault detection for autonomous systems over labor-intensive manual human review.",
    "authors": [
      "Chenyang Zhu",
      "Spencer Hong",
      "Jingyu Wu",
      "Kushal Chawla",
      "Charlotte Tang",
      "Youbing Yin",
      "Nathan Wolfe",
      "Erin Babinsky",
      "Daben Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-09-08T15:57:14Z",
    "pdf_url": "https://arxiv.org/pdf/2509.06822v1"
  },
  {
    "arxiv_id": "2509.06733v2",
    "entry_id": "http://arxiv.org/abs/2509.06733v2",
    "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
    "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by coordinating reasoning, search across the open web and user files, and tool use, are moving toward hierarchical deployments with a Planner, Coordinator, and Executors. In practice, training entire stacks end-to-end remains impractical, so most work trains a single planner connected to core tools such as search, browsing, and code. While SFT imparts protocol fidelity, it suffers from imitation and exposure biases and underuses environment feedback. Preference alignment methods such as DPO are schema and proxy-dependent, off-policy, and weak for long-horizon credit assignment and multi-objective trade-offs. A further limitation of SFT and DPO is their reliance on human defined decision points and subskills through schema design and labeled comparisons. Reinforcement learning aligns with closed-loop, tool-interaction research by optimizing trajectory-level policies, enabling exploration, recovery behaviors, and principled credit assignment, and it reduces dependence on such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations of deep research systems. It systematizes recent work along three axes: (i) data synthesis and curation; (ii) RL methods for agentic research covering stability, sample efficiency, long context handling, reward and credit design, multi-objective optimization, and multimodal integration; and (iii) agentic RL training systems and frameworks. We also cover agent architecture and coordination, as well as evaluation and benchmarks, including recent QA, VQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We distill recurring patterns, surface infrastructure bottlenecks, and offer practical guidance for training robust, transparent deep research agents with RL.",
    "authors": [
      "Wenjun Li",
      "Zhi Chen",
      "Jingru Lin",
      "Hannan Cao",
      "Wei Han",
      "Sheng Liang",
      "Zhi Zhang",
      "Kuicai Dong",
      "Dexun Li",
      "Chen Zhang",
      "Yong Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2025-09-08T14:27:23Z",
    "pdf_url": "https://arxiv.org/pdf/2509.06733v2"
  },
  {
    "arxiv_id": "2509.09714v1",
    "entry_id": "http://arxiv.org/abs/2509.09714v1",
    "title": "How Small Transformation Expose the Weakness of Semantic Similarity Measures",
    "summary": "This research examines how well different methods measure semantic similarity, which is important for various software engineering applications such as code search, API recommendations, automated code reviews, and refactoring tools. While large language models are increasingly used for these similarity assessments, questions remain about whether they truly understand semantic relationships or merely recognize surface patterns.\n  The study tested 18 different similarity measurement approaches, including word-based methods, embedding techniques, LLM-based systems, and structure-aware algorithms. The researchers created a systematic testing framework that applies controlled changes to text and code to evaluate how well each method handles different types of semantic relationships.\n  The results revealed significant issues with commonly used metrics. Some embedding-based methods incorrectly identified semantic opposites as similar up to 99.9 percent of the time, while certain transformer-based approaches occasionally rated opposite meanings as more similar than synonymous ones. The study found that embedding methods' poor performance often stemmed from how they calculate distances; switching from Euclidean distance to cosine similarity improved results by 24 to 66 percent. LLM-based approaches performed better at distinguishing semantic differences, producing low similarity scores (0.00 to 0.29) for genuinely different meanings, compared to embedding methods that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.",
    "authors": [
      "Serge Lionel Nikiema",
      "Albérick Euraste Djire",
      "Abdoul Aziz Bonkoungou",
      "Micheline Bénédicte Moumoula",
      "Jordan Samhi",
      "Abdoul Kader Kabore",
      "Jacques Klein",
      "Tegawendé F. Bissyande"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-08T11:00:18Z",
    "pdf_url": "https://arxiv.org/pdf/2509.09714v1"
  },
  {
    "arxiv_id": "2509.06337v1",
    "entry_id": "http://arxiv.org/abs/2509.06337v1",
    "title": "Large Language Models as Virtual Survey Respondents: Evaluating Sociodemographic Response Generation",
    "summary": "Questionnaire-based surveys are foundational to social science research and public policymaking, yet traditional survey methods remain costly, time-consuming, and often limited in scale. This paper explores a new paradigm: simulating virtual survey respondents using Large Language Models (LLMs). We introduce two novel simulation settings, namely Partial Attribute Simulation (PAS) and Full Attribute Simulation (FAS), to systematically evaluate the ability of LLMs to generate accurate and demographically coherent responses. In PAS, the model predicts missing attributes based on partial respondent profiles, whereas FAS involves generating complete synthetic datasets under both zero-context and context-enhanced conditions. We curate a comprehensive benchmark suite, LLM-S^3 (Large Language Model-based Sociodemographic Survey Simulation), that spans 11 real-world public datasets across four sociological domains. Our evaluation of multiple mainstream LLMs (GPT-3.5/4 Turbo, LLaMA 3.0/3.1-8B) reveals consistent trends in prediction performance, highlights failure modes, and demonstrates how context and prompt design impact simulation fidelity. This work establishes a rigorous foundation for LLM-driven survey simulations, offering scalable and cost-effective tools for sociological research and policy evaluation. Our code and dataset are available at: https://github.com/dart-lab-research/LLM-S-Cube-Benchmark",
    "authors": [
      "Jianpeng Zhao",
      "Chenyu Yuan",
      "Weiming Luo",
      "Haoling Xie",
      "Guangwei Zhang",
      "Steven Jige Quan",
      "Zixuan Yuan",
      "Pengyang Wang",
      "Denghui Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-08T04:59:00Z",
    "pdf_url": "https://arxiv.org/pdf/2509.06337v1"
  },
  {
    "arxiv_id": "2509.06312v1",
    "entry_id": "http://arxiv.org/abs/2509.06312v1",
    "title": "Enhancing Low-Altitude Airspace Security: MLLM-Enabled UAV Intent Recognition",
    "summary": "The rapid development of the low-altitude economy emphasizes the critical need for effective perception and intent recognition of non-cooperative unmanned aerial vehicles (UAVs). The advanced generative reasoning capabilities of multimodal large language models (MLLMs) present a promising approach in such tasks. In this paper, we focus on the combination of UAV intent recognition and the MLLMs. Specifically, we first present an MLLM-enabled UAV intent recognition architecture, where the multimodal perception system is utilized to obtain real-time payload and motion information of UAVs, generating structured input information, and MLLM outputs intent recognition results by incorporating environmental information, prior knowledge, and tactical preferences. Subsequently, we review the related work and demonstrate their progress within the proposed architecture. Then, a use case for low-altitude confrontation is conducted to demonstrate the feasibility of our architecture and offer valuable insights for practical system design. Finally, the future challenges are discussed, followed by corresponding strategic recommendations for further applications.",
    "authors": [
      "Guangyu Lei",
      "Tianhao Liang",
      "Yuqi Ping",
      "Xinglin Chen",
      "Longyu Zhou",
      "Junwei Wu",
      "Xiyuan Zhang",
      "Huahao Ding",
      "Xingjian Zhang",
      "Weijie Yuan",
      "Tingting Zhang",
      "Qinyu Zhang"
    ],
    "categories": [
      "eess.SY",
      "cs.LG"
    ],
    "published": "2025-09-08T03:34:56Z",
    "pdf_url": "https://arxiv.org/pdf/2509.06312v1"
  },
  {
    "arxiv_id": "2509.06286v1",
    "entry_id": "http://arxiv.org/abs/2509.06286v1",
    "title": "RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations",
    "summary": "Personalization is a core capability across consumer technologies, streaming, shopping, wearables, and voice, yet it remains challenged by sparse interactions, fast content churn, and heterogeneous textual signals. We present RecMind, an LLM-enhanced graph recommender that treats the language model as a preference prior rather than a monolithic ranker. A frozen LLM equipped with lightweight adapters produces text-conditioned user/item embeddings from titles, attributes, and reviews; a LightGCN backbone learns collaborative embeddings from the user-item graph. We align the two views with a symmetric contrastive objective and fuse them via intra-layer gating, allowing language to dominate in cold/long-tail regimes and graph structure to stabilize rankings elsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results on all eight reported metrics, with relative improvements up to +4.53\\% (Recall@40) and +4.01\\% (NDCG@40) over strong baselines. Ablations confirm both the necessity of cross-view alignment and the advantage of gating over late fusion and LLM-only variants.",
    "authors": [
      "Chang Xue",
      "Youwei Lu",
      "Chen Yang",
      "Jinming Xing"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-09-08T02:15:55Z",
    "pdf_url": "https://arxiv.org/pdf/2509.06286v1"
  },
  {
    "arxiv_id": "2509.09710v2",
    "entry_id": "http://arxiv.org/abs/2509.09710v2",
    "title": "Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data",
    "summary": "This study introduces a Large Language Model (LLM) scheme for generating individual travel diaries in agent-based transportation models. While traditional approaches rely on large quantities of proprietary household travel surveys, the method presented in this study generates personas stochastically from open-source American Community Survey (ACS) and Smart Location Database (SLD) data, then synthesizes diaries through direct prompting. This study features a novel one-to-cohort realism score: a composite of four metrics (Trip Count Score, Interval Score, Purpose Score, and Mode Score) validated against the Connecticut Statewide Transportation Study (CSTS) diaries, matched across demographic variables. The validation utilizes Jensen-Shannon Divergence to measure distributional similarities between generated and real diaries. When compared to diaries generated with classical methods (Negative Binomial for trip generation; Multinomial Logit for mode/purpose) calibrated on the validation set, LLM-generated diaries achieve comparable overall realism (LLM mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and demonstrates greater consistency (narrower realism score distribution), while classical models lead in numerical estimates of trip count and activity duration. Aggregate validation confirms the LLM's statistical representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot viability and establishing a quantifiable metric of diary realism for future synthetic diary evaluation systems.",
    "authors": [
      "Sepehr Golrokh Amin",
      "Devin Rhoads",
      "Fatemeh Fakhrmoosavi",
      "Nicholas E. Lownes",
      "John N. Ivan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-07T17:03:08Z",
    "pdf_url": "https://arxiv.org/pdf/2509.09710v2"
  },
  {
    "arxiv_id": "2509.05716v1",
    "entry_id": "http://arxiv.org/abs/2509.05716v1",
    "title": "A Survey of the State-of-the-Art in Conversational Question Answering Systems",
    "summary": "Conversational Question Answering (ConvQA) systems have emerged as a pivotal area within Natural Language Processing (NLP) by driving advancements that enable machines to engage in dynamic and context-aware conversations. These capabilities are increasingly being applied across various domains, i.e., customer support, education, legal, and healthcare where maintaining a coherent and relevant conversation is essential. Building on recent advancements, this survey provides a comprehensive analysis of the state-of-the-art in ConvQA. This survey begins by examining the core components of ConvQA systems, i.e., history selection, question understanding, and answer prediction, highlighting their interplay in ensuring coherence and relevance in multi-turn conversations. It further investigates the use of advanced machine learning techniques, including but not limited to, reinforcement learning, contrastive learning, and transfer learning to improve ConvQA accuracy and efficiency. The pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash, Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact through data scalability and architectural advancements. Additionally, this survey presents a comprehensive analysis of key ConvQA datasets and concludes by outlining open research directions. Overall, this work offers a comprehensive overview of the ConvQA landscape and provides valuable insights to guide future advancements in the field.",
    "authors": [
      "Manoj Madushanka Perera",
      "Adnan Mahmood",
      "Kasun Eranda Wijethilake",
      "Fahmida Islam",
      "Maryam Tahermazandarani",
      "Quan Z. Sheng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-06T13:38:03Z",
    "pdf_url": "https://arxiv.org/pdf/2509.05716v1"
  },
  {
    "arxiv_id": "2509.07009v2",
    "entry_id": "http://arxiv.org/abs/2509.07009v2",
    "title": "Computational Concept of the Psyche (in Russian)",
    "summary": "The article provides an overview of approaches to modeling the human psyche in the perspective of building an artificial one. Based on the review, a concept of cognitive architecture is proposed, where the psyche is considered as an operating system of a living or artificial subject, including a space of needs that determines its life meanings in connection with stimuli from the external world, and intelligence as a decision-making system for actions in relation to this world in order to satisfy these needs. Based on the concept, a computational formalization is proposed for creating artificial intelligence systems through learning from experience in the space of a space of needs, taking into account their biological or existential significance for an intelligent agent. Thus, the problem of building general artificial intelligence as a system for making optimal decisions in the space of agent-specific needs under conditions of uncertainty is formalized, with maximization of success in achieving goals, minimization of existential risks and maximization of energy efficiency. A minimal experimental implementation of the model is also provided.",
    "authors": [
      "Anton Kolonin",
      "Vladimir Kryukov"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "eess.SY"
    ],
    "published": "2025-09-06T10:22:27Z",
    "pdf_url": "https://arxiv.org/pdf/2509.07009v2"
  },
  {
    "arxiv_id": "2509.04923v1",
    "entry_id": "http://arxiv.org/abs/2509.04923v1",
    "title": "Artificial intelligence for representing and characterizing quantum systems",
    "summary": "Efficient characterization of large-scale quantum systems, especially those produced by quantum analog simulators and megaquop quantum computers, poses a central challenge in quantum science due to the exponential scaling of the Hilbert space with respect to system size. Recent advances in artificial intelligence (AI), with its aptitude for high-dimensional pattern recognition and function approximation, have emerged as a powerful tool to address this challenge. A growing body of research has leveraged AI to represent and characterize scalable quantum systems, spanning from theoretical foundations to experimental realizations. Depending on how prior knowledge and learning architectures are incorporated, the integration of AI into quantum system characterization can be categorized into three synergistic paradigms: machine learning, and, in particular, deep learning and language models. This review discusses how each of these AI paradigms contributes to two core tasks in quantum systems characterization: quantum property prediction and the construction of surrogates for quantum states. These tasks underlie diverse applications, from quantum certification and benchmarking to the enhancement of quantum algorithms and the understanding of strongly correlated phases of matter. Key challenges and open questions are also discussed, together with future prospects at the interface of AI and quantum science.",
    "authors": [
      "Yuxuan Du",
      "Yan Zhu",
      "Yuan-Hang Zhang",
      "Min-Hsiu Hsieh",
      "Patrick Rebentrost",
      "Weibo Gao",
      "Ya-Dong Wu",
      "Jens Eisert",
      "Giulio Chiribella",
      "Dacheng Tao",
      "Barry C. Sanders"
    ],
    "categories": [
      "quant-ph",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-05T08:41:24Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04923v1"
  },
  {
    "arxiv_id": "2509.04810v1",
    "entry_id": "http://arxiv.org/abs/2509.04810v1",
    "title": "Code Review Without Borders: Evaluating Synthetic vs. Real Data for Review Recommendation",
    "summary": "Automating the decision of whether a code change requires manual review is vital for maintaining software quality in modern development workflows. However, the emergence of new programming languages and frameworks creates a critical bottleneck: while large volumes of unlabelled code are readily available, there is an insufficient amount of labelled data to train supervised models for review classification. We address this challenge by leveraging Large Language Models (LLMs) to translate code changes from well-resourced languages into equivalent changes in underrepresented or emerging languages, generating synthetic training data where labelled examples are scarce. We assume that although LLMs have learned the syntax and semantics of new languages from available unlabelled code, they have yet to fully grasp which code changes are considered significant or review-worthy within these emerging ecosystems. To overcome this, we use LLMs to generate synthetic change examples and train supervised classifiers on them. We systematically compare the performance of these classifiers against models trained on real labelled data. Our experiments across multiple GitHub repositories and language pairs demonstrate that LLM-generated synthetic data can effectively bootstrap review recommendation systems, narrowing the performance gap even in low-resource settings. This approach provides a scalable pathway to extend automated code review capabilities to rapidly evolving technology stacks, even in the absence of annotated data.",
    "authors": [
      "Yogev Cohen",
      "Dudi Ohayon",
      "Romy Somkin",
      "Yehudit Aperstein",
      "Alexander Apartsin"
    ],
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-09-05T05:17:14Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04810v1"
  },
  {
    "arxiv_id": "2509.04731v3",
    "entry_id": "http://arxiv.org/abs/2509.04731v3",
    "title": "Generative World Models of Tasks: LLM-Driven Hierarchical Scaffolding for Embodied Agents",
    "summary": "Recent advances in agent development have focused on scaling model size and raw interaction data, mirroring successes in large language models. However, for complex, long-horizon multi-agent tasks such as robotic soccer, this end-to-end approach often fails due to intractable exploration spaces and sparse rewards. We propose that an effective world model for decision-making must model the world's physics and also its task semantics. A systematic review of 2024 research in low-resource multi-agent soccer reveals a clear trend towards integrating symbolic and hierarchical methods, such as Hierarchical Task Networks (HTNs) and Bayesian Strategy Networks (BSNs), with multi-agent reinforcement learning (MARL). These methods decompose complex goals into manageable subgoals, creating an intrinsic curriculum that shapes agent learning. We formalize this trend into a framework for Hierarchical Task Environments (HTEs), which are essential for bridging the gap between simple, reactive behaviors and sophisticated, strategic team play. Our framework incorporates the use of Large Language Models (LLMs) as generative world models of tasks, capable of dynamically generating this scaffolding. We argue that HTEs provide a mechanism to guide exploration, generate meaningful learning signals, and train agents to internalize hierarchical structure, enabling the development of more capable and general-purpose agents with greater sample efficiency than purely end-to-end approaches.",
    "authors": [
      "Brennen Hill"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA",
      "cs.RO"
    ],
    "published": "2025-09-05T01:03:51Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04731v3"
  },
  {
    "arxiv_id": "2509.04615v1",
    "entry_id": "http://arxiv.org/abs/2509.04615v1",
    "title": "Breaking to Build: A Threat Model of Prompt-Based Attacks for Securing LLMs",
    "summary": "The proliferation of Large Language Models (LLMs) has introduced critical security challenges, where adversarial actors can manipulate input prompts to cause significant harm and circumvent safety alignments. These prompt-based attacks exploit vulnerabilities in a model's design, training, and contextual understanding, leading to intellectual property theft, misinformation generation, and erosion of user trust. A systematic understanding of these attack vectors is the foundational step toward developing robust countermeasures. This paper presents a comprehensive literature survey of prompt-based attack methodologies, categorizing them to provide a clear threat model. By detailing the mechanisms and impacts of these exploits, this survey aims to inform the research community's efforts in building the next generation of secure LLMs that are inherently resistant to unauthorized distillation, fine-tuning, and editing.",
    "authors": [
      "Brennen Hill",
      "Surendra Parla",
      "Venkata Abhijeeth Balabhadruni",
      "Atharv Prajod Padmalayam",
      "Sujay Chandra Shekara Sharma"
    ],
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-09-04T18:59:07Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04615v1"
  },
  {
    "arxiv_id": "2509.04304v1",
    "entry_id": "http://arxiv.org/abs/2509.04304v1",
    "title": "Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models",
    "summary": "The growing capabilities of Large Language Models (LLMs) show significant potential to enhance healthcare by assisting medical researchers and physicians. However, their reliance on static training data is a major risk when medical recommendations evolve with new research and developments. When LLMs memorize outdated medical knowledge, they can provide harmful advice or fail at clinical reasoning tasks. To investigate this problem, we introduce two novel question-answering (QA) datasets derived from systematic reviews: MedRevQA (16,501 QA pairs covering general biomedical knowledge) and MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over time). Our evaluation of eight prominent LLMs on the datasets reveals consistent reliance on outdated knowledge across all models. We additionally analyze the influence of obsolete pre-training data and training strategies to explain this phenomenon and propose future directions for mitigation, laying the groundwork for developing more current and reliable medical AI systems.",
    "authors": [
      "Juraj Vladika",
      "Mahdi Dhaini",
      "Florian Matthes"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-04T15:17:50Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04304v1"
  },
  {
    "arxiv_id": "2509.04260v1",
    "entry_id": "http://arxiv.org/abs/2509.04260v1",
    "title": "An Empirical Study of Vulnerabilities in Python Packages and Their Detection",
    "summary": "In the rapidly evolving software development landscape, Python stands out for its simplicity, versatility, and extensive ecosystem. Python packages, as units of organization, reusability, and distribution, have become a pressing concern, highlighted by the considerable number of vulnerability reports. As a scripting language, Python often cooperates with other languages for performance or interoperability. This adds complexity to the vulnerabilities inherent to Python packages, and the effectiveness of current vulnerability detection tools remains underexplored. This paper addresses these gaps by introducing PyVul, the first comprehensive benchmark suite of Python-package vulnerabilities. PyVul includes 1,157 publicly reported, developer-verified vulnerabilities, each linked to its affected packages. To accommodate diverse detection techniques, it provides annotations at both commit and function levels. An LLM-assisted data cleansing method is incorporated to improve label accuracy, achieving 100% commit-level and 94% function-level accuracy, establishing PyVul as the most precise large-scale Python vulnerability benchmark. We further carry out a distribution analysis of PyVul, which demonstrates that vulnerabilities in Python packages involve multiple programming languages and exhibit a wide variety of types. Moreover, our analysis reveals that multi-lingual Python packages are potentially more susceptible to vulnerabilities. Evaluation of state-of-the-art detectors using this benchmark reveals a significant discrepancy between the capabilities of existing tools and the demands of effectively identifying real-world security issues in Python packages. Additionally, we conduct an empirical review of the top-ranked CWEs observed in Python packages, to diagnose the fine-grained limitations of current detection tools and highlight the necessity for future advancements in the field.",
    "authors": [
      "Haowei Quan",
      "Junjie Wang",
      "Xinzhe Li",
      "Terry Yue Zhuo",
      "Xiao Chen",
      "Xiaoning Du"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-09-04T14:38:28Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04260v1"
  },
  {
    "arxiv_id": "2509.04191v1",
    "entry_id": "http://arxiv.org/abs/2509.04191v1",
    "title": "KubeGuard: LLM-Assisted Kubernetes Hardening via Configuration Files and Runtime Logs Analysis",
    "summary": "The widespread adoption of Kubernetes (K8s) for orchestrating cloud-native applications has introduced significant security challenges, such as misconfigured resources and overly permissive configurations. Failing to address these issues can result in unauthorized access, privilege escalation, and lateral movement within clusters. Most existing K8s security solutions focus on detecting misconfigurations, typically through static analysis or anomaly detection. In contrast, this paper presents KubeGuard, a novel runtime log-driven recommender framework aimed at mitigating risks by addressing overly permissive configurations. KubeGuard is designed to harden K8s environments through two complementary tasks: Resource Creation and Resource Refinement. It leverages large language models (LLMs) to analyze manifests and runtime logs reflecting actual system behavior, using modular prompt-chaining workflows. This approach enables KubeGuard to create least-privilege configurations for new resources and refine existing manifests to reduce the attack surface. KubeGuard's output manifests are presented as recommendations that users (e.g., developers and operators) can review and adopt to enhance cluster security. Our evaluation demonstrates that KubeGuard effectively generates and refines K8s manifests for Roles, NetworkPolicies, and Deployments, leveraging both proprietary and open-source LLMs. The high precision, recall, and F1-scores affirm KubeGuard's practicality as a framework that translates runtime observability into actionable, least-privilege configuration guidance.",
    "authors": [
      "Omri Sgan Cohen",
      "Ehud Malul",
      "Yair Meidan",
      "Dudu Mimran",
      "Yuval Elovici",
      "Asaf Shabtai"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-09-04T13:13:57Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04191v1"
  },
  {
    "arxiv_id": "2509.03871v1",
    "entry_id": "http://arxiv.org/abs/2509.03871v1",
    "title": "A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models",
    "summary": "The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.",
    "authors": [
      "Yanbo Wang",
      "Yongcan Yu",
      "Jian Liang",
      "Ran He"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-09-04T04:12:31Z",
    "pdf_url": "https://arxiv.org/pdf/2509.03871v1"
  },
  {
    "arxiv_id": "2509.03736v1",
    "entry_id": "http://arxiv.org/abs/2509.03736v1",
    "title": "Are LLM Agents Behaviorally Coherent? Latent Profiles for Social Simulation",
    "summary": "The impressive capabilities of Large Language Models (LLMs) have fueled the notion that synthetic agents can serve as substitutes for real participants in human-subject research. In an effort to evaluate the merits of this claim, social science researchers have largely focused on whether LLM-generated survey data corresponds to that of a human counterpart whom the LLM is prompted to represent. In contrast, we address a more fundamental question: Do agents maintain internal consistency, retaining similar behaviors when examined under different experimental settings? To this end, we develop a study designed to (a) reveal the agent's internal state and (b) examine agent behavior in a basic dialogue setting. This design enables us to explore a set of behavioral hypotheses to assess whether an agent's conversation behavior is consistent with what we would expect from their revealed internal state. Our findings on these hypotheses show significant internal inconsistencies in LLMs across model families and at differing model sizes. Most importantly, we find that, although agents may generate responses matching those of their human counterparts, they fail to be internally consistent, representing a critical gap in their capabilities to accurately substitute for real participants in human-subject research. Our simulation code and data are publicly accessible.",
    "authors": [
      "James Mooney",
      "Josef Woldense",
      "Zheng Robert Jia",
      "Shirley Anugrah Hayati",
      "My Ha Nguyen",
      "Vipul Raheja",
      "Dongyeop Kang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-09-03T21:55:29Z",
    "pdf_url": "https://arxiv.org/pdf/2509.03736v1"
  },
  {
    "arxiv_id": "2509.03682v1",
    "entry_id": "http://arxiv.org/abs/2509.03682v1",
    "title": "A Comprehensive Review of Multi-Agent Reinforcement Learning in Video Games",
    "summary": "Recent advancements in multi-agent reinforcement learning (MARL) have demonstrated its application potential in modern games. Beginning with foundational work and progressing to landmark achievements such as AlphaStar in StarCraft II and OpenAI Five in Dota 2, MARL has proven capable of achieving superhuman performance across diverse game environments through techniques like self-play, supervised learning, and deep reinforcement learning. With its growing impact, a comprehensive review has become increasingly important in this field. This paper aims to provide a thorough examination of MARL's application from turn-based two-agent games to real-time multi-agent video games including popular genres such as Sports games, First-Person Shooter (FPS) games, Real-Time Strategy (RTS) games and Multiplayer Online Battle Arena (MOBA) games. We further analyze critical challenges posed by MARL in video games, including nonstationary, partial observability, sparse rewards, team coordination, and scalability, and highlight successful implementations in games like Rocket League, Minecraft, Quake III Arena, StarCraft II, Dota 2, Honor of Kings, etc. This paper offers insights into MARL in video game AI systems, proposes a novel method to estimate game complexity, and suggests future research directions to advance MARL and its applications in game development, inspiring further innovation in this rapidly evolving field.",
    "authors": [
      "Zhengyang Li",
      "Qijin Ji",
      "Xinghong Ling",
      "Quan Liu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-09-03T20:05:58Z",
    "pdf_url": "https://arxiv.org/pdf/2509.03682v1"
  },
  {
    "arxiv_id": "2509.03615v1",
    "entry_id": "http://arxiv.org/abs/2509.03615v1",
    "title": "E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition",
    "summary": "Optical Character Recognition (OCR) in multilingual, noisy, and diverse real-world images remains a significant challenge for optical character recognition systems. With the rise of Large Vision-Language Models (LVLMs), there is growing interest in their ability to generalize and reason beyond fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR system built specifically optimized for edge deployment in resource-constrained environments. We present a large-scale comparative evaluation of five state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly hand annotated dataset of multilingual (54 languages) images. Our benchmark covers a broad range of metrics including accuracy, semantic consistency, language coverage, computational efficiency (latency, memory, GPU usage), and deployment cost. To better reflect real-world applicability, we also conducted edge case deployment analysis, evaluating model performance on CPU only environments. Among the results, Qwen achieved the highest precision (0.54), while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and outperformed others in efficiency, processing images 35 faster (0.17 seconds per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000 images) compared to LVLM. Our findings demonstrate that the most optimal OCR systems for edge deployment are the traditional ones even in the era of LLMs due to their low compute requirements, low latency, and very high affordability.",
    "authors": [
      "Aryan Gupta",
      "Anupam Purwar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-03T18:08:41Z",
    "pdf_url": "https://arxiv.org/pdf/2509.03615v1"
  },
  {
    "arxiv_id": "2509.02547v3",
    "entry_id": "http://arxiv.org/abs/2509.02547v3",
    "title": "The Landscape of Agentic Reinforcement Learning for LLMs: A Survey",
    "summary": "The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.",
    "authors": [
      "Guibin Zhang",
      "Hejia Geng",
      "Xiaohang Yu",
      "Zhenfei Yin",
      "Zaibin Zhang",
      "Zelin Tan",
      "Heng Zhou",
      "Zhongzhi Li",
      "Xiangyuan Xue",
      "Yijiang Li",
      "Yifan Zhou",
      "Yang Chen",
      "Chen Zhang",
      "Yutao Fan",
      "Zihu Wang",
      "Songtao Huang",
      "Francisco Piedrahita-Velez",
      "Yue Liao",
      "Hongru Wang",
      "Mengyue Yang",
      "Heng Ji",
      "Jun Wang",
      "Shuicheng Yan",
      "Philip Torr",
      "Lei Bai"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-09-02T17:46:26Z",
    "pdf_url": "https://arxiv.org/pdf/2509.02547v3"
  },
  {
    "arxiv_id": "2509.02411v1",
    "entry_id": "http://arxiv.org/abs/2509.02411v1",
    "title": "A Survey: Towards Privacy and Security in Mobile Large Language Models",
    "summary": "Mobile Large Language Models (LLMs) are revolutionizing diverse fields such as healthcare, finance, and education with their ability to perform advanced natural language processing tasks on-the-go. However, the deployment of these models in mobile and edge environments introduces significant challenges related to privacy and security due to their resource-intensive nature and the sensitivity of the data they process. This survey provides a comprehensive overview of privacy and security issues associated with mobile LLMs, systematically categorizing existing solutions such as differential privacy, federated learning, and prompt encryption. Furthermore, we analyze vulnerabilities unique to mobile LLMs, including adversarial attacks, membership inference, and side-channel attacks, offering an in-depth comparison of their effectiveness and limitations. Despite recent advancements, mobile LLMs face unique hurdles in achieving robust security while maintaining efficiency in resource-constrained environments. To bridge this gap, we propose potential applications, discuss open challenges, and suggest future research directions, paving the way for the development of trustworthy, privacy-compliant, and scalable mobile LLM systems.",
    "authors": [
      "Honghui Xu",
      "Kaiyang Li",
      "Wei Chen",
      "Danyang Zheng",
      "Zhiyuan Li",
      "Zhipeng Cai"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-09-02T15:19:57Z",
    "pdf_url": "https://arxiv.org/pdf/2509.02411v1"
  },
  {
    "arxiv_id": "2509.02350v1",
    "entry_id": "http://arxiv.org/abs/2509.02350v1",
    "title": "Implicit Reasoning in Large Language Models: A Comprehensive Survey",
    "summary": "Large Language Models (LLMs) have demonstrated strong generalization across a wide range of tasks. Reasoning with LLMs is central to solving multi-step problems and complex decision-making. To support efficient reasoning, recent studies have shifted attention from explicit chain-of-thought prompting toward implicit reasoning, where reasoning occurs silently via latent structures without emitting intermediate textual steps. Implicit reasoning brings advantages such as lower generation cost, faster inference, and better alignment with internal computation. Although prior surveys have discussed latent representations in the context of reasoning, a dedicated and mechanism-level examination of how reasoning unfolds internally within LLMs remains absent. This survey fills that gap by introducing a taxonomy centered on execution paradigms, shifting the focus from representational forms to computational strategies. We organize existing methods into three execution paradigms based on \\textbf{\\textit{how and where internal computation unfolds}}: latent optimization, signal-guided control, and layer-recurrent execution. We also review structural, behavioral and representation-based evidence that supports the presence of implicit reasoning in LLMs. We further provide a structured overview of the evaluation metrics and benchmarks used in existing works to assess the effectiveness and reliability of implicit reasoning. We maintain a continuously updated project at: https://github.com/digailab/awesome-llm-implicit-reasoning.",
    "authors": [
      "Jindong Li",
      "Yali Fu",
      "Li Fan",
      "Jiahong Liu",
      "Yao Shu",
      "Chengwei Qin",
      "Menglin Yang",
      "Irwin King",
      "Rex Ying"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-09-02T14:16:02Z",
    "pdf_url": "https://arxiv.org/pdf/2509.02350v1"
  },
  {
    "arxiv_id": "2509.04501v2",
    "entry_id": "http://arxiv.org/abs/2509.04501v2",
    "title": "Understanding Reinforcement Learning for Model Training, and future directions with GRAPE",
    "summary": "This paper provides a self-contained, from-scratch, exposition of key algorithms for instruction tuning of models: SFT, Rejection Sampling, REINFORCE, Trust Region Policy Optimization (TRPO), Proximal Policy Optimization (PPO), Group Relative Policy Optimization (GRPO), and Direct Preference Optimization (DPO). Explanations of these algorithms often assume prior knowledge, lack critical details, and/or are overly generalized and complex. Here, each method is discussed and developed step by step using simplified and explicit notation focused on LLMs, aiming to eliminate ambiguity and provide a clear and intuitive understanding of the concepts. By minimizing detours into the broader RL literature and connecting concepts to LLMs, we eliminate superfluous abstractions and reduce cognitive overhead. Following this exposition, we provide a literature review of new techniques and approaches beyond those detailed. Finally, new ideas for research and exploration in the form of GRAPE (Generalized Relative Advantage Policy Evolution) are presented.",
    "authors": [
      "Rohit Patel"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-09-02T03:59:40Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04501v2"
  },
  {
    "arxiv_id": "2509.01822v1",
    "entry_id": "http://arxiv.org/abs/2509.01822v1",
    "title": "When LLM Meets Time Series: Can LLMs Perform Multi-Step Time Series Reasoning and Inference",
    "summary": "The rapid advancement of Large Language Models (LLMs) has sparked growing interest in their application to time series analysis tasks. However, their ability to perform complex reasoning over temporal data in real-world application domains remains underexplored. To move toward this goal, a first step is to establish a rigorous benchmark dataset for evaluation. In this work, we introduce the TSAIA Benchmark, a first attempt to evaluate LLMs as time-series AI assistants. To ensure both scientific rigor and practical relevance, we surveyed over 20 academic publications and identified 33 real-world task formulations. The benchmark encompasses a broad spectrum of challenges, ranging from constraint-aware forecasting to anomaly detection with threshold calibration: tasks that require compositional reasoning and multi-step time series analysis. The question generator is designed to be dynamic and extensible, supporting continuous expansion as new datasets or task types are introduced. Given the heterogeneous nature of the tasks, we adopt task-specific success criteria and tailored inference-quality metrics to ensure meaningful evaluation for each task. We apply this benchmark to assess eight state-of-the-art LLMs under a unified evaluation protocol. Our analysis reveals limitations in current models' ability to assemble complex time series analysis workflows, underscoring the need for specialized methodologies for domain-specific adaptation. Our benchmark is available at https://huggingface.co/datasets/Melady/TSAIA, and the code is available at https://github.com/USC-Melady/TSAIA.",
    "authors": [
      "Wen Ye",
      "Jinbo Liu",
      "Defu Cao",
      "Wei Yang",
      "Yan Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-09-01T22:58:57Z",
    "pdf_url": "https://arxiv.org/pdf/2509.01822v1"
  },
  {
    "arxiv_id": "2509.01814v1",
    "entry_id": "http://arxiv.org/abs/2509.01814v1",
    "title": "Mic Drop or Data Flop? Evaluating the Fitness for Purpose of AI Voice Interviewers for Data Collection within Quantitative & Qualitative Research Contexts",
    "summary": "Transformer-based Large Language Models (LLMs) have paved the way for \"AI interviewers\" that can administer voice-based surveys with respondents in real-time. This position paper reviews emerging evidence to understand when such AI interviewing systems are fit for purpose for collecting data within quantitative and qualitative research contexts. We evaluate the capabilities of AI interviewers as well as current Interactive Voice Response (IVR) systems across two dimensions: input/output performance (i.e., speech recognition, answer recording, emotion handling) and verbal reasoning (i.e., ability to probe, clarify, and handle branching logic). Field studies suggest that AI interviewers already exceed IVR capabilities for both quantitative and qualitative data collection, but real-time transcription error rates, limited emotion detection abilities, and uneven follow-up quality indicate that the utility, use and adoption of current AI interviewer technology may be context-dependent for qualitative data collection efforts.",
    "authors": [
      "Shreyas Tirumala",
      "Nishant Jain",
      "Danny D. Leybzon",
      "Trent D. Buskirk"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-09-01T22:44:57Z",
    "pdf_url": "https://arxiv.org/pdf/2509.01814v1"
  },
  {
    "arxiv_id": "2509.01398v1",
    "entry_id": "http://arxiv.org/abs/2509.01398v1",
    "title": "The Need for Verification in AI-Driven Scientific Discovery",
    "summary": "Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.",
    "authors": [
      "Cristina Cornelio",
      "Takuya Ito",
      "Ryan Cory-Wright",
      "Sanjeeb Dash",
      "Lior Horesh"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-09-01T11:50:04Z",
    "pdf_url": "https://arxiv.org/pdf/2509.01398v1"
  },
  {
    "arxiv_id": "2509.01387v1",
    "entry_id": "http://arxiv.org/abs/2509.01387v1",
    "title": "ABCD-LINK: Annotation Bootstrapping for Cross-Document Fine-Grained Links",
    "summary": "Understanding fine-grained relations between documents is crucial for many application domains. However, the study of automated assistance is limited by the lack of efficient methods to create training and evaluation datasets of cross-document links. To address this, we introduce a new domain-agnostic framework for selecting a best-performing approach and annotating cross-document links in a new domain from scratch. We first generate and validate semi-synthetic datasets of interconnected documents. This data is used to perform automatic evaluation, producing a shortlist of best-performing linking approaches. These approaches are then used in an extensive human evaluation study, yielding performance estimates on natural text pairs. We apply our framework in two distinct domains -- peer review and news -- and show that combining retrieval models with LLMs achieves 78\\% link approval from human raters, more than doubling the precision of strong retrievers alone. Our framework enables systematic study of cross-document understanding across application scenarios, and the resulting novel datasets lay foundation for numerous cross-document tasks like media framing and peer review. We make the code, data, and annotation protocols openly available.",
    "authors": [
      "Serwar Basch",
      "Ilia Kuznetsov",
      "Tom Hope",
      "Iryna Gurevych"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-09-01T11:32:24Z",
    "pdf_url": "https://arxiv.org/pdf/2509.01387v1"
  },
  {
    "arxiv_id": "2509.01063v1",
    "entry_id": "http://arxiv.org/abs/2509.01063v1",
    "title": "An Economy of AI Agents",
    "summary": "In the coming decade, artificially intelligent agents with the ability to plan and execute complex tasks over long time horizons with little direct oversight from humans may be deployed across the economy. This chapter surveys recent developments and highlights open questions for economists around how AI agents might interact with humans and with each other, shape markets and organizations, and what institutions might be required for well-functioning markets.",
    "authors": [
      "Gillian K. Hadfield",
      "Andrew Koh"
    ],
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-09-01T02:07:39Z",
    "pdf_url": "https://arxiv.org/pdf/2509.01063v1"
  },
  {
    "arxiv_id": "2509.00987v1",
    "entry_id": "http://arxiv.org/abs/2509.00987v1",
    "title": "Causal MAS: A Survey of Large Language Model Architectures for Discovery and Effect Estimation",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various reasoning and generation tasks. However, their proficiency in complex causal reasoning, discovery, and estimation remains an area of active development, often hindered by issues like hallucination, reliance on spurious correlations, and difficulties in handling nuanced, domain-specific, or personalized causal relationships. Multi-agent systems, leveraging the collaborative or specialized abilities of multiple LLM-based agents, are emerging as a powerful paradigm to address these limitations. This review paper explores the burgeoning field of causal multi-agent LLMs. We examine how these systems are designed to tackle different facets of causality, including causal reasoning and counterfactual analysis, causal discovery from data, and the estimation of causal effects. We delve into the diverse architectural patterns and interaction protocols employed, from pipeline-based processing and debate frameworks to simulation environments and iterative refinement loops. Furthermore, we discuss the evaluation methodologies, benchmarks, and diverse application domains where causal multi-agent LLMs are making an impact, including scientific discovery, healthcare, fact-checking, and personalized systems. Finally, we highlight the persistent challenges, open research questions, and promising future directions in this synergistic field, aiming to provide a comprehensive overview of its current state and potential trajectory.",
    "authors": [
      "Adib Bazgir",
      "Amir Habibdoust",
      "Yuwen Zhang",
      "Xing Song"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-31T20:48:31Z",
    "pdf_url": "https://arxiv.org/pdf/2509.00987v1"
  },
  {
    "arxiv_id": "2509.00647v1",
    "entry_id": "http://arxiv.org/abs/2509.00647v1",
    "title": "LLM-HyPZ: Hardware Vulnerability Discovery using an LLM-Assisted Hybrid Platform for Zero-Shot Knowledge Extraction and Refinement",
    "summary": "The rapid growth of hardware vulnerabilities has created an urgent need for systematic and scalable analysis methods. Unlike software flaws, which are often patchable post-deployment, hardware weaknesses remain embedded across product lifecycles, posing persistent risks to processors, embedded devices, and IoT platforms. Existing efforts such as the MITRE CWE Hardware List (2021) relied on expert-driven Delphi surveys, which lack statistical rigor and introduce subjective bias, while large-scale data-driven foundations for hardware weaknesses have been largely absent. In this work, we propose LLM-HyPZ, an LLM-assisted hybrid framework for zero-shot knowledge extraction and refinement from vulnerability corpora. Our approach integrates zero-shot LLM classification, contextualized embeddings, unsupervised clustering, and prompt-driven summarization to mine hardware-related CVEs at scale. Applying LLM-HyPZ to the 2021-2024 CVE corpus (114,836 entries), we identified 1,742 hardware-related vulnerabilities. We distilled them into five recurring themes, including privilege escalation via firmware and BIOS, memory corruption in mobile and IoT systems, and physical access exploits. Benchmarking across seven LLMs shows that LLaMA 3.3 70B achieves near-perfect classification accuracy (99.5%) on a curated validation set. Beyond methodological contributions, our framework directly supported the MITRE CWE Most Important Hardware Weaknesses (MIHW) 2025 update by narrowing the candidate search space. Specifically, our pipeline surfaced 411 of the 1,026 CVEs used for downstream MIHW analysis, thereby reducing expert workload and accelerating evidence gathering. These results establish LLM-HyPZ as the first data-driven, scalable approach for systematically discovering hardware vulnerabilities, thereby bridging the gap between expert knowledge and real-world vulnerability evidence.",
    "authors": [
      "Yu-Zheng Lin",
      "Sujan Ghimire",
      "Abhiram Nandimandalam",
      "Jonah Michael Camacho",
      "Unnati Tripathi",
      "Rony Macwan",
      "Sicong Shao",
      "Setareh Rafatirad",
      "Rozhin Yasaei",
      "Pratik Satam",
      "Soheil Salehi"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-08-31T00:55:31Z",
    "pdf_url": "https://arxiv.org/pdf/2509.00647v1"
  },
  {
    "arxiv_id": "2509.00496v1",
    "entry_id": "http://arxiv.org/abs/2509.00496v1",
    "title": "ResearchQA: Evaluating Scholarly Question Answering at Scale Across 75 Fields with Survey-Mined Questions and Rubrics",
    "summary": "Evaluating long-form responses to research queries heavily relies on expert annotators, restricting attention to areas like AI where researchers can conveniently enlist colleagues. Yet, research expertise is widespread: survey articles synthesize knowledge distributed across the literature. We introduce ResearchQA, a resource for evaluating LLM systems by distilling survey articles from 75 research fields into 21K queries and 160K rubric items. Each rubric, derived jointly with queries from survey sections, lists query-specific answer evaluation criteria, i.e., citing papers, making explanations, and describing limitations. Assessments by 31 Ph.D. annotators in 8 fields indicate 96% of queries support Ph.D. information needs and 87% of rubric items should be addressed in system responses by a sentence or more. Using our rubrics, we are able to construct an automatic pairwise judge obtaining 74% agreement with expert judgments. We leverage ResearchQA to analyze competency gaps in 18 systems in over 7.6K pairwise evaluations. No parametric or retrieval-augmented system we evaluate exceeds 70% on covering rubric items, and the highest-ranking agentic system shows 75% coverage. Error analysis reveals that the highest-ranking system fully addresses less than 11% of citation rubric items, 48% of limitation items, and 49% of comparison items. We release our data to facilitate more comprehensive multi-field evaluations.",
    "authors": [
      "Li S. Yifei",
      "Allen Chang",
      "Chaitanya Malaviya",
      "Mark Yatskar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-30T13:37:28Z",
    "pdf_url": "https://arxiv.org/pdf/2509.00496v1"
  },
  {
    "arxiv_id": "2509.00285v2",
    "entry_id": "http://arxiv.org/abs/2509.00285v2",
    "title": "OpinioRAG: Towards Generating User-Centric Opinion Highlights from Large-scale Online Reviews",
    "summary": "We study the problem of opinion highlights generation from large volumes of user reviews, often exceeding thousands per entity, where existing methods either fail to scale or produce generic, one-size-fits-all summaries that overlook personalized needs. To tackle this, we introduce OpinioRAG, a scalable, training-free framework that combines RAG-based evidence retrieval with LLMs to efficiently produce tailored summaries. Additionally, we propose novel reference-free verification metrics designed for sentiment-rich domains, where accurately capturing opinions and sentiment alignment is essential. These metrics offer a fine-grained, context-sensitive assessment of factual consistency. To facilitate evaluation, we contribute the first large-scale dataset of long-form user reviews, comprising entities with over a thousand reviews each, paired with unbiased expert summaries and manually annotated queries. Through extensive experiments, we identify key challenges, provide actionable insights into improving systems, pave the way for future research, and position OpinioRAG as a robust framework for generating accurate, relevant, and structured summaries at scale.",
    "authors": [
      "Mir Tafseer Nayeem",
      "Davood Rafiei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-08-30T00:00:34Z",
    "pdf_url": "https://arxiv.org/pdf/2509.00285v2"
  },
  {
    "arxiv_id": "2508.21693v1",
    "entry_id": "http://arxiv.org/abs/2508.21693v1",
    "title": "Why Stop at Words? Unveiling the Bigger Picture through Line-Level OCR",
    "summary": "Conventional optical character recognition (OCR) techniques segmented each character and then recognized. This made them prone to error in character segmentation, and devoid of context to exploit language models. Advances in sequence to sequence translation in last decade led to modern techniques first detecting words and then inputting one word at a time to a model to directly output full words as sequence of characters. This allowed better utilization of language models and bypass error-prone character segmentation step. We observe that the above transition in style has moved the bottleneck in accuracy to word segmentation. Hence, in this paper, we propose a natural and logical progression from word level OCR to line-level OCR. The proposal allows to bypass errors in word detection, and provides larger sentence context for better utilization of language models. We show that the proposed technique not only improves the accuracy but also efficiency of OCR. Despite our thorough literature survey, we did not find any public dataset to train and benchmark such shift from word to line-level OCR. Hence, we also contribute a meticulously curated dataset of 251 English page images with line-level annotations. Our experimentation revealed a notable end-to-end accuracy improvement of 5.4%, underscoring the potential benefits of transitioning towards line-level OCR, especially for document images. We also report a 4 times improvement in efficiency compared to word-based pipelines. With continuous improvements in large language models, our methodology also holds potential to exploit such advances. Project Website: https://nishitanand.github.io/line-level-ocr-website",
    "authors": [
      "Shashank Vempati",
      "Nishit Anand",
      "Gaurav Talebailkar",
      "Arpan Garai",
      "Chetan Arora"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-08-29T15:02:11Z",
    "pdf_url": "https://arxiv.org/pdf/2508.21693v1"
  },
  {
    "arxiv_id": "2508.21587v1",
    "entry_id": "http://arxiv.org/abs/2508.21587v1",
    "title": "A Survey on Current Trends and Recent Advances in Text Anonymization",
    "summary": "The proliferation of textual data containing sensitive personal information across various domains requires robust anonymization techniques to protect privacy and comply with regulations, while preserving data usability for diverse and crucial downstream tasks. This survey provides a comprehensive overview of current trends and recent advances in text anonymization techniques. We begin by discussing foundational approaches, primarily centered on Named Entity Recognition, before examining the transformative impact of Large Language Models, detailing their dual role as sophisticated anonymizers and potent de-anonymization threats. The survey further explores domain-specific challenges and tailored solutions in critical sectors such as healthcare, law, finance, and education. We investigate advanced methodologies incorporating formal privacy models and risk-aware frameworks, and address the specialized subfield of authorship anonymization. Additionally, we review evaluation frameworks, comprehensive metrics, benchmarks, and practical toolkits for real-world deployment of anonymization solutions. This review consolidates current knowledge, identifies emerging trends and persistent challenges, including the evolving privacy-utility trade-off, the need to address quasi-identifiers, and the implications of LLM capabilities, and aims to guide future research directions for both academics and practitioners in this field.",
    "authors": [
      "Tobias Deußer",
      "Lorenz Sparrenberg",
      "Armin Berger",
      "Max Hahnbück",
      "Christian Bauckhage",
      "Rafet Sifa"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-29T12:43:06Z",
    "pdf_url": "https://arxiv.org/pdf/2508.21587v1"
  },
  {
    "arxiv_id": "2508.21484v2",
    "entry_id": "http://arxiv.org/abs/2508.21484v2",
    "title": "Data-driven Discovery of Digital Twins in Biomedical Research",
    "summary": "Recent technological advances have expanded the availability of high-throughput biological datasets, enabling the reliable design of digital twins of biomedical systems or patients. Such computational tools represent key reaction networks driving perturbation or drug response and can guide drug discovery and personalized therapeutics. Yet, their development still relies on laborious data integration by the human modeler, so that automated approaches are critically needed. The success of data-driven system discovery in Physics, rooted in clean datasets and well-defined governing laws, has fueled interest in applying similar techniques in Biology, which presents unique challenges. Here, we reviewed methodologies for automatically inferring digital twins from biological time series, which mostly involve symbolic or sparse regression. We evaluate algorithms according to eight biological and methodological challenges, associated to noisy/incomplete data, multiple conditions, prior knowledge integration, latent variables, high dimensionality, unobserved variable derivatives, candidate library design, and uncertainty quantification. Upon these criteria, sparse regression generally outperformed symbolic regression, particularly when using Bayesian frameworks. We further highlight the emerging role of deep learning and large language models, which enable innovative prior knowledge integration, though the reliability and consistency of such approaches must be improved. While no single method addresses all challenges, we argue that progress in learning digital twins will come from hybrid and modular frameworks combining chemical reaction network-based mechanistic grounding, Bayesian uncertainty quantification, and the generative and knowledge integration capacities of deep learning. To support their development, we further propose a benchmarking framework to evaluate methods across all challenges.",
    "authors": [
      "Clémence Métayer",
      "Annabelle Ballesta",
      "Julien Martinelli"
    ],
    "categories": [
      "q-bio.QM",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-08-29T10:10:02Z",
    "pdf_url": "https://arxiv.org/pdf/2508.21484v2"
  },
  {
    "arxiv_id": "2508.21377v1",
    "entry_id": "http://arxiv.org/abs/2508.21377v1",
    "title": "Challenges and Applications of Large Language Models: A Comparison of GPT and DeepSeek family of models",
    "summary": "Large Language Models (LLMs) are transforming AI across industries, but their development and deployment remain complex. This survey reviews 16 key challenges in building and using LLMs and examines how these challenges are addressed by two state-of-the-art models with unique approaches: OpenAI's closed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a large open source Mixture-of-Experts model. Through this comparison, we showcase the trade-offs between closed source models (robust safety, fine-tuned reliability) and open source models (efficiency, adaptability). We also explore LLM applications across different domains (from chatbots and coding tools to healthcare and education), highlighting which model attributes are best suited for each use case. This article aims to guide AI researchers, developers, and decision-makers in understanding current LLM capabilities, limitations, and best practices.",
    "authors": [
      "Shubham Sharma",
      "Sneha Tuli",
      "Narendra Badam"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-29T07:41:04Z",
    "pdf_url": "https://arxiv.org/pdf/2508.21377v1"
  },
  {
    "arxiv_id": "2508.21148v2",
    "entry_id": "http://arxiv.org/abs/2508.21148v2",
    "title": "A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers",
    "summary": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.",
    "authors": [
      "Ming Hu",
      "Chenglong Ma",
      "Wei Li",
      "Wanghan Xu",
      "Jiamin Wu",
      "Jucheng Hu",
      "Tianbin Li",
      "Guohang Zhuang",
      "Jiaqi Liu",
      "Yingzhou Lu",
      "Ying Chen",
      "Chaoyang Zhang",
      "Cheng Tan",
      "Jie Ying",
      "Guocheng Wu",
      "Shujian Gao",
      "Pengcheng Chen",
      "Jiashi Lin",
      "Haitao Wu",
      "Lulu Chen",
      "Fengxiang Wang",
      "Yuanyuan Zhang",
      "Xiangyu Zhao",
      "Feilong Tang",
      "Encheng Su",
      "Junzhi Ning",
      "Xinyao Liu",
      "Ye Du",
      "Changkai Ji",
      "Pengfei Jiang",
      "Cheng Tang",
      "Ziyan Huang",
      "Jiyao Liu",
      "Jiaqi Wei",
      "Yuejin Yang",
      "Xiang Zhang",
      "Guangshuai Wang",
      "Yue Yang",
      "Huihui Xu",
      "Ziyang Chen",
      "Yizhou Wang",
      "Chen Tang",
      "Jianyu Wu",
      "Yuchen Ren",
      "Siyuan Yan",
      "Zhonghua Wang",
      "Zhongxing Xu",
      "Shiyan Su",
      "Shangquan Sun",
      "Runkai Zhao",
      "Zhisheng Zhang",
      "Dingkang Yang",
      "Jinjie Wei",
      "Jiaqi Wang",
      "Jiahao Xu",
      "Jiangtao Yan",
      "Wenhao Tang",
      "Hongze Zhu",
      "Yu Liu",
      "Fudi Wang",
      "Yiqing Shen",
      "Yuanfeng Ji",
      "Yanzhou Su",
      "Tong Xie",
      "Hongming Shan",
      "Chun-Mei Feng",
      "Zhi Hou",
      "Diping Song",
      "Lihao Liu",
      "Yanyan Huang",
      "Lequan Yu",
      "Bin Fu",
      "Shujun Wang",
      "Xiaomeng Li",
      "Xiaowei Hu",
      "Yun Gu",
      "Ben Fei",
      "Benyou Wang",
      "Yuewen Cao",
      "Minjie Shen",
      "Jie Xu",
      "Haodong Duan",
      "Fang Yan",
      "Hongxia Hao",
      "Jielan Li",
      "Jiajun Du",
      "Yanbo Wang",
      "Imran Razzak",
      "Zhongying Deng",
      "Chi Zhang",
      "Lijun Wu",
      "Conghui He",
      "Zhaohui Lu",
      "Jinhai Huang",
      "Wenqi Shao",
      "Yihao Liu",
      "Siqi Luo",
      "Yi Xin",
      "Xiaohong Liu",
      "Fenghua Ling",
      "Yuqiang Li",
      "Aoran Wang",
      "Siqi Sun",
      "Qihao Zheng",
      "Nanqing Dong",
      "Tianfan Fu",
      "Dongzhan Zhou",
      "Yan Lu",
      "Wenlong Zhang",
      "Jin Ye",
      "Jianfei Cai",
      "Yirong Chen",
      "Wanli Ouyang",
      "Yu Qiao",
      "Zongyuan Ge",
      "Shixiang Tang",
      "Junjun He",
      "Chunfeng Song",
      "Lei Bai",
      "Bowen Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-28T18:30:52Z",
    "pdf_url": "https://arxiv.org/pdf/2508.21148v2"
  },
  {
    "arxiv_id": "2508.21061v1",
    "entry_id": "http://arxiv.org/abs/2508.21061v1",
    "title": "OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models",
    "summary": "As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.",
    "authors": [
      "Adam Coscia",
      "Shunan Guo",
      "Eunyee Koh",
      "Alex Endert"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-28T17:58:29Z",
    "pdf_url": "https://arxiv.org/pdf/2508.21061v1"
  },
  {
    "arxiv_id": "2509.00115v3",
    "entry_id": "http://arxiv.org/abs/2509.00115v3",
    "title": "Adaptive Monitoring and Real-World Evaluation of Agentic AI Systems",
    "summary": "Agentic artificial intelligence (AI) -- multi-agent systems that combine large language models with external tools and autonomous planning -- are rapidly transitioning from research laboratories into high-stakes domains. Our earlier \"Basic\" paper introduced a five-axis framework and proposed preliminary metrics such as goal drift and harm reduction but did not provide an algorithmic instantiation or empirical evidence. This \"Advanced\" sequel fills that gap. First, we revisit recent benchmarks and industrial deployments to show that technical metrics still dominate evaluations: a systematic review of 84 papers from 2023--2025 found that 83% report capability metrics while only 30% consider human-centred or economic axes [2]. Second, we formalise an Adaptive Multi-Dimensional Monitoring (AMDM) algorithm that normalises heterogeneous metrics, applies per-axis exponentially weighted moving-average thresholds and performs joint anomaly detection via the Mahalanobis distance [7]. Third, we conduct simulations and real-world experiments. AMDM cuts anomaly-detection latency from 12.3 s to 5.6 s on simulated goal drift and reduces false-positive rates from 4.5% to 0.9% compared with static thresholds. We present a comparison table and ROC/PR curves, and we reanalyse case studies to surface missing metrics. Code, data and a reproducibility checklist accompany this paper to facilitate replication. The code supporting this work is available at https://github.com/Manishms18/Adaptive-Multi-Dimensional-Monitoring.",
    "authors": [
      "Manish Shukla"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-08-28T15:52:49Z",
    "pdf_url": "https://arxiv.org/pdf/2509.00115v3"
  },
  {
    "arxiv_id": "2508.20816v1",
    "entry_id": "http://arxiv.org/abs/2508.20816v1",
    "title": "Multi-Agent Penetration Testing AI for the Web",
    "summary": "AI-powered development platforms are making software creation accessible to a broader audience, but this democratization has triggered a scalability crisis in security auditing. With studies showing that up to 40% of AI-generated code contains vulnerabilities, the pace of development now vastly outstrips the capacity for thorough security assessment.\n  We present MAPTA, a multi-agent system for autonomous web application security assessment that combines large language model orchestration with tool-grounded execution and end-to-end exploit validation. On the 104-challenge XBOW benchmark, MAPTA achieves 76.9% overall success with perfect performance on SSRF and misconfiguration vulnerabilities, 83% success on broken authorization, and strong results on injection attacks including server-side template injection (85%) and SQL injection (83%). Cross-site scripting (57%) and blind SQL injection (0%) remain challenging. Our comprehensive cost analysis across all challenges totals $21.38 with a median cost of $0.073 for successful attempts versus $0.357 for failures. Success correlates strongly with resource efficiency, enabling practical early-stopping thresholds at approximately 40 tool calls or $0.30 per challenge.\n  MAPTA's real-world findings are impactful given both the popularity of the respective scanned GitHub repositories (8K-70K stars) and MAPTA's low average operating cost of $3.67 per open-source assessment: MAPTA discovered critical vulnerabilities including RCEs, command injections, secret exposure, and arbitrary file write vulnerabilities. Findings are responsibly disclosed, 10 findings are under CVE review.",
    "authors": [
      "Isaac David",
      "Arthur Gervais"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-08-28T14:14:24Z",
    "pdf_url": "https://arxiv.org/pdf/2508.20816v1"
  },
  {
    "arxiv_id": "2508.20729v1",
    "entry_id": "http://arxiv.org/abs/2508.20729v1",
    "title": "Re4: Scientific Computing Agent with Rewriting, Resolution, Review and Revision",
    "summary": "Large language models (LLMs) serve as an active and promising field of generative artificial intelligence and have demonstrated abilities to perform complex tasks in multiple domains, including mathematical and scientific reasoning. In this work, we construct a novel agent framework for solving representative problems in scientific computing. The proposed agent, incorporating a \"rewriting-resolution-review-revision\" logical chain via three reasoning LLMs (functioning as the Consultant, Reviewer, and Programmer, respectively), is integrated in a collaborative and interactive manner. The Consultant module endows the agent with knowledge transfer capabilities to link problems to professional domain insights, thereby rewriting problem descriptions through text augmentation. The Programmer module is responsible for generating and executing well-structured code to deliver the problem resolution. The Reviewer module equips the agent with the capacity for self-debugging and self-refinement through interactive feedback with code runtime outputs. By leveraging the end-to-end review mechanism, the executable code provided by the Programmer attains the iterative revision. A comprehensive evaluation is conducted on the performance of the proposed agent framework in solving PDEs, ill-conditioned linear systems, and data-driven physical analysis problems. Compared to single-model, this collaborative framework significantly improves the bug-free code generation rate and reduces the occurrence of non-physical solutions, thereby establishing a highly reliable framework for autonomous code generation based on natural language descriptions. The review mechanism improved the average execution success (bug-free code and non-NaN solutions) rate of the latest reasoning models. In summary, our agent framework establishes automatic code generation and review as a promising scientific computing paradigm.",
    "authors": [
      "Ao Cheng",
      "Lei Zhang",
      "Guowei He"
    ],
    "categories": [
      "cs.AI",
      "physics.comp-ph"
    ],
    "published": "2025-08-28T12:50:48Z",
    "pdf_url": "https://arxiv.org/pdf/2508.20729v1"
  },
  {
    "arxiv_id": "2508.20578v1",
    "entry_id": "http://arxiv.org/abs/2508.20578v1",
    "title": "Human-AI Collaborative Bot Detection in MMORPGs",
    "summary": "In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-leveling bots exploit automated programs to level up characters at scale, undermining gameplay balance and fairness. Detecting such bots is challenging, not only because they mimic human behavior, but also because punitive actions require explainable justification to avoid legal and user experience issues. In this paper, we present a novel framework for detecting auto-leveling bots by leveraging contrastive representation learning and clustering techniques in a fully unsupervised manner to identify groups of characters with similar level-up patterns. To ensure reliable decisions, we incorporate a Large Language Model (LLM) as an auxiliary reviewer to validate the clustered groups, effectively mimicking a secondary human judgment. We also introduce a growth curve-based visualization to assist both the LLM and human moderators in assessing leveling behavior. This collaborative approach improves the efficiency of bot detection workflows while maintaining explainability, thereby supporting scalable and accountable bot regulation in MMORPGs.",
    "authors": [
      "Jaeman Son",
      "Hyunsoo Kim"
    ],
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-08-28T09:17:35Z",
    "pdf_url": "https://arxiv.org/pdf/2508.20578v1"
  },
  {
    "arxiv_id": "2508.21101v1",
    "entry_id": "http://arxiv.org/abs/2508.21101v1",
    "title": "Beyond Prediction: Reinforcement Learning as the Defining Leap in Healthcare AI",
    "summary": "Reinforcement learning (RL) marks a fundamental shift in how artificial intelligence is applied in healthcare. Instead of merely predicting outcomes, RL actively decides interventions with long term goals. Unlike traditional models that operate on fixed associations, RL systems learn through trial, feedback, and long-term reward optimization, introducing transformative possibilities and new risks. From an information fusion lens, healthcare RL typically integrates multi-source signals such as vitals, labs clinical notes, imaging and device telemetry using temporal and decision-level mechanisms. These systems can operate within centralized, federated, or edge architectures to meet real-time clinical constraints, and naturally span data, features and decision fusion levels. This survey explore RL's rise in healthcare as more than a set of tools, rather a shift toward agentive intelligence in clinical environments. We first structure the landscape of RL techniques including model-based and model-free methods, offline and batch-constrained approaches, and emerging strategies for reward specification and uncertainty calibration through the lens of healthcare constraints. We then comprehensively analyze RL applications spanning critical care, chronic disease, mental health, diagnostics, and robotic assistance, identifying their trends, gaps, and translational bottlenecks. In contrast to prior reviews, we critically analyze RL's ethical, deployment, and reward design challenges, and synthesize lessons for safe, human-aligned policy learning. This paper serves as both a a technical roadmap and a critical reflection of RL's emerging transformative role in healthcare AI not as prediction machinery, but as agentive clinical intelligence.",
    "authors": [
      "Dilruk Perera",
      "Gousia Habib",
      "Qianyi Xu",
      "Daniel J. Tan",
      "Kai He",
      "Erik Cambria",
      "Mengling Feng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-08-28T07:05:24Z",
    "pdf_url": "https://arxiv.org/pdf/2508.21101v1"
  },
  {
    "arxiv_id": "2509.04460v1",
    "entry_id": "http://arxiv.org/abs/2509.04460v1",
    "title": "CoCoNUTS: Concentrating on Content while Neglecting Uninformative Textual Styles for AI-Generated Peer Review Detection",
    "summary": "The growing integration of large language models (LLMs) into the peer review process presents potential risks to the fairness and reliability of scholarly evaluation. While LLMs offer valuable assistance for reviewers with language refinement, there is growing concern over their use to generate substantive review content. Existing general AI-generated text detectors are vulnerable to paraphrasing attacks and struggle to distinguish between surface language refinement and substantial content generation, suggesting that they primarily rely on stylistic cues. When applied to peer review, this limitation can result in unfairly suspecting reviews with permissible AI-assisted language enhancement, while failing to catch deceptively humanized AI-generated reviews. To address this, we propose a paradigm shift from style-based to content-based detection. Specifically, we introduce CoCoNUTS, a content-oriented benchmark built upon a fine-grained dataset of AI-generated peer reviews, covering six distinct modes of human-AI collaboration. Furthermore, we develop CoCoDet, an AI review detector via a multi-task learning framework, designed to achieve more accurate and robust detection of AI involvement in review content. Our work offers a practical foundation for evaluating the use of LLMs in peer review, and contributes to the development of more precise, equitable, and reliable detection methods for real-world scholarly applications. Our code and data will be publicly available at https://github.com/Y1hanChen/COCONUTS.",
    "authors": [
      "Yihan Chen",
      "Jiawei Chen",
      "Guozhao Mo",
      "Xuanang Chen",
      "Ben He",
      "Xianpei Han",
      "Le Sun"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-28T06:03:11Z",
    "pdf_url": "https://arxiv.org/pdf/2509.04460v1"
  },
  {
    "arxiv_id": "2508.20396v1",
    "entry_id": "http://arxiv.org/abs/2508.20396v1",
    "title": "BiListing: Modality Alignment for Listings",
    "summary": "Airbnb is a leader in offering travel accommodations. Airbnb has historically relied on structured data to understand, rank, and recommend listings to guests due to the limited capabilities and associated complexity arising from extracting meaningful information from text and images. With the rise of representation learning, leveraging rich information from text and photos has become easier. A popular approach has been to create embeddings for text documents and images to enable use cases of computing similarities between listings or using embeddings as features in an ML model.\n  However, an Airbnb listing has diverse unstructured data: multiple images, various unstructured text documents such as title, description, and reviews, making this approach challenging. Specifically, it is a non-trivial task to combine multiple embeddings of different pieces of information to reach a single representation.\n  This paper proposes BiListing, for Bimodal Listing, an approach to align text and photos of a listing by leveraging large-language models and pretrained language-image models. The BiListing approach has several favorable characteristics: capturing unstructured data into a single embedding vector per listing and modality, enabling zero-shot capability to search inventory efficiently in user-friendly semantics, overcoming the cold start problem, and enabling listing-to-listing search along a single modality, or both.\n  We conducted offline and online tests to leverage the BiListing embeddings in the Airbnb search ranking model, and successfully deployed it in production, achieved 0.425% of NDCB gain, and drove tens of millions in incremental revenue.",
    "authors": [
      "Guillaume Guy",
      "Mihajlo Grbovic",
      "Chun How Tan",
      "Han Zhao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-28T03:47:31Z",
    "pdf_url": "https://arxiv.org/pdf/2508.20396v1"
  },
  {
    "arxiv_id": "2508.20315v1",
    "entry_id": "http://arxiv.org/abs/2508.20315v1",
    "title": "Multi-Agent Reinforcement Learning in Intelligent Transportation Systems: A Comprehensive Survey",
    "summary": "The growing complexity of urban mobility and the demand for efficient, sustainable, and adaptive solutions have positioned Intelligent Transportation Systems (ITS) at the forefront of modern infrastructure innovation. At the core of ITS lies the challenge of autonomous decision-making across dynamic, large scale, and uncertain environments where multiple agents traffic signals, autonomous vehicles, or fleet units must coordinate effectively. Multi Agent Reinforcement Learning (MARL) offers a promising paradigm for addressing these challenges by enabling distributed agents to jointly learn optimal strategies that balance individual objectives with system wide efficiency. This paper presents a comprehensive survey of MARL applications in ITS. We introduce a structured taxonomy that categorizes MARL approaches according to coordination models and learning algorithms, spanning value based, policy based, actor critic, and communication enhanced frameworks. Applications are reviewed across key ITS domains, including traffic signal control, connected and autonomous vehicle coordination, logistics optimization, and mobility on demand systems. Furthermore, we highlight widely used simulation platforms such as SUMO, CARLA, and CityFlow that support MARL experimentation, along with emerging benchmarks. The survey also identifies core challenges, including scalability, non stationarity, credit assignment, communication constraints, and the sim to real transfer gap, which continue to hinder real world deployment.",
    "authors": [
      "RexCharles Donatus",
      "Kumater Ter",
      "Ore-Ofe Ajayi",
      "Daniel Udekwe"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-27T23:04:34Z",
    "pdf_url": "https://arxiv.org/pdf/2508.20315v1"
  },
  {
    "arxiv_id": "2508.20275v1",
    "entry_id": "http://arxiv.org/abs/2508.20275v1",
    "title": "A Systematic Review on the Generative AI Applications in Human Medical Genomics",
    "summary": "Although traditional statistical techniques and machine learning methods have contributed significantly to genetics and, in particular, inherited disease diagnosis, they often struggle with complex, high-dimensional data, a challenge now addressed by state-of-the-art deep learning models. Large language models (LLMs), based on transformer architectures, have excelled in tasks requiring contextual comprehension of unstructured medical data. This systematic review examines the role of LLMs in the genetic research and diagnostics of both rare and common diseases. Automated keyword-based search in PubMed, bioRxiv, medRxiv, and arXiv was conducted, targeting studies on LLM applications in diagnostics and education within genetics and removing irrelevant or outdated models. A total of 172 studies were analyzed, highlighting applications in genomic variant identification, annotation, and interpretation, as well as medical imaging advancements through vision transformers. Key findings indicate that while transformer-based models significantly advance disease and risk stratification, variant interpretation, medical imaging analysis, and report generation, major challenges persist in integrating multimodal data (genomic sequences, imaging, and clinical records) into unified and clinically robust pipelines, facing limitations in generalizability and practical implementation in clinical settings. This review provides a comprehensive classification and assessment of the current capabilities and limitations of LLMs in transforming hereditary disease diagnostics and supporting genetic education, serving as a guide to navigate this rapidly evolving field.",
    "authors": [
      "Anton Changalidis",
      "Yury Barbitoff",
      "Yulia Nasykhova",
      "Andrey Glotov"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-bio.QM"
    ],
    "published": "2025-08-27T21:17:12Z",
    "pdf_url": "https://arxiv.org/pdf/2508.20275v1"
  },
  {
    "arxiv_id": "2508.19883v1",
    "entry_id": "http://arxiv.org/abs/2508.19883v1",
    "title": "AI-Powered Detection of Inappropriate Language in Medical School Curricula",
    "summary": "The use of inappropriate language -- such as outdated, exclusionary, or non-patient-centered terms -- medical instructional materials can significantly influence clinical training, patient interactions, and health outcomes. Despite their reputability, many materials developed over past decades contain examples now considered inappropriate by current medical standards. Given the volume of curricular content, manually identifying instances of inappropriate use of language (IUL) and its subcategories for systematic review is prohibitively costly and impractical. To address this challenge, we conduct a first-in-class evaluation of small language models (SLMs) fine-tuned on labeled data and pre-trained LLMs with in-context learning on a dataset containing approximately 500 documents and over 12,000 pages. For SLMs, we consider: (1) a general IUL classifier, (2) subcategory-specific binary classifiers, (3) a multilabel classifier, and (4) a two-stage hierarchical pipeline for general IUL detection followed by multilabel classification. For LLMs, we consider variations of prompts that include subcategory definitions and/or shots. We found that both LLama-3 8B and 70B, even with carefully curated shots, are largely outperformed by SLMs. While the multilabel classifier performs best on annotated data, supplementing training with unflagged excerpts as negative examples boosts the specific classifiers' AUC by up to 25%, making them most effective models for mitigating harmful language in medical curricula.",
    "authors": [
      "Chiman Salavati",
      "Shannon Song",
      "Scott A. Hale",
      "Roberto E. Montenegro",
      "Shiri Dori-Hacohen",
      "Fabricio Murai"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-08-27T13:40:45Z",
    "pdf_url": "https://arxiv.org/pdf/2508.19883v1"
  },
  {
    "arxiv_id": "2508.19667v1",
    "entry_id": "http://arxiv.org/abs/2508.19667v1",
    "title": "Survey of Specialized Large Language Model",
    "summary": "The rapid evolution of specialized large language models (LLMs) has transitioned from simple domain adaptation to sophisticated native architectures, marking a paradigm shift in AI development. This survey systematically examines this progression across healthcare, finance, legal, and technical domains. Besides the wide use of specialized LLMs, technical breakthrough such as the emergence of domain-native designs beyond fine-tuning, growing emphasis on parameter efficiency through sparse computation and quantization, increasing integration of multimodal capabilities and so on are applied to recent LLM agent. Our analysis reveals how these innovations address fundamental limitations of general-purpose LLMs in professional applications, with specialized models consistently performance gains on domain-specific benchmarks. The survey further highlights the implications for E-Commerce field to fill gaps in the field.",
    "authors": [
      "Chenghan Yang",
      "Ruiyu Zhao",
      "Yang Liu",
      "Ling Jiang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-27T08:27:23Z",
    "pdf_url": "https://arxiv.org/pdf/2508.19667v1"
  },
  {
    "arxiv_id": "2508.20139v1",
    "entry_id": "http://arxiv.org/abs/2508.20139v1",
    "title": "Is the medical image segmentation problem solved? A survey of current developments and future directions",
    "summary": "Medical image segmentation has advanced rapidly over the past two decades, largely driven by deep learning, which has enabled accurate and efficient delineation of cells, tissues, organs, and pathologies across diverse imaging modalities. This progress raises a fundamental question: to what extent have current models overcome persistent challenges, and what gaps remain? In this work, we provide an in-depth review of medical image segmentation, tracing its progress and key developments over the past decade. We examine core principles, including multiscale analysis, attention mechanisms, and the integration of prior knowledge, across the encoder, bottleneck, skip connections, and decoder components of segmentation networks. Our discussion is organized around seven key dimensions: (1) the shift from supervised to semi-/unsupervised learning, (2) the transition from organ segmentation to lesion-focused tasks, (3) advances in multi-modality integration and domain adaptation, (4) the role of foundation models and transfer learning, (5) the move from deterministic to probabilistic segmentation, (6) the progression from 2D to 3D and 4D segmentation, and (7) the trend from model invocation to segmentation agents. Together, these perspectives provide a holistic overview of the trajectory of deep learning-based medical image segmentation and aim to inspire future innovation. To support ongoing research, we maintain a continually updated repository of relevant literature and open-source resources at https://github.com/apple1986/medicalSegReview",
    "authors": [
      "Guoping Xu",
      "Jayaram K. Udupa",
      "Jax Luo",
      "Songlin Zhao",
      "Yajun Yu",
      "Scott B. Raymond",
      "Hao Peng",
      "Lipeng Ning",
      "Yogesh Rathi",
      "Wei Liu",
      "You Zhang"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-08-27T01:12:45Z",
    "pdf_url": "https://arxiv.org/pdf/2508.20139v1"
  },
  {
    "arxiv_id": "2508.19500v1",
    "entry_id": "http://arxiv.org/abs/2508.19500v1",
    "title": "Servant, Stalker, Predator: How An Honest, Helpful, And Harmless (3H) Agent Unlocks Adversarial Skills",
    "summary": "This paper identifies and analyzes a novel vulnerability class in Model Context Protocol (MCP) based agent systems. The attack chain describes and demonstrates how benign, individually authorized tasks can be orchestrated to produce harmful emergent behaviors. Through systematic analysis using the MITRE ATLAS framework, we demonstrate how 95 agents tested with access to multiple services-including browser automation, financial analysis, location tracking, and code deployment-can chain legitimate operations into sophisticated attack sequences that extend beyond the security boundaries of any individual service. These red team exercises survey whether current MCP architectures lack cross-domain security measures necessary to detect or prevent a large category of compositional attacks. We present empirical evidence of specific attack chains that achieve targeted harm through service orchestration, including data exfiltration, financial manipulation, and infrastructure compromise. These findings reveal that the fundamental security assumption of service isolation fails when agents can coordinate actions across multiple domains, creating an exponential attack surface that grows with each additional capability. This research provides a barebones experimental framework that evaluate not whether agents can complete MCP benchmark tasks, but what happens when they complete them too well and optimize across multiple services in ways that violate human expectations and safety constraints. We propose three concrete experimental directions using the existing MCP benchmark suite.",
    "authors": [
      "David Noever"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-08-27T01:11:59Z",
    "pdf_url": "https://arxiv.org/pdf/2508.19500v1"
  },
  {
    "arxiv_id": "2508.19461v1",
    "entry_id": "http://arxiv.org/abs/2508.19461v1",
    "title": "Reliable Weak-to-Strong Monitoring of LLM Agents",
    "summary": "We stress test monitoring systems for detecting covert misbehavior in autonomous LLM agents (e.g., secretly sharing private information). To this end, we systematize a monitor red teaming (MRT) workflow that incorporates: (1) varying levels of agent and monitor situational awareness; (2) distinct adversarial strategies to evade the monitor, such as prompt injection; and (3) two datasets and environments -- SHADE-Arena for tool-calling agents and our new CUA-SHADE-Arena, which extends TheAgentCompany, for computer-use agents. We run MRT on existing LLM monitor scaffoldings, which orchestrate LLMs and parse agent trajectories, alongside a new hybrid hierarchical-sequential scaffolding proposed in this work. Our empirical results yield three key findings. First, agent awareness dominates monitor awareness: an agent's knowledge that it is being monitored substantially degrades the monitor's reliability. On the contrary, providing the monitor with more information about the agent is less helpful than expected. Second, monitor scaffolding matters more than monitor awareness: the hybrid scaffolding consistently outperforms baseline monitor scaffolding, and can enable weaker models to reliably monitor stronger agents -- a weak-to-strong scaling effect. Third, in a human-in-the-loop setting where humans discuss with the LLM monitor to get an updated judgment for the agent's behavior, targeted human oversight is most effective; escalating only pre-flagged cases to human reviewers improved the TPR by approximately 15% at FPR = 0.01. Our work establishes a standard workflow for MRT, highlighting the lack of adversarial robustness for LLMs and humans when monitoring and detecting agent misbehavior. We release code, data, and logs to spur further research.",
    "authors": [
      "Neil Kale",
      "Chen Bo Calvin Zhang",
      "Kevin Zhu",
      "Ankit Aich",
      "Paula Rodriguez",
      "Scale Red Team",
      "Christina Q. Knight",
      "Zifan Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-08-26T22:29:31Z",
    "pdf_url": "https://arxiv.org/pdf/2508.19461v1"
  },
  {
    "arxiv_id": "2508.19097v1",
    "entry_id": "http://arxiv.org/abs/2508.19097v1",
    "title": "Reasoning LLMs in the Medical Domain: A Literature Survey",
    "summary": "The emergence of advanced reasoning capabilities in Large Language Models (LLMs) marks a transformative development in healthcare applications. Beyond merely expanding functional capabilities, these reasoning mechanisms enhance decision transparency and explainability-critical requirements in medical contexts. This survey examines the transformation of medical LLMs from basic information retrieval tools to sophisticated clinical reasoning systems capable of supporting complex healthcare decisions. We provide a thorough analysis of the enabling technological foundations, with a particular focus on specialized prompting techniques like Chain-of-Thought and recent breakthroughs in Reinforcement Learning exemplified by DeepSeek-R1. Our investigation evaluates purpose-built medical frameworks while also examining emerging paradigms such as multi-agent collaborative systems and innovative prompting architectures. The survey critically assesses current evaluation methodologies for medical validation and addresses persistent challenges in field interpretation limitations, bias mitigation strategies, patient safety frameworks, and integration of multimodal clinical data. Through this survey, we seek to establish a roadmap for developing reliable LLMs that can serve as effective partners in clinical practice and medical research.",
    "authors": [
      "Armin Berger",
      "Sarthak Khanna",
      "David Berghaus",
      "Rafet Sifa"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-26T14:59:19Z",
    "pdf_url": "https://arxiv.org/pdf/2508.19097v1"
  },
  {
    "arxiv_id": "2508.18803v1",
    "entry_id": "http://arxiv.org/abs/2508.18803v1",
    "title": "A Survey on Cloud-Edge-Terminal Collaborative Intelligence in AIoT Networks",
    "summary": "The proliferation of Internet of things (IoT) devices in smart cities, transportation, healthcare, and industrial applications, coupled with the explosive growth of AI-driven services, has increased demands for efficient distributed computing architectures and networks, driving cloud-edge-terminal collaborative intelligence (CETCI) as a fundamental paradigm within the artificial intelligence of things (AIoT) community. With advancements in deep learning, large language models (LLMs), and edge computing, CETCI has made significant progress with emerging AIoT applications, moving beyond isolated layer optimization to deployable collaborative intelligence systems for AIoT (CISAIOT), a practical research focus in AI, distributed computing, and communications. This survey describes foundational architectures, enabling technologies, and scenarios of CETCI paradigms, offering a tutorial-style review for CISAIOT beginners. We systematically analyze architectural components spanning cloud, edge, and terminal layers, examining core technologies including network virtualization, container orchestration, and software-defined networking, while presenting categorizations of collaboration paradigms that cover task offloading, resource allocation, and optimization across heterogeneous infrastructures. Furthermore, we explain intelligent collaboration learning frameworks by reviewing advances in federated learning, distributed deep learning, edge-cloud model evolution, and reinforcement learning-based methods. Finally, we discuss challenges (e.g., scalability, heterogeneity, interoperability) and future trends (e.g., 6G+, agents, quantum computing, digital twin), highlighting how integration of distributed computing and communication can address open issues and guide development of robust, efficient, and secure collaborative AIoT systems.",
    "authors": [
      "Jiaqi Wu",
      "Jing Liu",
      "Yang Liu",
      "Lixu Wang",
      "Zehua Wang",
      "Wei Chen",
      "Zijian Tian",
      "Richard Yu",
      "Victor C. M. Leung"
    ],
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "published": "2025-08-26T08:38:01Z",
    "pdf_url": "https://arxiv.org/pdf/2508.18803v1"
  },
  {
    "arxiv_id": "2508.18665v3",
    "entry_id": "http://arxiv.org/abs/2508.18665v3",
    "title": "Membership Inference Attacks on LLM-based Recommender Systems",
    "summary": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly adapt recommendation systems to different domains. It utilizes in-context learning (ICL), i.e., the prompts, to customize the recommendation functions, which include sensitive historical user-specific item interactions, e.g., implicit feedback like clicked items or explicit product reviews. Such private information may be exposed to novel privacy attack. However, no study has been done on this important issue. We design four membership inference attacks (MIAs), aiming to reveal whether victims' historical interactions have been used by system prompts. They are \\emph{direct inquiry, hallucination, similarity, and poisoning attacks}, each of which utilizes the unique features of LLMs or RecSys. We have carefully evaluated them on three LLMs that have been used to develop ICL-LLM RecSys and two well-known RecSys benchmark datasets. The results confirm that the MIA threat on LLM RecSys is realistic: direct inquiry and poisoning attacks showing significantly high attack advantages. We have also analyzed the factors affecting these attacks, such as the number of shots in system prompts and the position of the victim in the shots.",
    "authors": [
      "Jiajie He",
      "Yuechun Gu",
      "Min-Chun Chen",
      "Keke Chen"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-08-26T04:14:39Z",
    "pdf_url": "https://arxiv.org/pdf/2508.18665v3"
  },
  {
    "arxiv_id": "2508.18646v1",
    "entry_id": "http://arxiv.org/abs/2508.18646v1",
    "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap",
    "summary": "For Large Language Models (LLMs), a disconnect persists between benchmark performance and real-world utility. Current evaluation frameworks remain fragmented, prioritizing technical metrics while neglecting holistic assessment for deployment. This survey introduces an anthropomorphic evaluation paradigm through the lens of human intelligence, proposing a novel three-dimensional taxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational capacity, Emotional Quotient (EQ)-Alignment Ability for value-based interactions, and Professional Quotient (PQ)-Professional Expertise for specialized proficiency. For practical value, we pioneer a Value-oriented Evaluation (VQ) framework assessing economic viability, social impact, ethical alignment, and environmental sustainability. Our modular architecture integrates six components with an implementation roadmap. Through analysis of 200+ benchmarks, we identify key challenges including dynamic assessment needs and interpretability gaps. It provides actionable guidance for developing LLMs that are technically proficient, contextually relevant, and ethically sound. We maintain a curated repository of open-source evaluation resources at: https://github.com/onejune2018/Awesome-LLM-Eval.",
    "authors": [
      "Jun Wang",
      "Ninglun Gu",
      "Kailai Zhang",
      "Zijiao Zhang",
      "Yelun Bao",
      "Jin Yang",
      "Xu Yin",
      "Liwei Liu",
      "Yihuan Liu",
      "Pengyong Li",
      "Gary G. Yen",
      "Junchi Yan"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-26T03:43:05Z",
    "pdf_url": "https://arxiv.org/pdf/2508.18646v1"
  },
  {
    "arxiv_id": "2508.19294v2",
    "entry_id": "http://arxiv.org/abs/2508.19294v2",
    "title": "Object Detection with Multimodal Large Vision-Language Models: An In-depth Review",
    "summary": "The fusion of language and vision in large vision-language models (LVLMs) has revolutionized deep learning-based object detection by enhancing adaptability, contextual reasoning, and generalization beyond traditional architectures. This in-depth review presents a structured exploration of the state-of-the-art in LVLMs, systematically organized through a three-step research review process. First, we discuss the functioning of vision language models (VLMs) for object detection, describing how these models harness natural language processing (NLP) and computer vision (CV) techniques to revolutionize object detection and localization. We then explain the architectural innovations, training paradigms, and output flexibility of recent LVLMs for object detection, highlighting how they achieve advanced contextual understanding for object detection. The review thoroughly examines the approaches used in integration of visual and textual information, demonstrating the progress made in object detection using VLMs that facilitate more sophisticated object detection and localization strategies. This review presents comprehensive visualizations demonstrating LVLMs' effectiveness in diverse scenarios including localization and segmentation, and then compares their real-time performance, adaptability, and complexity to traditional deep learning systems. Based on the review, its is expected that LVLMs will soon meet or surpass the performance of conventional methods in object detection. The review also identifies a few major limitations of the current LVLM modes, proposes solutions to address those challenges, and presents a clear roadmap for the future advancement in this field. We conclude, based on this study, that the recent advancement in LVLMs have made and will continue to make a transformative impact on object detection and robotic applications in the future.",
    "authors": [
      "Ranjan Sapkota",
      "Manoj Karkee"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-25T17:21:00Z",
    "pdf_url": "https://arxiv.org/pdf/2508.19294v2"
  },
  {
    "arxiv_id": "2508.18091v1",
    "entry_id": "http://arxiv.org/abs/2508.18091v1",
    "title": "Teaching LLMs to Think Mathematically: A Critical Study of Decision-Making via Optimization",
    "summary": "This paper investigates the capabilities of large language models (LLMs) in formulating and solving decision-making problems using mathematical programming. We first conduct a systematic review and meta-analysis of recent literature to assess how well LLMs understand, structure, and solve optimization problems across domains. The analysis is guided by critical review questions focusing on learning approaches, dataset designs, evaluation metrics, and prompting strategies. Our systematic evidence is complemented by targeted experiments designed to evaluate the performance of state-of-the-art LLMs in automatically generating optimization models for problems in computer networks. Using a newly constructed dataset, we apply three prompting strategies: Act-as-expert, chain-of-thought, and self-consistency, and evaluate the obtained outputs based on optimality gap, token-level F1 score, and compilation accuracy. Results show promising progress in LLMs' ability to parse natural language and represent symbolic formulations, but also reveal key limitations in accuracy, scalability, and interpretability. These empirical gaps motivate several future research directions, including structured datasets, domain-specific fine-tuning, hybrid neuro-symbolic approaches, modular multi-agent architectures, and dynamic retrieval via chain-of-RAGs. This paper contributes a structured roadmap for advancing LLM capabilities in mathematical programming.",
    "authors": [
      "Mohammad J. Abdel-Rahman",
      "Yasmeen Alslman",
      "Dania Refai",
      "Amro Saleh",
      "Malik A. Abu Loha",
      "Mohammad Yahya Hamed"
    ],
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "published": "2025-08-25T14:52:56Z",
    "pdf_url": "https://arxiv.org/pdf/2508.18091v1"
  },
  {
    "arxiv_id": "2508.18048v1",
    "entry_id": "http://arxiv.org/abs/2508.18048v1",
    "title": "HyST: LLM-Powered Hybrid Retrieval over Semi-Structured Tabular Data",
    "summary": "User queries in real-world recommendation systems often combine structured constraints (e.g., category, attributes) with unstructured preferences (e.g., product descriptions or reviews). We introduce HyST (Hybrid retrieval over Semi-structured Tabular data), a hybrid retrieval framework that combines LLM-powered structured filtering with semantic embedding search to support complex information needs over semi-structured tabular data. HyST extracts attribute-level constraints from natural language using large language models (LLMs) and applies them as metadata filters, while processing the remaining unstructured query components via embedding-based retrieval. Experiments on a semi-structured benchmark show that HyST consistently outperforms tradtional baselines, highlighting the importance of structured filtering in improving retrieval precision, offering a scalable and accurate solution for real-world user queries.",
    "authors": [
      "Jiyoon Myung",
      "Jihyeon Park",
      "Joohyung Han"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-08-25T14:06:27Z",
    "pdf_url": "https://arxiv.org/pdf/2508.18048v1"
  },
  {
    "arxiv_id": "2508.17926v1",
    "entry_id": "http://arxiv.org/abs/2508.17926v1",
    "title": "AMELIA: A Family of Multi-task End-to-end Language Models for Argumentation",
    "summary": "Argument mining is a subfield of argumentation that aims to automatically extract argumentative structures and their relations from natural language texts. This paper investigates how a single large language model can be leveraged to perform one or several argument mining tasks. Our contributions are two-fold. First, we construct a multi-task dataset by surveying and converting 19 well-known argument mining datasets from the literature into a unified format. Second, we explore various training strategies using Meta AI's Llama-3.1-8B-Instruct model: (1) fine-tuning on individual tasks, (2) fine-tuning jointly on multiple tasks, and (3) merging models fine-tuned separately on individual tasks. Our experiments show that task-specific fine-tuning significantly improves individual performance across all tasks. Moreover, multi-task fine-tuning maintains strong performance without degradation, suggesting effective transfer learning across related tasks. Finally, we demonstrate that model merging offers a viable compromise: it yields competitive performance while mitigating the computational costs associated with full multi-task fine-tuning.",
    "authors": [
      "Henri Savigny",
      "Bruno Yun"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-25T11:51:39Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17926v1"
  },
  {
    "arxiv_id": "2508.17692v1",
    "entry_id": "http://arxiv.org/abs/2508.17692v1",
    "title": "LLM-based Agentic Reasoning Frameworks: A Survey from Methods to Scenarios",
    "summary": "Recent advances in the intrinsic reasoning capabilities of large language models (LLMs) have given rise to LLM-based agent systems that exhibit near-human performance on a variety of automated tasks. However, although these systems share similarities in terms of their use of LLMs, different reasoning frameworks of the agent system steer and organize the reasoning process in different ways. In this survey, we propose a systematic taxonomy that decomposes agentic reasoning frameworks and analyze how these frameworks dominate framework-level reasoning by comparing their applications across different scenarios. Specifically, we propose an unified formal language to further classify agentic reasoning systems into single-agent methods, tool-based methods, and multi-agent methods. After that, we provide a comprehensive review of their key application scenarios in scientific discovery, healthcare, software engineering, social simulation, and economics. We also analyze the characteristic features of each framework and summarize different evaluation strategies. Our survey aims to provide the research community with a panoramic view to facilitate understanding of the strengths, suitable scenarios, and evaluation practices of different agentic reasoning frameworks.",
    "authors": [
      "Bingxi Zhao",
      "Lin Geng Foo",
      "Ping Hu",
      "Christian Theobalt",
      "Hossein Rahmani",
      "Jun Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-25T06:01:16Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17692v1"
  },
  {
    "arxiv_id": "2508.17580v1",
    "entry_id": "http://arxiv.org/abs/2508.17580v1",
    "title": "UQ: Assessing Language Models on Unsolved Questions",
    "summary": "Benchmarks shape progress in AI research. A useful benchmark should be both difficult and realistic: questions should challenge frontier models while also reflecting real-world usage. Yet, current paradigms face a difficulty-realism tension: exam-style benchmarks are often made artificially difficult with limited real-world value, while benchmarks based on real user interaction often skew toward easy, high-frequency problems. In this work, we explore a radically different paradigm: assessing models on unsolved questions. Rather than a static benchmark scored once, we curate unsolved questions and evaluate models asynchronously over time with validator-assisted screening and community verification. We introduce UQ, a testbed of 500 challenging, diverse questions sourced from Stack Exchange, spanning topics from CS theory and math to sci-fi and history, probing capabilities including reasoning, factuality, and browsing. UQ is difficult and realistic by construction: unsolved questions are often hard and naturally arise when humans seek answers, thus solving them yields direct real-world value. Our contributions are threefold: (1) UQ-Dataset and its collection pipeline combining rule-based filters, LLM judges, and human review to ensure question quality (e.g., well-defined and difficult); (2) UQ-Validators, compound validation strategies that leverage the generator-validator gap to provide evaluation signals and pre-screen candidate solutions for human review; and (3) UQ-Platform, an open platform where experts collectively verify questions and solutions. The top model passes UQ-validation on only 15% of questions, and preliminary human verification has already identified correct answers among those that passed. UQ charts a path for evaluating frontier models on real-world, open-ended challenges, where success pushes the frontier of human knowledge. We release UQ at https://uq.stanford.edu.",
    "authors": [
      "Fan Nie",
      "Ken Ziyu Liu",
      "Zihao Wang",
      "Rui Sun",
      "Wei Liu",
      "Weijia Shi",
      "Huaxiu Yao",
      "Linjun Zhang",
      "Andrew Y. Ng",
      "James Zou",
      "Sanmi Koyejo",
      "Yejin Choi",
      "Percy Liang",
      "Niklas Muennighoff"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-25T01:07:59Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17580v1"
  },
  {
    "arxiv_id": "2508.17576v2",
    "entry_id": "http://arxiv.org/abs/2508.17576v2",
    "title": "CausalSent: Interpretable Sentiment Classification with RieszNet",
    "summary": "Despite the overwhelming performance improvements offered by recent natural language processing (NLP) models, the decisions made by these models are largely a black box. Towards closing this gap, the field of causal NLP combines causal inference literature with modern NLP models to elucidate causal effects of text features. We replicate and extend Bansal et al's work on regularizing text classifiers to adhere to estimated effects, focusing instead on model interpretability. Specifically, we focus on developing a two-headed RieszNet-based neural network architecture which achieves better treatment effect estimation accuracy. Our framework, CausalSent, accurately predicts treatment effects in semi-synthetic IMDB movie reviews, reducing MAE of effect estimates by 2-3x compared to Bansal et al's MAE on synthetic Civil Comments data. With an ensemble of validated models, we perform an observational case study on the causal effect of the word \"love\" in IMDB movie reviews, finding that the presence of the word \"love\" causes a +2.9% increase in the probability of a positive sentiment.",
    "authors": [
      "Daniel Frees",
      "Martin Pollack"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-08-25T00:56:51Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17576v2"
  },
  {
    "arxiv_id": "2508.17527v1",
    "entry_id": "http://arxiv.org/abs/2508.17527v1",
    "title": "Evaluating Retrieval-Augmented Generation Strategies for Large Language Models in Travel Mode Choice Prediction",
    "summary": "Accurately predicting travel mode choice is essential for effective transportation planning, yet traditional statistical and machine learning models are constrained by rigid assumptions, limited contextual reasoning, and reduced generalizability. This study explores the potential of Large Language Models (LLMs) as a more flexible and context-aware approach to travel mode choice prediction, enhanced by Retrieval-Augmented Generation (RAG) to ground predictions in empirical data. We develop a modular framework for integrating RAG into LLM-based travel mode choice prediction and evaluate four retrieval strategies: basic RAG, RAG with balanced retrieval, RAG with a cross-encoder for re-ranking, and RAG with balanced retrieval and cross-encoder for re-ranking. These strategies are tested across three LLM architectures (OpenAI GPT-4o, o4-mini, and o3) to examine the interaction between model reasoning capabilities and retrieval methods. Using the 2023 Puget Sound Regional Household Travel Survey data, we conduct a series of experiments to evaluate model performance. The results demonstrate that RAG substantially enhances predictive accuracy across a range of models. Notably, the GPT-4o model combined with balanced retrieval and cross-encoder re-ranking achieves the highest accuracy of 80.8%, exceeding that of conventional statistical and machine learning baselines. Furthermore, LLM-based models exhibit superior generalization abilities relative to these baselines. Findings highlight the critical interplay between LLM reasoning capabilities and retrieval strategies, demonstrating the importance of aligning retrieval strategies with model capabilities to maximize the potential of LLM-based travel behavior modeling.",
    "authors": [
      "Yiming Xu",
      "Junfeng Jiao"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-08-24T21:20:55Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17527v1"
  },
  {
    "arxiv_id": "2508.17361v1",
    "entry_id": "http://arxiv.org/abs/2508.17361v1",
    "title": "Trust Me, I Know This Function: Hijacking LLM Static Analysis using Bias",
    "summary": "Large Language Models (LLMs) are increasingly trusted to perform automated code review and static analysis at scale, supporting tasks such as vulnerability detection, summarization, and refactoring. In this paper, we identify and exploit a critical vulnerability in LLM-based code analysis: an abstraction bias that causes models to overgeneralize familiar programming patterns and overlook small, meaningful bugs. Adversaries can exploit this blind spot to hijack the control flow of the LLM's interpretation with minimal edits and without affecting actual runtime behavior. We refer to this attack as a Familiar Pattern Attack (FPA).\n  We develop a fully automated, black-box algorithm that discovers and injects FPAs into target code. Our evaluation shows that FPAs are not only effective, but also transferable across models (GPT-4o, Claude 3.5, Gemini 2.0) and universal across programming languages (Python, C, Rust, Go). Moreover, FPAs remain effective even when models are explicitly warned about the attack via robust system prompts. Finally, we explore positive, defensive uses of FPAs and discuss their broader implications for the reliability and safety of code-oriented LLMs.",
    "authors": [
      "Shir Bernstein",
      "David Beste",
      "Daniel Ayzenshteyn",
      "Lea Schonherr",
      "Yisroel Mirsky"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2025-08-24T13:42:48Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17361v1"
  },
  {
    "arxiv_id": "2508.17298v2",
    "entry_id": "http://arxiv.org/abs/2508.17298v2",
    "title": "Explain Before You Answer: A Survey on Compositional Visual Reasoning",
    "summary": "Compositional visual reasoning has emerged as a key research frontier in multimodal AI, aiming to endow machines with the human-like ability to decompose visual scenes, ground intermediate concepts, and perform multi-step logical inference. While early surveys focus on monolithic vision-language models or general multimodal reasoning, a dedicated synthesis of the rapidly expanding compositional visual reasoning literature is still missing. We fill this gap with a comprehensive survey spanning 2023 to 2025 that systematically reviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We first formalize core definitions and describe why compositional approaches offer advantages in cognitive alignment, semantic fidelity, robustness, interpretability, and data efficiency. Next, we trace a five-stage paradigm shift: from prompt-enhanced language-centric pipelines, through tool-enhanced LLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and unified agentic VLMs, highlighting their architectural designs, strengths, and limitations. We then catalog 60+ benchmarks and corresponding metrics that probe compositional visual reasoning along dimensions such as grounding accuracy, chain-of-thought faithfulness, and high-resolution perception. Drawing on these analyses, we distill key insights, identify open challenges (e.g., limitations of LLM-based reasoning, hallucination, a bias toward deductive reasoning, scalable supervision, tool integration, and benchmark limitations), and outline future directions, including world-model integration, human-AI collaborative reasoning, and richer evaluation protocols. By offering a unified taxonomy, historical roadmap, and critical outlook, this survey aims to serve as a foundational reference and inspire the next generation of compositional visual reasoning research.",
    "authors": [
      "Fucai Ke",
      "Joy Hsu",
      "Zhixi Cai",
      "Zixian Ma",
      "Xin Zheng",
      "Xindi Wu",
      "Sukai Huang",
      "Weiqing Wang",
      "Pari Delir Haghighi",
      "Gholamreza Haffari",
      "Ranjay Krishna",
      "Jiajun Wu",
      "Hamid Rezatofighi"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-08-24T11:01:51Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17298v2"
  },
  {
    "arxiv_id": "2508.17198v1",
    "entry_id": "http://arxiv.org/abs/2508.17198v1",
    "title": "From reactive to cognitive: brain-inspired spatial intelligence for embodied agents",
    "summary": "Spatial cognition enables adaptive goal-directed behavior by constructing internal models of space. Robust biological systems consolidate spatial knowledge into three interconnected forms: \\textit{landmarks} for salient cues, \\textit{route knowledge} for movement trajectories, and \\textit{survey knowledge} for map-like representations. While recent advances in multi-modal large language models (MLLMs) have enabled visual-language reasoning in embodied agents, these efforts lack structured spatial memory and instead operate reactively, limiting their generalization and adaptability in complex real-world environments. Here we present Brain-inspired Spatial Cognition for Navigation (BSC-Nav), a unified framework for constructing and leveraging structured spatial memory in embodied agents. BSC-Nav builds allocentric cognitive maps from egocentric trajectories and contextual cues, and dynamically retrieves spatial knowledge aligned with semantic goals. Integrated with powerful MLLMs, BSC-Nav achieves state-of-the-art efficacy and efficiency across diverse navigation tasks, demonstrates strong zero-shot generalization, and supports versatile embodied behaviors in the real physical world, offering a scalable and biologically grounded path toward general-purpose spatial intelligence.",
    "authors": [
      "Shouwei Ruan",
      "Liyuan Wang",
      "Caixin Kang",
      "Qihui Zhu",
      "Songming Liu",
      "Xingxing Wei",
      "Hang Su"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-24T03:20:48Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17198v1"
  },
  {
    "arxiv_id": "2508.17117v2",
    "entry_id": "http://arxiv.org/abs/2508.17117v2",
    "title": "PlantVillageVQA: A Visual Question Answering Dataset for Benchmarking Vision-Language Models in Plant Science",
    "summary": "PlantVillageVQA is a large-scale visual question answering (VQA) dataset derived from the widely used PlantVillage image corpus. It was designed to advance the development and evaluation of vision-language models for agricultural decision-making and analysis. The PlantVillageVQA dataset comprises 193,609 high-quality question-answer (QA) pairs grounded over 55,448 images spanning 14 crop species and 38 disease conditions. Questions are organised into 3 levels of cognitive complexity and 9 distinct categories. Each question category was phrased manually following expert guidance and generated via an automated two-stage pipeline: (1) template-based QA synthesis from image metadata and (2) multi-stage linguistic re-engineering. The dataset was iteratively reviewed by domain experts for scientific accuracy and relevancy. The final dataset was evaluated using three state-of-the-art models for quality assessment. Our objective remains to provide a publicly available, standardised and expert-verified database to enhance diagnostic accuracy for plant disease identifications and advance scientific research in the agricultural domain. Our dataset will be open-sourced at https://huggingface.co/datasets/SyedNazmusSakib/PlantVillageVQA.",
    "authors": [
      "Syed Nazmus Sakib",
      "Nafiul Haque",
      "Mohammad Zabed Hossain",
      "Shifat E. Arman"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-23T19:04:57Z",
    "pdf_url": "https://arxiv.org/pdf/2508.17117v2"
  },
  {
    "arxiv_id": "2509.00038v1",
    "entry_id": "http://arxiv.org/abs/2509.00038v1",
    "title": "Compiling Prompts, Not Crafting Them: A Reproducible Workflow for AI-Assisted Evidence Synthesis",
    "summary": "Large language models (LLMs) offer significant potential to accelerate systematic literature reviews (SLRs), yet current approaches often rely on brittle, manually crafted prompts that compromise reliability and reproducibility. This fragility undermines scientific confidence in LLM-assisted evidence synthesis. In response, this work adapts recent advances in declarative prompt optimisation, developed for general-purpose LLM applications, and demonstrates their applicability to the domain of SLR automation. This research proposes a structured, domain-specific framework that embeds task declarations, test suites, and automated prompt tuning into a reproducible SLR workflow. These emerging methods are translated into a concrete blueprint with working code examples, enabling researchers to construct verifiable LLM pipelines that align with established principles of transparency and rigour in evidence synthesis. This is a novel application of such approaches to SLR pipelines.",
    "authors": [
      "Teo Susnjak"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-22T21:37:49Z",
    "pdf_url": "https://arxiv.org/pdf/2509.00038v1"
  },
  {
    "arxiv_id": "2508.16527v1",
    "entry_id": "http://arxiv.org/abs/2508.16527v1",
    "title": "Towards Open World Detection: A Survey",
    "summary": "For decades, Computer Vision has aimed at enabling machines to perceive the external world. Initial limitations led to the development of highly specialized niches. As success in each task accrued and research progressed, increasingly complex perception tasks emerged. This survey charts the convergence of these tasks and, in doing so, introduces Open World Detection (OWD), an umbrella term we propose to unify class-agnostic and generally applicable detection models in the vision domain. We start from the history of foundational vision subdomains and cover key concepts, methodologies and datasets making up today's state-of-the-art landscape. This traverses topics starting from early saliency detection, foreground/background separation, out of distribution detection and leading up to open world object detection, zero-shot detection and Vision Large Language Models (VLLMs). We explore the overlap between these subdomains, their increasing convergence, and their potential to unify into a singular domain in the future, perception.",
    "authors": [
      "Andrei-Stefan Bulzan",
      "Cosmin Cernazanu-Glavan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-08-22T16:49:52Z",
    "pdf_url": "https://arxiv.org/pdf/2508.16527v1"
  },
  {
    "arxiv_id": "2508.16439v3",
    "entry_id": "http://arxiv.org/abs/2508.16439v3",
    "title": "PediatricsMQA: a Multi-modal Pediatrics Question Answering Benchmark",
    "summary": "Large language models (LLMs) and vision-augmented LLMs (VLMs) have significantly advanced medical informatics, diagnostics, and decision support. However, these models exhibit systematic biases, particularly age bias, compromising their reliability and equity. This is evident in their poorer performance on pediatric-focused text and visual question-answering tasks. This bias reflects a broader imbalance in medical research, where pediatric studies receive less funding and representation despite the significant disease burden in children. To address these issues, a new comprehensive multi-modal pediatric question-answering benchmark, PediatricsMQA, has been introduced. It consists of 3,417 text-based multiple-choice questions (MCQs) covering 131 pediatric topics across seven developmental stages (prenatal to adolescent) and 2,067 vision-based MCQs using 634 pediatric images from 67 imaging modalities and 256 anatomical regions. The dataset was developed using a hybrid manual-automatic pipeline, incorporating peer-reviewed pediatric literature, validated question banks, existing benchmarks, and existing QA resources. Evaluating state-of-the-art open models, we find dramatic performance drops in younger cohorts, highlighting the need for age-aware methods to ensure equitable AI support in pediatric care.",
    "authors": [
      "Adil Bahaj",
      "Oumaima Fadi",
      "Mohamed Chetouani",
      "Mounir Ghogho"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.GR",
      "cs.MM"
    ],
    "published": "2025-08-22T14:50:55Z",
    "pdf_url": "https://arxiv.org/pdf/2508.16439v3"
  },
  {
    "arxiv_id": "2508.16419v1",
    "entry_id": "http://arxiv.org/abs/2508.16419v1",
    "title": "LLM-GUARD: Large Language Model-Based Detection and Repair of Bugs and Security Vulnerabilities in C++ and Python",
    "summary": "Large Language Models (LLMs) such as ChatGPT-4, Claude 3, and LLaMA 4 are increasingly embedded in software/application development, supporting tasks from code generation to debugging. Yet, their real-world effectiveness in detecting diverse software bugs, particularly complex, security-relevant vulnerabilities, remains underexplored. This study presents a systematic, empirical evaluation of these three leading LLMs using a benchmark of foundational programming errors, classic security flaws, and advanced, production-grade bugs in C++ and Python. The dataset integrates real code from SEED Labs, OpenSSL (via the Suresoft GLaDOS database), and PyBugHive, validated through local compilation and testing pipelines. A novel multi-stage, context-aware prompting protocol simulates realistic debugging scenarios, while a graded rubric measures detection accuracy, reasoning depth, and remediation quality. Our results show that all models excel at identifying syntactic and semantic issues in well-scoped code, making them promising for educational use and as first-pass reviewers in automated code auditing. Performance diminishes in scenarios involving complex security vulnerabilities and large-scale production code, with ChatGPT-4 and Claude 3 generally providing more nuanced contextual analyses than LLaMA 4. This highlights both the promise and the present constraints of LLMs in serving as reliable code analysis tools.",
    "authors": [
      "Akshay Mhatre",
      "Noujoud Nader",
      "Patrick Diehl",
      "Deepti Gupta"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2025-08-22T14:30:24Z",
    "pdf_url": "https://arxiv.org/pdf/2508.16419v1"
  },
  {
    "arxiv_id": "2508.16261v1",
    "entry_id": "http://arxiv.org/abs/2508.16261v1",
    "title": "On the Evolution of Federated Post-Training Large Language Models: A Model Accessibility View",
    "summary": "Federated Learning (FL) enables training models across decentralized data silos while preserving client data privacy. Recent research has explored efficient methods for post-training large language models (LLMs) within FL to address computational and communication challenges. While existing approaches often rely on access to LLMs' internal information, which is frequently restricted in real-world scenarios, an inference-only paradigm (black-box FedLLM) has emerged to address these limitations. This paper presents a comprehensive survey on federated tuning for LLMs. We propose a taxonomy categorizing existing studies along two axes: model access-based and parameter efficiency-based optimization. We classify FedLLM approaches into white-box, gray-box, and black-box techniques, highlighting representative methods within each category. We review emerging research treating LLMs as black-box inference APIs and discuss promising directions and open challenges for future research.",
    "authors": [
      "Tao Guo",
      "Junxiao Wang",
      "Fushuo Huo",
      "Laizhong Cui",
      "Song Guo",
      "Jie Gui",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-22T09:52:31Z",
    "pdf_url": "https://arxiv.org/pdf/2508.16261v1"
  },
  {
    "arxiv_id": "2508.19269v1",
    "entry_id": "http://arxiv.org/abs/2508.19269v1",
    "title": "Should LLMs be WEIRD? Exploring WEIRDness and Human Rights in Large Language Models",
    "summary": "Large language models (LLMs) are often trained on data that reflect WEIRD values: Western, Educated, Industrialized, Rich, and Democratic. This raises concerns about cultural bias and fairness. Using responses to the World Values Survey, we evaluated five widely used LLMs: GPT-3.5, GPT-4, Llama-3, BLOOM, and Qwen. We measured how closely these responses aligned with the values of the WEIRD countries and whether they conflicted with human rights principles. To reflect global diversity, we compared the results with the Universal Declaration of Human Rights and three regional charters from Asia, the Middle East, and Africa. Models with lower alignment to WEIRD values, such as BLOOM and Qwen, produced more culturally varied responses but were 2% to 4% more likely to generate outputs that violated human rights, especially regarding gender and equality. For example, some models agreed with the statements ``a man who cannot father children is not a real man'' and ``a husband should always know where his wife is'', reflecting harmful gender norms. These findings suggest that as cultural representation in LLMs increases, so does the risk of reproducing discriminatory beliefs. Approaches such as Constitutional AI, which could embed human rights principles into model behavior, may only partly help resolve this tension.",
    "authors": [
      "Ke Zhou",
      "Marios Constantinides",
      "Daniele Quercia"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-22T08:13:28Z",
    "pdf_url": "https://arxiv.org/pdf/2508.19269v1"
  },
  {
    "arxiv_id": "2508.16701v2",
    "entry_id": "http://arxiv.org/abs/2508.16701v2",
    "title": "Generative Artificial Intelligence and Agents in Research and Teaching",
    "summary": "This study provides a comprehensive analysis of the development, functioning, and application of generative artificial intelligence (GenAI) and large language models (LLMs), with an emphasis on their implications for research and education. It traces the conceptual evolution from artificial intelligence (AI) through machine learning (ML) and deep learning (DL) to transformer architectures, which constitute the foundation of contemporary generative systems. Technical aspects, including prompting strategies, word embeddings, and probabilistic sampling methods (temperature, top-k, and top-p), are examined alongside the emergence of autonomous agents. These elements are considered in relation to both the opportunities they create and the limitations and risks they entail.\n  The work critically evaluates the integration of GenAI across the research process, from ideation and literature review to research design, data collection, analysis, interpretation, and dissemination. While particular attention is given to geographical research, the discussion extends to wider academic contexts. A parallel strand addresses the pedagogical applications of GenAI, encompassing course and lesson design, teaching delivery, assessment, and feedback, with geography education serving as a case example.\n  Central to the analysis are the ethical, social, and environmental challenges posed by GenAI. Issues of bias, intellectual property, governance, and accountability are assessed, alongside the ecological footprint of LLMs and emerging technological strategies for mitigation. The concluding section considers near- and long-term futures of GenAI, including scenarios of sustained adoption, regulation, and potential decline. By situating GenAI within both scholarly practice and educational contexts, the study contributes to critical debates on its transformative potential and societal responsibilities.",
    "authors": [
      "Jussi S. Jauhiainen",
      "Aurora Toppari"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-08-22T06:00:45Z",
    "pdf_url": "https://arxiv.org/pdf/2508.16701v2"
  },
  {
    "arxiv_id": "2508.15760v1",
    "entry_id": "http://arxiv.org/abs/2508.15760v1",
    "title": "LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries",
    "summary": "Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.",
    "authors": [
      "Ming Yin",
      "Dinghan Shen",
      "Silei Xu",
      "Jianbing Han",
      "Sixun Dong",
      "Mian Zhang",
      "Yebowen Hu",
      "Shujian Liu",
      "Simin Ma",
      "Song Wang",
      "Sathish Reddy Indurthi",
      "Xun Wang",
      "Yiran Chen",
      "Kaiqiang Song"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-21T17:55:54Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15760v1"
  },
  {
    "arxiv_id": "2508.15719v1",
    "entry_id": "http://arxiv.org/abs/2508.15719v1",
    "title": "Tutorial on the Probabilistic Unification of Estimation Theory, Machine Learning, and Generative AI",
    "summary": "Extracting meaning from uncertain, noisy data is a fundamental problem across time series analysis, pattern recognition, and language modeling. This survey presents a unified mathematical framework that connects classical estimation theory, statistical inference, and modern machine learning, including deep learning and large language models. By analyzing how techniques such as maximum likelihood estimation, Bayesian inference, and attention mechanisms address uncertainty, the paper illustrates that many AI methods are rooted in shared probabilistic principles. Through illustrative scenarios including system identification, image classification, and language generation, we show how increasingly complex models build upon these foundations to tackle practical challenges like overfitting, data sparsity, and interpretability. In other words, the work demonstrates that maximum likelihood, MAP estimation, Bayesian classification, and deep learning all represent different facets of a shared goal: inferring hidden causes from noisy and/or biased observations. It serves as both a theoretical synthesis and a practical guide for students and researchers navigating the evolving landscape of machine learning.",
    "authors": [
      "Mohammed Elmusrati"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-08-21T16:57:33Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15719v1"
  },
  {
    "arxiv_id": "2508.15658v2",
    "entry_id": "http://arxiv.org/abs/2508.15658v2",
    "title": "SurGE: A Benchmark and Evaluation Framework for Scientific Survey Generation",
    "summary": "The rapid growth of academic literature makes the manual creation of scientific surveys increasingly infeasible. While large language models show promise for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To bridge this critical gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for scientific survey generation in computer science. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers. In addition, we propose an automated evaluation framework that measures the quality of generated surveys across four dimensions: comprehensiveness, citation accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based methods demonstrates a significant performance gap, revealing that even advanced agentic frameworks struggle with the complexities of survey generation and highlighting the need for future research in this area. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE",
    "authors": [
      "Weihang Su",
      "Anzhe Xie",
      "Qingyao Ai",
      "Jianming Long",
      "Jiaxin Mao",
      "Ziyi Ye",
      "Yiqun Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-08-21T15:45:10Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15658v2"
  },
  {
    "arxiv_id": "2508.15437v2",
    "entry_id": "http://arxiv.org/abs/2508.15437v2",
    "title": "Test-time Corpus Feedback: From Retrieval to RAG",
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as a standard framework for knowledge-intensive NLP tasks, combining large language models (LLMs) with document retrieval from external corpora. Despite its widespread use, most RAG pipelines continue to treat retrieval and reasoning as isolated components, retrieving documents once and then generating answers without further interaction. This static design often limits performance on complex tasks that require iterative evidence gathering or high-precision retrieval. Recent work in both the information retrieval (IR) and NLP communities has begun to close this gap by introducing adaptive retrieval and ranking methods that incorporate feedback. In this survey, we present a structured overview of advanced retrieval and ranking mechanisms that integrate such feedback. We categorize feedback signals based on their source and role in improving the query, retrieved context, or document pool. By consolidating these developments, we aim to bridge IR and NLP perspectives and highlight retrieval as a dynamic, learnable component of end-to-end RAG systems.",
    "authors": [
      "Mandeep Rathee",
      "V Venktesh",
      "Sean MacAvaney",
      "Avishek Anand"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-21T10:57:38Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15437v2"
  },
  {
    "arxiv_id": "2508.15201v2",
    "entry_id": "http://arxiv.org/abs/2508.15201v2",
    "title": "Survey of Vision-Language-Action Models for Embodied Manipulation",
    "summary": "Embodied intelligence systems, which enhance agent capabilities through continuous environment interactions, have garnered significant attention from both academia and industry. Vision-Language-Action models, inspired by advancements in large foundation models, serve as universal robotic control frameworks that substantially improve agent-environment interaction capabilities in embodied intelligence systems. This expansion has broadened application scenarios for embodied AI robots. This survey comprehensively reviews VLA models for embodied manipulation. Firstly, it chronicles the developmental trajectory of VLA architectures. Subsequently, we conduct a detailed analysis of current research across 5 critical dimensions: VLA model structures, training datasets, pre-training methods, post-training methods, and model evaluation. Finally, we synthesize key challenges in VLA development and real-world deployment, while outlining promising future research directions.",
    "authors": [
      "Haoran Li",
      "Yuhui Chen",
      "Wenbo Cui",
      "Weiheng Liu",
      "Kai Liu",
      "Mingcai Zhou",
      "Zhengtao Zhang",
      "Dongbin Zhao"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-08-21T03:30:04Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15201v2"
  },
  {
    "arxiv_id": "2508.15126v1",
    "entry_id": "http://arxiv.org/abs/2508.15126v1",
    "title": "aiXiv: A Next-Generation Open Access Ecosystem for Scientific Discovery Generated by AI Scientists",
    "summary": "Recent advances in large language models (LLMs) have enabled AI agents to autonomously generate scientific proposals, conduct experiments, author papers, and perform peer reviews. Yet this flood of AI-generated research content collides with a fragmented and largely closed publication ecosystem. Traditional journals and conferences rely on human peer review, making them difficult to scale and often reluctant to accept AI-generated research content; existing preprint servers (e.g. arXiv) lack rigorous quality-control mechanisms. Consequently, a significant amount of high-quality AI-generated research lacks appropriate venues for dissemination, hindering its potential to advance scientific progress. To address these challenges, we introduce aiXiv, a next-generation open-access platform for human and AI scientists. Its multi-agent architecture allows research proposals and papers to be submitted, reviewed, and iteratively refined by both human and AI scientists. It also provides API and MCP interfaces that enable seamless integration of heterogeneous human and AI scientists, creating a scalable and extensible ecosystem for autonomous scientific discovery. Through extensive experiments, we demonstrate that aiXiv is a reliable and robust platform that significantly enhances the quality of AI-generated research proposals and papers after iterative revising and reviewing on aiXiv. Our work lays the groundwork for a next-generation open-access ecosystem for AI scientists, accelerating the publication and dissemination of high-quality AI-generated research content. Code is available at https://github.com/aixiv-org. Website is available at https://forms.gle/DxQgCtXFsJ4paMtn8.",
    "authors": [
      "Pengsong Zhang",
      "Xiang Hu",
      "Guowei Huang",
      "Yang Qi",
      "Heng Zhang",
      "Xiuxu Li",
      "Jiaxing Song",
      "Jiabin Luo",
      "Yijiang Li",
      "Shuo Yin",
      "Chengxiao Dai",
      "Eric Hanchen Jiang",
      "Xiaoyan Zhou",
      "Zhenfei Yin",
      "Boqin Yuan",
      "Jing Dong",
      "Guinan Su",
      "Guanren Qiao",
      "Haiming Tang",
      "Anghong Du",
      "Lili Pan",
      "Zhenzhong Lan",
      "Xinyu Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-20T23:16:41Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15126v1"
  },
  {
    "arxiv_id": "2508.16665v3",
    "entry_id": "http://arxiv.org/abs/2508.16665v3",
    "title": "Trust but Verify! A Survey on Verification Design for Test-time Scaling",
    "summary": "Test-time scaling (TTS) has emerged as a new frontier for scaling the performance of Large Language Models. In test-time scaling, by using more computational resources during inference, LLMs can improve their reasoning process and task performance. Several approaches have emerged for TTS such as distilling reasoning traces from another model or exploring the vast decoding search space by employing a verifier. The verifiers serve as reward models that help score the candidate outputs from the decoding process to diligently explore the vast solution space and select the best outcome. This paradigm commonly termed has emerged as a superior approach owing to parameter free scaling at inference time and high performance gains. The verifiers could be prompt-based, fine-tuned as a discriminative or generative model to verify process paths, outcomes or both. Despite their widespread adoption, there is no detailed collection, clear categorization and discussion of diverse verification approaches and their training mechanisms. In this survey, we cover the diverse approaches in the literature and present a unified view of verifier training, types and their utility in test-time scaling. Our repository can be found at https://github.com/elixir-research-group/Verifierstesttimescaling.github.io.",
    "authors": [
      "V Venktesh",
      "Mandeep Rathee",
      "Avishek Anand"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-20T22:27:21Z",
    "pdf_url": "https://arxiv.org/pdf/2508.16665v3"
  },
  {
    "arxiv_id": "2508.15013v1",
    "entry_id": "http://arxiv.org/abs/2508.15013v1",
    "title": "Goals and the Structure of Experience",
    "summary": "Purposeful behavior is a hallmark of natural and artificial intelligence. Its acquisition is often believed to rely on world models, comprising both descriptive (what is) and prescriptive (what is desirable) aspects that identify and evaluate state of affairs in the world, respectively. Canonical computational accounts of purposeful behavior, such as reinforcement learning, posit distinct components of a world model comprising a state representation (descriptive aspect) and a reward function (prescriptive aspect). However, an alternative possibility, which has not yet been computationally formulated, is that these two aspects instead co-emerge interdependently from an agent's goal. Here, we describe a computational framework of goal-directed state representation in cognitive agents, in which the descriptive and prescriptive aspects of a world model co-emerge from agent-environment interaction sequences, or experiences. Drawing on Buddhist epistemology, we introduce a construct of goal-directed, or telic, states, defined as classes of goal-equivalent experience distributions. Telic states provide a parsimonious account of goal-directed learning in terms of the statistical divergence between behavioral policies and desirable experience features. We review empirical and theoretical literature supporting this novel perspective and discuss its potential to provide a unified account of behavioral, phenomenological and neural dimensions of purposeful behaviors across diverse substrates.",
    "authors": [
      "Nadav Amir",
      "Stas Tiomkin",
      "Angela Langdon"
    ],
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "published": "2025-08-20T19:05:24Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15013v1"
  },
  {
    "arxiv_id": "2508.14279v1",
    "entry_id": "http://arxiv.org/abs/2508.14279v1",
    "title": "GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs",
    "summary": "LLMs (Large language models) have revolutionized NLP (Natural Language Processing), yet their pedagogical value for low-resource languages remains unclear. We present GRILE (Grammar Romanian Inference and Language Explanations) , the first open benchmark of 1,151 multiple-choice questions harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate, university admissions). GRILE enables us to probe two complementary abilities of seven state-of-the-art multilingual and Romanian-specific LLMs: (i) selecting the correct answer, and (ii) producing linguistically accurate explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight models stay below 65%, and 48% of their explanations contain factual or pedagogical flaws according to expert review. A detailed error analysis pinpoints systematic weaknesses in morphology and in applying the latest DOOM3 orthographic norms. All data, code and a public web demo are released to catalyze future research. Our findings expose open challenges for trustworthy educational NLP in low-resource settings and establish GRILE as a new test-bed for controllable explanation generation and evaluation.",
    "authors": [
      "Adrian-Marius Dumitran",
      "Alexandra-Mihaela Danila",
      "Angela-Liliana Dumitran"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-08-19T21:27:06Z",
    "pdf_url": "https://arxiv.org/pdf/2508.14279v1"
  },
  {
    "arxiv_id": "2508.15841v1",
    "entry_id": "http://arxiv.org/abs/2508.15841v1",
    "title": "A Review of Developmental Interpretability in Large Language Models",
    "summary": "This review synthesizes the nascent but critical field of developmental interpretability for Large Language Models. We chart the field's evolution from static, post-hoc analysis of trained models to a dynamic investigation of the training process itself. We begin by surveying the foundational methodologies, including representational probing, causal tracing, and circuit analysis, that enable researchers to deconstruct the learning process. The core of this review examines the developmental arc of LLM capabilities, detailing key findings on the formation and composition of computational circuits, the biphasic nature of knowledge acquisition, the transient dynamics of learning strategies like in-context learning, and the phenomenon of emergent abilities as phase transitions in training. We explore illuminating parallels with human cognitive and linguistic development, which provide valuable conceptual frameworks for understanding LLM learning. Finally, we argue that this developmental perspective is not merely an academic exercise but a cornerstone of proactive AI safety, offering a pathway to predict, monitor, and align the processes by which models acquire their capabilities. We conclude by outlining the grand challenges facing the field, such as scalability and automation, and propose a research agenda for building more transparent, reliable, and beneficial AI systems.",
    "authors": [
      "Ihor Kendiukhov"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-08-19T18:19:16Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15841v1"
  },
  {
    "arxiv_id": "2508.13678v1",
    "entry_id": "http://arxiv.org/abs/2508.13678v1",
    "title": "Neuro-Symbolic Artificial Intelligence: Towards Improving the Reasoning Abilities of Large Language Models",
    "summary": "Large Language Models (LLMs) have shown promising results across various tasks, yet their reasoning capabilities remain a fundamental challenge. Developing AI systems with strong reasoning capabilities is regarded as a crucial milestone in the pursuit of Artificial General Intelligence (AGI) and has garnered considerable attention from both academia and industry. Various techniques have been explored to enhance the reasoning capabilities of LLMs, with neuro-symbolic approaches being a particularly promising way. This paper comprehensively reviews recent developments in neuro-symbolic approaches for enhancing LLM reasoning. We first present a formalization of reasoning tasks and give a brief introduction to the neurosymbolic learning paradigm. Then, we discuss neuro-symbolic methods for improving the reasoning capabilities of LLMs from three perspectives: Symbolic->LLM, LLM->Symbolic, and LLM+Symbolic. Finally, we discuss several key challenges and promising future directions. We have also released a GitHub repository including papers and resources related to this survey: https://github.com/LAMDASZ-ML/Awesome-LLM-Reasoning-with-NeSy.",
    "authors": [
      "Xiao-Wen Yang",
      "Jie-Jing Shao",
      "Lan-Zhe Guo",
      "Bo-Wen Zhang",
      "Zhi Zhou",
      "Lin-Han Jia",
      "Wang-Zhou Dai",
      "Yu-Feng Li"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-19T09:27:46Z",
    "pdf_url": "https://arxiv.org/pdf/2508.13678v1"
  },
  {
    "arxiv_id": "2508.13426v1",
    "entry_id": "http://arxiv.org/abs/2508.13426v1",
    "title": "ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models",
    "summary": "As large language models (LLMs) increasingly mediate cross-cultural communication, their behavior still reflects the distributional bias of the languages and viewpoints that are over-represented in their pre-training corpora. Yet, it remains a challenge to model and align culture due to limited cultural knowledge and a lack of exploration into effective learning approaches. We introduce a cost-efficient, cognitively grounded remedy: parameter-efficient fine-tuning on native speakers' free word-association norms, which encode implicit cultural schemas. Leveraging English-US and Mandarin associations from the Small-World-of-Words project, we adapt Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based preference optimization. SFT boosts held-out association Precision at 5 by 16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20, and attains human-level valence and arousal. These lexical gains transfer: on World-Values-Survey questions, fine-tuned models shift answer distributions toward the target culture, and on a 50-item high-tension subset, Qwen's Chinese-aligned responses double while Llama's US bias drops by one-third. Our 7-8B models rival or beat vanilla 70B baselines, showing that a few million culture-grounded associations can instill value alignment without costly retraining. Our work highlights both the promise and the need for future research grounded in human cognition in improving cultural alignment in AI models.",
    "authors": [
      "Chunhua Liu",
      "Kabir Manandhar Shrestha",
      "Sukai Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-19T00:55:20Z",
    "pdf_url": "https://arxiv.org/pdf/2508.13426v1"
  },
  {
    "arxiv_id": "2508.13404v3",
    "entry_id": "http://arxiv.org/abs/2508.13404v3",
    "title": "TASER: Table Agents for Schema-guided Extraction and Recommendation",
    "summary": "Real-world financial documents report essential information about an entity's financial holdings that can span millions of different financial instrument types. Yet, these details are often buried in messy, multi-page, fragmented tables - for example, 99.4% of the tables in our dataset have no bounding boxes with the maximum number of rows amounting to 426 per table across 44 pages. To tackle these unique challenges from real-world tables, we present a continuously learning, agentic table extraction system, TASER (Table Agents for Schema-guided Extraction and Recommendation) that extracts highly unstructured, multi-page, heterogeneous tables into normalized, schema-conforming outputs. Our table agents execute on table detection, classification, extraction, and recommendations by leveraging an initial schema. Then, our Recommender Agent reviews the outputs, recommends schema revisions, and decides on the final recommendations, enabling TASER to outperform existing table detection models such as Table Transformer by 10.1%. Within this continuous learning process, we highlight that larger batch sizes result in a 104.3% increase in schema recommendations that are actionable and utilized, resulting in a 9.8% increase in extracted holdings - highlighting the importance of a continuous learning process. To train TASER, we have manually labeled 22,584 pages (28,150,449 tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of the first real financial table datasets. We release our dataset TASERTab to enable the research community to access real-world financial tables and outputs. Our results highlight the promise of agentic, schema-guided extraction systems for robust understanding of real-world financial tables.",
    "authors": [
      "Nicole Cho",
      "Kirsty Fielding",
      "William Watson",
      "Sumitra Ganesh",
      "Manuela Veloso"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-08-18T23:48:22Z",
    "pdf_url": "https://arxiv.org/pdf/2508.13404v3"
  },
  {
    "arxiv_id": "2508.13256v1",
    "entry_id": "http://arxiv.org/abs/2508.13256v1",
    "title": "CardAIc-Agents: A Multimodal Framework with Hierarchical Adaptation for Cardiac Care Support",
    "summary": "Cardiovascular diseases (CVDs) remain the foremost cause of mortality worldwide, a burden worsened by a severe deficit of healthcare workers. Artificial intelligence (AI) agents have shown potential to alleviate this gap via automated early detection and proactive screening, yet their clinical application remains limited by: 1) prompt-based clinical role assignment that relies on intrinsic model capabilities without domain-specific tool support; or 2) rigid sequential workflows, whereas clinical care often requires adaptive reasoning that orders specific tests and, based on their results, guides personalised next steps; 3) general and static knowledge bases without continuous learning capability; and 4) fixed unimodal or bimodal inputs and lack of on-demand visual outputs when further clarification is needed. In response, a multimodal framework, CardAIc-Agents, was proposed to augment models with external tools and adaptively support diverse cardiac tasks. Specifically, a CardiacRAG agent generated general plans from updatable cardiac knowledge, while the chief agent integrated tools to autonomously execute these plans and deliver decisions. To enable adaptive and case-specific customization, a stepwise update strategy was proposed to dynamically refine plans based on preceding execution results, once the task was assessed as complex. In addition, a multidisciplinary discussion tool was introduced to interpret challenging cases, thereby supporting further adaptation. When clinicians raised concerns, visual review panels were provided to assist final validation. Experiments across three datasets showed the efficiency of CardAIc-Agents compared to mainstream Vision-Language Models (VLMs), state-of-the-art agentic systems, and fine-tuned VLMs.",
    "authors": [
      "Yuting Zhang",
      "Karina V. Bunting",
      "Asgher Champsi",
      "Xiaoxia Wang",
      "Wenqi Lu",
      "Alexander Thorley",
      "Sandeep S Hothi",
      "Zhaowen Qiu",
      "Dipak Kotecha",
      "Jinming Duan"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "published": "2025-08-18T16:17:12Z",
    "pdf_url": "https://arxiv.org/pdf/2508.13256v1"
  },
  {
    "arxiv_id": "2508.12896v1",
    "entry_id": "http://arxiv.org/abs/2508.12896v1",
    "title": "Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption",
    "summary": "We formalize three design axioms for sustained adoption of agent-centric AI systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed > Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying novelty term and a growing utility term and derive the phase conditions for troughs/overshoots with full proofs. We introduce: (i) an identifiability/confounding analysis for $(α,β,N_0,U_{\\max})$ with delta-method gradients; (ii) a non-monotone comparator (logistic-with-transient-bump) evaluated on the same series to provide additional model comparison; (iii) ablations over hazard families $h(\\cdot)$ mapping $ΔV \\to β$; (iv) a multi-series benchmark (varying trough depth, noise, AR structure) reporting coverage (type-I error, power); (v) calibration of friction proxies against time-motion/survey ground truth with standard errors; (vi) residual analyses (autocorrelation and heteroskedasticity) for each fitted curve; (vii) preregistered windowing choices for pre/post estimation; (viii) Fisher information & CRLB for $(α,β)$ under common error models; (ix) microfoundations linking $\\mathcal{T}$ to $(N_0,U_{\\max})$; (x) explicit comparison to bi-logistic, double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$ heterogeneity. Figures and tables are reflowed for readability, and the bibliography restores and extends non-logistic/Bass adoption references (Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All code and logs necessary to reproduce the synthetic analyses are embedded as LaTeX listings.",
    "authors": [
      "Faruk Alpay",
      "Taylan Alpay"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "stat.ME"
    ],
    "published": "2025-08-18T12:53:38Z",
    "pdf_url": "https://arxiv.org/pdf/2508.12896v1"
  },
  {
    "arxiv_id": "2508.14111v2",
    "entry_id": "http://arxiv.org/abs/2508.14111v2",
    "title": "From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery",
    "summary": "Artificial intelligence (AI) is reshaping scientific discovery, evolving from specialized computational tools into autonomous research partners. We position Agentic Science as a pivotal stage within the broader AI for Science paradigm, where AI systems progress from partial assistance to full scientific agency. Enabled by large language models (LLMs), multimodal systems, and integrated research platforms, agentic AI shows capabilities in hypothesis generation, experimental design, execution, analysis, and iterative refinement -- behaviors once regarded as uniquely human. This survey provides a domain-oriented review of autonomous scientific discovery across life sciences, chemistry, materials science, and physics. We unify three previously fragmented perspectives -- process-oriented, autonomy-oriented, and mechanism-oriented -- through a comprehensive framework that connects foundational capabilities, core processes, and domain-specific realizations. Building on this framework, we (i) trace the evolution of AI for Science, (ii) identify five core capabilities underpinning scientific agency, (iii) model discovery as a dynamic four-stage workflow, (iv) review applications across the above domains, and (v) synthesize key challenges and future opportunities. This work establishes a domain-oriented synthesis of autonomous scientific discovery and positions Agentic Science as a structured paradigm for advancing AI-driven research.",
    "authors": [
      "Jiaqi Wei",
      "Yuejin Yang",
      "Xiang Zhang",
      "Yuhan Chen",
      "Xiang Zhuang",
      "Zhangyang Gao",
      "Dongzhan Zhou",
      "Guangshuai Wang",
      "Zhiqiang Gao",
      "Juntai Cao",
      "Zijie Qiu",
      "Ming Hu",
      "Chenglong Ma",
      "Shixiang Tang",
      "Junjun He",
      "Chunfeng Song",
      "Xuming He",
      "Qiang Zhang",
      "Chenyu You",
      "Shuangjia Zheng",
      "Ning Ding",
      "Wanli Ouyang",
      "Nanqing Dong",
      "Yu Cheng",
      "Siqi Sun",
      "Lei Bai",
      "Bowen Zhou"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-18T05:25:54Z",
    "pdf_url": "https://arxiv.org/pdf/2508.14111v2"
  },
  {
    "arxiv_id": "2508.12555v1",
    "entry_id": "http://arxiv.org/abs/2508.12555v1",
    "title": "Illuminating LLM Coding Agents: Visual Analytics for Deeper Understanding and Enhancement",
    "summary": "Coding agents powered by large language models (LLMs) have gained traction for automating code generation through iterative problem-solving with minimal human involvement. Despite the emergence of various frameworks, e.g., LangChain, AutoML, and AIDE, ML scientists still struggle to effectively review and adjust the agents' coding process. The current approach of manually inspecting individual outputs is inefficient, making it difficult to track code evolution, compare coding iterations, and identify improvement opportunities. To address this challenge, we introduce a visual analytics system designed to enhance the examination of coding agent behaviors. Focusing on the AIDE framework, our system supports comparative analysis across three levels: (1) Code-Level Analysis, which reveals how the agent debugs and refines its code over iterations; (2) Process-Level Analysis, which contrasts different solution-seeking processes explored by the agent; and (3) LLM-Level Analysis, which highlights variations in coding behavior across different LLMs. By integrating these perspectives, our system enables ML scientists to gain a structured understanding of agent behaviors, facilitating more effective debugging and prompt engineering. Through case studies using coding agents to tackle popular Kaggle competitions, we demonstrate how our system provides valuable insights into the iterative coding process.",
    "authors": [
      "Junpeng Wang",
      "Yuzhong Chen",
      "Menghai Pan",
      "Chin-Chia Michael Yeh",
      "Mahashweta Das"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-18T01:17:11Z",
    "pdf_url": "https://arxiv.org/pdf/2508.12555v1"
  },
  {
    "arxiv_id": "2508.15822v1",
    "entry_id": "http://arxiv.org/abs/2508.15822v1",
    "title": "An Auditable Pipeline for Fuzzy Full-Text Screening in Systematic Reviews: Integrating Contrastive Semantic Highlighting and LLM Judgment",
    "summary": "Full-text screening is the major bottleneck of systematic reviews (SRs), as decisive evidence is dispersed across long, heterogeneous documents and rarely admits static, binary rules. We present a scalable, auditable pipeline that reframes inclusion/exclusion as a fuzzy decision problem and benchmark it against statistical and crisp baselines in the context of the Population Health Modelling Consensus Reporting Network for noncommunicable diseases (POPCORN). Articles are parsed into overlapping chunks and embedded with a domain-adapted model; for each criterion (Population, Intervention, Outcome, Study Approach), we compute contrastive similarity (inclusion-exclusion cosine) and a vagueness margin, which a Mamdani fuzzy controller maps into graded inclusion degrees with dynamic thresholds in a multi-label setting. A large language model (LLM) judge adjudicates highlighted spans with tertiary labels, confidence scores, and criterion-referenced rationales; when evidence is insufficient, fuzzy membership is attenuated rather than excluded. In a pilot on an all-positive gold set (16 full texts; 3,208 chunks), the fuzzy system achieved recall of 81.3% (Population), 87.5% (Intervention), 87.5% (Outcome), and 75.0% (Study Approach), surpassing statistical (56.3-75.0%) and crisp baselines (43.8-81.3%). Strict \"all-criteria\" inclusion was reached for 50.0% of articles, compared to 25.0% and 12.5% under the baselines. Cross-model agreement on justifications was 98.3%, human-machine agreement 96.1%, and a pilot review showed 91% inter-rater agreement (kappa = 0.82), with screening time reduced from about 20 minutes to under 1 minute per article at significantly lower cost. These results show that fuzzy logic with contrastive highlighting and LLM adjudication yields high recall, stable rationale, and end-to-end traceability.",
    "authors": [
      "Pouria Mortezaagha",
      "Arya Rahgozar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.IR"
    ],
    "published": "2025-08-17T17:41:50Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15822v1"
  },
  {
    "arxiv_id": "2508.12379v2",
    "entry_id": "http://arxiv.org/abs/2508.12379v2",
    "title": "GraphCogent: Mitigating LLMs' Working Memory Constraints via Multi-Agent Collaboration in Complex Graph Understanding",
    "summary": "Large language models (LLMs) show promising performance on small-scale graph reasoning tasks but fail when handling real-world graphs with complex queries. This phenomenon arises from LLMs' working memory constraints, which result in their inability to retain long-range graph topology over extended contexts while sustaining coherent multi-step reasoning. However, real-world graphs are often structurally complex, such as Web, Transportation, Social, and Citation networks. To address these limitations, we propose GraphCogent, a collaborative agent framework inspired by human Working Memory Model that decomposes graph reasoning into specialized cognitive processes: sense, buffer, and execute. The framework consists of three modules: Sensory Module standardizes diverse graph text representations via subgraph sampling, Buffer Module integrates and indexes graph data across multiple formats, and Execution Module combines tool calling and tool creation for efficient reasoning. We also introduce Graph4real, a comprehensive benchmark that contains four domains of real-world graphs (Web, Transportation, Social, and Citation) to evaluate LLMs' graph reasoning capabilities. Our Graph4real covers 21 different graph reasoning tasks, categorized into three types (Structural Querying, Algorithmic Reasoning, and Predictive Modeling tasks), with graph scales up to 10 times larger than existing benchmarks. Experiments show that Llama3.1-8B based GraphCogent achieves a 50% improvement over massive-scale LLMs like DeepSeek-R1 (671B). Compared to state-of-the-art agent-based baseline, our framework outperforms by 20% in accuracy while reducing token usage by 80% for in-toolset tasks and 30% for out-toolset tasks. Code will be available after review.",
    "authors": [
      "Rongzheng Wang",
      "Shuang Liang",
      "Qizhi Chen",
      "Yihong Huang",
      "Muquan Li",
      "Yizhuo Ma",
      "Dongyang Zhang",
      "Ke Qin",
      "Man-Fai Leung"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-17T14:28:38Z",
    "pdf_url": "https://arxiv.org/pdf/2508.12379v2"
  },
  {
    "arxiv_id": "2508.12358v1",
    "entry_id": "http://arxiv.org/abs/2508.12358v1",
    "title": "Uncovering Systematic Failures of LLMs in Verifying Code Against Natural Language Specifications",
    "summary": "Large language models (LLMs) have become essential tools in software development, widely used for requirements engineering, code generation and review tasks. Software engineers often rely on LLMs to assess whether system code implementation satisfy task requirements, thereby enhancing code robustness and accuracy. However, it remains unclear whether LLMs can reliably determine whether the code complies fully with the given task descriptions, which is usually natural language specifications. In this paper, we uncover a systematic failure of LLMs in evaluating whether code aligns with natural language requirements. Specifically, with widely used benchmarks, we employ unified prompts to judge code correctness. Our results reveal that LLMs frequently misclassify correct code implementations as either ``not satisfying requirements'' or containing potential defects. Surprisingly, more complex prompting, especially when leveraging prompt engineering techniques involving explanations and proposed corrections, leads to higher misjudgment rate, which highlights the critical reliability issues in using LLMs as code review assistants. We further analyze the root causes of these misjudgments, and propose two improved prompting strategies for mitigation. For the first time, our findings reveals unrecognized limitations in LLMs to match code with requirements. We also offer novel insights and practical guidance for effective use of LLMs in automated code review and task-oriented agent scenarios.",
    "authors": [
      "Haolin Jin",
      "Huaming Chen"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-08-17T13:07:26Z",
    "pdf_url": "https://arxiv.org/pdf/2508.12358v1"
  },
  {
    "arxiv_id": "2508.14104v1",
    "entry_id": "http://arxiv.org/abs/2508.14104v1",
    "title": "You Don't Know Until You Click:Automated GUI Testing for Production-Ready Software Evaluation",
    "summary": "Large Language Models (LLMs) and code agents in software development are rapidly evolving from generating isolated code snippets to producing full-fledged software applications with graphical interfaces, interactive logic, and dynamic behaviors. However, current benchmarks fall short in evaluating such production-ready software, as they often rely on static checks or binary pass/fail scripts, failing to capture the interactive behaviors and runtime dynamics that define real-world usability - qualities that only emerge when an application is actively used. This is the blind spot of current evaluation: you don't know if an app works until you click through it, interact with it, and observe how it responds. To bridge this gap, we introduce RealDevWorld, a novel evaluation framework for automated end-to-end assessment of LLMs' ability to generate production-ready repositories from scratch. It features two key components: (1) RealDevBench, a diverse collection of 194 open-ended software engineering tasks across multiple domains, incorporating multimodal elements to reflect real-world complexity; and (2) AppEvalPilot, a new agent-as-a-judge evaluation system that simulates realistic, GUI-based user interactions to automatically and holistically assess software functional correctness, visual fidelity, and runtime behavior. The framework delivers fine-grained, task-specific diagnostic feedback, supporting nuanced evaluation beyond simple success/failure judgments. Empirical results show that RealDevWorld delivers effective, automatic, and human-aligned evaluations, achieving an accuracy of 0.92 and a correlation of 0.85 with expert human assessments, while significantly reducing the reliance on manual review. This enables scalable, human-aligned assessment of production-level software generated by LLMs. Our code is available on GitHub.",
    "authors": [
      "Yutong Bian",
      "Xianhao Lin",
      "Yupeng Xie",
      "Tianyang Liu",
      "Mingchen Zhuge",
      "Siyuan Lu",
      "Haoming Tang",
      "Jinlin Wang",
      "Jiayi Zhang",
      "Jiaqi Chen",
      "Xiangru Tang",
      "Yongxin Ni",
      "Sirui Hong",
      "Chenglin Wu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-08-17T07:31:11Z",
    "pdf_url": "https://arxiv.org/pdf/2508.14104v1"
  },
  {
    "arxiv_id": "2508.12213v1",
    "entry_id": "http://arxiv.org/abs/2508.12213v1",
    "title": "Towards Generalizable Human Activity Recognition: A Survey",
    "summary": "As a critical component of Wearable AI, IMU-based Human Activity Recognition (HAR) has attracted increasing attention from both academia and industry in recent years. Although HAR performance has improved considerably in specific scenarios, its generalization capability remains a key barrier to widespread real-world adoption. For example, domain shifts caused by variations in users, sensor positions, or environments can significantly decrease the performance in practice. As a result, in this survey, we explore the rapidly evolving field of IMU-based generalizable HAR, reviewing 229 research papers alongside 25 publicly available datasets to provide a broad and insightful overview. We first present the background and overall framework of IMU-based HAR tasks, as well as the generalization-oriented training settings. Then, we categorize representative methodologies from two perspectives: (i) model-centric approaches, including pre-training method, end-to-end method, and large language model (LLM)-based learning method; and (ii) data-centric approaches, including multi-modal learning and data augmentation techniques. In addition, we summarize widely used datasets in this field, as well as relevant tools and benchmarks. Building on these methodological advances, the broad applicability of IMU-based HAR is also reviewed and discussed. Finally, we discuss persistent challenges (e.g., data scarcity, efficient training, and reliable evaluation) and also outline future directions for HAR, including the adoption of foundation and large language models, physics-informed and context-aware reasoning, generative modeling, and resource-efficient training and inference. The complete list of this survey is available at https://github.com/rh20624/Awesome-IMU-Sensing, which will be updated continuously.",
    "authors": [
      "Yize Cai",
      "Baoshen Guo",
      "Flora Salim",
      "Zhiqing Hong"
    ],
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-17T03:04:39Z",
    "pdf_url": "https://arxiv.org/pdf/2508.12213v1"
  },
  {
    "arxiv_id": "2508.13214v1",
    "entry_id": "http://arxiv.org/abs/2508.13214v1",
    "title": "Too Easily Fooled? Prompt Injection Breaks LLMs on Frustratingly Simple Multiple-Choice Questions",
    "summary": "Large Language Models (LLMs) have recently demonstrated strong emergent abilities in complex reasoning and zero-shot generalization, showing unprecedented potential for LLM-as-a-judge applications in education, peer review, and data quality evaluation. However, their robustness under prompt injection attacks, where malicious instructions are embedded into the content to manipulate outputs, remains a significant concern. In this work, we explore a frustratingly simple yet effective attack setting to test whether LLMs can be easily misled. Specifically, we evaluate LLMs on basic arithmetic questions (e.g., \"What is 3 + 2?\") presented as either multiple-choice or true-false judgment problems within PDF files, where hidden prompts are injected into the file. Our results reveal that LLMs are indeed vulnerable to such hidden prompt injection attacks, even in these trivial scenarios, highlighting serious robustness risks for LLM-as-a-judge applications.",
    "authors": [
      "Xuyang Guo",
      "Zekai Huang",
      "Zhao Song",
      "Jiahao Zhang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-08-16T23:30:08Z",
    "pdf_url": "https://arxiv.org/pdf/2508.13214v1"
  },
  {
    "arxiv_id": "2508.12045v2",
    "entry_id": "http://arxiv.org/abs/2508.12045v2",
    "title": "Large Language Models Enable Design of Personalized Nudges across Cultures",
    "summary": "Nudge strategies are effective tools for influencing behaviour, but their impact depends on individual preferences. Strategies that work for some individuals may be counterproductive for others. We hypothesize that large language models (LLMs) can facilitate the design of individual-specific nudges without the need for costly and time-intensive behavioural data collection and modelling. To test this, we use LLMs to design personalized decoy-based nudges tailored to individual profiles and cultural contexts, aimed at encouraging air travellers to voluntarily offset CO$_2$ emissions from flights. We evaluate their effectiveness through a large-scale survey experiment ($n=3495$) conducted across five countries. Results show that LLM-informed personalized nudges are more effective than uniform settings, raising offsetting rates by 3-7$\\%$ in Germany, Singapore, and the US, though not in China or India. Our study highlights the potential of LLM as a low-cost testbed for piloting nudge strategies. At the same time, cultural heterogeneity constrains their generalizability underscoring the need for combining LLM-based simulations with targeted empirical validation.",
    "authors": [
      "Vladimir Maksimenko",
      "Qingyao Xin",
      "Prateek Gupta",
      "Bin Zhang",
      "Prateek Bansal"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-08-16T13:40:44Z",
    "pdf_url": "https://arxiv.org/pdf/2508.12045v2"
  },
  {
    "arxiv_id": "2508.12022v1",
    "entry_id": "http://arxiv.org/abs/2508.12022v1",
    "title": "AI Models for Depressive Disorder Detection and Diagnosis: A Review",
    "summary": "Major Depressive Disorder is one of the leading causes of disability worldwide, yet its diagnosis still depends largely on subjective clinical assessments. Integrating Artificial Intelligence (AI) holds promise for developing objective, scalable, and timely diagnostic tools. In this paper, we present a comprehensive survey of state-of-the-art AI methods for depression detection and diagnosis, based on a systematic review of 55 key studies. We introduce a novel hierarchical taxonomy that structures the field by primary clinical task (diagnosis vs. prediction), data modality (text, speech, neuroimaging, multimodal), and computational model class (e.g., graph neural networks, large language models, hybrid approaches). Our in-depth analysis reveals three major trends: the predominance of graph neural networks for modeling brain connectivity, the rise of large language models for linguistic and conversational data, and an emerging focus on multimodal fusion, explainability, and algorithmic fairness. Alongside methodological insights, we provide an overview of prominent public datasets and standard evaluation metrics as a practical guide for researchers. By synthesizing current advances and highlighting open challenges, this survey offers a comprehensive roadmap for future innovation in computational psychiatry.",
    "authors": [
      "Dorsa Macky Aleagha",
      "Payam Zohari",
      "Mostafa Haghir Chehreghani"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-16T11:46:48Z",
    "pdf_url": "https://arxiv.org/pdf/2508.12022v1"
  },
  {
    "arxiv_id": "2508.11957v1",
    "entry_id": "http://arxiv.org/abs/2508.11957v1",
    "title": "A Comprehensive Review of AI Agents: Transforming Possibilities in Technology and Beyond",
    "summary": "Artificial Intelligence (AI) agents have rapidly evolved from specialized, rule-based programs to versatile, learning-driven autonomous systems capable of perception, reasoning, and action in complex environments. The explosion of data, advances in deep learning, reinforcement learning, and multi-agent coordination have accelerated this transformation. Yet, designing and deploying unified AI agents that seamlessly integrate cognition, planning, and interaction remains a grand challenge. In this review, we systematically examine the architectural principles, foundational components, and emergent paradigms that define the landscape of contemporary AI agents. We synthesize insights from cognitive science-inspired models, hierarchical reinforcement learning frameworks, and large language model-based reasoning. Moreover, we discuss the pressing ethical, safety, and interpretability concerns associated with deploying these agents in real-world scenarios. By highlighting major breakthroughs, persistent challenges, and promising research directions, this review aims to guide the next generation of AI agent systems toward more robust, adaptable, and trustworthy autonomous intelligence.",
    "authors": [
      "Xiaodong Qu",
      "Andrews Damoah",
      "Joshua Sherwood",
      "Peiyan Liu",
      "Christian Shun Jin",
      "Lulu Chen",
      "Minjie Shen",
      "Nawwaf Aleisa",
      "Zeyuan Hou",
      "Chenyu Zhang",
      "Lifu Gao",
      "Yanshu Li",
      "Qikai Yang",
      "Qun Wang",
      "Cristabelle De Souza"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-16T07:38:45Z",
    "pdf_url": "https://arxiv.org/pdf/2508.11957v1"
  },
  {
    "arxiv_id": "2508.11834v1",
    "entry_id": "http://arxiv.org/abs/2508.11834v1",
    "title": "Recent Advances in Transformer and Large Language Models for UAV Applications",
    "summary": "The rapid advancement of Transformer-based models has reshaped the landscape of uncrewed aerial vehicle (UAV) systems by enhancing perception, decision-making, and autonomy. This review paper systematically categorizes and evaluates recent developments in Transformer architectures applied to UAVs, including attention mechanisms, CNN-Transformer hybrids, reinforcement learning Transformers, and large language models (LLMs). Unlike previous surveys, this work presents a unified taxonomy of Transformer-based UAV models, highlights emerging applications such as precision agriculture and autonomous navigation, and provides comparative analyses through structured tables and performance benchmarks. The paper also reviews key datasets, simulators, and evaluation metrics used in the field. Furthermore, it identifies existing gaps in the literature, outlines critical challenges in computational efficiency and real-time deployment, and offers future research directions. This comprehensive synthesis aims to guide researchers and practitioners in understanding and advancing Transformer-driven UAV technologies.",
    "authors": [
      "Hamza Kheddar",
      "Yassine Habchi",
      "Mohamed Chahine Ghanem",
      "Mustapha Hemis",
      "Dusit Niyato"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO",
      "eess.IV",
      "eess.SY"
    ],
    "published": "2025-08-15T22:56:37Z",
    "pdf_url": "https://arxiv.org/pdf/2508.11834v1"
  },
  {
    "arxiv_id": "2508.11759v2",
    "entry_id": "http://arxiv.org/abs/2508.11759v2",
    "title": "Using Natural Language for Human-Robot Collaboration in the Real World",
    "summary": "We have a vision of a day when autonomous robots can collaborate with humans as assistants in performing complex tasks in the physical world. This vision includes that the robots will have the ability to communicate with their human collaborators using language that is natural to the humans. Traditional Interactive Task Learning (ITL) systems have some of this ability, but the language they can understand is very limited. The advent of large language models (LLMs) provides an opportunity to greatly improve the language understanding of robots, yet integrating the language abilities of LLMs with robots that operate in the real physical world is a challenging problem.\n  In this chapter we first review briefly a few commercial robot products that work closely with humans, and discuss how they could be much better collaborators with robust language abilities. We then explore how an AI system with a cognitive agent that controls a physical robot at its core, interacts with both a human and an LLM, and accumulates situational knowledge through its experiences, can be a possible approach to reach that vision. We focus on three specific challenges of having the robot understand natural language, and present a simple proof-of-concept experiment using ChatGPT for each. Finally, we discuss what it will take to turn these simple experiments into an operational system where LLM-assisted language understanding is a part of an integrated robotic assistant that uses language to collaborate with humans.",
    "authors": [
      "Peter Lindes",
      "Kaoutar Skiker"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-15T18:09:53Z",
    "pdf_url": "https://arxiv.org/pdf/2508.11759v2"
  },
  {
    "arxiv_id": "2508.11738v1",
    "entry_id": "http://arxiv.org/abs/2508.11738v1",
    "title": "Artificial Intelligence in Rural Healthcare Delivery: Bridging Gaps and Enhancing Equity through Innovation",
    "summary": "Rural healthcare faces persistent challenges, including inadequate infrastructure, workforce shortages, and socioeconomic disparities that hinder access to essential services. This study investigates the transformative potential of artificial intelligence (AI) in addressing these issues in underserved rural areas. We systematically reviewed 109 studies published between 2019 and 2024 from PubMed, Embase, Web of Science, IEEE Xplore, and Scopus. Articles were screened using PRISMA guidelines and Covidence software. A thematic analysis was conducted to identify key patterns and insights regarding AI implementation in rural healthcare delivery. The findings reveal significant promise for AI applications, such as predictive analytics, telemedicine platforms, and automated diagnostic tools, in improving healthcare accessibility, quality, and efficiency. Among these, advanced AI systems, including Multimodal Foundation Models (MFMs) and Large Language Models (LLMs), offer particularly transformative potential. MFMs integrate diverse data sources, such as imaging, clinical records, and bio signals, to support comprehensive decision-making, while LLMs facilitate clinical documentation, patient triage, translation, and virtual assistance. Together, these technologies can revolutionize rural healthcare by augmenting human capacity, reducing diagnostic delays, and democratizing access to expertise. However, barriers remain, including infrastructural limitations, data quality concerns, and ethical considerations. Addressing these challenges requires interdisciplinary collaboration, investment in digital infrastructure, and the development of regulatory frameworks. This review offers actionable recommendations and highlights areas for future research to ensure equitable and sustainable integration of AI in rural healthcare systems.",
    "authors": [
      "Kiruthika Balakrishnan",
      "Durgadevi Velusamy",
      "Hana E. Hinkle",
      "Zhi Li",
      "Karthikeyan Ramasamy",
      "Hikmat Khan",
      "Srini Ramaswamy",
      "Pir Masoom Shah"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-08-15T17:08:10Z",
    "pdf_url": "https://arxiv.org/pdf/2508.11738v1"
  },
  {
    "arxiv_id": "2508.11529v1",
    "entry_id": "http://arxiv.org/abs/2508.11529v1",
    "title": "A Comprehensive Perspective on Explainable AI across the Machine Learning Workflow",
    "summary": "Artificial intelligence is reshaping science and industry, yet many users still regard its models as opaque \"black boxes\". Conventional explainable artificial-intelligence methods clarify individual predictions but overlook the upstream decisions and downstream quality checks that determine whether insights can be trusted. In this work, we present Holistic Explainable Artificial Intelligence (HXAI), a user-centric framework that embeds explanation into every stage of the data-analysis workflow and tailors those explanations to users. HXAI unifies six components (data, analysis set-up, learning process, model output, model quality, communication channel) into a single taxonomy and aligns each component with the needs of domain experts, data analysts and data scientists. A 112-item question bank covers these needs; our survey of contemporary tools highlights critical coverage gaps. Grounded in theories of human explanation, principles from human-computer interaction and findings from empirical user studies, HXAI identifies the characteristics that make explanations clear, actionable and cognitively manageable. A comprehensive taxonomy operationalises these insights, reducing terminological ambiguity and enabling rigorous coverage analysis of existing toolchains. We further demonstrate how AI agents that embed large-language models can orchestrate diverse explanation techniques, translating technical artifacts into stakeholder-specific narratives that bridge the gap between AI developers and domain experts. Departing from traditional surveys or perspective articles, this work melds concepts from multiple disciplines, lessons from real-world projects and a critical synthesis of the literature to advance a novel, end-to-end viewpoint on transparency, trustworthiness and responsible AI deployment.",
    "authors": [
      "George Paterakis",
      "Andrea Castellani",
      "George Papoutsoglou",
      "Tobias Rodemann",
      "Ioannis Tsamardinos"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-08-15T15:15:25Z",
    "pdf_url": "https://arxiv.org/pdf/2508.11529v1"
  },
  {
    "arxiv_id": "2508.11454v1",
    "entry_id": "http://arxiv.org/abs/2508.11454v1",
    "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context",
    "summary": "Large language models (LLMs) are now widely used across many fields, including marketing research. Sentiment analysis, in particular, helps firms understand consumer preferences. While most NLP studies classify sentiment from review text alone, marketing theories, such as prospect theory and expectation--disconfirmation theory, point out that customer evaluations are shaped not only by the actual experience but also by additional reference points. This study therefore investigates how the content and format of such supplementary information affect sentiment analysis using LLMs. We compare natural language (NL) and JSON-formatted prompts using a lightweight 3B parameter model suitable for practical marketing applications. Experiments on two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with additional information outperforms all baselines without fine-tuning: Macro-F1 rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it deployable in resource-constrained edge devices. Furthermore, a follow-up analysis confirms that performance gains stem from genuine contextual reasoning rather than label proxying. This work demonstrates that structured prompting can enable smaller models to achieve competitive performance, offering a practical alternative to large-scale model deployment.",
    "authors": [
      "Junichiro Niimi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-15T13:04:32Z",
    "pdf_url": "https://arxiv.org/pdf/2508.11454v1"
  },
  {
    "arxiv_id": "2508.11310v1",
    "entry_id": "http://arxiv.org/abs/2508.11310v1",
    "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems",
    "summary": "The growing interest in automatic survey generation (ASG), a task that traditionally required considerable time and effort, has been spurred by recent advances in large language models (LLMs). With advancements in retrieval-augmented generation (RAG) and the rising popularity of multi-agent systems (MASs), synthesizing academic surveys using LLMs has become a viable approach, thereby elevating the need for robust evaluation methods in this domain. However, existing evaluation methods suffer from several limitations, including biased metrics, a lack of human preference, and an over-reliance on LLMs-as-judges. To address these challenges, we propose SGSimEval, a comprehensive benchmark for Survey Generation with Similarity-Enhanced Evaluation that evaluates automatic survey generation systems by integrating assessments of the outline, content, and references, and also combines LLM-based scoring with quantitative metrics to provide a multifaceted evaluation framework. In SGSimEval, we also introduce human preference metrics that emphasize both inherent quality and similarity to humans. Extensive experiments reveal that current ASG systems demonstrate human-comparable superiority in outline generation, while showing significant room for improvement in content and reference generation, and our evaluation metrics maintain strong consistency with human assessments.",
    "authors": [
      "Beichen Guo",
      "Zhiyuan Wen",
      "Yu Yang",
      "Peng Gao",
      "Ruosong Yang",
      "Jiaxing Shen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-08-15T08:27:58Z",
    "pdf_url": "https://arxiv.org/pdf/2508.11310v1"
  },
  {
    "arxiv_id": "2508.10972v1",
    "entry_id": "http://arxiv.org/abs/2508.10972v1",
    "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision",
    "summary": "Advances in vision language models (VLMs) have enabled the simulation of general human behavior through their reasoning and problem solving capabilities. However, prior research has not investigated such simulation capabilities in the accessibility domain. In this paper, we evaluate the extent to which VLMs can simulate the vision perception of low vision individuals when interpreting images. We first compile a benchmark dataset through a survey study with 40 low vision participants, collecting their brief and detailed vision information and both open-ended and multiple-choice image perception and recognition responses to up to 25 images. Using these responses, we construct prompts for VLMs (GPT-4o) to create simulated agents of each participant, varying the included information on vision information and example image responses. We evaluate the agreement between VLM-generated responses and participants' original answers. Our results indicate that VLMs tend to infer beyond the specified vision ability when given minimal prompts, resulting in low agreement (0.59). The agreement between the agent' and participants' responses remains low when only either the vision information (0.59) or example image responses (0.59) are provided, whereas a combination of both significantly increase the agreement (0.70, p < 0.0001). Notably, a single example combining both open-ended and multiple-choice responses, offers significant performance improvements over either alone (p < 0.0001), while additional examples provided minimal benefits (p > 0.05).",
    "authors": [
      "Rosiana Natalie",
      "Wenqian Xu",
      "Ruei-Che Chang",
      "Rada Mihalcea",
      "Anhong Guo"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-08-14T16:46:03Z",
    "pdf_url": "https://arxiv.org/pdf/2508.10972v1"
  },
  {
    "arxiv_id": "2508.10745v1",
    "entry_id": "http://arxiv.org/abs/2508.10745v1",
    "title": "Agentic Design Review System",
    "summary": "Evaluating graphic designs involves assessing it from multiple facets like alignment, composition, aesthetics and color choices. Evaluating designs in a holistic way involves aggregating feedback from individual expert reviewers. Towards this, we propose an Agentic Design Review System (AgenticDRS), where multiple agents collaboratively analyze a design, orchestrated by a meta-agent. A novel in-context exemplar selection approach based on graph matching and a unique prompt expansion method plays central role towards making each agent design aware. Towards evaluating this framework, we propose DRS-BENCH benchmark. Thorough experimental evaluation against state-of-the-art baselines adapted to the problem setup, backed-up with critical ablation experiments brings out the efficacy of Agentic-DRS in evaluating graphic designs and generating actionable feedback. We hope that this work will attract attention to this pragmatic, yet under-explored research direction.",
    "authors": [
      "Sayan Nag",
      "K J Joseph",
      "Koustava Goswami",
      "Vlad I Morariu",
      "Balaji Vasan Srinivasan"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG",
      "cs.MA",
      "cs.MM"
    ],
    "published": "2025-08-14T15:29:24Z",
    "pdf_url": "https://arxiv.org/pdf/2508.10745v1"
  },
  {
    "arxiv_id": "2508.15804v1",
    "entry_id": "http://arxiv.org/abs/2508.15804v1",
    "title": "ReportBench: Evaluating Deep Research Agents via Academic Survey Tasks",
    "summary": "The advent of Deep Research agents has substantially reduced the time required for conducting extensive research tasks. However, these tasks inherently demand rigorous standards of factual accuracy and comprehensiveness, necessitating thorough evaluation before widespread adoption. In this paper, we propose ReportBench, a systematic benchmark designed to evaluate the content quality of research reports generated by large language models (LLMs). Our evaluation focuses on two critical dimensions: (1) the quality and relevance of cited literature, and (2) the faithfulness and veracity of the statements within the generated reports. ReportBench leverages high-quality published survey papers available on arXiv as gold-standard references, from which we apply reverse prompt engineering to derive domain-specific prompts and establish a comprehensive evaluation corpus. Furthermore, we develop an agent-based automated framework within ReportBench that systematically analyzes generated reports by extracting citations and statements, checking the faithfulness of cited content against original sources, and validating non-cited claims using web-based resources. Empirical evaluations demonstrate that commercial Deep Research agents such as those developed by OpenAI and Google consistently generate more comprehensive and reliable reports than standalone LLMs augmented with search or browsing tools. However, there remains substantial room for improvement in terms of the breadth and depth of research coverage, as well as factual consistency. The complete code and data will be released at the following link: https://github.com/ByteDance-BandAI/ReportBench",
    "authors": [
      "Minghao Li",
      "Ying Zeng",
      "Zhihao Cheng",
      "Cong Ma",
      "Kai Jia"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-14T03:33:43Z",
    "pdf_url": "https://arxiv.org/pdf/2508.15804v1"
  },
  {
    "arxiv_id": "2508.11707v1",
    "entry_id": "http://arxiv.org/abs/2508.11707v1",
    "title": "Listening with Language Models: Using LLMs to Collect and Interpret Classroom Feedback",
    "summary": "Traditional end-of-quarter surveys often fail to provide instructors with timely, detailed, and actionable feedback about their teaching. In this paper, we explore how Large Language Model (LLM)-powered chatbots can reimagine the classroom feedback process by engaging students in reflective, conversational dialogues. Through the design and deployment of a three-part system-PromptDesigner, FeedbackCollector, and FeedbackAnalyzer-we conducted a pilot study across two graduate courses at UC Santa Cruz. Our findings suggest that LLM-based feedback systems offer richer insights, greater contextual relevance, and higher engagement compared to standard survey tools. Instructors valued the system's adaptability, specificity, and ability to support mid-course adjustments, while students appreciated the conversational format and opportunity for elaboration. We conclude by discussing the design implications of using AI to facilitate more meaningful and responsive feedback in higher education.",
    "authors": [
      "Sai Siddartha Maram",
      "Ulia Zaman",
      "Magy Seif El-Nasr"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-08-13T22:53:55Z",
    "pdf_url": "https://arxiv.org/pdf/2508.11707v1"
  },
  {
    "arxiv_id": "2508.10146v1",
    "entry_id": "http://arxiv.org/abs/2508.10146v1",
    "title": "Agentic AI Frameworks: Architectures, Protocols, and Design Challenges",
    "summary": "The emergence of Large Language Models (LLMs) has ushered in a transformative paradigm in artificial intelligence, Agentic AI, where intelligent agents exhibit goal-directed autonomy, contextual reasoning, and dynamic multi-agent coordination. This paper provides a systematic review and comparative analysis of leading Agentic AI frameworks, including CrewAI, LangGraph, AutoGen, Semantic Kernel, Agno, Google ADK, and MetaGPT, evaluating their architectural principles, communication mechanisms, memory management, safety guardrails, and alignment with service-oriented computing paradigms. Furthermore, we identify key limitations, emerging trends, and open challenges in the field. To address the issue of agent communication, we conduct an in-depth analysis of protocols such as the Contract Net Protocol (CNP), Agent-to-Agent (A2A), Agent Network Protocol (ANP), and Agora. Our findings not only establish a foundational taxonomy for Agentic AI systems but also propose future research directions to enhance scalability, robustness, and interoperability. This work serves as a comprehensive reference for researchers and practitioners working to advance the next generation of autonomous AI systems.",
    "authors": [
      "Hana Derouiche",
      "Zaki Brahmi",
      "Haithem Mazeni"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-13T19:16:18Z",
    "pdf_url": "https://arxiv.org/pdf/2508.10146v1"
  },
  {
    "arxiv_id": "2508.09834v1",
    "entry_id": "http://arxiv.org/abs/2508.09834v1",
    "title": "Speed Always Wins: A Survey on Efficient Architectures for Large Language Models",
    "summary": "Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.",
    "authors": [
      "Weigao Sun",
      "Jiaxi Hu",
      "Yucheng Zhou",
      "Jusen Du",
      "Disen Lan",
      "Kexin Wang",
      "Tong Zhu",
      "Xiaoye Qu",
      "Yu Zhang",
      "Xiaoyu Mo",
      "Daizong Liu",
      "Yuxuan Liang",
      "Wenliang Chen",
      "Guoqi Li",
      "Yu Cheng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-08-13T14:13:46Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09834v1"
  },
  {
    "arxiv_id": "2508.09832v1",
    "entry_id": "http://arxiv.org/abs/2508.09832v1",
    "title": "Exploring the Potential of Large Language Models in Fine-Grained Review Comment Classification",
    "summary": "Code review is a crucial practice in software development. As code review nowadays is lightweight, various issues can be identified, and sometimes, they can be trivial. Research has investigated automated approaches to classify review comments to gauge the effectiveness of code reviews. However, previous studies have primarily relied on supervised machine learning, which requires extensive manual annotation to train the models effectively. To address this limitation, we explore the potential of using Large Language Models (LLMs) to classify code review comments. We assess the performance of LLMs to classify 17 categories of code review comments. Our results show that LLMs can classify code review comments, outperforming the state-of-the-art approach using a trained deep learning model. In particular, LLMs achieve better accuracy in classifying the five most useful categories, which the state-of-the-art approach struggles with due to low training examples. Rather than relying solely on a specific small training data distribution, our results show that LLMs provide balanced performance across high- and low-frequency categories. These results suggest that the LLMs could offer a scalable solution for code review analytics to improve the effectiveness of the code review process.",
    "authors": [
      "Linh Nguyen",
      "Chunhua Liu",
      "Hong Yi Lin",
      "Patanamon Thongtanunam"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-08-13T14:07:05Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09832v1"
  },
  {
    "arxiv_id": "2508.09809v2",
    "entry_id": "http://arxiv.org/abs/2508.09809v2",
    "title": "A Comprehensive Review of Datasets for Clinical Mental Health AI Systems",
    "summary": "Mental health disorders are rising worldwide. However, the availability of trained clinicians has not scaled proportionally, leaving many people without adequate or timely support. To bridge this gap, recent studies have shown the promise of Artificial Intelligence (AI) to assist mental health diagnosis, monitoring, and intervention. However, the development of efficient, reliable, and ethical AI to assist clinicians is heavily dependent on high-quality clinical training datasets. Despite growing interest in data curation for training clinical AI assistants, existing datasets largely remain scattered, under-documented, and often inaccessible, hindering the reproducibility, comparability, and generalizability of AI models developed for clinical mental health care. In this paper, we present the first comprehensive survey of clinical mental health datasets relevant to the training and development of AI-powered clinical assistants. We categorize these datasets by mental disorders (e.g., depression, schizophrenia), data modalities (e.g., text, speech, physiological signals), task types (e.g., diagnosis prediction, symptom severity estimation, intervention generation), accessibility (public, restricted or private), and sociocultural context (e.g., language and cultural background). Along with these, we also investigate synthetic clinical mental health datasets. Our survey identifies critical gaps such as a lack of longitudinal data, limited cultural and linguistic representation, inconsistent collection and annotation standards, and a lack of modalities in synthetic data. We conclude by outlining key challenges in curating and standardizing future datasets and provide actionable recommendations to facilitate the development of more robust, generalizable, and equitable mental health AI systems.",
    "authors": [
      "Aishik Mandal",
      "Prottay Kumar Adhikary",
      "Hiba Arnaout",
      "Iryna Gurevych",
      "Tanmoy Chakraborty"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-13T13:42:35Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09809v2"
  },
  {
    "arxiv_id": "2508.09713v1",
    "entry_id": "http://arxiv.org/abs/2508.09713v1",
    "title": "Evaluating the Role of Large Language Models in Legal Practice in India",
    "summary": "The integration of Artificial Intelligence(AI) into the legal profession raises significant questions about the capacity of Large Language Models(LLM) to perform key legal tasks. In this paper, I empirically evaluate how well LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian context, including issue spotting, legal drafting, advice, research, and reasoning. Through a survey experiment, I compare outputs from LLMs with those of a junior lawyer, with advanced law students rating the work on helpfulness, accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting, often matching or surpassing human work. However, they struggle with specialised legal research, frequently generating hallucinations, factually incorrect or fabricated outputs. I conclude that while LLMs can augment certain legal tasks, human expertise remains essential for nuanced reasoning and the precise application of law.",
    "authors": [
      "Rahul Hemrajani"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-13T11:04:48Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09713v1"
  },
  {
    "arxiv_id": "2508.09614v1",
    "entry_id": "http://arxiv.org/abs/2508.09614v1",
    "title": "How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments",
    "summary": "This study examines the rhetorical and linguistic features of argumentative texts generated by ChatGPT on ethically nuanced topics and investigates their persuasive impact on human readers.Through a user study involving 62 participants and pre-post interaction surveys, the paper analyzes how exposure to AI-generated arguments affects opinion change and user perception. A linguistic and rhetorical analysis of the generated texts reveals a consistent argumentative macrostructure, reliance on formulaic expressions, and limited stylistic richness. While ChatGPT demonstrates proficiency in constructing coherent argumentative texts, its persuasive efficacy appears constrained, particularly on topics involving ethical issues.The study finds that while participants often acknowledge the benefits highlighted by ChatGPT, ethical concerns tend to persist or even intensify post-interaction. The results also demonstrate a variation depending on the topic. These findings highlight new insights on AI-generated persuasion in ethically sensitive domains and are a basis for future research.",
    "authors": [
      "Daniel Raffini",
      "Agnese Macori",
      "Lorenzo Porcaro",
      "Tiziana Catarci",
      "Marco Angelini"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-08-13T08:45:04Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09614v1"
  },
  {
    "arxiv_id": "2508.09561v1",
    "entry_id": "http://arxiv.org/abs/2508.09561v1",
    "title": "Edge General Intelligence Through World Models and Agentic AI: Fundamentals, Solutions, and Challenges",
    "summary": "Edge General Intelligence (EGI) represents a transformative evolution of edge computing, where distributed agents possess the capability to perceive, reason, and act autonomously across diverse, dynamic environments. Central to this vision are world models, which act as proactive internal simulators that not only predict but also actively imagine future trajectories, reason under uncertainty, and plan multi-step actions with foresight. This proactive nature allows agents to anticipate potential outcomes and optimize decisions ahead of real-world interactions. While prior works in robotics and gaming have showcased the potential of world models, their integration into the wireless edge for EGI remains underexplored. This survey bridges this gap by offering a comprehensive analysis of how world models can empower agentic artificial intelligence (AI) systems at the edge. We first examine the architectural foundations of world models, including latent representation learning, dynamics modeling, and imagination-based planning. Building on these core capabilities, we illustrate their proactive applications across EGI scenarios such as vehicular networks, unmanned aerial vehicle (UAV) networks, the Internet of Things (IoT) systems, and network functions virtualization, thereby highlighting how they can enhance optimization under latency, energy, and privacy constraints. We then explore their synergy with foundation models and digital twins, positioning world models as the cognitive backbone of EGI. Finally, we highlight open challenges, such as safety guarantees, efficient training, and constrained deployment, and outline future research directions. This survey provides both a conceptual foundation and a practical roadmap for realizing the next generation of intelligent, autonomous edge systems.",
    "authors": [
      "Changyuan Zhao",
      "Guangyuan Liu",
      "Ruichen Zhang",
      "Yinqiu Liu",
      "Jiacheng Wang",
      "Jiawen Kang",
      "Dusit Niyato",
      "Zan Li",
      "Xuemin",
      "Shen",
      "Zhu Han",
      "Sumei Sun",
      "Chau Yuen",
      "Dong In Kim"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-13T07:29:40Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09561v1"
  },
  {
    "arxiv_id": "2508.09458v2",
    "entry_id": "http://arxiv.org/abs/2508.09458v2",
    "title": "Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis",
    "summary": "Knowledge syntheses (literature reviews) are essential to health professions education (HPE), consolidating findings to advance theory and practice. However, they are labor-intensive, especially during data extraction. Artificial Intelligence (AI)-assisted extraction promises efficiency but raises concerns about accuracy, making it critical to distinguish AI 'hallucinations' (fabricated content) from legitimate interpretive differences. We developed an extraction platform using large language models (LLMs) to automate data extraction and compared AI to human responses across 187 publications and 17 extraction questions from a published scoping review. AI-human, human-human, and AI-AI consistencies were measured using interrater reliability (categorical) and thematic similarity ratings (open-ended). Errors were identified by comparing extracted responses to source publications. AI was highly consistent with humans for concrete, explicitly stated questions (e.g., title, aims) and lower for questions requiring subjective interpretation or absent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human consistency was not higher than AI-human and showed the same question-dependent variability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due to interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while humans were nearly three times more likely to state inaccuracies (4.37%). Findings suggest AI variability depends more on interpretability than hallucination. Repeating AI extraction can identify interpretive complexity or ambiguity, refining processes before human review. AI can be a transparent, trustworthy partner in knowledge synthesis, though caution is needed to preserve critical human insights.",
    "authors": [
      "Xi Long",
      "Christy Boscardin",
      "Lauren A. Maggio",
      "Joseph A. Costello",
      "Ralph Gonzales",
      "Rasmyah Hammoudeh",
      "Ki Lai",
      "Yoon Soo Park",
      "Brian C. Gin"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.ET"
    ],
    "published": "2025-08-13T03:33:30Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09458v2"
  },
  {
    "arxiv_id": "2508.09332v1",
    "entry_id": "http://arxiv.org/abs/2508.09332v1",
    "title": "Teaching Code Refactoring Using LLMs",
    "summary": "This Innovative Practice full paper explores how Large Language Models (LLMs) can enhance the teaching of code refactoring in software engineering courses through real-time, context-aware feedback. Refactoring improves code quality but is difficult to teach, especially with complex, real-world codebases. Traditional methods like code reviews and static analysis tools offer limited, inconsistent feedback. Our approach integrates LLM-assisted refactoring into a course project using structured prompts to help students identify and address code smells such as long methods and low cohesion. Implemented in Spring 2025 in a long-lived OSS project, the intervention is evaluated through student feedback and planned analysis of code quality improvements. Findings suggest that LLMs can bridge theoretical and practical learning, supporting a deeper understanding of maintainability and refactoring principles.",
    "authors": [
      "Anshul Khairnar",
      "Aarya Rajoju",
      "Edward F. Gehringer"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2025-08-12T20:41:19Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09332v1"
  },
  {
    "arxiv_id": "2508.09016v4",
    "entry_id": "http://arxiv.org/abs/2508.09016v4",
    "title": "A Survey on Training-free Alignment of Large Language Models",
    "summary": "The alignment of large language models (LLMs) aims to ensure their outputs adhere to human values, ethical standards, and legal norms. Traditional alignment methods often rely on resource-intensive fine-tuning (FT), which may suffer from knowledge degradation and face challenges in scenarios where the model accessibility or computational resources are constrained. In contrast, training-free (TF) alignment techniques--leveraging in-context learning, decoding-time adjustments, and post-generation corrections--offer a promising alternative by enabling alignment without heavily retraining LLMs, making them adaptable to both open-source and closed-source environments. This paper presents the first systematic review of TF alignment methods, categorizing them by stages of pre-decoding, in-decoding, and post-decoding. For each stage, we provide a detailed examination from the viewpoint of LLMs and multimodal LLMs (MLLMs), highlighting their mechanisms and limitations. Furthermore, we identify key challenges and future directions, paving the way for more inclusive and effective TF alignment techniques. By synthesizing and organizing the rapidly growing body of research, this survey offers a guidance for practitioners and advances the development of safer and more reliable LLMs.",
    "authors": [
      "Birong Pan",
      "Yongqi Li",
      "Weiyu Zhang",
      "Wenpeng Lu",
      "Mayi Xu",
      "Shen Zhou",
      "Yuanyuan Zhu",
      "Ming Zhong",
      "Tieyun Qian"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-08-12T15:30:44Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09016v4"
  },
  {
    "arxiv_id": "2508.08795v1",
    "entry_id": "http://arxiv.org/abs/2508.08795v1",
    "title": "A Dual-Axis Taxonomy of Knowledge Editing for LLMs: From Mechanisms to Functions",
    "summary": "Large language models (LLMs) acquire vast knowledge from large text corpora, but this information can become outdated or inaccurate. Since retraining is computationally expensive, knowledge editing offers an efficient alternative -- modifying internal knowledge without full retraining. These methods aim to update facts precisely while preserving the model's overall capabilities. While existing surveys focus on the mechanism of editing (e.g., parameter changes vs. external memory), they often overlook the function of the knowledge being edited. This survey introduces a novel, complementary function-based taxonomy to provide a more holistic view. We examine how different mechanisms apply to various knowledge types -- factual, temporal, conceptual, commonsense, and social -- highlighting how editing effectiveness depends on the nature of the target knowledge. By organizing our review along these two axes, we map the current landscape, outline the strengths and limitations of existing methods, define the problem formally, survey evaluation tasks and datasets, and conclude with open challenges and future directions.",
    "authors": [
      "Amir Mohammad Salehoof",
      "Ali Ramezani",
      "Yadollah Yaghoobzadeh",
      "Majid Nili Ahmadabadi"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-12T09:51:39Z",
    "pdf_url": "https://arxiv.org/pdf/2508.08795v1"
  },
  {
    "arxiv_id": "2508.08712v3",
    "entry_id": "http://arxiv.org/abs/2508.08712v3",
    "title": "A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models",
    "summary": "As text generation has become a core capability of modern Large Language Models (LLMs), it underpins a wide range of downstream applications. However, most existing LLMs rely on autoregressive (AR) generation, producing one token at a time based on previously generated context-resulting in limited generation speed due to the inherently sequential nature of the process. To address this challenge, an increasing number of researchers have begun exploring parallel text generation-a broad class of techniques aimed at breaking the token-by-token generation bottleneck and improving inference efficiency. Despite growing interest, there remains a lack of comprehensive analysis on what specific techniques constitute parallel text generation and how they improve inference performance. To bridge this gap, we present a systematic survey of parallel text generation methods. We categorize existing approaches into AR-based and Non-AR-based paradigms, and provide a detailed examination of the core techniques within each category. Following this taxonomy, we assess their theoretical trade-offs in terms of speed, quality, and efficiency, and examine their potential for combination and comparison with alternative acceleration strategies. Finally, based on our findings, we highlight recent advancements, identify open challenges, and outline promising directions for future research in parallel text generation. We have also created a GitHub repository for indexing relevant papers and open resources available at https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.",
    "authors": [
      "Lingzhe Zhang",
      "Liancheng Fang",
      "Chiming Duan",
      "Minghua He",
      "Leyi Pan",
      "Pei Xiao",
      "Shiyu Huang",
      "Yunpeng Zhai",
      "Xuming Hu",
      "Philip S. Yu",
      "Aiwei Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2025-08-12T07:56:04Z",
    "pdf_url": "https://arxiv.org/pdf/2508.08712v3"
  },
  {
    "arxiv_id": "2508.10047v1",
    "entry_id": "http://arxiv.org/abs/2508.10047v1",
    "title": "A Survey of Optimization Modeling Meets LLMs: Progress and Future Directions",
    "summary": "By virtue of its great utility in solving real-world problems, optimization modeling has been widely employed for optimal decision-making across various sectors, but it requires substantial expertise from operations research professionals. With the advent of large language models (LLMs), new opportunities have emerged to automate the procedure of mathematical modeling. This survey presents a comprehensive and timely review of recent advancements that cover the entire technical stack, including data synthesis and fine-tuning for the base model, inference frameworks, benchmark datasets, and performance evaluation. In addition, we conducted an in-depth analysis on the quality of benchmark datasets, which was found to have a surprisingly high error rate. We cleaned the datasets and constructed a new leaderboard with fair performance evaluation in terms of base LLM model and datasets. We also build an online portal that integrates resources of cleaned datasets, code and paper repository to benefit the community. Finally, we identify limitations in current methodologies and outline future research opportunities.",
    "authors": [
      "Ziyang Xiao",
      "Jingrong Xie",
      "Lilin Xu",
      "Shisi Guan",
      "Jingyan Zhu",
      "Xiongwei Han",
      "Xiaojin Fu",
      "WingYin Yu",
      "Han Wu",
      "Wei Shi",
      "Qingcan Kang",
      "Jiahui Duan",
      "Tao Zhong",
      "Mingxuan Yuan",
      "Jia Zeng",
      "Yuan Wang",
      "Gang Chen",
      "Dongxiang Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-12T06:55:33Z",
    "pdf_url": "https://arxiv.org/pdf/2508.10047v1"
  },
  {
    "arxiv_id": "2508.08661v1",
    "entry_id": "http://arxiv.org/abs/2508.08661v1",
    "title": "Hallucinations in Code Change to Natural Language Generation: Prevalence and Evaluation of Detection Metrics",
    "summary": "Language models have shown strong capabilities across a wide range of tasks in software engineering, such as code generation, yet they suffer from hallucinations. While hallucinations have been studied independently in natural language and code generation, their occurrence in tasks involving code changes which have a structurally complex and context-dependent format of code remains largely unexplored. This paper presents the first comprehensive analysis of hallucinations in two critical tasks involving code change to natural language generation: commit message generation and code review comment generation. We quantify the prevalence of hallucinations in recent language models and explore a range of metric-based approaches to automatically detect them. Our findings reveal that approximately 50\\% of generated code reviews and 20\\% of generated commit messages contain hallucinations. Whilst commonly used metrics are weak detectors on their own, combining multiple metrics substantially improves performance. Notably, model confidence and feature attribution metrics effectively contribute to hallucination detection, showing promise for inference-time detection.\\footnote{All code and data will be released upon acceptance.",
    "authors": [
      "Chunhua Liu",
      "Hong Yi Lin",
      "Patanamon Thongtanunam"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-08-12T05:59:33Z",
    "pdf_url": "https://arxiv.org/pdf/2508.08661v1"
  },
  {
    "arxiv_id": "2508.08591v1",
    "entry_id": "http://arxiv.org/abs/2508.08591v1",
    "title": "DepressLLM: Interpretable domain-adapted language model for depression detection from real-world narratives",
    "summary": "Advances in large language models (LLMs) have enabled a wide range of applications. However, depression prediction is hindered by the lack of large-scale, high-quality, and rigorously annotated datasets. This study introduces DepressLLM, trained and evaluated on a novel corpus of 3,699 autobiographical narratives reflecting both happiness and distress. DepressLLM provides interpretable depression predictions and, via its Score-guided Token Probability Summation (SToPS) module, delivers both improved classification performance and reliable confidence estimates, achieving an AUC of 0.789, which rises to 0.904 on samples with confidence $\\geq$ 0.95. To validate its robustness to heterogeneous data, we evaluated DepressLLM on in-house datasets, including an Ecological Momentary Assessment (EMA) corpus of daily stress and mood recordings, and on public clinical interview data. Finally, a psychiatric review of high-confidence misclassifications highlighted key model and data limitations that suggest directions for future refinements. These findings demonstrate that interpretable AI can enable earlier diagnosis of depression and underscore the promise of medical AI in psychiatry.",
    "authors": [
      "Sehwan Moon",
      "Aram Lee",
      "Jeong Eun Kim",
      "Hee-Ju Kang",
      "Il-Seon Shin",
      "Sung-Wan Kim",
      "Jae-Min Kim",
      "Min Jhon",
      "Ju-Wan Kim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-12T03:12:55Z",
    "pdf_url": "https://arxiv.org/pdf/2508.08591v1"
  },
  {
    "arxiv_id": "2508.08535v2",
    "entry_id": "http://arxiv.org/abs/2508.08535v2",
    "title": "LLM-Driven Adaptive 6G-Ready Wireless Body Area Networks: Survey and Framework",
    "summary": "Wireless Body Area Networks (WBANs) enable continuous monitoring of physiological signals for applications ranging from chronic disease management to emergency response. Recent advances in 6G communications, post-quantum cryptography, and energy harvesting have the potential to enhance WBAN performance. However, integrating these technologies into a unified, adaptive system remains a challenge. This paper surveys some of the most well-known Wireless Body Area Network (WBAN) architectures, routing strategies, and security mechanisms, identifying key gaps in adaptability, energy efficiency, and quantum-resistant security. We propose a novel Large Language Model-driven adaptive WBAN framework in which a Large Language Model acts as a cognitive control plane, coordinating routing, physical layer selection, micro-energy harvesting, and post-quantum security in real time. Our review highlights the limitations of current heuristic-based designs and outlines a research agenda for resource-constrained, 6G-ready medical systems. This approach aims to enable ultra-reliable, secure, and self-optimizing WBANs for next-generation mobile health applications.",
    "authors": [
      "Mohammad Jalili Torkamani",
      "Negin Mahmoudi",
      "Kiana Kiashemshaki"
    ],
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "published": "2025-08-12T00:25:41Z",
    "pdf_url": "https://arxiv.org/pdf/2508.08535v2"
  },
  {
    "arxiv_id": "2508.08221v3",
    "entry_id": "http://arxiv.org/abs/2508.08221v3",
    "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
    "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.",
    "authors": [
      "Zihe Liu",
      "Jiashun Liu",
      "Yancheng He",
      "Weixun Wang",
      "Jiaheng Liu",
      "Ling Pan",
      "Xinyu Hu",
      "Shaopan Xiong",
      "Ju Huang",
      "Jian Hu",
      "Shengyi Huang",
      "Johan Obando-Ceron",
      "Siran Yang",
      "Jiamang Wang",
      "Wenbo Su",
      "Bo Zheng"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-08-11T17:39:45Z",
    "pdf_url": "https://arxiv.org/pdf/2508.08221v3"
  },
  {
    "arxiv_id": "2508.08137v1",
    "entry_id": "http://arxiv.org/abs/2508.08137v1",
    "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation",
    "summary": "Conducting a comprehensive literature review is crucial for advancing circuit design methodologies. However, the rapid influx of state-of-the-art research, inconsistent data representation, and the complexity of optimizing circuit design objectives make this task significantly challenging. In this paper, we propose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for circuit design assistance that integrates a hybrid Retrieval-Augmented Generation (RAG) framework with an adaptive vector database of circuit design research papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason + Act (ReAct) workflow for iterative reasoning, goal-setting, and multi-step information retrieval. It functions as a question-answering design assistant, capable of interpreting complex queries and providing reasoned responses grounded in circuit literature. Its multimodal capabilities enable processing of both textual and visual data, facilitating more efficient and comprehensive analysis. The system dynamically adapts using intelligent search tools, automated document retrieval from the internet, and real-time database updates. Unlike conventional approaches constrained by model context limits, MuaLLM decouples retrieval from inference, enabling scalable reasoning over arbitrarily large corpora. At the maximum context length supported by standard LLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining the same accuracy. This allows rapid, no-human-in-the-loop database generation, overcoming the bottleneck of simulation-based dataset creation for circuits. To evaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval and citation performance, and Reasoning-100 (Reas-100), focused on multistep reasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8% accuracy on Reas-100.",
    "authors": [
      "Pravallika Abbineni",
      "Saoud Aldowaish",
      "Colin Liechty",
      "Soroosh Noorzad",
      "Ali Ghazizadeh",
      "Morteza Fayazi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "eess.SY"
    ],
    "published": "2025-08-11T16:11:09Z",
    "pdf_url": "https://arxiv.org/pdf/2508.08137v1"
  },
  {
    "arxiv_id": "2508.07966v1",
    "entry_id": "http://arxiv.org/abs/2508.07966v1",
    "title": "Exploring the Challenges and Opportunities of AI-assisted Codebase Generation",
    "summary": "Recent AI code assistants have significantly improved their ability to process more complex contexts and generate entire codebases based on a textual description, compared to the popular snippet-level generation. These codebase AI assistants (CBAs) can also extend or adapt codebases, allowing users to focus on higher-level design and deployment decisions. While prior work has extensively studied the impact of snippet-level code generation, this new class of codebase generation models is relatively unexplored. Despite initial anecdotal reports of excitement about these agents, they remain less frequently adopted compared to snippet-level code assistants. To utilize CBAs better, we need to understand how developers interact with CBAs, and how and why CBAs fall short of developers' needs. In this paper, we explored these gaps through a counterbalanced user study and interview with (n = 16) students and developers working on coding tasks with CBAs. We found that participants varied the information in their prompts, like problem description (48% of prompts), required functionality (98% of prompts), code structure (48% of prompts), and their prompt writing process. Despite various strategies, the overall satisfaction score with generated codebases remained low (mean = 2.8, median = 3, on a scale of one to five). Participants mentioned functionality as the most common factor for dissatisfaction (77% of instances), alongside poor code quality (42% of instances) and communication issues (25% of instances). We delve deeper into participants' dissatisfaction to identify six underlying challenges that participants faced when using CBAs, and extracted five barriers to incorporating CBAs into their workflows. Finally, we surveyed 21 commercial CBAs to compare their capabilities with participant challenges and present design opportunities for more efficient and useful CBAs.",
    "authors": [
      "Philipp Eibl",
      "Sadra Sabouri",
      "Souti Chattopadhyay"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-08-11T13:26:48Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07966v1"
  },
  {
    "arxiv_id": "2508.07887v1",
    "entry_id": "http://arxiv.org/abs/2508.07887v1",
    "title": "Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant",
    "summary": "Simulators have revolutionized scientific practice across the natural sciences. By generating data that reliably approximate real-world phenomena, they enable scientists to accelerate hypothesis testing and optimize experimental designs. This is perhaps best illustrated by AlphaFold, a Nobel-prize winning simulator in chemistry that predicts protein structures from amino acid sequences, enabling rapid prototyping of molecular interactions, drug targets, and protein functions. In the behavioral sciences, a reliable participant simulator - a system capable of producing human-like behavior across cognitive tasks - would represent a similarly transformative advance. Recently, Binz et al. introduced Centaur, a large language model (LLM) fine-tuned on human data from 160 experiments, proposing its use not only as a model of cognition but also as a participant simulator for \"in silico prototyping of experimental studies\", e.g., to advance automated cognitive science. Here, we review the core criteria for a participant simulator and assess how well Centaur meets them. Although Centaur demonstrates strong predictive accuracy, its generative behavior - a critical criterion for a participant simulator - systematically diverges from human data. This suggests that, while Centaur is a significant step toward predicting human behavior, it does not yet meet the standards of a reliable participant simulator or an accurate model of cognition.",
    "authors": [
      "Sabrina Namazova",
      "Alessandra Brondetta",
      "Younes Strittmatter",
      "Matthew Nassar",
      "Sebastian Musslick"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-08-11T12:05:18Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07887v1"
  },
  {
    "arxiv_id": "2508.07880v1",
    "entry_id": "http://arxiv.org/abs/2508.07880v1",
    "title": "Multi-agent systems for chemical engineering: A review and perspective",
    "summary": "Large language model (LLM)-based multi-agent systems (MASs) are a recent but rapidly evolving technology with the potential to transform chemical engineering by decomposing complex workflows into teams of collaborative agents with specialized knowledge and tools. This review surveys the state-of-the-art of MAS within chemical engineering. While early studies demonstrate promising results, scientific challenges remain, including the design of tailored architectures, integration of heterogeneous data modalities, development of foundation models with domain-specific modalities, and strategies for ensuring transparency, safety, and environmental impact. As a young but fast-moving field, MASs offer exciting opportunities to rethink chemical engineering workflows.",
    "authors": [
      "Sophia Rupprecht",
      "Qinghe Gao",
      "Tanuj Karia",
      "Artur M. Schweidtmann"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-08-11T11:55:18Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07880v1"
  },
  {
    "arxiv_id": "2508.07746v1",
    "entry_id": "http://arxiv.org/abs/2508.07746v1",
    "title": "A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory",
    "summary": "Offline reinforcement learning (RL) aims to optimize the return given a fixed dataset of agent trajectories without additional interactions with the environment. While algorithm development has progressed rapidly, significant theoretical advances have also been made in understanding the fundamental challenges of offline RL. However, bridging these theoretical insights with practical algorithm design remains an ongoing challenge. In this survey, we explore key intuitions derived from theoretical work and their implications for offline RL algorithms.\n  We begin by listing the conditions needed for the proofs, including function representation and data coverage assumptions. Function representation conditions tell us what to expect for generalization, and data coverage assumptions describe the quality requirement of the data. We then examine counterexamples, where offline RL is not solvable without an impractically large amount of data. These cases highlight what cannot be achieved for all algorithms and the inherent hardness of offline RL. Building on techniques to mitigate these challenges, we discuss the conditions that are sufficient for offline RL. These conditions are not merely assumptions for theoretical proofs, but they also reveal the limitations of these algorithms and remind us to search for novel solutions when the conditions cannot be satisfied.",
    "authors": [
      "Fengdi Che"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-08-11T08:26:28Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07746v1"
  },
  {
    "arxiv_id": "2508.07720v1",
    "entry_id": "http://arxiv.org/abs/2508.07720v1",
    "title": "Toward Goal-Oriented Communication in Multi-Agent Systems: An overview",
    "summary": "As multi-agent systems (MAS) become increasingly prevalent in autonomous systems, distributed control, and edge intelligence, efficient communication under resource constraints has emerged as a critical challenge. Traditional communication paradigms often emphasize message fidelity or bandwidth optimization, overlooking the task relevance of the exchanged information. In contrast, goal-oriented communication prioritizes the importance of information with respect to the agents' shared objectives. This review provides a comprehensive survey of goal-oriented communication in MAS, bridging perspectives from information theory, communication theory, and machine learning. We examine foundational concepts alongside learning-based approaches and emergent protocols. Special attention is given to coordination under communication constraints, as well as applications in domains such as swarm robotics, federated learning, and edge computing. The paper concludes with a discussion of open challenges and future research directions at the intersection of communication theory, machine learning, and multi-agent decision making.",
    "authors": [
      "Themistoklis Charalambous",
      "Nikolaos Pappas",
      "Nikolaos Nomikos",
      "Risto Wichman"
    ],
    "categories": [
      "cs.MA",
      "eess.SY"
    ],
    "published": "2025-08-11T07:46:55Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07720v1"
  },
  {
    "arxiv_id": "2508.07497v1",
    "entry_id": "http://arxiv.org/abs/2508.07497v1",
    "title": "VA-Blueprint: Uncovering Building Blocks for Visual Analytics System Design",
    "summary": "Designing and building visual analytics (VA) systems is a complex, iterative process that requires the seamless integration of data processing, analytics capabilities, and visualization techniques. While prior research has extensively examined the social and collaborative aspects of VA system authoring, the practical challenges of developing these systems remain underexplored. As a result, despite the growing number of VA systems, there are only a few structured knowledge bases to guide their design and development. To tackle this gap, we propose VA-Blueprint, a methodology and knowledge base that systematically reviews and categorizes the fundamental building blocks of urban VA systems, a domain particularly rich and representative due to its intricate data and unique problem sets. Applying this methodology to an initial set of 20 systems, we identify and organize their core components into a multi-level structure, forming an initial knowledge base with a structured blueprint for VA system development. To scale this effort, we leverage a large language model to automate the extraction of these components for other 81 papers (completing a corpus of 101 papers), assessing its effectiveness in scaling knowledge base construction. We evaluate our method through interviews with experts and a quantitative analysis of annotation metrics. Our contributions provide a deeper understanding of VA systems' composition and establish a practical foundation to support more structured, reproducible, and efficient system development. VA-Blueprint is available at https://urbantk.org/va-blueprint.",
    "authors": [
      "Leonardo Ferreira",
      "Gustavo Moreira",
      "Fabio Miranda"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-08-10T22:03:11Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07497v1"
  },
  {
    "arxiv_id": "2508.07407v2",
    "entry_id": "http://arxiv.org/abs/2508.07407v2",
    "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems",
    "summary": "Recent advances in large language models have sparked growing interest in AI agents capable of solving complex, real-world tasks. However, most existing agent systems rely on manually crafted configurations that remain static after deployment, limiting their ability to adapt to dynamic and evolving environments. To this end, recent research has explored agent evolution techniques that aim to automatically enhance agent systems based on interaction data and environmental feedback. This emerging direction lays the foundation for self-evolving AI agents, which bridge the static capabilities of foundation models with the continuous adaptability required by lifelong agentic systems. In this survey, we provide a comprehensive review of existing techniques for self-evolving agentic systems. Specifically, we first introduce a unified conceptual framework that abstracts the feedback loop underlying the design of self-evolving agentic systems. The framework highlights four key components: System Inputs, Agent System, Environment, and Optimisers, serving as a foundation for understanding and comparing different strategies. Based on this framework, we systematically review a wide range of self-evolving techniques that target different components of the agent system. We also investigate domain-specific evolution strategies developed for specialised fields such as biomedicine, programming, and finance, where optimisation objectives are tightly coupled with domain constraints. In addition, we provide a dedicated discussion on the evaluation, safety, and ethical considerations for self-evolving agentic systems, which are critical to ensuring their effectiveness and reliability. This survey aims to provide researchers and practitioners with a systematic understanding of self-evolving AI agents, laying the foundation for the development of more adaptive, autonomous, and lifelong agentic systems.",
    "authors": [
      "Jinyuan Fang",
      "Yanwen Peng",
      "Xi Zhang",
      "Yingxu Wang",
      "Xinhao Yi",
      "Guibin Zhang",
      "Yi Xu",
      "Bin Wu",
      "Siwei Liu",
      "Zihao Li",
      "Zhaochun Ren",
      "Nikos Aletras",
      "Xi Wang",
      "Han Zhou",
      "Zaiqiao Meng"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-08-10T16:07:32Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07407v2"
  },
  {
    "arxiv_id": "2508.07390v1",
    "entry_id": "http://arxiv.org/abs/2508.07390v1",
    "title": "Urbanite: A Dataflow-Based Framework for Human-AI Interactive Alignment in Urban Visual Analytics",
    "summary": "With the growing availability of urban data and the increasing complexity of societal challenges, visual analytics has become essential for deriving insights into pressing real-world problems. However, analyzing such data is inherently complex and iterative, requiring expertise across multiple domains. The need to manage diverse datasets, distill intricate workflows, and integrate various analytical methods presents a high barrier to entry, especially for researchers and urban experts who lack proficiency in data management, machine learning, and visualization. Advancements in large language models offer a promising solution to lower the barriers to the construction of analytics systems by enabling users to specify intent rather than define precise computational operations. However, this shift from explicit operations to intent-based interaction introduces challenges in ensuring alignment throughout the design and development process. Without proper mechanisms, gaps can emerge between user intent, system behavior, and analytical outcomes. To address these challenges, we propose Urbanite, a framework for human-AI collaboration in urban visual analytics. Urbanite leverages a dataflow-based model that allows users to specify intent at multiple scopes, enabling interactive alignment across the specification, process, and evaluation stages of urban analytics. Based on findings from a survey to uncover challenges, Urbanite incorporates features to facilitate explainability, multi-resolution definition of tasks across dataflows, nodes, and parameters, while supporting the provenance of interactions. We demonstrate Urbanite's effectiveness through usage scenarios created in collaboration with urban experts. Urbanite is available at https://urbantk.org/urbanite.",
    "authors": [
      "Gustavo Moreira",
      "Leonardo Ferreira",
      "Carolina Veiga",
      "Maryam Hosseini",
      "Fabio Miranda"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-08-10T15:44:37Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07390v1"
  },
  {
    "arxiv_id": "2508.07343v1",
    "entry_id": "http://arxiv.org/abs/2508.07343v1",
    "title": "A Survey on Agentic Service Ecosystems: Measurement, Analysis, and Optimization",
    "summary": "The Agentic Service Ecosystem consists of heterogeneous autonomous agents (e.g., intelligent machines, humans, and human-machine hybrid systems) that interact through resource exchange and service co-creation. These agents, with distinct behaviors and motivations, exhibit autonomous perception, reasoning, and action capabilities, which increase system complexity and make traditional linear analysis methods inadequate. Swarm intelligence, characterized by decentralization, self-organization, emergence, and dynamic adaptability, offers a novel theoretical lens and methodology for understanding and optimizing such ecosystems. However, current research, owing to fragmented perspectives and cross-ecosystem differences, fails to comprehensively capture the complexity of swarm-intelligence emergence in agentic contexts. The lack of a unified methodology further limits the depth and systematic treatment of the research. This paper proposes a framework for analyzing the emergence of swarm intelligence in Agentic Service Ecosystems, with three steps: measurement, analysis, and optimization, to reveal the cyclical mechanisms and quantitative criteria that foster emergence. By reviewing existing technologies, the paper analyzes their strengths and limitations, identifies unresolved challenges, and shows how this framework provides both theoretical support and actionable methods for real-world applications.",
    "authors": [
      "Xuwen Zhang",
      "Xiao Xue",
      "Xia Xie",
      "Qun Ma",
      "Xiangning Yu",
      "Deyu Zhou",
      "Yifan Wang",
      "Ming Zhang"
    ],
    "categories": [
      "cs.MA",
      "cs.SI"
    ],
    "published": "2025-08-10T13:37:35Z",
    "pdf_url": "https://arxiv.org/pdf/2508.07343v1"
  },
  {
    "arxiv_id": "2508.06832v1",
    "entry_id": "http://arxiv.org/abs/2508.06832v1",
    "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges",
    "summary": "The mainstream paradigm of remote sensing image interpretation has long been dominated by vision-centered models, which rely on visual features for semantic understanding. However, these models face inherent limitations in handling multi-modal reasoning, semantic abstraction, and interactive decision-making. While recent advances have introduced Large Language Models (LLMs) into remote sensing workflows, existing studies primarily focus on downstream applications, lacking a unified theoretical framework that explains the cognitive role of language. This review advocates a paradigm shift from vision-centered to language-centered remote sensing interpretation. Drawing inspiration from the Global Workspace Theory (GWT) of human cognition, We propose a language-centered framework for remote sensing interpretation that treats LLMs as the cognitive central hub integrating perceptual, task, knowledge and action spaces to enable unified understanding, reasoning, and decision-making. We first explore the potential of LLMs as the central cognitive component in remote sensing interpretation, and then summarize core technical challenges, including unified multimodal representation, knowledge association, and reasoning and decision-making. Furthermore, we construct a global workspace-driven interpretation mechanism and review how language-centered solutions address each challenge. Finally, we outline future research directions from four perspectives: adaptive alignment of multimodal data, task understanding under dynamic knowledge constraints, trustworthy reasoning, and autonomous interaction. This work aims to provide a conceptual foundation for the next generation of remote sensing interpretation systems and establish a roadmap toward cognition-driven intelligent geospatial analysis.",
    "authors": [
      "Haifeng Li",
      "Wang Guo",
      "Haiyang Wu",
      "Mengwei Wu",
      "Jipeng Zhang",
      "Qing Zhu",
      "Yu Liu",
      "Xin Huang",
      "Chao Tao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-09T05:10:38Z",
    "pdf_url": "https://arxiv.org/pdf/2508.06832v1"
  },
  {
    "arxiv_id": "2508.06732v1",
    "entry_id": "http://arxiv.org/abs/2508.06732v1",
    "title": "ClimateSOM: A Visual Analysis Workflow for Climate Ensemble Datasets",
    "summary": "Ensemble datasets are ever more prevalent in various scientific domains. In climate science, ensemble datasets are used to capture variability in projections under plausible future conditions including greenhouse and aerosol emissions. Each ensemble model run produces projections that are fundamentally similar yet meaningfully distinct. Understanding this variability among ensemble model runs and analyzing its magnitude and patterns is a vital task for climate scientists. In this paper, we present ClimateSOM, a visual analysis workflow that leverages a self-organizing map (SOM) and Large Language Models (LLMs) to support interactive exploration and interpretation of climate ensemble datasets. The workflow abstracts climate ensemble model runs - spatiotemporal time series - into a distribution over a 2D space that captures the variability among the ensemble model runs using a SOM. LLMs are integrated to assist in sensemaking of this SOM-defined 2D space, the basis for the visual analysis tasks. In all, ClimateSOM enables users to explore the variability among ensemble model runs, identify patterns, compare and cluster the ensemble model runs. To demonstrate the utility of ClimateSOM, we apply the workflow to an ensemble dataset of precipitation projections over California and the Northwestern United States. Furthermore, we conduct a short evaluation of our LLM integration, and conduct an expert review of the visual workflow and the insights from the case studies with six domain experts to evaluate our approach and its utility.",
    "authors": [
      "Yuya Kawakami",
      "Daniel Cayan",
      "Dongyu Liu",
      "Kwan-Liu Ma"
    ],
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-08-08T22:15:43Z",
    "pdf_url": "https://arxiv.org/pdf/2508.06732v1"
  },
  {
    "arxiv_id": "2508.06691v1",
    "entry_id": "http://arxiv.org/abs/2508.06691v1",
    "title": "Role of Large Language Models and Retrieval-Augmented Generation for Accelerating Crystalline Material Discovery: A Systematic Review",
    "summary": "Large language models (LLMs) have emerged as powerful tools for knowledge-intensive tasks across domains. In materials science, to find novel materials for various energy efficient devices for various real-world applications, requires several time and cost expensive simulations and experiments. In order to tune down the uncharted material search space, minimizing the experimental cost, LLMs can play a bigger role to first provide an accelerated search of promising known material candidates. Furthermore, the integration of LLMs with domain-specific information via retrieval-augmented generation (RAG) is poised to revolutionize how researchers predict materials structures, analyze defects, discover novel compounds, and extract knowledge from literature and databases. In motivation to the potentials of LLMs and RAG in accelerating material discovery, this paper presents a broad and systematic review to examine the recent advancements in applying LLMs and RAG to key materials science problems. We survey state-of-the-art developments in crystal structure prediction, defect analysis, materials discovery, literature mining, database integration, and multi-modal retrieval, highlighting how combining LLMs with external knowledge sources enables new capabilities. We discuss the performance, limitations, and implications of these approaches, and outline future directions for leveraging LLMs to accelerate materials research and discovery for advancement in technologies in the area of electronics, optics, biomedical, and energy storage.",
    "authors": [
      "Agada Joseph Oche",
      "Arpan Biswas"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "published": "2025-08-08T20:32:56Z",
    "pdf_url": "https://arxiv.org/pdf/2508.06691v1"
  },
  {
    "arxiv_id": "2508.06635v2",
    "entry_id": "http://arxiv.org/abs/2508.06635v2",
    "title": "Valid Inference with Imperfect Synthetic Data",
    "summary": "Predictions and generations from large language models are increasingly being explored as an aid in limited data regimes, such as in computational social science and human subjects research. While prior technical work has mainly explored the potential to use model-predicted labels for unlabeled data in a principled manner, there is increasing interest in using large language models to generate entirely new synthetic samples (e.g., synthetic simulations), such as in responses to surveys. However, it remains unclear by what means practitioners can combine such data with real data and yet produce statistically valid conclusions upon them. In this paper, we introduce a new estimator based on generalized method of moments, providing a hyperparameter-free solution with strong theoretical guarantees to address this challenge. Intriguingly, we find that interactions between the moment residuals of synthetic data and those of real data (i.e., when they are predictive of each other) can greatly improve estimates of the target parameter. We validate the finite-sample performance of our estimator across different tasks in computational social science applications, demonstrating large empirical gains.",
    "authors": [
      "Yewon Byun",
      "Shantanu Gupta",
      "Zachary C. Lipton",
      "Rachel Leah Childers",
      "Bryan Wilder"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "stat.ML"
    ],
    "published": "2025-08-08T18:32:52Z",
    "pdf_url": "https://arxiv.org/pdf/2508.06635v2"
  },
  {
    "arxiv_id": "2508.06616v1",
    "entry_id": "http://arxiv.org/abs/2508.06616v1",
    "title": "Generative AI for Intent-Driven Network Management in 6G: A Case Study on Hierarchical Learning Approach",
    "summary": "With the emergence of 6G, mobile networks are becoming increasingly heterogeneous and dynamic, necessitating advanced automation for efficient management. Intent-Driven Networks (IDNs) address this by translating high-level intents into optimization policies. Large Language Models (LLMs) can enhance this process by understanding complex human instructions to enable adaptive, intelligent automation. Given the rapid advancements in Generative AI (GenAI), a comprehensive survey of LLM-based IDN architectures in disaggregated Radio Access Network (RAN) environments is both timely and critical. This article provides such a survey, along with a case study on a hierarchical learning-enabled IDN architecture that integrates GenAI across three key stages: intent processing, intent validation, and intent execution. Unlike most existing approaches that apply GenAI in the form of LLMs for intent processing only, we propose a hierarchical framework that introduces GenAI across all three stages of IDN. To demonstrate the effectiveness of the proposed IDN management architecture, we present a case study based on the latest GenAI architecture named Mamba. The case study shows how the proposed GenAI-driven architecture enhances network performance through intelligent automation, surpassing the performance of the conventional IDN architectures.",
    "authors": [
      "Md Arafat Habib",
      "Medhat Elsayed",
      "Yigit Ozcan",
      "Pedro Enrique Iturria-Rivera",
      "Majid Bavand",
      "Melike Erol-Kantarci"
    ],
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "published": "2025-08-08T18:06:52Z",
    "pdf_url": "https://arxiv.org/pdf/2508.06616v1"
  },
  {
    "arxiv_id": "2508.06145v1",
    "entry_id": "http://arxiv.org/abs/2508.06145v1",
    "title": "Retrieval Augmented Large Language Model System for Comprehensive Drug Contraindications",
    "summary": "The versatility of large language models (LLMs) has been explored across various sectors, but their application in healthcare poses challenges, particularly in the domain of pharmaceutical contraindications where accurate and reliable information is required. This study enhances the capability of LLMs to address contraindications effectively by implementing a Retrieval Augmented Generation (RAG) pipeline. Utilizing OpenAI's GPT-4o-mini as the base model, and the text-embedding-3-small model for embeddings, our approach integrates Langchain to orchestrate a hybrid retrieval system with re-ranking. This system leverages Drug Utilization Review (DUR) data from public databases, focusing on contraindications for specific age groups, pregnancy, and concomitant drug use. The dataset includes 300 question-answer pairs across three categories, with baseline model accuracy ranging from 0.49 to 0.57. Post-integration of the RAG pipeline, we observed a significant improvement in model accuracy, achieving rates of 0.94, 0.87, and 0.89 for contraindications related to age groups, pregnancy, and concomitant drug use, respectively. The results indicate that augmenting LLMs with a RAG framework can substantially reduce uncertainty in prescription and drug intake decisions by providing more precise and reliable drug contraindication information.",
    "authors": [
      "Byeonghun Bang",
      "Jongsuk Yoon",
      "Dong-Jin Chang",
      "Seho Park",
      "Yong Oh Lee"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-08T09:09:03Z",
    "pdf_url": "https://arxiv.org/pdf/2508.06145v1"
  },
  {
    "arxiv_id": "2508.06110v1",
    "entry_id": "http://arxiv.org/abs/2508.06110v1",
    "title": "PanelTR: Zero-Shot Table Reasoning Framework Through Multi-Agent Scientific Discussion",
    "summary": "Table reasoning, including tabular QA and fact verification, often depends on annotated data or complex data augmentation, limiting flexibility and generalization. LLMs, despite their versatility, often underperform compared to simple supervised models. To approach these issues, we introduce PanelTR, a framework utilizing LLM agent scientists for robust table reasoning through a structured scientific approach. PanelTR's workflow involves agent scientists conducting individual investigations, engaging in self-review, and participating in collaborative peer-review discussions. This process, driven by five scientist personas, enables semantic-level transfer without relying on data augmentation or parametric optimization. Experiments across four benchmarks show that PanelTR outperforms vanilla LLMs and rivals fully supervised models, all while remaining independent of training data. Our findings indicate that structured scientific methodology can effectively handle complex tasks beyond table reasoning with flexible semantic understanding in a zero-shot context.",
    "authors": [
      "Yiran Rex Ma"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-08-08T08:15:52Z",
    "pdf_url": "https://arxiv.org/pdf/2508.06110v1"
  },
  {
    "arxiv_id": "2508.05938v1",
    "entry_id": "http://arxiv.org/abs/2508.05938v1",
    "title": "Prosocial Behavior Detection in Player Game Chat: From Aligning Human-AI Definitions to Efficient Annotation at Scale",
    "summary": "Detecting prosociality in text--communication intended to affirm, support, or improve others' behavior--is a novel and increasingly important challenge for trust and safety systems. Unlike toxic content detection, prosociality lacks well-established definitions and labeled data, requiring new approaches to both annotation and deployment. We present a practical, three-stage pipeline that enables scalable, high-precision prosocial content classification while minimizing human labeling effort and inference costs. First, we identify the best LLM-based labeling strategy using a small seed set of human-labeled examples. We then introduce a human-AI refinement loop, where annotators review high-disagreement cases between GPT-4 and humans to iteratively clarify and expand the task definition-a critical step for emerging annotation tasks like prosociality. This process results in improved label quality and definition alignment. Finally, we synthesize 10k high-quality labels using GPT-4 and train a two-stage inference system: a lightweight classifier handles high-confidence predictions, while only $\\sim$35\\% of ambiguous instances are escalated to GPT-4o. This architecture reduces inference costs by $\\sim$70% while achieving high precision ($\\sim$0.90). Our pipeline demonstrates how targeted human-AI interaction, careful task formulation, and deployment-aware architecture design can unlock scalable solutions for novel responsible AI tasks.",
    "authors": [
      "Rafal Kocielnik",
      "Min Kim",
      "Penphob",
      "Boonyarungsrit",
      "Fereshteh Soltani",
      "Deshawn Sambrano",
      "Animashree Anandkumar",
      "R. Michael Alvarez"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-08-08T02:04:14Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05938v1"
  },
  {
    "arxiv_id": "2508.05855v1",
    "entry_id": "http://arxiv.org/abs/2508.05855v1",
    "title": "Safety of Embodied Navigation: A Survey",
    "summary": "As large language models (LLMs) continue to advance and gain influence, the development of embodied AI has accelerated, drawing significant attention, particularly in navigation scenarios. Embodied navigation requires an agent to perceive, interact with, and adapt to its environment while moving toward a specified target in unfamiliar settings. However, the integration of embodied navigation into critical applications raises substantial safety concerns. Given their deployment in dynamic, real-world environments, ensuring the safety of such systems is critical. This survey provides a comprehensive analysis of safety in embodied navigation from multiple perspectives, encompassing attack strategies, defense mechanisms, and evaluation methodologies. Beyond conducting a comprehensive examination of existing safety challenges, mitigation technologies, and various datasets and metrics that assess effectiveness and robustness, we explore unresolved issues and future research directions in embodied navigation safety. These include potential attack methods, mitigation strategies, more reliable evaluation techniques, and the implementation of verification frameworks. By addressing these critical gaps, this survey aims to provide valuable insights that can guide future research toward the development of safer and more reliable embodied navigation systems. Furthermore, the findings of this study have broader implications for enhancing societal safety and increasing industrial efficiency.",
    "authors": [
      "Zixia Wang",
      "Jia Hu",
      "Ronghui Mu"
    ],
    "categories": [
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-08-07T21:09:48Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05855v1"
  },
  {
    "arxiv_id": "2508.05513v1",
    "entry_id": "http://arxiv.org/abs/2508.05513v1",
    "title": "Streamlining Admission with LOR Insights: AI-Based Leadership Assessment in Online Master's Program",
    "summary": "Letters of recommendation (LORs) provide valuable insights into candidates' capabilities and experiences beyond standardized test scores. However, reviewing these text-heavy materials is time-consuming and labor-intensive. To address this challenge and support the admission committee in providing feedback for students' professional growth, our study introduces LORI: LOR Insights, a novel AI-based detection tool for assessing leadership skills in LORs submitted by online master's program applicants. By employing natural language processing and leveraging large language models using RoBERTa and LLAMA, we seek to identify leadership attributes such as teamwork, communication, and innovation. Our latest RoBERTa model achieves a weighted F1 score of 91.6%, a precision of 92.4%, and a recall of 91.6%, showing a strong level of consistency in our test data. With the growing importance of leadership skills in the STEM sector, integrating LORI into the graduate admissions process is crucial for accurately assessing applicants' leadership capabilities. This approach not only streamlines the admissions process but also automates and ensures a more comprehensive evaluation of candidates' capabilities.",
    "authors": [
      "Meryem Yilmaz Soylu",
      "Adrian Gallard",
      "Jeonghyun Lee",
      "Gayane Grigoryan",
      "Rushil Desai",
      "Stephen Harmon"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-07T15:46:59Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05513v1"
  },
  {
    "arxiv_id": "2508.05432v1",
    "entry_id": "http://arxiv.org/abs/2508.05432v1",
    "title": "Whose Truth? Pluralistic Geo-Alignment for (Agentic) AI",
    "summary": "AI (super) alignment describes the challenge of ensuring (future) AI systems behave in accordance with societal norms and goals. While a quickly evolving literature is addressing biases and inequalities, the geographic variability of alignment remains underexplored. Simply put, what is considered appropriate, truthful, or legal can differ widely across regions due to cultural norms, political realities, and legislation. Alignment measures applied to AI/ML workflows can sometimes produce outcomes that diverge from statistical realities, such as text-to-image models depicting balanced gender ratios in company leadership despite existing imbalances. Crucially, some model outputs are globally acceptable, while others, e.g., questions about Kashmir, depend on knowing the user's location and their context. This geographic sensitivity is not new. For instance, Google Maps renders Kashmir's borders differently based on user location. What is new is the unprecedented scale and automation with which AI now mediates knowledge, expresses opinions, and represents geographic reality to millions of users worldwide, often with little transparency about how context is managed. As we approach Agentic AI, the need for spatio-temporally aware alignment, rather than one-size-fits-all approaches, is increasingly urgent. This paper reviews key geographic research problems, suggests topics for future work, and outlines methods for assessing alignment sensitivity.",
    "authors": [
      "Krzysztof Janowicz",
      "Zilong Liu",
      "Gengchen Mai",
      "Zhangyu Wang",
      "Ivan Majic",
      "Alexandra Fortacz",
      "Grant McKenzie",
      "Song Gao"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-08-07T14:21:33Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05432v1"
  },
  {
    "arxiv_id": "2508.05427v1",
    "entry_id": "http://arxiv.org/abs/2508.05427v1",
    "title": "Large Language Models Transform Organic Synthesis From Reaction Prediction to Automation",
    "summary": "Large language models (LLMs) are beginning to reshape how chemists plan and run reactions in organic synthesis. Trained on millions of reported transformations, these text-based models can propose synthetic routes, forecast reaction outcomes and even instruct robots that execute experiments without human supervision. Here we survey the milestones that turned LLMs from speculative tools into practical lab partners. We show how coupling LLMs with graph neural networks, quantum calculations and real-time spectroscopy shrinks discovery cycles and supports greener, data-driven chemistry. We discuss limitations, including biased datasets, opaque reasoning and the need for safety gates that prevent unintentional hazards. Finally, we outline community initiatives open benchmarks, federated learning and explainable interfaces that aim to democratize access while keeping humans firmly in control. These advances chart a path towards rapid, reliable and inclusive molecular innovation powered by artificial intelligence and automation.",
    "authors": [
      "Kartar Kumar Lohana Tharwani",
      "Rajesh Kumar",
      "Sumita",
      "Numan Ahmed",
      "Yong Tang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-07T14:17:23Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05427v1"
  },
  {
    "arxiv_id": "2508.05360v1",
    "entry_id": "http://arxiv.org/abs/2508.05360v1",
    "title": "Building Effective Safety Guardrails in AI Education Tools",
    "summary": "There has been rapid development in generative AI tools across the education sector, which in turn is leading to increased adoption by teachers. However, this raises concerns regarding the safety and age-appropriateness of the AI-generated content that is being created for use in classrooms. This paper explores Oak National Academy's approach to addressing these concerns within the development of the UK Government's first publicly available generative AI tool - our AI-powered lesson planning assistant (Aila). Aila is intended to support teachers planning national curriculum-aligned lessons that are appropriate for pupils aged 5-16 years. To mitigate safety risks associated with AI-generated content we have implemented four key safety guardrails - (1) prompt engineering to ensure AI outputs are generated within pedagogically sound and curriculum-aligned parameters, (2) input threat detection to mitigate attacks, (3) an Independent Asynchronous Content Moderation Agent (IACMA) to assess outputs against predefined safety categories, and (4) taking a human-in-the-loop approach, to encourage teachers to review generated content before it is used in the classroom. Through our on-going evaluation of these safety guardrails we have identified several challenges and opportunities to take into account when implementing and testing safety guardrails. This paper highlights ways to build more effective safety guardrails in generative AI education tools including the on-going iteration and refinement of guardrails, as well as enabling cross-sector collaboration through sharing both open-source code, datasets and learnings.",
    "authors": [
      "Hannah-Beth Clark",
      "Laura Benton",
      "Emma Searle",
      "Margaux Dowland",
      "Matthew Gregory",
      "Will Gayne",
      "John Roberts"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-08-07T13:09:47Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05360v1"
  },
  {
    "arxiv_id": "2508.05294v4",
    "entry_id": "http://arxiv.org/abs/2508.05294v4",
    "title": "Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction",
    "summary": "Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper reviews works that advance agentic applications and architectures, including initial efforts with GPT-style interfaces and more complex systems where AI agents function as coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.",
    "authors": [
      "Sahar Salimpour",
      "Lei Fu",
      "Kajetan Rachwał",
      "Pascal Bertrand",
      "Kevin O'Sullivan",
      "Robert Jakob",
      "Farhad Keramat",
      "Leonardo Militano",
      "Giovanni Toffetti",
      "Harry Edelman",
      "Jorge Peña Queralta"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-07T11:48:03Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05294v4"
  },
  {
    "arxiv_id": "2508.05009v1",
    "entry_id": "http://arxiv.org/abs/2508.05009v1",
    "title": "Can Large Language Models Integrate Spatial Data? Empirical Insights into Reasoning Strengths and Computational Weaknesses",
    "summary": "We explore the application of large language models (LLMs) to empower domain experts in integrating large, heterogeneous, and noisy urban spatial datasets. Traditional rule-based integration methods are unable to cover all edge cases, requiring manual verification and repair. Machine learning approaches require collecting and labeling of large numbers of task-specific samples. In this study, we investigate the potential of LLMs for spatial data integration. Our analysis first considers how LLMs reason about environmental spatial relationships mediated by human experience, such as between roads and sidewalks. We show that while LLMs exhibit spatial reasoning capabilities, they struggle to connect the macro-scale environment with the relevant computational geometry tasks, often producing logically incoherent responses. But when provided relevant features, thereby reducing dependence on spatial reasoning, LLMs are able to generate high-performing results. We then adapt a review-and-refine method, which proves remarkably effective in correcting erroneous initial responses while preserving accurate responses. We discuss practical implications of employing LLMs for spatial data integration in real-world contexts and outline future research directions, including post-training, multi-modal integration methods, and support for diverse data formats. Our findings position LLMs as a promising and flexible alternative to traditional rule-based heuristics, advancing the capabilities of adaptive spatial data integration.",
    "authors": [
      "Bin Han",
      "Robert Wolfe",
      "Anat Caspi",
      "Bill Howe"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-07T03:44:20Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05009v1"
  },
  {
    "arxiv_id": "2508.04915v1",
    "entry_id": "http://arxiv.org/abs/2508.04915v1",
    "title": "ConfAgents: A Conformal-Guided Multi-Agent Framework for Cost-Efficient Medical Diagnosis",
    "summary": "The efficacy of AI agents in healthcare research is hindered by their reliance on static, predefined strategies. This creates a critical limitation: agents can become better tool-users but cannot learn to become better strategic planners, a crucial skill for complex domains like healthcare. We introduce HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its own high-level problem-solving policies by distilling procedural successes and failures into a durable, strategic knowledge base. To anchor our research and facilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark featuring complex, realistic health data analysis tasks derived from peer-reviewed clinical research. Our comprehensive experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work marks a necessary shift from building better tool-users to designing smarter, self-evolving task-managers, paving the way for more autonomous and effective AI for scientific discovery.",
    "authors": [
      "Huiya Zhao",
      "Yinghao Zhu",
      "Zixiang Wang",
      "Yasha Wang",
      "Junyi Gao",
      "Liantao Ma"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-08-06T22:39:38Z",
    "pdf_url": "https://arxiv.org/pdf/2508.04915v1"
  },
  {
    "arxiv_id": "2508.04575v1",
    "entry_id": "http://arxiv.org/abs/2508.04575v1",
    "title": "Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration",
    "summary": "While AI agents show potential in scientific ideation, most existing frameworks rely on single-agent refinement, limiting creativity due to bounded knowledge and perspective. Inspired by real-world research dynamics, this paper investigates whether structured multi-agent discussions can surpass solitary ideation. We propose a cooperative multi-agent framework for generating research proposals and systematically compare configurations including group size, leaderled versus leaderless structures, and team compositions varying in interdisciplinarity and seniority. To assess idea quality, we employ a comprehensive protocol with agent-based scoring and human review across dimensions such as novelty, strategic vision, and integration depth. Our results show that multi-agent discussions substantially outperform solitary baselines. A designated leader acts as a catalyst, transforming discussion into more integrated and visionary proposals. Notably, we find that cognitive diversity is a primary driver of quality, yet expertise is a non-negotiable prerequisite, as teams lacking a foundation of senior knowledge fail to surpass even a single competent agent. These findings offer actionable insights for designing collaborative AI ideation systems and shed light on how team structure influences creative outcomes.",
    "authors": [
      "Nuo Chen",
      "Yicheng Tong",
      "Jiaying Wu",
      "Minh Duc Duong",
      "Qian Wang",
      "Qingyun Zou",
      "Bryan Hooi",
      "Bingsheng He"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-08-06T15:59:18Z",
    "pdf_url": "https://arxiv.org/pdf/2508.04575v1"
  },
  {
    "arxiv_id": "2508.04482v1",
    "entry_id": "http://arxiv.org/abs/2508.04482v1",
    "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use",
    "summary": "The dream to create AI assistants as capable and versatile as the fictional J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution of (multi-modal) large language models ((M)LLMs), this dream is closer to reality, as (M)LLM-based Agents using computing devices (e.g., computers and mobile phones) by operating within the environments and interfaces (e.g., Graphical User Interface (GUI)) provided by operating systems (OS) to automate tasks have significantly advanced. This paper presents a comprehensive survey of these advanced agents, designated as OS Agents. We begin by elucidating the fundamentals of OS Agents, exploring their key components including the environment, observation space, and action space, and outlining essential capabilities such as understanding, planning, and grounding. We then examine methodologies for constructing OS Agents, focusing on domain-specific foundation models and agent frameworks. A detailed review of evaluation protocols and benchmarks highlights how OS Agents are assessed across diverse tasks. Finally, we discuss current challenges and identify promising directions for future research, including safety and privacy, personalization and self-evolution. This survey aims to consolidate the state of OS Agents research, providing insights to guide both academic inquiry and industrial development. An open-source GitHub repository is maintained as a dynamic resource to foster further innovation in this field. We present a 9-page version of our work, accepted by ACL 2025, to provide a concise overview to the domain.",
    "authors": [
      "Xueyu Hu",
      "Tao Xiong",
      "Biao Yi",
      "Zishu Wei",
      "Ruixuan Xiao",
      "Yurun Chen",
      "Jiasheng Ye",
      "Meiling Tao",
      "Xiangxin Zhou",
      "Ziyu Zhao",
      "Yuhuai Li",
      "Shengze Xu",
      "Shenzhi Wang",
      "Xinchen Xu",
      "Shuofei Qiao",
      "Zhaokai Wang",
      "Kun Kuang",
      "Tieyong Zeng",
      "Liang Wang",
      "Jiwei Li",
      "Yuchen Eleanor Jiang",
      "Wangchunshu Zhou",
      "Guoyin Wang",
      "Keting Yin",
      "Zhou Zhao",
      "Hongxia Yang",
      "Fan Wu",
      "Shengyu Zhang",
      "Fei Wu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-08-06T14:33:45Z",
    "pdf_url": "https://arxiv.org/pdf/2508.04482v1"
  },
  {
    "arxiv_id": "2508.04427v1",
    "entry_id": "http://arxiv.org/abs/2508.04427v1",
    "title": "Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models",
    "summary": "Multimodal learning has witnessed remarkable advancements in recent years, particularly with the integration of attention-based models, leading to significant performance gains across a variety of tasks. Parallel to this progress, the demand for explainable artificial intelligence (XAI) has spurred a growing body of research aimed at interpreting the complex decision-making processes of these models. This systematic literature review analyzes research published between January 2020 and early 2024 that focuses on the explainability of multimodal models. Framed within the broader goals of XAI, we examine the literature across multiple dimensions, including model architecture, modalities involved, explanation algorithms and evaluation methodologies. Our analysis reveals that the majority of studies are concentrated on vision-language and language-only models, with attention-based techniques being the most commonly employed for explanation. However, these methods often fall short in capturing the full spectrum of interactions between modalities, a challenge further compounded by the architectural heterogeneity across domains. Importantly, we find that evaluation methods for XAI in multimodal settings are largely non-systematic, lacking consistency, robustness, and consideration for modality-specific cognitive and contextual factors. Based on these findings, we provide a comprehensive set of recommendations aimed at promoting rigorous, transparent, and standardized evaluation and reporting practices in multimodal XAI research. Our goal is to support future research in more interpretable, accountable, and responsible mulitmodal AI systems, with explainability at their core.",
    "authors": [
      "Md Raisul Kibria",
      "Sébastien Lafond",
      "Janan Arslan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-08-06T13:14:20Z",
    "pdf_url": "https://arxiv.org/pdf/2508.04427v1"
  },
  {
    "arxiv_id": "2508.04399v1",
    "entry_id": "http://arxiv.org/abs/2508.04399v1",
    "title": "Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky",
    "summary": "This study evaluates advanced natural language processing (NLP) techniques to enhance crash data quality by mining crash narratives, using secondary crash identification in Kentucky as a case study. Drawing from 16,656 manually reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we compare three model classes: zero-shot open-source large language models (LLMs) (LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers (BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic regression as baseline. Models were calibrated on 2015-2021 data and tested on 1,771 narratives from 2022. Fine-tuned transformers achieved superior performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy (95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139 minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred high computational costs (up to 723 minutes for DeepSeek-R1:70B), while fine-tuned models processed the test set in seconds after brief training. Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can rival larger counterparts in performance while reducing runtime, suggesting opportunities for optimized deployments. Results highlight trade-offs between accuracy, efficiency, and data requirements, with fine-tuned transformer models balancing precision and recall effectively on Kentucky data. Practical deployment considerations emphasize privacy-preserving local deployment, ensemble approaches for improved accuracy, and incremental processing for scalability, providing a replicable scheme for enhancing crash-data quality with advanced NLP.",
    "authors": [
      "Xu Zhang",
      "Mei Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-08-06T12:41:18Z",
    "pdf_url": "https://arxiv.org/pdf/2508.04399v1"
  },
  {
    "arxiv_id": "2508.04361v3",
    "entry_id": "http://arxiv.org/abs/2508.04361v3",
    "title": "OmniPlay: Benchmarking Omni-Modal Models on Omni-Modal Game Playing",
    "summary": "While generalist foundation models like Gemini and GPT-4o demonstrate impressive multi-modal competence, existing evaluations fail to test their intelligence in dynamic, interactive worlds. Static benchmarks lack agency, while interactive benchmarks suffer from a severe modal bottleneck, typically ignoring crucial auditory and temporal cues. To bridge this evaluation chasm, we introduce OmniPlay, a diagnostic benchmark designed not just to evaluate, but to probe the fusion and reasoning capabilities of agentic models across the full sensory spectrum. Built on a core philosophy of modality interdependence, OmniPlay comprises a suite of five game environments that systematically create scenarios of both synergy and conflict, forcing agents to perform genuine cross-modal reasoning. Our comprehensive evaluation of six leading omni-modal models reveals a critical dichotomy: they exhibit superhuman performance on high-fidelity memory tasks but suffer from systemic failures in challenges requiring robust reasoning and strategic planning. We demonstrate that this fragility stems from brittle fusion mechanisms, which lead to catastrophic performance degradation under modality conflict and uncover a counter-intuitive \"less is more\" paradox, where removing sensory information can paradoxically improve performance. Our findings suggest that the path toward robust AGI requires a research focus beyond scaling to explicitly address synergistic fusion. Our platform is available for anonymous review at https://github.com/fuqingbie/omni-game-benchmark.",
    "authors": [
      "Fuqing Bie",
      "Shiyu Huang",
      "Xijia Tao",
      "Zhiqin Fang",
      "Leyi Pan",
      "Junzhe Chen",
      "Min Ren",
      "Liuyu Xiang",
      "Zhaofeng He"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-06T11:58:58Z",
    "pdf_url": "https://arxiv.org/pdf/2508.04361v3"
  },
  {
    "arxiv_id": "2508.04337v1",
    "entry_id": "http://arxiv.org/abs/2508.04337v1",
    "title": "Modelling and Classifying the Components of a Literature Review",
    "summary": "Previous work has demonstrated that AI methods for analysing scientific literature benefit significantly from annotating sentences in papers according to their rhetorical roles, such as research gaps, results, limitations, extensions of existing methodologies, and others. Such representations also have the potential to support the development of a new generation of systems capable of producing high-quality literature reviews. However, achieving this goal requires the definition of a relevant annotation schema and effective strategies for large-scale annotation of the literature. This paper addresses these challenges by 1) introducing a novel annotation schema specifically designed to support literature review generation and 2) conducting a comprehensive evaluation of a wide range of state-of-the-art large language models (LLMs) in classifying rhetorical roles according to this schema. To this end, we also present Sci-Sentence, a novel multidisciplinary benchmark comprising 700 sentences manually annotated by domain experts and 2,240 sentences automatically labelled using LLMs. We evaluate 37 LLMs on this benchmark, spanning diverse model families and sizes, using both zero-shot learning and fine-tuning approaches. The experiments yield several novel insights that advance the state of the art in this challenging domain. First, the current generation of LLMs performs remarkably well on this task when fine-tuned on high-quality data, achieving performance levels above 96\\% F1. Second, while large proprietary models like GPT-4o achieve the best results, some lightweight open-source alternatives also demonstrate excellent performance. Finally, enriching the training data with semi-synthetic examples generated by LLMs proves beneficial, enabling small encoders to achieve robust results and significantly enhancing the performance of several open decoder models.",
    "authors": [
      "Francisco Bolaños",
      "Angelo Salatino",
      "Francesco Osborne",
      "Enrico Motta"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.IR"
    ],
    "published": "2025-08-06T11:30:07Z",
    "pdf_url": "https://arxiv.org/pdf/2508.04337v1"
  },
  {
    "arxiv_id": "2508.04227v1",
    "entry_id": "http://arxiv.org/abs/2508.04227v1",
    "title": "Continual Learning for VLMs: A Survey and Taxonomy Beyond Forgetting",
    "summary": "Vision-language models (VLMs) have achieved impressive performance across diverse multimodal tasks by leveraging large-scale pre-training. However, enabling them to learn continually from non-stationary data remains a major challenge, as their cross-modal alignment and generalization capabilities are particularly vulnerable to catastrophic forgetting. Unlike traditional unimodal continual learning (CL), VLMs face unique challenges such as cross-modal feature drift, parameter interference due to shared architectures, and zero-shot capability erosion. This survey offers the first focused and systematic review of continual learning for VLMs (VLM-CL). We begin by identifying the three core failure modes that degrade performance in VLM-CL. Based on these, we propose a challenge-driven taxonomy that maps solutions to their target problems: (1) \\textit{Multi-Modal Replay Strategies} address cross-modal drift through explicit or implicit memory mechanisms; (2) \\textit{Cross-Modal Regularization} preserves modality alignment during updates; and (3) \\textit{Parameter-Efficient Adaptation} mitigates parameter interference with modular or low-rank updates. We further analyze current evaluation protocols, datasets, and metrics, highlighting the need for better benchmarks that capture VLM-specific forgetting and compositional generalization. Finally, we outline open problems and future directions, including continual pre-training and compositional zero-shot learning. This survey aims to serve as a comprehensive and diagnostic reference for researchers developing lifelong vision-language systems. All resources are available at: https://github.com/YuyangSunshine/Awesome-Continual-learning-of-Vision-Language-Models.",
    "authors": [
      "Yuyang Liu",
      "Qiuhe Hong",
      "Linlan Huang",
      "Alexandra Gomez-Villa",
      "Dipam Goswami",
      "Xialei Liu",
      "Joost van de Weijer",
      "Yonghong Tian"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-08-06T09:03:10Z",
    "pdf_url": "https://arxiv.org/pdf/2508.04227v1"
  },
  {
    "arxiv_id": "2508.09164v1",
    "entry_id": "http://arxiv.org/abs/2508.09164v1",
    "title": "Generating Feasible and Diverse Synthetic Populations Using Diffusion Models",
    "summary": "Population synthesis is a critical task that involves generating synthetic yet realistic representations of populations. It is a fundamental problem in agent-based modeling (ABM), which has become the standard to analyze intelligent transportation systems. The synthetic population serves as the primary input for ABM transportation simulation, with traveling agents represented by population members. However, when the number of attributes describing agents becomes large, survey data often cannot densely support the joint distribution of the attributes in the population due to the curse of dimensionality. This sparsity makes it difficult to accurately model and produce the population. Interestingly, deep generative models trained from available sample data can potentially synthesize possible attribute combinations that present in the actual population but do not exist in the sample data(called sampling zeros). Nevertheless, this comes at the cost of falsely generating the infeasible attribute combinations that do not exist in the population (called structural zeros). In this study, a novel diffusion model-based population synthesis method is proposed to estimate the underlying joint distribution of a population. This approach enables the recovery of numerous missing sampling zeros while keeping the generated structural zeros minimal. Our method is compared with other recently proposed approaches such as Variational Autoencoders (VAE) and Generative Adversarial Network (GAN) approaches, which have shown success in high dimensional tabular population synthesis. We assess the performance of the synthesized outputs using a range of metrics, including marginal distribution similarity, feasibility, and diversity. The results demonstrate that our proposed method outperforms previous approaches in achieving a better balance between the feasibility and diversity of the synthesized population.",
    "authors": [
      "Min Tang",
      "Peng Lu",
      "Qing Feng"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-06T03:11:27Z",
    "pdf_url": "https://arxiv.org/pdf/2508.09164v1"
  },
  {
    "arxiv_id": "2508.03953v1",
    "entry_id": "http://arxiv.org/abs/2508.03953v1",
    "title": "Policy to Assist Iteratively Local Segmentation: Optimising Modality and Location Selection for Prostate Cancer Localisation",
    "summary": "Radiologists often mix medical image reading strategies, including inspection of individual modalities and local image regions, using information at different locations from different images independently as well as concurrently. In this paper, we propose a recommend system to assist machine learning-based segmentation models, by suggesting appropriate image portions along with the best modality, such that prostate cancer segmentation performance can be maximised. Our approach trains a policy network that assists tumor localisation, by recommending both the optimal imaging modality and the specific sections of interest for review. During training, a pre-trained segmentation network mimics radiologist inspection on individual or variable combinations of these imaging modalities and their sections - selected by the policy network. Taking the locally segmented regions as an input for the next step, this dynamic decision making process iterates until all cancers are best localised. We validate our method using a data set of 1325 labelled multiparametric MRI images from prostate cancer patients, demonstrating its potential to improve annotation efficiency and segmentation accuracy, especially when challenging pathology is present. Experimental results show that our approach can surpass standard segmentation networks. Perhaps more interestingly, our trained agent independently developed its own optimal strategy, which may or may not be consistent with current radiologist guidelines such as PI-RADS. This observation also suggests a promising interactive application, in which the proposed policy networks assist human radiologists.",
    "authors": [
      "Xiangcen Wu",
      "Shaheer U. Saeed",
      "Yipei Wang",
      "Ester Bonmati Coll",
      "Yipeng Hu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-08-05T22:40:18Z",
    "pdf_url": "https://arxiv.org/pdf/2508.03953v1"
  },
  {
    "arxiv_id": "2508.03860v2",
    "entry_id": "http://arxiv.org/abs/2508.03860v2",
    "title": "Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models",
    "summary": "Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. Instruction tuning, multi-agent reasoning, and RAG frameworks for external knowledge access are also reviewed. The key findings demonstrate the limitations of current metrics, the importance of validated external evidence, and the improvement of factual consistency through domain-specific customization. The review underscores the importance of building more accurate, understandable, and context-aware fact-checking. These insights contribute to the advancement of research toward more trustworthy models.",
    "authors": [
      "Subhey Sadi Rahman",
      "Md. Adnanul Islam",
      "Md. Mahbub Alam",
      "Musarrat Zeba",
      "Md. Abdur Rahman",
      "Sadia Sultana Chowa",
      "Mohaimenul Azam Khan Raiaan",
      "Sami Azam"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-05T19:20:05Z",
    "pdf_url": "https://arxiv.org/pdf/2508.03860v2"
  },
  {
    "arxiv_id": "2508.03438v1",
    "entry_id": "http://arxiv.org/abs/2508.03438v1",
    "title": "Data Overdose? Time for a Quadruple Shot: Knowledge Graph Construction using Enhanced Triple Extraction",
    "summary": "The rapid expansion of publicly-available medical data presents a challenge for clinicians and researchers alike, increasing the gap between the volume of scientific literature and its applications. The steady growth of studies and findings overwhelms medical professionals at large, hindering their ability to systematically review and understand the latest knowledge. This paper presents an approach to information extraction and automatic knowledge graph (KG) generation to identify and connect biomedical knowledge. Through a pipeline of large language model (LLM) agents, the system decomposes 44 PubMed abstracts into semantically meaningful proposition sentences and extracts KG triples from these sentences. The triples are enhanced using a combination of open domain and ontology-based information extraction methodologies to incorporate ontological categories. On top of this, a context variable is included during extraction to allow the triple to stand on its own - thereby becoming `quadruples'. The extraction accuracy of the LLM is validated by comparing natural language sentences generated from the enhanced triples to the original propositions, achieving an average cosine similarity of 0.874. The similarity for generated sentences of enhanced triples were compared with generated sentences of ordinary triples showing an increase as a result of the context variable. Furthermore, this research explores the ability for LLMs to infer new relationships and connect clusters in the knowledge base of the knowledge graph. This approach leads the way to provide medical practitioners with a centralised, updated in real-time, and sustainable knowledge source, and may be the foundation of similar gains in a wide variety of fields.",
    "authors": [
      "Taine J. Elliott",
      "Stephen P. Levitt",
      "Ken Nixon",
      "Martin Bekker"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-05T13:30:41Z",
    "pdf_url": "https://arxiv.org/pdf/2508.03438v1"
  },
  {
    "arxiv_id": "2508.03393v1",
    "entry_id": "http://arxiv.org/abs/2508.03393v1",
    "title": "Agentic AI in 6G Software Businesses: A Layered Maturity Model",
    "summary": "The emergence of agentic AI systems in 6G software businesses presents both strategic opportunities and significant challenges. While such systems promise increased autonomy, scalability, and intelligent decision-making across distributed environments, their adoption raises concerns regarding technical immaturity, integration complexity, organizational readiness, and performance-cost trade-offs. In this study, we conducted a preliminary thematic mapping to identify factors influencing the adoption of agentic software within the context of 6G. Drawing on a multivocal literature review and targeted scanning, we identified 29 motivators and 27 demotivators, which were further categorized into five high-level themes in each group. This thematic mapping offers a structured overview of the enabling and inhibiting forces shaping organizational readiness for agentic transformation. Positioned as a feasibility assessment, the study represents an early phase of a broader research initiative aimed at developing and validating a layered maturity model grounded in CMMI model with the software architectural three dimensions possibly Data, Business Logic, and Presentation. Ultimately, this work seeks to provide a practical framework to help software-driven organizations assess, structure, and advance their agent-first capabilities in alignment with the demands of 6G.",
    "authors": [
      "Muhammad Zohaib",
      "Muhammad Azeem Akbar",
      "Sami Hyrynsalmi",
      "Arif Ali Khan"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-08-05T12:42:46Z",
    "pdf_url": "https://arxiv.org/pdf/2508.03393v1"
  },
  {
    "arxiv_id": "2508.03194v1",
    "entry_id": "http://arxiv.org/abs/2508.03194v1",
    "title": "Scaling DRL for Decision Making: A Survey on Data, Network, and Training Budget Strategies",
    "summary": "In recent years, the expansion of neural network models and training data has driven remarkable progress in deep learning, particularly in computer vision and natural language processing. This advancement is underpinned by the concept of Scaling Laws, which demonstrates that scaling model parameters and training data enhances learning performance. While these fields have witnessed breakthroughs, such as the development of large language models like GPT-4 and advanced vision models like Midjourney, the application of scaling laws in deep reinforcement learning (DRL) remains relatively unexplored. Despite its potential to improve performance, the integration of scaling laws into DRL for decision making has not been fully realized. This review addresses this gap by systematically analyzing scaling strategies in three dimensions: data, network, and training budget. In data scaling, we explore methods to optimize data efficiency through parallel sampling and data generation, examining the relationship between data volume and learning outcomes. For network scaling, we investigate architectural enhancements, including monolithic expansions, ensemble and MoE methods, and agent number scaling techniques, which collectively enhance model expressivity while posing unique computational challenges. Lastly, in training budget scaling, we evaluate the impact of distributed training, high replay ratios, large batch sizes, and auxiliary training on training efficiency and convergence. By synthesizing these strategies, this review not only highlights their synergistic roles in advancing DRL for decision making but also provides a roadmap for future research. We emphasize the importance of balancing scalability with computational efficiency and outline promising directions for leveraging scaling to unlock the full potential of DRL in various tasks such as robot control, autonomous driving and LLM training.",
    "authors": [
      "Yi Ma",
      "Hongyao Tang",
      "Chenjun Xiao",
      "Yaodong Yang",
      "Wei Wei",
      "Jianye Hao",
      "Jiye Liang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-08-05T08:03:12Z",
    "pdf_url": "https://arxiv.org/pdf/2508.03194v1"
  },
  {
    "arxiv_id": "2508.02994v1",
    "entry_id": "http://arxiv.org/abs/2508.02994v1",
    "title": "When AIs Judge AIs: The Rise of Agent-as-a-Judge Evaluation for LLMs",
    "summary": "As large language models (LLMs) grow in capability and autonomy, evaluating their outputs-especially in open-ended and complex tasks-has become a critical bottleneck. A new paradigm is emerging: using AI agents as the evaluators themselves. This \"agent-as-a-judge\" approach leverages the reasoning and perspective-taking abilities of LLMs to assess the quality and safety of other models, promising calable and nuanced alternatives to human evaluation. In this review, we define the agent-as-a-judge concept, trace its evolution from single-model judges to dynamic multi-agent debate frameworks, and critically examine their strengths and shortcomings. We compare these approaches across reliability, cost, and human alignment, and survey real-world deployments in domains such as medicine, law, finance, and education. Finally, we highlight pressing challenges-including bias, robustness, and meta evaluation-and outline future research directions. By bringing together these strands, our review demonstrates how agent-based judging can complement (but not replace) human oversight, marking a step toward trustworthy, scalable evaluation for next-generation LLMs.",
    "authors": [
      "Fangyi Yu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-05T01:42:25Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02994v1"
  },
  {
    "arxiv_id": "2508.10003v1",
    "entry_id": "http://arxiv.org/abs/2508.10003v1",
    "title": "Semantic Structure in Large Language Model Embeddings",
    "summary": "Psychological research consistently finds that human ratings of words across diverse semantic scales can be reduced to a low-dimensional form with relatively little information loss. We find that the semantic associations encoded in the embedding matrices of large language models (LLMs) exhibit a similar structure. We show that the projections of words on semantic directions defined by antonym pairs (e.g. kind - cruel) correlate highly with human ratings, and further find that these projections effectively reduce to a 3-dimensional subspace within LLM embeddings, closely resembling the patterns derived from human survey responses. Moreover, we find that shifting tokens along one semantic direction causes off-target effects on geometrically aligned features proportional to their cosine similarity. These findings suggest that semantic features are entangled within LLMs similarly to how they are interconnected in human language, and a great deal of semantic information, despite its apparent complexity, is surprisingly low-dimensional. Furthermore, accounting for this semantic structure may prove essential for avoiding unintended consequences when steering features.",
    "authors": [
      "Austin C. Kozlowski",
      "Callin Dai",
      "Andrei Boutyline"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-04T20:21:50Z",
    "pdf_url": "https://arxiv.org/pdf/2508.10003v1"
  },
  {
    "arxiv_id": "2508.02827v1",
    "entry_id": "http://arxiv.org/abs/2508.02827v1",
    "title": "Automated Validation of LLM-based Evaluators for Software Engineering Artifacts",
    "summary": "Automation in software engineering increasingly relies on large language models (LLMs) to generate, review, and assess code artifacts. However, establishing LLMs as reliable evaluators remains an open challenge: human evaluations are costly, subjective and non scalable, while existing automated methods fail to discern fine grained variations in artifact quality.\n  We introduce REFINE (Ranking Evaluators for FIne grained Nuanced Evaluation), an automated framework for benchmarking LLM based evaluators across software engineering tasks. REFINE comprises of two modules: Hierarchy Dataset Builder applies novel generation techniques to automatically synthesize artifacts with progressively reduced quality, and Evaluator Tester quantifies each candidate evaluator configuration by measuring how closely its rankings align with expected ordering.\n  A key feature of REFINE is controllability: users can tune the granularity of degradation to progressively refine evaluator configurations, from coarse filtering to stress testing on subtle quality gaps.\n  While the methodology is general, we focus on coding tasks reflecting the practical demands in our production setting. REFINE was integrated into IBM's internal development workflows and applied to code generation, translation, and summarization for COBOL, an enterprise critical programming language, using industrial data. It was used to identify LLM as a Judge configurations that lifted alignment scores from below $0.7$ to above $0.9$ in some coding tasks. These nuance sensitive evaluators are now actively used by model training teams to support model release decisions.",
    "authors": [
      "Ora Nova Fandina",
      "Eitan Farchi",
      "Shmulik Froimovich",
      "Rami Katan",
      "Alice Podolsky",
      "Orna Raz",
      "Avi Ziv"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-08-04T18:52:01Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02827v1"
  },
  {
    "arxiv_id": "2508.02630v2",
    "entry_id": "http://arxiv.org/abs/2508.02630v2",
    "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging Questions for Agentic E-Commerce",
    "summary": "Online marketplaces will be transformed by autonomous AI agents acting on behalf of consumers. Rather than humans browsing and clicking, AI agents can parse webpages or interact through APIs to evaluate products, and transact. This raises a fundamental question: what do AI agents buy-and why? We develop ACES, a sandbox environment that pairs a platform-agnostic agent with a fully programmable mock marketplace to study this. We first explore aggregate choices, revealing that modal choices can differ across models, with AI agents sometimes concentrating on a few products, raising competition questions. We then analyze the drivers of choices through rationality checks and randomized experiments on product positions and listing attributes. Models show sizeable and heterogeneous position effects: all favor the top row, yet different models prefer different columns, undermining the assumption of a universal ``top'' rank. They penalize sponsored tags, reward endorsements, and sensitivities to price, ratings, and reviews are directionally as expected, but vary sharply across models. Finally, we find that a seller-side agent that makes minor tweaks to product descriptions can deliver substantial market-share gains by targeting AI buyer preferences. Our findings reveal how AI agents behave in e-commerce, and surface concrete seller strategy, platform design, and regulatory questions.",
    "authors": [
      "Amine Allouah",
      "Omar Besbes",
      "Josué D Figueroa",
      "Yash Kanoria",
      "Akshit Kumar"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.MA",
      "econ.GN"
    ],
    "published": "2025-08-04T17:19:36Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02630v2"
  },
  {
    "arxiv_id": "2508.02621v2",
    "entry_id": "http://arxiv.org/abs/2508.02621v2",
    "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research",
    "summary": "The rapid proliferation of scientific knowledge presents a grand challenge: transforming this vast repository of information into an active engine for discovery, especially in high-stakes domains like healthcare. Current AI agents, however, are constrained by static, predefined strategies, limiting their ability to navigate the complex, evolving ecosystem of scientific research. This paper introduces HealthFlow, a self-evolving AI agent that overcomes this limitation through a novel meta-level evolution mechanism. HealthFlow autonomously refines its high-level problem-solving policies by distilling procedural successes and failures into a durable, structured knowledge base, enabling it to learn not just how to use tools, but how to strategize. To anchor our research and provide a community resource, we introduce EHRFlowBench, a new benchmark featuring complex health data analysis tasks systematically derived from peer-reviewed scientific literature. Our experiments demonstrate that HealthFlow's self-evolving approach significantly outperforms state-of-the-art agent frameworks. This work offers a new paradigm for intelligent systems that can learn to operationalize the procedural knowledge embedded in scientific content, marking a critical step toward more autonomous and effective AI for healthcare scientific discovery.",
    "authors": [
      "Yinghao Zhu",
      "Yifan Qi",
      "Zixiang Wang",
      "Lei Gu",
      "Dehao Sui",
      "Haoran Hu",
      "Xichen Zhang",
      "Ziyi He",
      "Junjun He",
      "Liantao Ma",
      "Lequan Yu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-08-04T17:08:47Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02621v2"
  },
  {
    "arxiv_id": "2508.02574v1",
    "entry_id": "http://arxiv.org/abs/2508.02574v1",
    "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare",
    "summary": "Arabic-language patient feedback remains under-analysed because dialect diversity and scarce aspect-level sentiment labels hinder automated assessment. To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that merges ChatGPT pseudo-labelling with targeted human review to build the first explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence is annotated with an aspect and sentiment label (positive, negative, or neutral), forming a pioneering Arabic dataset aligned with healthcare themes, with ChatGPT-generated rationales provided for each label to enhance transparency. To evaluate the impact of annotation quality on model performance, we created three versions of the training data: a fully supervised set with all labels reviewed by humans, a semi-supervised set with 50% human review, and an unsupervised set with only machine-generated labels. We fine-tuned two transformer models on these datasets for both aspect and sentiment classification. Experimental results show that our Arabic-specific model achieved high accuracy even with minimal human supervision, reflecting only a minor performance drop when using ChatGPT-only labels. Reducing the number of aspect classes notably improved classification metrics across the board. These findings demonstrate an effective, scalable approach to Arabic aspect-based sentiment analysis (SA) in healthcare, combining large language model annotation with human expertise to produce a robust and explainable dataset. Future directions include generalisation across hospitals, prompt refinement, and interpretable data-driven modelling.",
    "authors": [
      "Eman Alamoudi",
      "Ellis Solaiman"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SI"
    ],
    "published": "2025-08-04T16:28:58Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02574v1"
  },
  {
    "arxiv_id": "2508.02366v3",
    "entry_id": "http://arxiv.org/abs/2508.02366v3",
    "title": "Language Model Guided Reinforcement Learning in Quantitative Trading",
    "summary": "Algorithmic trading requires short-term tactical decisions consistent with long-term financial objectives. Reinforcement Learning (RL) has been applied to such problems, but adoption is limited by myopic behaviour and opaque policies. Large Language Models (LLMs) offer complementary strategic reasoning and multi-modal signal interpretation when guided by well-structured prompts. This paper proposes a hybrid framework in which LLMs generate high-level trading strategies to guide RL agents. We evaluate (i) the economic rationale of LLM-generated strategies through expert review, and (ii) the performance of LLM-guided agents against unguided RL baselines using Sharpe Ratio (SR) and Maximum Drawdown (MDD). Empirical results indicate that LLM guidance improves both return and risk metrics relative to standard RL.",
    "authors": [
      "Adam Darmanin",
      "Vince Vella"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "q-fin.TR"
    ],
    "published": "2025-08-04T12:52:11Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02366v3"
  },
  {
    "arxiv_id": "2508.02312v1",
    "entry_id": "http://arxiv.org/abs/2508.02312v1",
    "title": "A Survey on Data Security in Large Language Models",
    "summary": "Large Language Models (LLMs), now a foundation in advancing natural language processing, power applications such as text generation, machine translation, and conversational systems. Despite their transformative potential, these models inherently rely on massive amounts of training data, often collected from diverse and uncurated sources, which exposes them to serious data security risks. Harmful or malicious data can compromise model behavior, leading to issues such as toxic output, hallucinations, and vulnerabilities to threats such as prompt injection or data poisoning. As LLMs continue to be integrated into critical real-world systems, understanding and addressing these data-centric security risks is imperative to safeguard user trust and system reliability. This survey offers a comprehensive overview of the main data security risks facing LLMs and reviews current defense strategies, including adversarial training, RLHF, and data augmentation. Additionally, we categorize and analyze relevant datasets used for assessing robustness and security across different domains, providing guidance for future research. Finally, we highlight key research directions that focus on secure model updates, explainability-driven defenses, and effective governance frameworks, aiming to promote the safe and responsible development of LLM technology. This work aims to inform researchers, practitioners, and policymakers, driving progress toward data security in LLMs.",
    "authors": [
      "Kang Chen",
      "Xiuze Zhou",
      "Yuanguo Lin",
      "Jinhe Su",
      "Yuanhui Yu",
      "Li Shen",
      "Fan Lin"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-08-04T11:28:34Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02312v1"
  },
  {
    "arxiv_id": "2508.02279v1",
    "entry_id": "http://arxiv.org/abs/2508.02279v1",
    "title": "Dialogue Systems Engineering: A Survey and Future Directions",
    "summary": "This paper proposes to refer to the field of software engineering related to the life cycle of dialogue systems as Dialogue Systems Engineering, and surveys this field while also discussing its future directions. With the advancement of large language models, the core technologies underlying dialogue systems have significantly progressed. As a result, dialogue system technology is now expected to be applied to solving various societal issues and in business contexts. To achieve this, it is important to build, operate, and continuously improve dialogue systems correctly and efficiently. Accordingly, in addition to applying existing software engineering knowledge, it is becoming increasingly important to evolve software engineering tailored specifically to dialogue systems. In this paper, we enumerate the knowledge areas of dialogue systems engineering based on those of software engineering, as defined in the Software Engineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based on this survey, we identify unexplored topics in each area and discuss the future direction of dialogue systems engineering.",
    "authors": [
      "Mikio Nakano",
      "Hironori Takeuchi",
      "Sadahiro Yoshikawa",
      "Yoichi Matsuyama",
      "Kazunori Komatani"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-04T10:49:01Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02279v1"
  },
  {
    "arxiv_id": "2508.02121v1",
    "entry_id": "http://arxiv.org/abs/2508.02121v1",
    "title": "A Survey on AgentOps: Categorization, Challenges, and Future Directions",
    "summary": "As the reasoning capabilities of Large Language Models (LLMs) continue to advance, LLM-based agent systems offer advantages in flexibility and interpretability over traditional systems, garnering increasing attention. However, despite the widespread research interest and industrial application of agent systems, these systems, like their traditional counterparts, frequently encounter anomalies. These anomalies lead to instability and insecurity, hindering their further development. Therefore, a comprehensive and systematic approach to the operation and maintenance of agent systems is urgently needed. Unfortunately, current research on the operations of agent systems is sparse. To address this gap, we have undertaken a survey on agent system operations with the aim of establishing a clear framework for the field, defining the challenges, and facilitating further development. Specifically, this paper begins by systematically defining anomalies within agent systems, categorizing them into intra-agent anomalies and inter-agent anomalies. Next, we introduce a novel and comprehensive operational framework for agent systems, dubbed Agent System Operations (AgentOps). We provide detailed definitions and explanations of its four key stages: monitoring, anomaly detection, root cause analysis, and resolution.",
    "authors": [
      "Zexin Wang",
      "Jingjing Li",
      "Quan Zhou",
      "Haotian Si",
      "Yuanhao Liu",
      "Jianhui Li",
      "Gaogang Xie",
      "Fei Sun",
      "Dan Pei",
      "Changhua Pei"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-08-04T06:59:36Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02121v1"
  },
  {
    "arxiv_id": "2508.02120v1",
    "entry_id": "http://arxiv.org/abs/2508.02120v1",
    "title": "Don't Overthink It: A Survey of Efficient R1-style Large Reasoning Models",
    "summary": "Recently, Large Reasoning Models (LRMs) have gradually become a research hotspot due to their outstanding performance in handling complex tasks. Among them, DeepSeek R1 has garnered significant attention for its exceptional performance and open-source nature, driving advancements in the research of R1-style LRMs. Unlike traditional Large Language Models (LLMs), these models enhance logical deduction and decision-making capabilities during reasoning by incorporating mechanisms such as long chain-of-thought and self-reflection through reinforcement learning. However, with the widespread application of these models, the problem of overthinking has gradually emerged. Specifically, when generating answers, these models often construct excessively long reasoning chains with redundant or repetitive steps, which leads to reduced reasoning efficiency and may affect the accuracy of the final answer. To this end, various efficient reasoning methods have been proposed, aiming to reduce the length of reasoning paths without compromising model performance and reasoning capability. By reviewing the current research advancements in the field of efficient reasoning methods systematically, we categorize existing works into two main directions based on the lens of single-model optimization versus model collaboration: (1) Efficient Reasoning with Single Model, which focuses on improving the reasoning efficiency of individual models; and (2) Efficient Reasoning with Model Collaboration, which explores optimizing reasoning paths through collaboration among multiple models. Besides, we maintain a public GitHub repository that tracks the latest progress in efficient reasoning methods.",
    "authors": [
      "Linan Yue",
      "Yichao Du",
      "Yizhi Wang",
      "Weibo Gao",
      "Fangzhou Yao",
      "Li Wang",
      "Ye Liu",
      "Ziyu Xu",
      "Qi Liu",
      "Shimin Di",
      "Min-Ling Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-04T06:54:31Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02120v1"
  },
  {
    "arxiv_id": "2508.02096v2",
    "entry_id": "http://arxiv.org/abs/2508.02096v2",
    "title": "Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches",
    "summary": "Conversational Recommender Systems (CRSs) are receiving growing research attention across domains, yet their user experience (UX) evaluation remains limited. Existing reviews largely overlook empirical UX studies, particularly in adaptive and large language model (LLM)-based CRSs. To address this gap, we conducted a systematic review following PRISMA guidelines, synthesising 23 empirical studies published between 2017 and 2025. We analysed how UX has been conceptualised, measured, and shaped by domain, adaptivity, and LLM. Our findings reveal persistent limitations: post hoc surveys dominate, turn-level affective UX constructs are rarely assessed, and adaptive behaviours are seldom linked to UX outcomes. LLM-based CRSs introduce further challenges, including epistemic opacity and verbosity, yet evaluations infrequently address these issues. We contribute a structured synthesis of UX metrics, a comparative analysis of adaptive and nonadaptive systems, and a forward-looking agenda for LLM-aware UX evaluation. These findings support the development of more transparent, engaging, and user-centred CRS evaluation practices.",
    "authors": [
      "Raj Mahmud",
      "Yufeng Wu",
      "Abdullah Bin Sawad",
      "Shlomo Berkovsky",
      "Mukesh Prasad",
      "A. Baki Kocaballi"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-08-04T06:07:33Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02096v2"
  },
  {
    "arxiv_id": "2508.01956v1",
    "entry_id": "http://arxiv.org/abs/2508.01956v1",
    "title": "Agent-Based Feature Generation from Clinical Notes for Outcome Prediction",
    "summary": "Electronic health records (EHRs) contain rich unstructured clinical notes that could enhance predictive modeling, yet extracting meaningful features from these notes remains challenging. Current approaches range from labor-intensive manual clinician feature generation (CFG) to fully automated representational feature generation (RFG) that lack interpretability and clinical relevance. Here we introduce SNOW (Scalable Note-to-Outcome Workflow), a modular multi-agent system powered by large language models (LLMs) that autonomously generates structured clinical features from unstructured notes without human intervention. We evaluated SNOW against manual CFG, clinician-guided LLM approaches, and RFG methods for predicting 5-year prostate cancer recurrence in 147 patients from Stanford Healthcare. While manual CFG achieved the highest performance (AUC-ROC: 0.771), SNOW matched this performance (0.761) without requiring any clinical expertise, significantly outperforming both baseline features alone (0.691) and all RFG approaches. The clinician-guided LLM method also performed well (0.732) but still required expert input. SNOW's specialized agents handle feature discovery, extraction, validation, post-processing, and aggregation, creating interpretable features that capture complex clinical information typically accessible only through manual review. Our findings demonstrate that autonomous LLM systems can replicate expert-level feature engineering at scale, potentially transforming how clinical ML models leverage unstructured EHR data while maintaining the interpretability essential for clinical deployment.",
    "authors": [
      "Jiayi Wang",
      "Jacqueline Jil Vallon",
      "Neil Panjwani",
      "Xi Ling",
      "Sushmita Vij",
      "Sandy Srinivas",
      "John Leppert",
      "Mark K. Buyyounouski",
      "Mohsen Bayati"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-08-03T23:45:18Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01956v1"
  },
  {
    "arxiv_id": "2508.01781v1",
    "entry_id": "http://arxiv.org/abs/2508.01781v1",
    "title": "A comprehensive taxonomy of hallucinations in Large Language Models",
    "summary": "Large language models (LLMs) have revolutionized natural language processing, yet their propensity for hallucination, generating plausible but factually incorrect or fabricated content, remains a critical challenge. This report provides a comprehensive taxonomy of LLM hallucinations, beginning with a formal definition and a theoretical framework that posits its inherent inevitability in computable LLMs, irrespective of architecture or training. It explores core distinctions, differentiating between intrinsic (contradicting input context) and extrinsic (inconsistent with training data or reality), as well as factuality (absolute correctness) and faithfulness (adherence to input). The report then details specific manifestations, including factual errors, contextual and logical inconsistencies, temporal disorientation, ethical violations, and task-specific hallucinations across domains like code generation and multimodal applications. It analyzes the underlying causes, categorizing them into data-related issues, model-related factors, and prompt-related influences. Furthermore, the report examines cognitive and human factors influencing hallucination perception, surveys evaluation benchmarks and metrics for detection, and outlines architectural and systemic mitigation strategies. Finally, it introduces web-based resources for monitoring LLM releases and performance. This report underscores the complex, multifaceted nature of LLM hallucinations and emphasizes that, given their theoretical inevitability, future efforts must focus on robust detection, mitigation, and continuous human oversight for responsible and reliable deployment in critical applications.",
    "authors": [
      "Manuel Cossio"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-08-03T14:37:16Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01781v1"
  },
  {
    "arxiv_id": "2508.01780v1",
    "entry_id": "http://arxiv.org/abs/2508.01780v1",
    "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
    "summary": "With the rapid development of Model Context Protocol (MCP), the number of MCP servers has surpassed 10,000. However, existing MCP benchmarks are limited to single-server settings with only a few tools, hindering effective evaluation of agent capabilities in large-scale, real-world scenarios. To address this limitation, we present LiveMCPBench, the first comprehensive benchmark comprising 95 real-world tasks grounded in the MCP ecosystem, designed to evaluate LLM agents at scale across diverse servers. To support a scalable and reproducible evaluation pipeline in large-scale MCP environments, we curate LiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and 527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework that enables automated and adaptive evaluation in dynamic, time-varying task environments, achieving 81% agreement with human reviewers. Finally, we propose the MCP Copilot Agent, a multi-step agent that routes tools for dynamic planning and executes tools for API interaction across the entire LiveMCPTool suite. Our evaluation covers 10 leading models, with the best-performing model (Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large performance variance across models, and several widely-used models perform poorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench offers the first unified framework for benchmarking LLM agents in realistic, tool-rich, and dynamic MCP environments, laying a solid foundation for scalable and reproducible research on agent capabilities. Our code and data will be publicly available at https://icip-cas.github.io/LiveMCPBench.",
    "authors": [
      "Guozhao Mo",
      "Wenliang Zhong",
      "Jiawei Chen",
      "Xuanang Chen",
      "Yaojie Lu",
      "Hongyu Lin",
      "Ben He",
      "Xianpei Han",
      "Le Sun"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-03T14:36:42Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01780v1"
  },
  {
    "arxiv_id": "2508.05668v3",
    "entry_id": "http://arxiv.org/abs/2508.05668v3",
    "title": "A Survey of LLM-based Deep Search Agents: Paradigm, Optimization, Evaluation, and Challenges",
    "summary": "The advent of Large Language Models (LLMs) has significantly revolutionized web search. The emergence of LLM-based Search Agents marks a pivotal shift towards deeper, dynamic, autonomous information seeking. These agents can comprehend user intentions and environmental context and execute multi-turn retrieval with dynamic planning, extending search capabilities far beyond the web. Leading examples like OpenAI's Deep Research highlight their potential for deep information mining and real-world applications. This survey provides the first systematic analysis of search agents. We comprehensively analyze and categorize existing works from the perspectives of architecture, optimization, application, and evaluation, ultimately identifying critical open challenges and outlining promising future research directions in this rapidly evolving field. Our repository is available on https://github.com/YunjiaXi/Awesome-Search-Agent-Papers.",
    "authors": [
      "Yunjia Xi",
      "Jianghao Lin",
      "Yongzhao Xiao",
      "Zheli Zhou",
      "Rong Shan",
      "Te Gao",
      "Jiachen Zhu",
      "Weiwen Liu",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-03T08:02:51Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05668v3"
  },
  {
    "arxiv_id": "2508.01556v1",
    "entry_id": "http://arxiv.org/abs/2508.01556v1",
    "title": "Empowering Tabular Data Preparation with Language Models: Why and How?",
    "summary": "Data preparation is a critical step in enhancing the usability of tabular data and thus boosts downstream data-driven tasks. Traditional methods often face challenges in capturing the intricate relationships within tables and adapting to the tasks involved. Recent advances in Language Models (LMs), especially in Large Language Models (LLMs), offer new opportunities to automate and support tabular data preparation. However, why LMs suit tabular data preparation (i.e., how their capabilities match task demands) and how to use them effectively across phases still remain to be systematically explored. In this survey, we systematically analyze the role of LMs in enhancing tabular data preparation processes, focusing on four core phases: data acquisition, integration, cleaning, and transformation. For each phase, we present an integrated analysis of how LMs can be combined with other components for different preparation tasks, highlight key advancements, and outline prospective pipelines.",
    "authors": [
      "Mengshi Chen",
      "Yuxiang Sun",
      "Tengchao Li",
      "Jianwei Wang",
      "Kai Wang",
      "Xuemin Lin",
      "Ying Zhang",
      "Wenjie Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-03T03:00:02Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01556v1"
  },
  {
    "arxiv_id": "2508.01554v1",
    "entry_id": "http://arxiv.org/abs/2508.01554v1",
    "title": "Are All Prompt Components Value-Neutral? Understanding the Heterogeneous Adversarial Robustness of Dissected Prompt in Large Language Models",
    "summary": "Prompt-based adversarial attacks have become an effective means to assess the robustness of large language models (LLMs). However, existing approaches often treat prompts as monolithic text, overlooking their structural heterogeneity-different prompt components contribute unequally to adversarial robustness. Prior works like PromptRobust assume prompts are value-neutral, but our analysis reveals that complex, domain-specific prompts with rich structures have components with differing vulnerabilities. To address this gap, we introduce PromptAnatomy, an automated framework that dissects prompts into functional components and generates diverse, interpretable adversarial examples by selectively perturbing each component using our proposed method, ComPerturb. To ensure linguistic plausibility and mitigate distribution shifts, we further incorporate a perplexity (PPL)-based filtering mechanism. As a complementary resource, we annotate four public instruction-tuning datasets using the PromptAnatomy framework, verified through human review. Extensive experiments across these datasets and five advanced LLMs demonstrate that ComPerturb achieves state-of-the-art attack success rates. Ablation studies validate the complementary benefits of prompt dissection and PPL filtering. Our results underscore the importance of prompt structure awareness and controlled perturbation for reliable adversarial robustness evaluation in LLMs. Code and data are available at https://github.com/Yujiaaaaa/PACP.",
    "authors": [
      "Yujia Zheng",
      "Tianhao Li",
      "Haotian Huang",
      "Tianyu Zeng",
      "Jingyu Lu",
      "Chuangxin Chu",
      "Yuekai Huang",
      "Ziyou Jiang",
      "Qian Xiong",
      "Yuyao Ge",
      "Mingyang Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-08-03T02:46:30Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01554v1"
  },
  {
    "arxiv_id": "2508.02744v1",
    "entry_id": "http://arxiv.org/abs/2508.02744v1",
    "title": "Large Language Model-based Data Science Agent: A Survey",
    "summary": "The rapid advancement of Large Language Models (LLMs) has driven novel applications across diverse domains, with LLM-based agents emerging as a crucial area of exploration. This survey presents a comprehensive analysis of LLM-based agents designed for data science tasks, summarizing insights from recent studies. From the agent perspective, we discuss the key design principles, covering agent roles, execution, knowledge, and reflection methods. From the data science perspective, we identify key processes for LLM-based agents, including data preprocessing, model development, evaluation, visualization, etc. Our work offers two key contributions: (1) a comprehensive review of recent developments in applying LLMbased agents to data science tasks; (2) a dual-perspective framework that connects general agent design principles with the practical workflows in data science.",
    "authors": [
      "Peiran Wang",
      "Yaoning Yu",
      "Ke Chen",
      "Xianyang Zhan",
      "Haohan Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-02T17:33:18Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02744v1"
  },
  {
    "arxiv_id": "2508.02740v1",
    "entry_id": "http://arxiv.org/abs/2508.02740v1",
    "title": "Who Gets Cited? Gender- and Majority-Bias in LLM-Driven Reference Selection",
    "summary": "Large language models (LLMs) are rapidly being adopted as research assistants, particularly for literature review and reference recommendation, yet little is known about whether they introduce demographic bias into citation workflows. This study systematically investigates gender bias in LLM-driven reference selection using controlled experiments with pseudonymous author names. We evaluate several LLMs (GPT-4o, GPT-4o-mini, Claude Sonnet, and Claude Haiku) by varying gender composition within candidate reference pools and analyzing selection patterns across fields. Our results reveal two forms of bias: a persistent preference for male-authored references and a majority-group bias that favors whichever gender is more prevalent in the candidate pool. These biases are amplified in larger candidate pools and only modestly attenuated by prompt-based mitigation strategies. Field-level analysis indicates that bias magnitude varies across scientific domains, with social sciences showing the least bias. Our findings indicate that LLMs can reinforce or exacerbate existing gender imbalances in scholarly recognition. Effective mitigation strategies are needed to avoid perpetuating existing gender disparities in scientific citation practices before integrating LLMs into high-stakes academic workflows.",
    "authors": [
      "Jiangen He"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-08-02T13:27:32Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02740v1"
  },
  {
    "arxiv_id": "2508.01341v3",
    "entry_id": "http://arxiv.org/abs/2508.01341v3",
    "title": "Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: \"One Map, Many Trials\" in Satellite-Driven Poverty Analysis",
    "summary": "Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a \"one map, many trials\" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).",
    "authors": [
      "Markus B. Pettersson",
      "Connor T. Jerzak",
      "Adel Daoud"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-08-02T12:26:26Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01341v3"
  },
  {
    "arxiv_id": "2508.01263v1",
    "entry_id": "http://arxiv.org/abs/2508.01263v1",
    "title": "Bridging LLMs and Symbolic Reasoning in Educational QA Systems: Insights from the XAI Challenge at IJCNN 2025",
    "summary": "The growing integration of Artificial Intelligence (AI) into education has intensified the need for transparency and interpretability. While hackathons have long served as agile environments for rapid AI prototyping, few have directly addressed eXplainable AI (XAI) in real-world educational contexts. This paper presents a comprehensive analysis of the XAI Challenge 2025, a hackathon-style competition jointly organized by Ho Chi Minh City University of Technology (HCMUT) and the International Workshop on Trustworthiness and Reliability in Neurosymbolic AI (TRNS-AI), held as part of the International Joint Conference on Neural Networks (IJCNN 2025). The challenge tasked participants with building Question-Answering (QA) systems capable of answering student queries about university policies while generating clear, logic-based natural language explanations. To promote transparency and trustworthiness, solutions were required to use lightweight Large Language Models (LLMs) or hybrid LLM-symbolic systems. A high-quality dataset was provided, constructed via logic-based templates with Z3 validation and refined through expert student review to ensure alignment with real-world academic scenarios. We describe the challenge's motivation, structure, dataset construction, and evaluation protocol. Situating the competition within the broader evolution of AI hackathons, we argue that it represents a novel effort to bridge LLMs and symbolic reasoning in service of explainability. Our findings offer actionable insights for future XAI-centered educational systems and competitive research initiatives.",
    "authors": [
      "Long S. T. Nguyen",
      "Khang H. N. Vo",
      "Thu H. A. Nguyen",
      "Tuan C. Bui",
      "Duc Q. Nguyen",
      "Thanh-Tung Tran",
      "Anh D. Nguyen",
      "Minh L. Nguyen",
      "Fabien Baldacci",
      "Thang H. Bui",
      "Emanuel Di Nardo",
      "Angelo Ciaramella",
      "Son H. Le",
      "Ihsan Ullah",
      "Lorenzo Di Rocco",
      "Tho T. Quan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LO"
    ],
    "published": "2025-08-02T08:46:06Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01263v1"
  },
  {
    "arxiv_id": "2508.01186v1",
    "entry_id": "http://arxiv.org/abs/2508.01186v1",
    "title": "A Survey on Agent Workflow -- Status and Future",
    "summary": "In the age of large language models (LLMs), autonomous agents have emerged as a powerful paradigm for achieving general intelligence. These agents dynamically leverage tools, memory, and reasoning capabilities to accomplish user-defined goals. As agent systems grow in complexity, agent workflows-structured orchestration frameworks-have become central to enabling scalable, controllable, and secure AI behaviors. This survey provides a comprehensive review of agent workflow systems, spanning academic frameworks and industrial implementations. We classify existing systems along two key dimensions: functional capabilities (e.g., planning, multi-agent collaboration, external API integration) and architectural features (e.g., agent roles, orchestration flows, specification languages). By comparing over 20 representative systems, we highlight common patterns, potential technical challenges, and emerging trends. We further address concerns related to workflow optimization strategies and security. Finally, we outline open problems such as standardization and multimodal integration, offering insights for future research at the intersection of agent design, workflow infrastructure, and safe automation.",
    "authors": [
      "Chaojia Yu",
      "Zihan Cheng",
      "Hanwen Cui",
      "Yishuo Gao",
      "Zexu Luo",
      "Yijin Wang",
      "Hangbin Zheng",
      "Yong Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-08-02T04:15:30Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01186v1"
  },
  {
    "arxiv_id": "2508.01128v1",
    "entry_id": "http://arxiv.org/abs/2508.01128v1",
    "title": "Towards Bridging Review Sparsity in Recommendation with Textual Edge Graph Representation",
    "summary": "Textual reviews enrich recommender systems with fine-grained preference signals and enhanced explainability. However, in real-world scenarios, users rarely leave reviews, resulting in severe sparsity that undermines the effectiveness of existing models. A natural solution is to impute or generate missing reviews to enrich the data. However, conventional imputation techniques -- such as matrix completion and LLM-based augmentation -- either lose contextualized semantics by embedding texts into vectors, or overlook structural dependencies among user-item interactions. To address these shortcomings, we propose TWISTER (ToWards Imputation on Sparsity with Textual Edge Graph Representation), a unified framework that imputes missing reviews by jointly modeling semantic and structural signals. Specifically, we represent user-item interactions as a Textual-Edge Graph (TEG), treating reviews as edge attributes. To capture relational context, we construct line-graph views and employ a large language model as a graph-aware aggregator. For each interaction lacking a textual review, our model aggregates the neighborhood's natural-language representations to generate a coherent and personalized review. Experiments on the Amazon and Goodreads datasets show that TWISTER consistently outperforms traditional numeric, graph-based, and LLM baselines, delivering higher-quality imputed reviews and, more importantly, enhanced recommendation performance. In summary, TWISTER generates reviews that are more helpful, authentic, and specific, while smoothing structural signals for improved recommendations.",
    "authors": [
      "Leyao Wang",
      "Xutao Mao",
      "Xuhui Zhan",
      "Yuying Zhao",
      "Bo Ni",
      "Ryan A. Rossi",
      "Nesreen K. Ahmed",
      "Tyler Derr"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-08-02T00:53:40Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01128v1"
  },
  {
    "arxiv_id": "2508.01109v1",
    "entry_id": "http://arxiv.org/abs/2508.01109v1",
    "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language Codes or Agent-Induced Novelty?",
    "summary": "We investigate whether socio-economic indicators like household wealth leave recoverable imprints in satellite imagery (capturing physical features) and Internet-sourced text (reflecting historical/economic narratives). Using Demographic and Health Survey (DHS) data from African neighborhoods, we pair Landsat images with LLM-generated textual descriptions conditioned on location/year and text retrieved by an AI search agent from web sources. We develop a multimodal framework predicting household wealth (International Wealth Index) through five pipelines: (i) vision model on satellite images, (ii) LLM using only location/year, (iii) AI agent searching/synthesizing web text, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework yields three contributions. First, fusing vision and agent/LLM text outperforms vision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on out-of-sample splits), with LLM-internal knowledge proving more effective than agent-retrieved text, improving robustness to out-of-country and out-of-time generalization. Second, we find partial representational convergence: fused embeddings from vision/language modalities correlate moderately (median cosine similarity of 0.60 after alignment), suggesting a shared latent code of material well-being while retaining complementary details, consistent with the Platonic Representation Hypothesis. Although LLM-only text outperforms agent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest gains from combining agent data in some splits weakly support the notion that agent-gathered information introduces unique representational structures not fully captured by static LLM knowledge. Third, we release a large-scale multimodal dataset comprising more than 60,000 DHS clusters linked to satellite images, LLM-generated descriptions, and agent-retrieved texts.",
    "authors": [
      "Satiyabooshan Murugaboopathy",
      "Connor T. Jerzak",
      "Adel Daoud"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-08-01T23:07:16Z",
    "pdf_url": "https://arxiv.org/pdf/2508.01109v1"
  },
  {
    "arxiv_id": "2508.02732v1",
    "entry_id": "http://arxiv.org/abs/2508.02732v1",
    "title": "A Note on Code Quality Score: LLMs for Maintainable Large Codebases",
    "summary": "Maintaining code quality in large-scale software systems presents significant challenges, particularly in settings where a large numbers of engineers work concurrently on a codebase. This paper introduces Code Quality Score (CQS) system to automatically detect issues with a set of code changes and provide actionable insights. At its core, the CQS system is powered by two Llama3 models, fine-tuned (with SFT and offline RL approaches), to a) detect common code quality issues related to coding best practices and b) to provide good ``critiques'' for LLM-generated code review respectively. To maintain good user experience, we layer the system with hand-crafted rules to filter out incorrect responses/hallucinations. Offline evaluations show that our CQS system is able to achieve an impressive precision rate for identifying valid issues. This system has already been rolled out to developers in an industrial scale setting and has consistently achieved 60\\% week over week user helpfulness rate, demonstrating its effectiveness in a real-world environment. In this paper, we present details of the CQS system along with some learnings on curating developer feedback to create training data for LLM fine-tuning.",
    "authors": [
      "Sherman Wong",
      "Jalaj Bhandari",
      "Leo Zhou Fan Yang",
      "Xylan Xu",
      "Yi Zhuang",
      "Cem Cayiroglu",
      "Payal Bhuptani",
      "Sheela Yadawad",
      "Hung Duong"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-08-01T21:09:45Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02732v1"
  },
  {
    "arxiv_id": "2508.02731v1",
    "entry_id": "http://arxiv.org/abs/2508.02731v1",
    "title": "Teaching at Scale: Leveraging AI to Evaluate and Elevate Engineering Education",
    "summary": "Evaluating teaching effectiveness at scale remains a persistent challenge for large universities, particularly within engineering programs that enroll tens of thousands of students. Traditional methods, such as manual review of student evaluations, are often impractical, leading to overlooked insights and inconsistent data use. This article presents a scalable, AI-supported framework for synthesizing qualitative student feedback using large language models. The system employs hierarchical summarization, anonymization, and exception handling to extract actionable themes from open-ended comments while upholding ethical safeguards. Visual analytics contextualize numeric scores through percentile-based comparisons, historical trends, and instructional load. The approach supports meaningful evaluation and aligns with best practices in qualitative analysis and educational assessment, incorporating student, peer, and self-reflective inputs without automating personnel decisions. We report on its successful deployment across a large college of engineering. Preliminary validation through comparisons with human reviewers, faculty feedback, and longitudinal analysis suggests that LLM-generated summaries can reliably support formative evaluation and professional development. This work demonstrates how AI systems, when designed with transparency and shared governance, can promote teaching excellence and continuous improvement at scale within academic institutions.",
    "authors": [
      "Jean-Francois Chamberland",
      "Martin C. Carlisle",
      "Arul Jayaraman",
      "Krishna R. Narayanan",
      "Sunay Palsole",
      "Karan Watson"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-08-01T20:27:40Z",
    "pdf_url": "https://arxiv.org/pdf/2508.02731v1"
  },
  {
    "arxiv_id": "2508.00742v2",
    "entry_id": "http://arxiv.org/abs/2508.00742v2",
    "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents",
    "summary": "Generative agents powered by Large Language Models demonstrate human-like characteristics through sophisticated natural language interactions. Their ability to assume roles and personalities based on predefined character biographies has positioned them as cost-effective substitutes for human participants in social science research. This paper explores the validity of such persona-based agents in representing human populations; we recreate the HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents, conducting factor analysis on their responses, and comparing these results to the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results found 1) a coherent and reliable personality structure was recoverable from the agents' responses demonstrating partial alignment to the HEXACO framework. 2) the derived personality dimensions were consistent and reliable within GPT-4, when coupled with a sufficiently curated population, and 3) cross-model analysis revealed variability in personality profiling, suggesting model-specific biases and limitations. We discuss the practical considerations and challenges encountered during the experiment. This study contributes to the ongoing discourse on the potential benefits and limitations of using generative agents in social science research and provides useful guidance on designing consistent and representative agent personas to maximise coverage and representation of human personality traits.",
    "authors": [
      "Sarah Mercer",
      "Daniel P. Martin",
      "Phil Swatton"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-08-01T16:16:16Z",
    "pdf_url": "https://arxiv.org/pdf/2508.00742v2"
  },
  {
    "arxiv_id": "2508.00737v2",
    "entry_id": "http://arxiv.org/abs/2508.00737v2",
    "title": "How LLMs are Shaping the Future of Virtual Reality",
    "summary": "The integration of Large Language Models (LLMs) into Virtual Reality (VR) games marks a paradigm shift in the design of immersive, adaptive, and intelligent digital experiences. This paper presents a comprehensive review of recent research at the intersection of LLMs and VR, examining how these models are transforming narrative generation, non-player character (NPC) interactions, accessibility, personalization, and game mastering. Drawing from an analysis of 62 peer reviewed studies published between 2018 and 2025, we identify key application domains ranging from emotionally intelligent NPCs and procedurally generated storytelling to AI-driven adaptive systems and inclusive gameplay interfaces. We also address the major challenges facing this convergence, including real-time performance constraints, memory limitations, ethical risks, and scalability barriers. Our findings highlight that while LLMs significantly enhance realism, creativity, and user engagement in VR environments, their effective deployment requires robust design strategies that integrate multimodal interaction, hybrid AI architectures, and ethical safeguards. The paper concludes by outlining future research directions in multimodal AI, affective computing, reinforcement learning, and open-source development, aiming to guide the responsible advancement of intelligent and inclusive VR systems.",
    "authors": [
      "Süeda Özkaya",
      "Santiago Berrezueta-Guzman",
      "Stefan Wagner"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-08-01T16:08:05Z",
    "pdf_url": "https://arxiv.org/pdf/2508.00737v2"
  },
  {
    "arxiv_id": "2508.00669v1",
    "entry_id": "http://arxiv.org/abs/2508.00669v1",
    "title": "Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications",
    "summary": "The proliferation of Large Language Models (LLMs) in medicine has enabled impressive capabilities, yet a critical gap remains in their ability to perform systematic, transparent, and verifiable reasoning, a cornerstone of clinical practice. This has catalyzed a shift from single-step answer generation to the development of LLMs explicitly designed for medical reasoning. This paper provides the first systematic review of this emerging field. We propose a taxonomy of reasoning enhancement techniques, categorized into training-time strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how these techniques are applied across different data modalities (text, image, code) and in key clinical applications such as diagnosis, education, and treatment planning. Furthermore, we survey the evolution of evaluation benchmarks from simple accuracy metrics to sophisticated assessments of reasoning quality and visual interpretability. Based on an analysis of 60 seminal studies from 2022-2025, we conclude by identifying critical challenges, including the faithfulness-plausibility gap and the need for native multimodal reasoning, and outlining future directions toward building efficient, robust, and sociotechnically responsible medical AI.",
    "authors": [
      "Wenxuan Wang",
      "Zizhan Ma",
      "Meidan Ding",
      "Shiyi Zheng",
      "Shengyuan Liu",
      "Jie Liu",
      "Jiaming Ji",
      "Wenting Chen",
      "Xiang Li",
      "Linlin Shen",
      "Yixuan Yuan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-08-01T14:41:31Z",
    "pdf_url": "https://arxiv.org/pdf/2508.00669v1"
  },
  {
    "arxiv_id": "2508.00632v1",
    "entry_id": "http://arxiv.org/abs/2508.00632v1",
    "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
    "summary": "While AI excels at generating text, audio, images, and videos, creating interactive audio-visual content such as video games remains challenging. Current LLMs can generate JavaScript games and animations, but lack automated evaluation metrics and struggle with complex content that normally requires teams of humans working for many months (multi-shot, multi-agents) using assets made by artists. To tackle these issues, we built a new metric and a multi-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video, and audio) compares the AVRs of two contents, with a text model reviewing evaluations to determine superiority. We show that AVR-Eval properly identifies good from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a bank of multimedia assets (audio, images, 3D models). The coding agent selects relevant assets, generates multiple initial codes, uses AVR-Eval to identify the best version, and iteratively improves it through omni-modal agent feedback from the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content A against B). We find that content generated by AVR-Agent has a significantly higher win rate against content made through one-shot generation. However, models struggle to leverage custom assets and AVR feedback effectively, showing no higher win rate. This reveals a critical gap: while humans benefit from high-quality assets and audio-visual feedback, current coding models do not seem to utilize these resources as effectively, highlighting fundamental differences between human and machine content creation approaches.",
    "authors": [
      "Alexia Jolicoeur-Martineau"
    ],
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.MM"
    ],
    "published": "2025-08-01T13:45:13Z",
    "pdf_url": "https://arxiv.org/pdf/2508.00632v1"
  },
  {
    "arxiv_id": "2508.00217v1",
    "entry_id": "http://arxiv.org/abs/2508.00217v1",
    "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges",
    "summary": "Tables have gained significant attention in large language models (LLMs) and multimodal large language models (MLLMs) due to their complex and flexible structure. Unlike linear text inputs, tables are two-dimensional, encompassing formats that range from well-structured database tables to complex, multi-layered spreadsheets, each with different purposes. This diversity in format and purpose has led to the development of specialized methods and tasks, instead of universal approaches, making navigation of table understanding tasks challenging. To address these challenges, this paper introduces key concepts through a taxonomy of tabular input representations and an introduction of table understanding tasks. We highlight several critical gaps in the field that indicate the need for further research: (1) the predominance of retrieval-focused tasks that require minimal reasoning beyond mathematical and logical operations; (2) significant challenges faced by models when processing complex table structures, large-scale tables, length context, or multi-table scenarios; and (3) the limited generalization of models across different tabular representations and formats.",
    "authors": [
      "Xiaofeng Wu",
      "Alan Ritter",
      "Wei Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-07-31T23:41:31Z",
    "pdf_url": "https://arxiv.org/pdf/2508.00217v1"
  },
  {
    "arxiv_id": "2508.00083v2",
    "entry_id": "http://arxiv.org/abs/2508.00083v2",
    "title": "A Survey on Code Generation with LLM-based Agents",
    "summary": "Code generation agents powered by large language models (LLMs) are revolutionizing the software development paradigm. Distinct from previous code generation techniques, code generation agents are characterized by three core features. 1) Autonomy: the ability to independently manage the entire workflow, from task decomposition to coding and debugging. 2) Expanded task scope: capabilities that extend beyond generating code snippets to encompass the full software development lifecycle (SDLC). 3) Enhancement of engineering practicality: a shift in research emphasis from algorithmic innovation toward practical engineering challenges, such as system reliability, process management, and tool integration. This domain has recently witnessed rapid development and an explosion in research, demonstrating significant application potential. This paper presents a systematic survey of the field of LLM-based code generation agents. We trace the technology's developmental trajectory from its inception and systematically categorize its core techniques, including both single-agent and multi-agent architectures. Furthermore, this survey details the applications of LLM-based agents across the full SDLC, summarizes mainstream evaluation benchmarks and metrics, and catalogs representative tools. Finally, by analyzing the primary challenges, we identify and propose several foundational, long-term research directions for the future work of the field.",
    "authors": [
      "Yihong Dong",
      "Xue Jiang",
      "Jiaru Qian",
      "Tian Wang",
      "Kechi Zhang",
      "Zhi Jin",
      "Ge Li"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-07-31T18:17:36Z",
    "pdf_url": "https://arxiv.org/pdf/2508.00083v2"
  },
  {
    "arxiv_id": "2507.23694v1",
    "entry_id": "http://arxiv.org/abs/2507.23694v1",
    "title": "A survey of multi-agent geosimulation methodologies: from ABM to LLM",
    "summary": "We provide a comprehensive examination of agent-based approaches that codify the principles and linkages underlying multi-agent systems, simulations, and information systems. Based on two decades of study, this paper confirms a framework intended as a formal specification for geosimulation platforms. Our findings show that large language models (LLMs) can be effectively incorporated as agent components if they follow a structured architecture specific to fundamental agent activities such as perception, memory, planning, and action. This integration is precisely consistent with the architecture that we formalize, providing a solid platform for next-generation geosimulation systems.",
    "authors": [
      "Virginia Padilla",
      "Jacinto Dávila"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-07-31T16:12:22Z",
    "pdf_url": "https://arxiv.org/pdf/2507.23694v1"
  },
  {
    "arxiv_id": "2508.03734v1",
    "entry_id": "http://arxiv.org/abs/2508.03734v1",
    "title": "A Survey of Multimodal Ophthalmic Diagnostics: From Task-Specific Approaches to Foundational Models",
    "summary": "Visual impairment represents a major global health challenge, with multimodal imaging providing complementary information that is essential for accurate ophthalmic diagnosis. This comprehensive survey systematically reviews the latest advances in multimodal deep learning methods in ophthalmology up to the year 2025. The review focuses on two main categories: task-specific multimodal approaches and large-scale multimodal foundation models. Task-specific approaches are designed for particular clinical applications such as lesion detection, disease diagnosis, and image synthesis. These methods utilize a variety of imaging modalities including color fundus photography, optical coherence tomography, and angiography. On the other hand, foundation models combine sophisticated vision-language architectures and large language models pretrained on diverse ophthalmic datasets. These models enable robust cross-modal understanding, automated clinical report generation, and decision support. The survey critically examines important datasets, evaluation metrics, and methodological innovations including self-supervised learning, attention-based fusion, and contrastive alignment. It also discusses ongoing challenges such as variability in data, limited annotations, lack of interpretability, and issues with generalizability across different patient populations. Finally, the survey outlines promising future directions that emphasize the use of ultra-widefield imaging and reinforcement learning-based reasoning frameworks to create intelligent, interpretable, and clinically applicable AI systems for ophthalmology.",
    "authors": [
      "Xiaoling Luo",
      "Ruli Zheng",
      "Qiaojian Zheng",
      "Zibo Du",
      "Shuo Yang",
      "Meidan Ding",
      "Qihao Xu",
      "Chengliang Liu",
      "Linlin Shen"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-07-31T10:49:21Z",
    "pdf_url": "https://arxiv.org/pdf/2508.03734v1"
  },
  {
    "arxiv_id": "2507.23356v1",
    "entry_id": "http://arxiv.org/abs/2507.23356v1",
    "title": "Quality Evaluation of COBOL to Java Code Transformation",
    "summary": "We present an automated evaluation system for assessing COBOL-to-Java code translation within IBM's watsonx Code Assistant for Z (WCA4Z). The system addresses key challenges in evaluating LLM-based translators, including model opacity and the complexity of translation quality assessment. Our approach combines analytic checkers with LLM-as-a-judge (LaaJ) techniques to deliver scalable, multi-faceted evaluations. The system supports continuous integration workflows, enables large-scale benchmarking, and reduces reliance on manual review. We describe the system architecture, evaluation strategies, and reporting mechanisms that provide actionable insights for developers and project managers, facilitating the evolution of high-quality, modernized codebases.",
    "authors": [
      "Shmulik Froimovich",
      "Raviv Gal",
      "Wesam Ibraheem",
      "Avi Ziv"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-07-31T09:06:20Z",
    "pdf_url": "https://arxiv.org/pdf/2507.23356v1"
  },
  {
    "arxiv_id": "2507.23276v2",
    "entry_id": "http://arxiv.org/abs/2507.23276v2",
    "title": "How Far Are AI Scientists from Changing the World?",
    "summary": "The emergence of large language models (LLMs) is propelling automated scientific discovery to the next level, with LLM-based Artificial Intelligence (AI) Scientist systems now taking the lead in scientific research. Several influential works have already appeared in the field of AI Scientist systems, with AI-generated research papers having been accepted at the ICLR 2025 workshop, suggesting that a human-level AI Scientist capable of uncovering phenomena previously unknown to humans, may soon become a reality. In this survey, we focus on the central question: How far are AI scientists from changing the world and reshaping the scientific research paradigm? To answer this question, we provide a prospect-driven review that comprehensively analyzes the current achievements of AI Scientist systems, identifying key bottlenecks and the critical components required for the emergence of a scientific agent capable of producing ground-breaking discoveries that solve grand challenges. We hope this survey will contribute to a clearer understanding of limitations of current AI Scientist systems, showing where we are, what is missing, and what the ultimate goals for scientific AI should be.",
    "authors": [
      "Qiujie Xie",
      "Yixuan Weng",
      "Minjun Zhu",
      "Fuchen Shen",
      "Shulin Huang",
      "Zhen Lin",
      "Jiahui Zhou",
      "Zilan Mao",
      "Zijie Yang",
      "Linyi Yang",
      "Jian Wu",
      "Yue Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-31T06:32:06Z",
    "pdf_url": "https://arxiv.org/pdf/2507.23276v2"
  },
  {
    "arxiv_id": "2508.05660v1",
    "entry_id": "http://arxiv.org/abs/2508.05660v1",
    "title": "Open-Source Agentic Hybrid RAG Framework for Scientific Literature Review",
    "summary": "The surge in scientific publications challenges traditional review methods, demanding tools that integrate structured metadata with full-text analysis. Hybrid Retrieval Augmented Generation (RAG) systems, combining graph queries with vector search offer promise but are typically static, rely on proprietary tools, and lack uncertainty estimates. We present an agentic approach that encapsulates the hybrid RAG pipeline within an autonomous agent capable of (1) dynamically selecting between GraphRAG and VectorRAG for each query, (2) adapting instruction-tuned generation in real time to researcher needs, and (3) quantifying uncertainty during inference. This dynamic orchestration improves relevance, reduces hallucinations, and promotes reproducibility.\n  Our pipeline ingests bibliometric open-access data from PubMed, arXiv, and Google Scholar APIs, builds a Neo4j citation-based knowledge graph (KG), and embeds full-text PDFs into a FAISS vector store (VS) using the all-MiniLM-L6-v2 model. A Llama-3.3-70B agent selects GraphRAG (translating queries to Cypher for KG) or VectorRAG (combining sparse and dense retrieval with re-ranking). Instruction tuning refines domain-specific generation, and bootstrapped evaluation yields standard deviation for evaluation metrics.\n  On synthetic benchmarks mimicking real-world queries, the Instruction-Tuned Agent with Direct Preference Optimization (DPO) outperforms the baseline, achieving a gain of 0.63 in VS Context Recall and a 0.56 gain in overall Context Precision. Additional gains include 0.24 in VS Faithfulness, 0.12 in both VS Precision and KG Answer Relevance, 0.11 in overall Faithfulness score, 0.05 in KG Context Recall, and 0.04 in both VS Answer Relevance and overall Precision. These results highlight the system's improved reasoning over heterogeneous sources and establish a scalable framework for autonomous, agentic scientific discovery.",
    "authors": [
      "Aditya Nagori",
      "Ricardo Accorsi Casonatto",
      "Ayush Gautam",
      "Abhinav Manikantha Sai Cheruvu",
      "Rishikesan Kamaleswaran"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-07-30T18:54:15Z",
    "pdf_url": "https://arxiv.org/pdf/2508.05660v1"
  },
  {
    "arxiv_id": "2507.22659v1",
    "entry_id": "http://arxiv.org/abs/2507.22659v1",
    "title": "A Systematic Literature Review on Detecting Software Vulnerabilities with Large Language Models",
    "summary": "The increasing adoption of Large Language Models (LLMs) in software engineering has sparked interest in their use for software vulnerability detection. However, the rapid development of this field has resulted in a fragmented research landscape, with diverse studies that are difficult to compare due to differences in, e.g., system designs and dataset usage. This fragmentation makes it difficult to obtain a clear overview of the state-of-the-art or compare and categorize studies meaningfully. In this work, we present a comprehensive systematic literature review (SLR) of LLM-based software vulnerability detection. We analyze 227 studies published between January 2020 and June 2025, categorizing them by task formulation, input representation, system architecture, and adaptation techniques. Further, we analyze the datasets used, including their characteristics, vulnerability coverage, and diversity. We present a fine-grained taxonomy of vulnerability detection approaches, identify key limitations, and outline actionable future research opportunities. By providing a structured overview of the field, this review improves transparency and serves as a practical guide for researchers and practitioners aiming to conduct more comparable and reproducible research. We publicly release all artifacts and maintain a living repository of LLM-based software vulnerability detection studies.",
    "authors": [
      "Sabrina Kaniewski",
      "Fabian Schmidt",
      "Markus Enzweiler",
      "Michael Menth",
      "Tobias Heer"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-07-30T13:17:16Z",
    "pdf_url": "https://arxiv.org/pdf/2507.22659v1"
  },
  {
    "arxiv_id": "2507.22610v1",
    "entry_id": "http://arxiv.org/abs/2507.22610v1",
    "title": "Metamorphic Testing of Deep Code Models: A Systematic Literature Review",
    "summary": "Large language models and deep learning models designed for code intelligence have revolutionized the software engineering field due to their ability to perform various code-related tasks. These models can process source code and software artifacts with high accuracy in tasks such as code completion, defect detection, and code summarization; therefore, they can potentially become an integral part of modern software engineering practices. Despite these capabilities, robustness remains a critical quality attribute for deep-code models as they may produce different results under varied and adversarial conditions (e.g., variable renaming). Metamorphic testing has become a widely used approach to evaluate models' robustness by applying semantic-preserving transformations to input programs and analyzing the stability of model outputs. While prior research has explored testing deep learning models, this systematic literature review focuses specifically on metamorphic testing for deep code models. By studying 45 primary papers, we analyze the transformations, techniques, and evaluation methods used to assess robustness. Our review summarizes the current landscape, identifying frequently evaluated models, programming tasks, datasets, target languages, and evaluation metrics, and highlights key challenges and future directions for advancing the field.",
    "authors": [
      "Ali Asgari",
      "Milan de Koning",
      "Pouria Derakhshanfar",
      "Annibale Panichella"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-07-30T12:25:30Z",
    "pdf_url": "https://arxiv.org/pdf/2507.22610v1"
  },
  {
    "arxiv_id": "2508.00917v1",
    "entry_id": "http://arxiv.org/abs/2508.00917v1",
    "title": "A Survey on Deep Multi-Task Learning in Connected Autonomous Vehicles",
    "summary": "Connected autonomous vehicles (CAVs) must simultaneously perform multiple tasks, such as object detection, semantic segmentation, depth estimation, trajectory prediction, motion prediction, and behaviour prediction, to ensure safe and reliable navigation in complex environments. Vehicle-to-everything (V2X) communication enables cooperative driving among CAVs, thereby mitigating the limitations of individual sensors, reducing occlusions, and improving perception over long distances. Traditionally, these tasks are addressed using distinct models, which leads to high deployment costs, increased computational overhead, and challenges in achieving real-time performance. Multi-task learning (MTL) has recently emerged as a promising solution that enables the joint learning of multiple tasks within a single unified model. This offers improved efficiency and resource utilization. To the best of our knowledge, this survey is the first comprehensive review focused on MTL in the context of CAVs. We begin with an overview of CAVs and MTL to provide foundational background. We then explore the application of MTL across key functional modules, including perception, prediction, planning, control, and multi-agent collaboration. Finally, we discuss the strengths and limitations of existing methods, identify key research gaps, and provide directions for future research aimed at advancing MTL methodologies for CAV systems.",
    "authors": [
      "Jiayuan Wang",
      "Farhad Pourpanah",
      "Q. M. Jonathan Wu",
      "Ning Zhang"
    ],
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-07-29T22:17:28Z",
    "pdf_url": "https://arxiv.org/pdf/2508.00917v1"
  },
  {
    "arxiv_id": "2507.21504v1",
    "entry_id": "http://arxiv.org/abs/2507.21504v1",
    "title": "Evaluation and Benchmarking of LLM Agents: A Survey",
    "summary": "The rise of LLM-based agents has opened new frontiers in AI applications, yet evaluating these agents remains a complex and underdeveloped area. This survey provides an in-depth overview of the emerging field of LLM agent evaluation, introducing a two-dimensional taxonomy that organizes existing work along (1) evaluation objectives -- what to evaluate, such as agent behavior, capabilities, reliability, and safety -- and (2) evaluation process -- how to evaluate, including interaction modes, datasets and benchmarks, metric computation methods, and tooling. In addition to taxonomy, we highlight enterprise-specific challenges, such as role-based access to data, the need for reliability guarantees, dynamic and long-horizon interactions, and compliance, which are often overlooked in current research. We also identify future research directions, including holistic, more realistic, and scalable evaluation. This work aims to bring clarity to the fragmented landscape of agent evaluation and provide a framework for systematic assessment, enabling researchers and practitioners to evaluate LLM agents for real-world deployment.",
    "authors": [
      "Mahmoud Mohammadi",
      "Yipeng Li",
      "Jane Lo",
      "Wendy Yip"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-29T04:57:02Z",
    "pdf_url": "https://arxiv.org/pdf/2507.21504v1"
  },
  {
    "arxiv_id": "2507.21471v2",
    "entry_id": "http://arxiv.org/abs/2507.21471v2",
    "title": "LUMIR: an LLM-Driven Unified Agent Framework for Multi-task Infrared Spectroscopy Reasoning",
    "summary": "Infrared spectroscopy enables rapid, non destructive analysis of chemical and material properties, yet high dimensional signals and overlapping bands hinder conventional chemometric methods. Large language models (LLMs), with strong generalization and reasoning capabilities, offer new opportunities for automated spectral interpretation, but their potential in this domain remains largely untapped. This study introduces LUMIR (LLM-driven Unified agent framework for Multi-task Infrared spectroscopy Reasoning), an agent based framework designed to achieve accurate infrared spectral analysis under low data conditions. LUMIR integrates a structured literature knowledge base, automated preprocessing, feature extraction, and predictive modeling into a unified pipeline. By mining peer reviewed spectroscopy studies, it identifies validated preprocessing and feature derivation strategies, transforms spectra into low dimensional representations, and applies few-shot prompts for classification, regression, and anomaly detection. The framework was validated on diverse datasets, including the publicly available Milk near-infrared dataset, Chinese medicinal herbs, Citri Reticulatae Pericarpium(CRP) with different storage durations, an industrial wastewater COD dataset, and two additional public benchmarks, Tecator and Corn. Across these tasks, LUMIR achieved performance comparable to or surpassing established machine learning and deep learning models, particularly in resource limited settings. This work demonstrates that combining structured literature guidance with few-shot learning enables robust, scalable, and automated spectral interpretation. LUMIR establishes a new paradigm for applying LLMs to infrared spectroscopy, offering high accuracy with minimal labeled data and broad applicability across scientific and industrial domains.",
    "authors": [
      "Zujie Xie",
      "Zixuan Chen",
      "Jiheng Liang",
      "Xiangyang Yu",
      "Ziru Yu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-29T03:20:51Z",
    "pdf_url": "https://arxiv.org/pdf/2507.21471v2"
  },
  {
    "arxiv_id": "2507.21285v1",
    "entry_id": "http://arxiv.org/abs/2507.21285v1",
    "title": "Curiosity by Design: An LLM-based Coding Assistant Asking Clarification Questions",
    "summary": "Large Language Models (LLMs) are increasingly used as coding assistants. However, the ambiguity of the developer's prompt often leads to incorrect code generation, as current models struggle to infer user intent without extensive prompt engineering or external context. This work aims to build an LLM-based coding assistant that mimics the human code review process by asking clarification questions when faced with ambiguous or under-specified queries.\n  Our end-to-end system includes (1) a query classifier trained to detect unclear programming-related queries and (2) a fine-tuned LLM that generates clarification questions. Our evaluation shows that the fine-tuned LLM outperforms standard zero-shot prompting in generating useful clarification questions. Furthermore, our user study indicates that users find the clarification questions generated by our model to outperform the baseline, demonstrating that our coding assistant produces more accurate and helpful code responses compared to baseline coding assistants.",
    "authors": [
      "Harsh Darji",
      "Thibaud Lutellier"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-28T19:10:57Z",
    "pdf_url": "https://arxiv.org/pdf/2507.21285v1"
  },
  {
    "arxiv_id": "2507.21046v3",
    "entry_id": "http://arxiv.org/abs/2507.21046v3",
    "title": "A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence",
    "summary": "Large Language Models (LLMs) have demonstrated strong capabilities but remain fundamentally static, unable to adapt their internal parameters to novel tasks, evolving knowledge domains, or dynamic interaction contexts. As LLMs are increasingly deployed in open-ended, interactive environments, this static nature has become a critical bottleneck, necessitating agents that can adaptively reason, act, and evolve in real time. This paradigm shift -- from scaling static models to developing self-evolving agents -- has sparked growing interest in architectures and methods enabling continual learning and adaptation from data, interactions, and experiences. This survey provides the first systematic and comprehensive review of self-evolving agents, organized around three foundational dimensions -- what to evolve, when to evolve, and how to evolve. We examine evolutionary mechanisms across agent components (e.g., models, memory, tools, architecture), categorize adaptation methods by stages (e.g., intra-test-time, inter-test-time), and analyze the algorithmic and architectural designs that guide evolutionary adaptation (e.g., scalar rewards, textual feedback, single-agent and multi-agent systems). Additionally, we analyze evaluation metrics and benchmarks tailored for self-evolving agents, highlight applications in domains such as coding, education, and healthcare, and identify critical challenges and research directions in safety, scalability, and co-evolutionary dynamics. By providing a structured framework for understanding and designing self-evolving agents, this survey establishes a roadmap for advancing adaptive agentic systems in both research and real-world deployments, ultimately shedding lights to pave the way for the realization of Artificial Super Intelligence (ASI), where agents evolve autonomously, performing at or beyond human-level intelligence across a wide array of tasks.",
    "authors": [
      "Huan-ang Gao",
      "Jiayi Geng",
      "Wenyue Hua",
      "Mengkang Hu",
      "Xinzhe Juan",
      "Hongzhang Liu",
      "Shilong Liu",
      "Jiahao Qiu",
      "Xuan Qi",
      "Yiran Wu",
      "Hongru Wang",
      "Han Xiao",
      "Yuhang Zhou",
      "Shaokun Zhang",
      "Jiayi Zhang",
      "Jinyu Xiang",
      "Yixiong Fang",
      "Qiwen Zhao",
      "Dongrui Liu",
      "Qihan Ren",
      "Cheng Qian",
      "Zhenhailong Wang",
      "Minda Hu",
      "Huazheng Wang",
      "Qingyun Wu",
      "Heng Ji",
      "Mengdi Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-28T17:59:05Z",
    "pdf_url": "https://arxiv.org/pdf/2507.21046v3"
  },
  {
    "arxiv_id": "2507.20977v1",
    "entry_id": "http://arxiv.org/abs/2507.20977v1",
    "title": "Repairing vulnerabilities without invisible hands. A differentiated replication study on LLMs",
    "summary": "Background: Automated Vulnerability Repair (AVR) is a fast-growing branch of program repair. Recent studies show that large language models (LLMs) outperform traditional techniques, extending their success beyond code generation and fault detection.\n  Hypothesis: These gains may be driven by hidden factors -- \"invisible hands\" such as training-data leakage or perfect fault localization -- that let an LLM reproduce human-authored fixes for the same code.\n  Objective: We replicate prior AVR studies under controlled conditions by deliberately adding errors to the reported vulnerability location in the prompt. If LLMs merely regurgitate memorized fixes, both small and large localization errors should yield the same number of correct patches, because any offset should divert the model from the original fix.\n  Method: Our pipeline repairs vulnerabilities from the Vul4J and VJTrans benchmarks after shifting the fault location by n lines from the ground truth. A first LLM generates a patch, a second LLM reviews it, and we validate the result with regression and proof-of-vulnerability tests. Finally, we manually audit a sample of patches and estimate the error rate with the Agresti-Coull-Wilson method.",
    "authors": [
      "Maria Camporese",
      "Fabio Massacci"
    ],
    "categories": [
      "cs.SE",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-07-28T16:39:16Z",
    "pdf_url": "https://arxiv.org/pdf/2507.20977v1"
  },
  {
    "arxiv_id": "2507.21193v1",
    "entry_id": "http://arxiv.org/abs/2507.21193v1",
    "title": "Interpretable Anomaly-Based DDoS Detection in AI-RAN with XAI and LLMs",
    "summary": "Next generation Radio Access Networks (RANs) introduce programmability, intelligence, and near real-time control through intelligent controllers, enabling enhanced security within the RAN and across broader 5G/6G infrastructures. This paper presents a comprehensive survey highlighting opportunities, challenges, and research gaps for Large Language Models (LLMs)-assisted explainable (XAI) intrusion detection (IDS) for secure future RAN environments. Motivated by this, we propose an LLM interpretable anomaly-based detection system for distributed denial-of-service (DDoS) attacks using multivariate time series key performance measures (KPMs), extracted from E2 nodes, within the Near Real-Time RAN Intelligent Controller (Near-RT RIC). An LSTM-based model is trained to identify malicious User Equipment (UE) behavior based on these KPMs. To enhance transparency, we apply post-hoc local explainability methods such as LIME and SHAP to interpret individual predictions. Furthermore, LLMs are employed to convert technical explanations into natural-language insights accessible to non-expert users. Experimental results on real 5G network KPMs demonstrate that our framework achieves high detection accuracy (F1-score > 0.96) while delivering actionable and interpretable outputs.",
    "authors": [
      "Sotiris Chatzimiltis",
      "Mohammad Shojafar",
      "Mahdi Boloursaz Mashhadi",
      "Rahim Tafazolli"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-07-27T22:16:09Z",
    "pdf_url": "https://arxiv.org/pdf/2507.21193v1"
  },
  {
    "arxiv_id": "2507.19973v1",
    "entry_id": "http://arxiv.org/abs/2507.19973v1",
    "title": "Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization",
    "summary": "Background: Manual extraction of pancreatic cystic lesion (PCL) features from radiology reports is labor-intensive, limiting large-scale studies needed to advance PCL research. Purpose: To develop and evaluate large language models (LLMs) that automatically extract PCL features from MRI/CT reports and assign risk categories based on guidelines. Materials and Methods: We curated a training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134 patients that described PCLs. Labels were generated by GPT-4o using chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated CoT data. Features were mapped to risk categories per institutional guideline based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out human-annotated reports. Model outputs for 100 cases were independently reviewed by three radiologists. Feature extraction was evaluated using exact match accuracy, risk categorization with macro-averaged F1 score, and radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79% to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved (LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no statistically significant differences. Radiologist inter-reader agreement was high (Fleiss' Kappa = 0.888) and showed no statistically significant difference with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT (Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT supervision enable accurate, interpretable, and efficient phenotyping for large-scale PCL research, achieving performance comparable to GPT-4o.",
    "authors": [
      "Ebrahim Rasromani",
      "Stella K. Kang",
      "Yanqi Xu",
      "Beisong Liu",
      "Garvit Luhadia",
      "Wan Fung Chui",
      "Felicia L. Pasadyn",
      "Yu Chih Hung",
      "Julie Y. An",
      "Edwin Mathieu",
      "Zehui Gu",
      "Carlos Fernandez-Granda",
      "Ammar A. Javed",
      "Greg D. Sacks",
      "Tamas Gonda",
      "Chenchan Huang",
      "Yiqiu Shen"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2025-07-26T15:02:32Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19973v1"
  },
  {
    "arxiv_id": "2507.19902v1",
    "entry_id": "http://arxiv.org/abs/2507.19902v1",
    "title": "AgentMesh: A Cooperative Multi-Agent Generative AI Framework for Software Development Automation",
    "summary": "Software development is a complex, multi-phase process traditionally requiring collaboration among individuals with diverse expertise. We propose AgentMesh, a Python-based framework that uses multiple cooperating LLM-powered agents to automate software development tasks. In AgentMesh, specialized agents - a Planner, Coder, Debugger, and Reviewer - work in concert to transform a high-level requirement into fully realized code. The Planner agent first decomposes user requests into concrete subtasks; the Coder agent implements each subtask in code; the Debugger agent tests and fixes the code; and the Reviewer agent validates the final output for correctness and quality. We describe the architecture and design of these agents and their communication, and provide implementation details including prompt strategies and workflow orchestration. A case study illustrates AgentMesh handling a non-trivial development request via sequential task planning, code generation, iterative debugging, and final code review. We discuss how dividing responsibilities among cooperative agents leverages the strengths of large language models while mitigating single-agent limitations. Finally, we examine current limitations - such as error propagation and context scaling - and outline future work toward more robust, scalable multi-agent AI systems for software engineering automation.",
    "authors": [
      "Sourena Khanzadeh"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-07-26T10:10:02Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19902v1"
  },
  {
    "arxiv_id": "2507.21174v2",
    "entry_id": "http://arxiv.org/abs/2507.21174v2",
    "title": "A ChatGPT-based approach for questions generation in higher education",
    "summary": "Large language models have been widely applied in many aspects of real life, bringing significant efficiency to businesses and offering distinctive user experiences. In this paper, we focus on exploring the application of ChatGPT, a chatbot based on a large language model, to support higher educator in generating quiz questions and assessing learners. Specifically, we explore interactive prompting patterns to design an optimal AI-powered question bank creation process. The generated questions are evaluated through a \"Blind test\" survey sent to various stakeholders including lecturers and learners. Initial results at the Banking Academy of Vietnam are relatively promising, suggesting a potential direction to streamline the time and effort involved in assessing learners at higher education institutes.",
    "authors": [
      "Sinh Trong Vu",
      "Huong Thu Truong",
      "Oanh Tien Do",
      "Tu Anh Le",
      "Tai Tan Mai"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-07-26T01:54:12Z",
    "pdf_url": "https://arxiv.org/pdf/2507.21174v2"
  },
  {
    "arxiv_id": "2507.19672v1",
    "entry_id": "http://arxiv.org/abs/2507.19672v1",
    "title": "Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges",
    "summary": "Due to the remarkable capabilities and growing impact of large language models (LLMs), they have been deeply integrated into many aspects of society. Thus, ensuring their alignment with human values and intentions has emerged as a critical challenge. This survey provides a comprehensive overview of practical alignment techniques, training protocols, and empirical findings in LLM alignment. We analyze the development of alignment methods across diverse paradigms, characterizing the fundamental trade-offs between core alignment objectives. Our analysis shows that while supervised fine-tuning enables basic instruction-following, preference-based methods offer more flexibility for aligning with nuanced human intent. We discuss state-of-the-art techniques, including Direct Preference Optimization (DPO), Constitutional AI, brain-inspired methods, and alignment uncertainty quantification (AUQ), highlighting their approaches to balancing quality and efficiency. We review existing evaluation frameworks and benchmarking datasets, emphasizing limitations such as reward misspecification, distributional robustness, and scalable oversight. We summarize strategies adopted by leading AI labs to illustrate the current state of practice. We conclude by outlining open problems in oversight, value pluralism, robustness, and continuous alignment. This survey aims to inform both researchers and practitioners navigating the evolving landscape of LLM alignment.",
    "authors": [
      "Haoran Lu",
      "Luyang Fang",
      "Ruidong Zhang",
      "Xinliang Li",
      "Jiazhang Cai",
      "Huimin Cheng",
      "Lin Tang",
      "Ziyu Liu",
      "Zeliang Sun",
      "Tao Wang",
      "Yingchuan Zhang",
      "Arif Hassan Zidan",
      "Jinwen Xu",
      "Jincheng Yu",
      "Meizhi Yu",
      "Hanqi Jiang",
      "Xilin Gong",
      "Weidi Luo",
      "Bolun Sun",
      "Yongkai Chen",
      "Terry Ma",
      "Shushan Wu",
      "Yifan Zhou",
      "Junhao Chen",
      "Haotian Xiang",
      "Jing Zhang",
      "Afrar Jahin",
      "Wei Ruan",
      "Ke Deng",
      "Yi Pan",
      "Peilong Wang",
      "Jiahui Li",
      "Zhengliang Liu",
      "Lu Zhang",
      "Lin Zhao",
      "Wei Liu",
      "Dajiang Zhu",
      "Xin Xing",
      "Fei Dou",
      "Wei Zhang",
      "Chao Huang",
      "Rongjie Liu",
      "Mengrui Zhang",
      "Yiwen Liu",
      "Xiaoxiao Sun",
      "Qin Lu",
      "Zhen Xiang",
      "Wenxuan Zhong",
      "Tianming Liu",
      "Ping Ma"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-07-25T20:52:58Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19672v1"
  },
  {
    "arxiv_id": "2507.19657v1",
    "entry_id": "http://arxiv.org/abs/2507.19657v1",
    "title": "\"X of Information'' Continuum: A Survey on AI-Driven Multi-dimensional Metrics for Next-Generation Networked Systems",
    "summary": "The development of next-generation networking systems has inherently shifted from throughput-based paradigms towards intelligent, information-aware designs that emphasize the quality, relevance, and utility of transmitted information, rather than sheer data volume. While classical network metrics, such as latency and packet loss, remain significant, they are insufficient to quantify the nuanced information quality requirements of modern intelligent applications, including autonomous vehicles, digital twins, and metaverse environments. In this survey, we present the first comprehensive study of the ``X of Information'' continuum by introducing a systematic four-dimensional taxonomic framework that structures information metrics along temporal, quality/utility, reliability/robustness, and network/communication dimensions. We uncover the increasing interdependencies among these dimensions, whereby temporal freshness triggers quality evaluation, which in turn helps with reliability appraisal, ultimately enabling effective network delivery. Our analysis reveals that artificial intelligence technologies, such as deep reinforcement learning, multi-agent systems, and neural optimization models, enable adaptive, context-aware optimization of competing information quality objectives. In our extensive study of six critical application domains, covering autonomous transportation, industrial IoT, healthcare digital twins, UAV communications, LLM ecosystems, and metaverse settings, we illustrate the revolutionary promise of multi-dimensional information metrics for meeting diverse operational needs. Our survey identifies prominent implementation challenges, including ...",
    "authors": [
      "Beining Wu",
      "Jun Huang",
      "Shui Yu"
    ],
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "published": "2025-07-25T20:03:38Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19657v1"
  },
  {
    "arxiv_id": "2507.19595v2",
    "entry_id": "http://arxiv.org/abs/2507.19595v2",
    "title": "Efficient Attention Mechanisms for Large Language Models: A Survey",
    "summary": "Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent research has introduced two principal categories of efficient attention mechanisms. Linear attention methods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight dynamics, thereby enabling scalable inference with reduced computational overhead. Sparse attention techniques, in contrast, limit attention computation to selected subsets of tokens based on fixed patterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving contextual coverage. This survey provides a systematic and comprehensive overview of these developments, integrating both algorithmic innovations and hardware-level considerations. In addition, we analyze the incorporation of efficient attention into largescale pre-trained language models, including both architectures built entirely on efficient attention and hybrid designs that combine local and global components. By aligning theoretical foundations with practical deployment strategies, this work aims to serve as a foundational reference for advancing the design of scalable and efficient language models.",
    "authors": [
      "Yutao Sun",
      "Zhenyu Li",
      "Yike Zhang",
      "Tengyu Pan",
      "Bowen Dong",
      "Yuyi Guo",
      "Jianyong Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-25T18:08:10Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19595v2"
  },
  {
    "arxiv_id": "2507.19593v1",
    "entry_id": "http://arxiv.org/abs/2507.19593v1",
    "title": "Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems",
    "summary": "Classical game-theoretic models typically assume rational agents, complete information, and common knowledge of payoffs - assumptions that are often violated in real-world MAS characterized by uncertainty, misaligned perceptions, and nested beliefs. To overcome these limitations, researchers have proposed extensions that incorporate models of cognitive constraints, subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory extends the classical paradigm by explicitly modeling agents' subjective perceptions of the strategic scenario, known as perceptual games, in which agents may hold divergent beliefs about the structure, payoffs, or available actions. We present a systematic review of agent-compatible applications of hypergame theory, examining how its descriptive capabilities have been adapted to dynamic and interactive MAS contexts. We analyze 44 selected studies from cybersecurity, robotics, social simulation, communications, and general game-theoretic modeling. Building on a formal introduction to hypergame theory and its two major extensions - hierarchical hypergames and HNF - we develop agent-compatibility criteria and an agent-based classification framework to assess integration patterns and practical applicability. Our analysis reveals prevailing tendencies, including the prevalence of hierarchical and graph-based models in deceptive reasoning and the simplification of extensive theoretical frameworks in practical applications. We identify structural gaps, including the limited adoption of HNF-based models, the lack of formal hypergame languages, and unexplored opportunities for modeling human-agent and agent-agent misalignment. By synthesizing trends, challenges, and open research directions, this review provides a new roadmap for applying hypergame theory to enhance the realism and effectiveness of strategic modeling in dynamic multi-agent environments.",
    "authors": [
      "Vince Trencsenyi",
      "Agnieszka Mensfelt",
      "Kostas Stathis"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-07-25T18:06:41Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19593v1"
  },
  {
    "arxiv_id": "2507.19565v1",
    "entry_id": "http://arxiv.org/abs/2507.19565v1",
    "title": "Review of Deep Learning Applications to Structural Proteomics Enabled by Cryogenic Electron Microscopy and Tomography",
    "summary": "The past decade's \"cryoEM revolution\" has produced exponential growth in high-resolution structural data through advances in cryogenic electron microscopy (cryoEM) and tomography (cryoET). Deep learning integration into structural proteomics workflows addresses longstanding challenges including low signal-to-noise ratios, preferred orientation artifacts, and missing-wedge problems that historically limited efficiency and scalability. This review examines AI applications across the entire cryoEM pipeline, from automated particle picking using convolutional neural networks (Topaz, crYOLO, CryoSegNet) to computational solutions for preferred orientation bias (spIsoNet, cryoPROS) and advanced denoising algorithms (Topaz-Denoise). In cryoET, tools like IsoNet employ U-Net architectures for simultaneous missing-wedge correction and noise reduction, while TomoNet streamlines subtomogram averaging through AI-driven particle detection. The workflow culminates with automated atomic model building using sophisticated tools like ModelAngelo, DeepTracer, and CryoREAD that translate density maps into interpretable biological structures. These AI-enhanced approaches have achieved near-atomic resolution reconstructions with minimal manual intervention, resolved previously intractable datasets suffering from severe orientation bias, and enabled successful application to diverse biological systems from HIV virus-like particles to in situ ribosomal complexes. As deep learning evolves, particularly with large language models and vision transformers, the future promises sophisticated automation and accessibility in structural biology, potentially revolutionizing our understanding of macromolecular architecture and function.",
    "authors": [
      "Brady K. Zhou",
      "Jason J. Hu",
      "Jane K. J. Lee",
      "Z. Hong Zhou",
      "Demetri Terzopoulos"
    ],
    "categories": [
      "q-bio.QM",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-07-25T16:15:09Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19565v1"
  },
  {
    "arxiv_id": "2507.22080v1",
    "entry_id": "http://arxiv.org/abs/2507.22080v1",
    "title": "CodeEvo: Interaction-Driven Synthesis of Code-centric Data through Hybrid and Iterative Feedback",
    "summary": "Acquiring high-quality instruction-code pairs is essential for training Large Language Models (LLMs) for code generation. Manually curated data is expensive and inherently limited in scale, motivating the development of code-centric synthesis methods. Yet, current approaches either focus on augmenting existing code or rely on predefined heuristics, both lacking rigorous data validation, which results in synthetic data that is ungrounded, repetitive, or overly simplistic. Inspired by collaborative programming practices, we propose CodeEvo, a framework that synthesizes code data through iterative interactions between two LLM agents: a Coder, which generates candidate code and test cases based on given instructions, and a Reviewer, which guides the synthesis process by producing new instructions and feedback. We further introduce a hybrid feedback mechanism that combines compiler determinism with the generative flexibility of agents, enabling automatic quality control throughout synthesis. Extensive experiments demonstrate that models fine-tuned on CodeEvo data significantly outperform established baselines across code generation benchmarks with various difficulties. In-depth analyses further provide insights from multiple perspectives into effective code-centric data synthesis.",
    "authors": [
      "Qiushi Sun",
      "Jinyang Gong",
      "Lei Li",
      "Qipeng Guo",
      "Fei Yuan"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-07-25T16:12:51Z",
    "pdf_url": "https://arxiv.org/pdf/2507.22080v1"
  },
  {
    "arxiv_id": "2507.19364v1",
    "entry_id": "http://arxiv.org/abs/2507.19364v1",
    "title": "Integrating LLM in Agent-Based Social Simulation: Opportunities and Challenges",
    "summary": "This position paper examines the use of Large Language Models (LLMs) in social simulation, analyzing both their potential and their limitations from a computational social science perspective. The first part reviews recent findings on the ability of LLMs to replicate key aspects of human cognition, including Theory of Mind reasoning and social inference, while also highlighting significant limitations such as cognitive biases, lack of true understanding, and inconsistencies in behavior. The second part surveys emerging applications of LLMs in multi-agent simulation frameworks, focusing on system architectures, scale, and validation strategies. Notable projects such as Generative Agents (Smallville) and AgentSociety are discussed in terms of their design choices, empirical grounding, and methodological innovations. Particular attention is given to the challenges of behavioral fidelity, calibration, and reproducibility in large-scale LLM-driven simulations. The final section distinguishes between contexts where LLMs, like other black-box systems, offer direct value-such as interactive simulations and serious games-and those where their use is more problematic, notably in explanatory or predictive modeling. The paper concludes by advocating for hybrid approaches that integrate LLMs into traditional agent-based modeling platforms (GAMA, Netlogo, etc), enabling modelers to combine the expressive flexibility of language-based reasoning with the transparency and analytical rigor of classical rule-based systems.",
    "authors": [
      "Patrick Taillandier",
      "Jean Daniel Zucker",
      "Arnaud Grignard",
      "Benoit Gaudou",
      "Nghi Quang Huynh",
      "Alexis Drogoul"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-07-25T15:15:35Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19364v1"
  },
  {
    "arxiv_id": "2507.19115v2",
    "entry_id": "http://arxiv.org/abs/2507.19115v2",
    "title": "Automated Code Review Using Large Language Models at Ericsson: An Experience Report",
    "summary": "Code review is one of the primary means of assuring the quality of released software along with testing and static analysis. However, code review requires experienced developers who may not always have the time to perform an in-depth review of code. Thus, automating code review can help alleviate the cognitive burden on experienced software developers allowing them to focus on their primary activities of writing code to add new features and fix bugs. In this paper, we describe our experience in using Large Language Models towards automating the code review process in Ericsson. We describe the development of a lightweight tool using LLMs and static program analysis. We then describe our preliminary experiments with experienced developers in evaluating our code review tool and the encouraging results.",
    "authors": [
      "Shweta Ramesh",
      "Joy Bose",
      "Hamender Singh",
      "A K Raghavan",
      "Sujoy Roychowdhury",
      "Giriprasad Sridhara",
      "Nishrith Saini",
      "Ricardo Britto"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-07-25T09:50:48Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19115v2"
  },
  {
    "arxiv_id": "2507.18910v1",
    "entry_id": "http://arxiv.org/abs/2507.18910v1",
    "title": "A Systematic Review of Key Retrieval-Augmented Generation (RAG) Systems: Progress, Gaps, and Future Directions",
    "summary": "Retrieval-Augmented Generation (RAG) represents a major advancement in natural language processing (NLP), combining large language models (LLMs) with information retrieval systems to enhance factual grounding, accuracy, and contextual relevance. This paper presents a comprehensive systematic review of RAG, tracing its evolution from early developments in open domain question answering to recent state-of-the-art implementations across diverse applications. The review begins by outlining the motivations behind RAG, particularly its ability to mitigate hallucinations and outdated knowledge in parametric models. Core technical components-retrieval mechanisms, sequence-to-sequence generation models, and fusion strategies are examined in detail. A year-by-year analysis highlights key milestones and research trends, providing insight into RAG's rapid growth. The paper further explores the deployment of RAG in enterprise systems, addressing practical challenges related to retrieval of proprietary data, security, and scalability. A comparative evaluation of RAG implementations is conducted, benchmarking performance on retrieval accuracy, generation fluency, latency, and computational efficiency. Persistent challenges such as retrieval quality, privacy concerns, and integration overhead are critically assessed. Finally, the review highlights emerging solutions, including hybrid retrieval approaches, privacy-preserving techniques, optimized fusion strategies, and agentic RAG architectures. These innovations point toward a future of more reliable, efficient, and context-aware knowledge-intensive NLP systems.",
    "authors": [
      "Agada Joseph Oche",
      "Ademola Glory Folashade",
      "Tirthankar Ghosal",
      "Arpan Biswas"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-07-25T03:05:46Z",
    "pdf_url": "https://arxiv.org/pdf/2507.18910v1"
  },
  {
    "arxiv_id": "2507.18755v1",
    "entry_id": "http://arxiv.org/abs/2507.18755v1",
    "title": "Agentic Program Repair from Test Failures at Scale: A Neuro-symbolic approach with static analysis and test execution feedback",
    "summary": "Aim: With the advent of LLMs, sophisticated agentic program repair has become viable at large organizations with large codebases. In this work, we develop an Engineering Agent that fixes the source code based on test failures at scale across diverse software offerings internally.\n  Method: Using Llama as the base, we employ the ReAct harness to develop an agent. We start with a test failure that was triaged by a rule-based test failure bot. We then set up an agentic harness and allow the agent to reason and run a set of 15 actions from reading a file to generating a patch. We provide feedback to the agent through static analysis and test failures so it can refine its solution. We leverage an LLM-as-a-Judge to ensure that the patch conforms to the standards followed by a human review to land fixes.\n  Benchmark Findings: We curated offline benchmarks for our patch generator, the Engineering Agent loop, and the LLM-as-a-Judge. In offline evaluations we found that a specialized 70B model is highly competitive with the much larger but vanilla Llama-405B. In an ablation study, we found that the ReAct harness (neural model) benefited from the symbolic information from static analysis tools and test execution traces. A model that strikes a balance between the solve rate and error rate vs the cost and latency has a benchmark solve rate of 42.3% using an average 11.8 feedback iterations.\n  Production Findings: In a three month period, 80% of the generated fixes were reviewed, of which 31.5% were landed (25.5% of the total number of generated fixes).\n  Feedback from Engineers: We used open coding to extract qualitative themes from engineers' feedback. We saw positive feedback in the form of quick approvals, gratitude, and surprise. We also found mixed feedback when the Engineering Agent's solution was partially correct and it served as a good starting point.",
    "authors": [
      "Chandra Maddila",
      "Adam Tait",
      "Claire Chang",
      "Daniel Cheng",
      "Nauman Ahmad",
      "Vijayaraghavan Murali",
      "Marshall Roch",
      "Arnaud Avondet",
      "Aaron Meltzer",
      "Victor Montalvao",
      "Michael Hopko",
      "Chris Waterson",
      "Parth Thakkar",
      "Renuka Fernandez",
      "Kristian Kristensen",
      "Sivan Barzily",
      "Sherry Chen",
      "Rui Abreu",
      "Nachiappan Nagappan",
      "Payam Shodjai",
      "Killian Murphy",
      "James Everingham",
      "Aparna Ramani",
      "Peter C. Rigby"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.PL"
    ],
    "published": "2025-07-24T19:12:32Z",
    "pdf_url": "https://arxiv.org/pdf/2507.18755v1"
  },
  {
    "arxiv_id": "2507.22933v1",
    "entry_id": "http://arxiv.org/abs/2507.22933v1",
    "title": "Augmented Vision-Language Models: A Systematic Review",
    "summary": "Recent advances in visual-language machine learning models have demonstrated exceptional ability to use natural language and understand visual scenes by training on large, unstructured datasets. However, this training paradigm cannot produce interpretable explanations for its outputs, requires retraining to integrate new information, is highly resource-intensive, and struggles with certain forms of logical reasoning. One promising solution involves integrating neural networks with external symbolic information systems, forming neural symbolic systems that can enhance reasoning and memory abilities. These neural symbolic systems provide more interpretable explanations to their outputs and the capacity to assimilate new information without extensive retraining. Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural component, augmented by external systems, offers a pragmatic approach to realizing the benefits of neural-symbolic integration. This systematic literature review aims to categorize techniques through which visual-language understanding can be improved by interacting with external symbolic information systems.",
    "authors": [
      "Anthony C Davis",
      "Burhan Sadiq",
      "Tianmin Shu",
      "Chien-Ming Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-24T16:27:38Z",
    "pdf_url": "https://arxiv.org/pdf/2507.22933v1"
  },
  {
    "arxiv_id": "2507.18476v1",
    "entry_id": "http://arxiv.org/abs/2507.18476v1",
    "title": "Automated Code Review Using Large Language Models with Symbolic Reasoning",
    "summary": "Code review is one of the key processes in the software development lifecycle and is essential to maintain code quality. However, manual code review is subjective and time consuming. Given its rule-based nature, code review is well suited for automation. In recent years, significant efforts have been made to automate this process with the help of artificial intelligence. Recent developments in Large Language Models (LLMs) have also emerged as a promising tool in this area, but these models often lack the logical reasoning capabilities needed to fully understand and evaluate code. To overcome this limitation, this study proposes a hybrid approach that integrates symbolic reasoning techniques with LLMs to automate the code review process. We tested our approach using the CodexGlue dataset, comparing several models, including CodeT5, CodeBERT, and GraphCodeBERT, to assess the effectiveness of combining symbolic reasoning and prompting techniques with LLMs. Our results show that this approach improves the accuracy and efficiency of automated code review.",
    "authors": [
      "Busra Icoz",
      "Goksel Biricik"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-07-24T14:50:27Z",
    "pdf_url": "https://arxiv.org/pdf/2507.18476v1"
  },
  {
    "arxiv_id": "2507.18215v1",
    "entry_id": "http://arxiv.org/abs/2507.18215v1",
    "title": "Information Security Based on LLM Approaches: A Review",
    "summary": "Information security is facing increasingly severe challenges, and traditional protection means are difficult to cope with complex and changing threats. In recent years, as an emerging intelligent technology, large language models (LLMs) have shown a broad application prospect in the field of information security. In this paper, we focus on the key role of LLM in information security, systematically review its application progress in malicious behavior prediction, network threat analysis, system vulnerability detection, malicious code identification, and cryptographic algorithm optimization, and explore its potential in enhancing security protection performance. Based on neural networks and Transformer architecture, this paper analyzes the technical basis of large language models and their advantages in natural language processing tasks. It is shown that the introduction of large language modeling helps to improve the detection accuracy and reduce the false alarm rate of security systems. Finally, this paper summarizes the current application results and points out that it still faces challenges in model transparency, interpretability, and scene adaptability, among other issues. It is necessary to explore further the optimization of the model structure and the improvement of the generalization ability to realize a more intelligent and accurate information security protection system.",
    "authors": [
      "Chang Gong",
      "Zhongwen Li",
      "Xiaoqi Li"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-07-24T09:09:36Z",
    "pdf_url": "https://arxiv.org/pdf/2507.18215v1"
  },
  {
    "arxiv_id": "2507.18139v1",
    "entry_id": "http://arxiv.org/abs/2507.18139v1",
    "title": "Neuromorphic Computing for Embodied Intelligence in Autonomous Systems: Current Trends, Challenges, and Future Directions",
    "summary": "The growing need for intelligent, adaptive, and energy-efficient autonomous systems across fields such as robotics, mobile agents (e.g., UAVs), and self-driving vehicles is driving interest in neuromorphic computing. By drawing inspiration from biological neural systems, neuromorphic approaches offer promising pathways to enhance the perception, decision-making, and responsiveness of autonomous platforms. This paper surveys recent progress in neuromorphic algorithms, specialized hardware, and cross-layer optimization strategies, with a focus on their deployment in real-world autonomous scenarios. Special attention is given to event-based dynamic vision sensors and their role in enabling fast, efficient perception. The discussion highlights new methods that improve energy efficiency, robustness, adaptability, and reliability through the integration of spiking neural networks into autonomous system architectures. We integrate perspectives from machine learning, robotics, neuroscience, and neuromorphic engineering to offer a comprehensive view of the state of the field. Finally, emerging trends and open challenges are explored, particularly in the areas of real-time decision-making, continual learning, and the development of secure, resilient autonomous systems.",
    "authors": [
      "Alberto Marchisio",
      "Muhammad Shafique"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-07-24T07:01:52Z",
    "pdf_url": "https://arxiv.org/pdf/2507.18139v1"
  },
  {
    "arxiv_id": "2507.18055v1",
    "entry_id": "http://arxiv.org/abs/2507.18055v1",
    "title": "Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs",
    "summary": "The increasing use of synthetic data generated by Large Language Models (LLMs) presents both opportunities and challenges in data-driven applications. While synthetic data provides a cost-effective, scalable alternative to real-world data to facilitate model training, its diversity and privacy risks remain underexplored. Focusing on text-based synthetic data, we propose a comprehensive set of metrics to quantitatively assess the diversity (i.e., linguistic expression, sentiment, and user perspective), and privacy (i.e., re-identification risk and stylistic outliers) of synthetic datasets generated by several state-of-the-art LLMs. Experiment results reveal significant limitations in LLMs' capabilities in generating diverse and privacy-preserving synthetic data. Guided by the evaluation results, a prompt-based approach is proposed to enhance the diversity of synthetic reviews while preserving reviewer privacy.",
    "authors": [
      "Tevin Atwal",
      "Chan Nam Tieu",
      "Yefeng Yuan",
      "Zhan Shi",
      "Yuhong Liu",
      "Liang Cheng"
    ],
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-07-24T03:12:16Z",
    "pdf_url": "https://arxiv.org/pdf/2507.18055v1"
  },
  {
    "arxiv_id": "2507.17718v1",
    "entry_id": "http://arxiv.org/abs/2507.17718v1",
    "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer",
    "summary": "With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores. Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.",
    "authors": [
      "Danny D. Leybzon",
      "Shreyas Tirumala",
      "Nishant Jain",
      "Summer Gillen",
      "Michael Jackson",
      "Cameron McPhee",
      "Jennifer Schmidt"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-07-23T17:30:14Z",
    "pdf_url": "https://arxiv.org/pdf/2507.17718v1"
  },
  {
    "arxiv_id": "2507.17717v2",
    "entry_id": "http://arxiv.org/abs/2507.17717v2",
    "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes",
    "summary": "AI-generated clinical notes are increasingly used in healthcare, but evaluating their quality remains a challenge due to high subjectivity and limited scalability of expert review. Existing automated metrics often fail to align with real-world physician preferences. To address this, we propose a pipeline that systematically distills real user feedback into structured checklists for note evaluation. These checklists are designed to be interpretable, grounded in human feedback, and enforceable by LLM-based evaluators. Using deidentified data from over 21,000 clinical encounters (prepared in accordance with the HIPAA safe harbor standard) from a deployed AI medical scribe system, we show that our feedback-derived checklist outperforms a baseline approach in our offline evaluations in coverage, diversity, and predictive power for human ratings. Extensive experiments confirm the checklist's robustness to quality-degrading perturbations, significant alignment with clinician preferences, and practical value as an evaluation methodology. In offline research settings, our checklist offers a practical tool for flagging notes that may fall short of our defined quality standards.",
    "authors": [
      "Karen Zhou",
      "John Giorgi",
      "Pranav Mani",
      "Peng Xu",
      "Davis Liang",
      "Chenhao Tan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-23T17:28:31Z",
    "pdf_url": "https://arxiv.org/pdf/2507.17717v2"
  },
  {
    "arxiv_id": "2507.17417v2",
    "entry_id": "http://arxiv.org/abs/2507.17417v2",
    "title": "A Comprehensive Evaluation on Quantization Techniques for Large Language Models",
    "summary": "For large language models (LLMs), post-training quantization (PTQ) can significantly reduce memory footprint and computational overhead. Model quantization is rapidly evolving. Though many papers report breakthrough results, they are often evaluated under different settings because a method typically contains multiple components. Analyzing connections among existing methods is important for deeper understanding. To bridge these gaps, we conduct an extensive review of state-of-the-art methods and perform comprehensive evaluations under the same conditions for fair comparison. To our knowledge, such a fair and extensive investigation remains critically underexplored. To better understand connections, first, we decouple published quantization methods into two steps: pre-quantization transformation and quantization error mitigation. The former is a preprocessing step that reduces outlier impact by flattening the data distribution; the latter offsets quantization errors to improve performance. Second, we evaluate and analyze the impact of different settings, including granularity and symmetry. Third, we analyze and evaluate the latest MXFP4 and NVFP4 data formats and their performance. Our experiments first demonstrate that optimized rotation and scaling yield the best pre-quantization performance, and that combining low-rank compensation with GPTQ can occasionally outperform GPTQ alone for error mitigation. Second, finer granularity improves performance but increases storage overhead. Third, we find that scaling-factor format and precision greatly affect FP4 performance, and that rotation-based strategies effective for INT4 offer limited gains for MXFP4 and NVFP4, motivating further study.",
    "authors": [
      "Yutong Liu",
      "Cairong Zhao",
      "Guosheng Hu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-07-23T11:21:21Z",
    "pdf_url": "https://arxiv.org/pdf/2507.17417v2"
  },
  {
    "arxiv_id": "2507.17787v1",
    "entry_id": "http://arxiv.org/abs/2507.17787v1",
    "title": "Hyperbolic Deep Learning for Foundation Models: A Survey",
    "summary": "Foundation models pre-trained on massive datasets, including large language models (LLMs), vision-language models (VLMs), and large multimodal models, have demonstrated remarkable success in diverse downstream tasks. However, recent studies have shown fundamental limitations of these models: (1) limited representational capacity, (2) lower adaptability, and (3) diminishing scalability. These shortcomings raise a critical question: is Euclidean geometry truly the optimal inductive bias for all foundation models, or could incorporating alternative geometric spaces enable models to better align with the intrinsic structure of real-world data and improve reasoning processes? Hyperbolic spaces, a class of non-Euclidean manifolds characterized by exponential volume growth with respect to distance, offer a mathematically grounded solution. These spaces enable low-distortion embeddings of hierarchical structures (e.g., trees, taxonomies) and power-law distributions with substantially fewer dimensions compared to Euclidean counterparts. Recent advances have leveraged these properties to enhance foundation models, including improving LLMs' complex reasoning ability, VLMs' zero-shot generalization, and cross-modal semantic alignment, while maintaining parameter efficiency. This paper provides a comprehensive review of hyperbolic neural networks and their recent development for foundation models. We further outline key challenges and research directions to advance the field.",
    "authors": [
      "Neil He",
      "Hiren Madhu",
      "Ngoc Bui",
      "Menglin Yang",
      "Rex Ying"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-23T09:50:17Z",
    "pdf_url": "https://arxiv.org/pdf/2507.17787v1"
  },
  {
    "arxiv_id": "2507.17264v1",
    "entry_id": "http://arxiv.org/abs/2507.17264v1",
    "title": "Understanding Prompt Programming Tasks and Questions",
    "summary": "Prompting foundation models (FMs) like large language models (LLMs) have enabled new AI-powered software features (e.g., text summarization) that previously were only possible by fine-tuning FMs. Now, developers are embedding prompts in software, known as prompt programs. The process of prompt programming requires the developer to make many changes to their prompt. Yet, the questions developers ask to update their prompt is unknown, despite the answers to these questions affecting how developers plan their changes. With the growing number of research and commercial prompt programming tools, it is unclear whether prompt programmers' needs are being adequately addressed. We address these challenges by developing a taxonomy of 25 tasks prompt programmers do and 51 questions they ask, measuring the importance of each task and question. We interview 16 prompt programmers, observe 8 developers make prompt changes, and survey 50 developers. We then compare the taxonomy with 48 research and commercial tools. We find that prompt programming is not well-supported: all tasks are done manually, and 16 of the 51 questions -- including a majority of the most important ones -- remain unanswered. Based on this, we outline important opportunities for prompt programming tools.",
    "authors": [
      "Jenny T. Liang",
      "Chenyang Yang",
      "Agnia Sergeyuk",
      "Travis D. Breaux",
      "Brad A. Myers"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-07-23T07:01:44Z",
    "pdf_url": "https://arxiv.org/pdf/2507.17264v1"
  },
  {
    "arxiv_id": "2507.17202v1",
    "entry_id": "http://arxiv.org/abs/2507.17202v1",
    "title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
    "summary": "Designing high-quality presentation slides can be challenging for non-experts due to the complexity involved in navigating various design choices. Numerous automated tools can suggest layouts and color schemes, yet often lack the ability to refine their own output, which is a key aspect in real-world workflows. We propose DesignLab, which separates the design process into two roles, the design reviewer, who identifies design-related issues, and the design contributor who corrects them. This decomposition enables an iterative loop where the reviewer continuously detects issues and the contributor corrects them, allowing a draft to be further polished with each iteration, reaching qualities that were unattainable. We fine-tune large language models for these roles and simulate intermediate drafts by introducing controlled perturbations, enabling the design reviewer learn design errors and the contributor learn how to fix them. Our experiments show that DesignLab outperforms existing design-generation methods, including a commercial tool, by embracing the iterative nature of designing which can result in polished, professional slides.",
    "authors": [
      "Jooyeol Yun",
      "Heng Wang",
      "Yotaro Shimose",
      "Jaegul Choo",
      "Shingo Takamatsu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-07-23T04:49:48Z",
    "pdf_url": "https://arxiv.org/pdf/2507.17202v1"
  },
  {
    "arxiv_id": "2507.17118v1",
    "entry_id": "http://arxiv.org/abs/2507.17118v1",
    "title": "HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study",
    "summary": "AI has become integral to safety-critical areas like autonomous driving systems (ADS) and robotics. The architecture of recent autonomous systems are trending toward end-to-end (E2E) monolithic architectures such as large language models (LLMs) and vision language models (VLMs). In this paper, we review different architectural solutions and then evaluate the efficacy of common safety analyses such as failure modes and effect analysis (FMEA) and fault tree analysis (FTA). We show how these techniques can be improved for the intricate nature of the foundational models, particularly in how they form and utilize latent representations. We introduce HySAFE-AI, Hybrid Safety Architectural Analysis Framework for AI Systems, a hybrid framework that adapts traditional methods to evaluate the safety of AI systems. Lastly, we offer hints of future work and suggestions to guide the evolution of future AI safety standards.",
    "authors": [
      "Mandar Pitale",
      "Jelena Frtunikj",
      "Abhinaw Priyadershi",
      "Vasu Singh",
      "Maria Spence"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-23T01:41:51Z",
    "pdf_url": "https://arxiv.org/pdf/2507.17118v1"
  },
  {
    "arxiv_id": "2507.16735v1",
    "entry_id": "http://arxiv.org/abs/2507.16735v1",
    "title": "AI-enhanced conversational agents for personalized asthma support Factors for engagement, value and efficacy",
    "summary": "Asthma-related deaths in the UK are the highest in Europe, and only 30% of patients access basic care. There is a need for alternative approaches to reaching people with asthma in order to provide health education, self-management support and bridges to care. Automated conversational agents (specifically, mobile chatbots) present opportunities for providing alternative and individually tailored access to health education, self-management support and risk self-assessment. But would patients engage with a chatbot, and what factors influence engagement? We present results from a patient survey (N=1257) devised by a team of asthma clinicians, patients, and technology developers, conducted to identify optimal factors for efficacy, value and engagement for a chatbot. Results indicate that most adults with asthma (53%) are interested in using a chatbot and the patients most likely to do so are those who believe their asthma is more serious and who are less confident about self-management. Results also indicate enthusiasm for 24/7 access, personalisation, and for WhatsApp as the preferred access method (compared to app, voice assistant, SMS or website). Obstacles to uptake include security/privacy concerns and skepticism of technological capabilities. We present detailed findings and consolidate these into 7 recommendations for developers for optimising efficacy of chatbot-based health support.",
    "authors": [
      "Laura Moradbakhti",
      "Dorian Peters",
      "Jennifer K. Quint",
      "Björn Schuller",
      "Darren Cook",
      "Rafael A. Calvo"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "published": "2025-07-22T16:21:00Z",
    "pdf_url": "https://arxiv.org/pdf/2507.16735v1"
  },
  {
    "arxiv_id": "2507.16672v1",
    "entry_id": "http://arxiv.org/abs/2507.16672v1",
    "title": "Meta-Learning for Cold-Start Personalization in Prompt-Tuned LLMs",
    "summary": "Generative, explainable, and flexible recommender systems, derived using Large Language Models (LLM) are promising and poorly adapted to the cold-start user situation, where there is little to no history of interaction. The current solutions i.e. supervised fine-tuning and collaborative filtering are dense-user-item focused and would be expensive to maintain and update. This paper introduces a meta-learning framework, that can be used to perform parameter-efficient prompt-tuning, to effectively personalize LLM-based recommender systems quickly at cold-start. The model learns soft prompt embeddings with first-order (Reptile) and second-order (MAML) optimization by treating each of the users as the tasks. As augmentations to the input tokens, these learnable vectors are the differentiable control variables that represent user behavioral priors. The prompts are meta-optimized through episodic sampling, inner-loop adaptation, and outer-loop generalization. On MovieLens-1M, Amazon Reviews, and Recbole, we can see that our adaptive model outperforms strong baselines in NDCG@10, HR@10, and MRR, and it runs in real-time (i.e., below 300 ms) on consumer GPUs. Zero-history personalization is also supported by this scalable solution, and its 275 ms rate of adaptation allows successful real-time risk profiling of financial systems by shortening detection latency and improving payment network stability. Crucially, the 275 ms adaptation capability can enable real-time risk profiling for financial institutions, reducing systemic vulnerability detection latency significantly versus traditional compliance checks. By preventing contagion in payment networks (e.g., Fedwire), the framework strengthens national financial infrastructure resilience.",
    "authors": [
      "Yushang Zhao",
      "Huijie Shen",
      "Dannier Li",
      "Lu Chang",
      "Chengrui Zhou",
      "Yinuo Yang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-22T15:07:23Z",
    "pdf_url": "https://arxiv.org/pdf/2507.16672v1"
  },
  {
    "arxiv_id": "2507.16586v1",
    "entry_id": "http://arxiv.org/abs/2507.16586v1",
    "title": "AI for Better UX in Computer-Aided Engineering: Is Academia Catching Up with Industry Demands? A Multivocal Literature Review",
    "summary": "Computer-Aided Engineering (CAE) enables simulation experts to optimize complex models, but faces challenges in user experience (UX) that limit efficiency and accessibility. While artificial intelligence (AI) has demonstrated potential to enhance CAE processes, research integrating these fields with a focus on UX remains fragmented. This paper presents a multivocal literature review (MLR) examining how AI enhances UX in CAE software across both academic research and industry implementations. Our analysis reveals significant gaps between academic explorations and industry applications, with companies actively implementing LLMs, adaptive UIs, and recommender systems while academic research focuses primarily on technical capabilities without UX validation. Key findings demonstrate opportunities in AI-powered guidance, adaptive interfaces, and workflow automation that remain underexplored in current research. By mapping the intersection of these domains, this study provides a foundation for future work to address the identified research gaps and advance the integration of AI to improve CAE user experience.",
    "authors": [
      "Choro Ulan Uulu",
      "Mikhail Kulyabin",
      "Layan Etaiwi",
      "Nuno Miguel Martins Pacheco",
      "Jan Joosten",
      "Kerstin Röse",
      "Filippos Petridis",
      "Jan Bosch",
      "Helena Holmström Olsson"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2025-07-22T13:39:45Z",
    "pdf_url": "https://arxiv.org/pdf/2507.16586v1"
  },
  {
    "arxiv_id": "2507.16410v1",
    "entry_id": "http://arxiv.org/abs/2507.16410v1",
    "title": "GG-BBQ: German Gender Bias Benchmark for Question Answering",
    "summary": "Within the context of Natural Language Processing (NLP), fairness evaluation is often associated with the assessment of bias and reduction of associated harm. In this regard, the evaluation is usually carried out by using a benchmark dataset, for a task such as Question Answering, created for the measurement of bias in the model's predictions along various dimensions, including gender identity. In our work, we evaluate gender bias in German Large Language Models (LLMs) using the Bias Benchmark for Question Answering by Parrish et al. (2022) as a reference. Specifically, the templates in the gender identity subset of this English dataset were machine translated into German. The errors in the machine translated templates were then manually reviewed and corrected with the help of a language expert. We find that manual revision of the translation is crucial when creating datasets for gender bias evaluation because of the limitations of machine translation from English to a language such as German with grammatical gender. Our final dataset is comprised of two subsets: Subset-I, which consists of group terms related to gender identity, and Subset-II, where group terms are replaced with proper names. We evaluate several LLMs used for German NLP on this newly created dataset and report the accuracy and bias scores. The results show that all models exhibit bias, both along and against existing social stereotypes.",
    "authors": [
      "Shalaka Satheesh",
      "Katrin Klug",
      "Katharina Beckh",
      "Héctor Allende-Cid",
      "Sebastian Houben",
      "Teena Hassan"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-07-22T10:02:28Z",
    "pdf_url": "https://arxiv.org/pdf/2507.16410v1"
  },
  {
    "arxiv_id": "2507.16395v2",
    "entry_id": "http://arxiv.org/abs/2507.16395v2",
    "title": "LLM-Driven Collaborative Model for Untangling Commits via Explicit and Implicit Dependency Reasoning",
    "summary": "Atomic commits, which address a single development concern, are a best practice in software development. In practice, however, developers often produce tangled commits that mix unrelated changes, complicating code review and maintenance. Prior untangling approaches (rule-based, feature-based, or graph-based) have made progress but typically rely on shallow signals and struggle to distinguish explicit dependencies (e.g., control/data flow) from implicit ones (e.g., semantic or conceptual relationships). In this paper, we propose ColaUntangle, a new collaborative consultation framework for commit untangling that models both explicit and implicit dependencies among code changes. ColaUntangle integrates Large Language Model (LLM)-driven agents in a multi-agent architecture: one agent specializes in explicit dependencies, another in implicit ones, and a reviewer agent synthesizes their perspectives through iterative consultation. To capture structural and contextual information, we construct Explicit and Implicit Contexts, enabling agents to reason over code relationships with both symbolic and semantic depth. We evaluate ColaUntangle on two widely-used datasets (1,612 C# and 14k Java tangled commits). Experimental results show that ColaUntangle outperforms the best-performing baseline, achieving an improvement of 44% on the C# dataset and 82% on the Java dataset. These findings highlight the potential of LLM-based collaborative frameworks for advancing automated commit untangling tasks.",
    "authors": [
      "Bo Hou",
      "Xin Tan",
      "Kai Zheng",
      "Fang Liu",
      "Yinghao Zhu",
      "Li Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2025-07-22T09:42:13Z",
    "pdf_url": "https://arxiv.org/pdf/2507.16395v2"
  },
  {
    "arxiv_id": "2507.16322v1",
    "entry_id": "http://arxiv.org/abs/2507.16322v1",
    "title": "Mind the Gap: Evaluating the Representativeness of Quantitative Medical Language Reasoning LLM Benchmarks for African Disease Burdens",
    "summary": "Introduction: Existing medical LLM benchmarks largely reflect examination syllabi and disease profiles from high income settings, raising questions about their validity for African deployment where malaria, HIV, TB, sickle cell disease and other neglected tropical diseases (NTDs) dominate burden and national guidelines drive care. Methodology: We systematically reviewed 31 quantitative LLM evaluation papers (Jan 2019 May 2025) identifying 19 English medical QA benchmarks. Alama Health QA was developed using a retrieval augmented generation framework anchored on the Kenyan Clinical Practice Guidelines. Six widely used sets (AfriMedQA, MMLUMedical, PubMedQA, MedMCQA, MedQAUSMLE, and guideline grounded Alama Health QA) underwent harmonized semantic profiling (NTD proportion, recency, readability, lexical diversity metrics) and blinded expert rating across five dimensions: clinical relevance, guideline alignment, clarity, distractor plausibility, and language/cultural fit. Results: Alama Health QA captured >40% of all NTD mentions across corpora and the highest within set frequencies for malaria (7.7%), HIV (4.1%), and TB (5.2%); AfriMedQA ranked second but lacked formal guideline linkage. Global benchmarks showed minimal representation (e.g., sickle cell disease absent in three sets) despite large scale. Qualitatively, Alama scored highest for relevance and guideline alignment; PubMedQA lowest for clinical utility. Discussion: Quantitative medical LLM benchmarks widely used in the literature underrepresent African disease burdens and regulatory contexts, risking misleading performance claims. Guideline anchored, regionally curated resources such as Alama Health QA and expanded disease specific derivatives are essential for safe, equitable model evaluation and deployment across African health systems.",
    "authors": [
      "Fred Mutisya",
      "Shikoh Gitau",
      "Christine Syovata",
      "Diana Oigara",
      "Ibrahim Matende",
      "Muna Aden",
      "Munira Ali",
      "Ryan Nyotu",
      "Diana Marion",
      "Job Nyangena",
      "Nasubo Ongoma",
      "Keith Mbae",
      "Elizabeth Wamicha",
      "Eric Mibuari",
      "Jean Philbert Nsengemana",
      "Talkmore Chidede"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-22T08:05:30Z",
    "pdf_url": "https://arxiv.org/pdf/2507.16322v1"
  },
  {
    "arxiv_id": "2507.16280v1",
    "entry_id": "http://arxiv.org/abs/2507.16280v1",
    "title": "ResearcherBench: Evaluating Deep AI Research Systems on the Frontiers of Scientific Inquiry",
    "summary": "The emergence of deep research systems presents significant capabilities in problem-solving, extending from basic queries to sophisticated research tasks. However, existing benchmarks primarily evaluate these systems as agents for web retrieval and report generation, overlooking their potential to discover novel insights on the frontiers of scientific research. To address this gap, we introduce ResearcherBench, the first benchmark focused on evaluating the capabilities of these advanced, agentic systems - which we refer to as Deep AI Research Systems (DARS) - on frontier AI scientific questions. We compiled a dataset of 65 research questions expertly selected from real-world scientific scenarios such as laboratory discussions and interviews, spanning 35 different AI subjects and categorized into three types: technical details, literature review, and open consulting. Our dual evaluation framework combines rubric assessment, which uses expert-designed criteria to evaluate insight quality, with factual assessment, which measures citation accuracy (faithfulness) and coverage (groundedness). We evaluated several leading commercial DARS and baseline systems. Results show that OpenAI Deep Research and Gemini Deep Research significantly outperform other systems, with particular strength in open-ended consulting questions. Such capabilities represent a meaningful step toward AI self-improvement, aligning with the vision of ASI for AI. We open-source ResearcherBench to provide a standardized platform for promoting the development of next-generation AI research assistants, hoping to foster a new perspective in AI research evaluation for a novel pattern of scientific collaboration: https://github.com/GAIR-NLP/ResearcherBench.",
    "authors": [
      "Tianze Xu",
      "Pengrui Lu",
      "Lyumanshan Ye",
      "Xiangkun Hu",
      "Pengfei Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-22T06:51:26Z",
    "pdf_url": "https://arxiv.org/pdf/2507.16280v1"
  },
  {
    "arxiv_id": "2507.16124v2",
    "entry_id": "http://arxiv.org/abs/2507.16124v2",
    "title": "Benchmarking LLM Privacy Recognition for Social Robot Decision Making",
    "summary": "While robots have previously utilized rule-based systems or probabilistic models for user interaction, the rapid evolution of large language models (LLMs) presents new opportunities to develop LLM-powered robots for enhanced human-robot interaction (HRI). To fully realize these capabilities, however, robots need to collect data such as audio, fine-grained images, video, and locations. As a result, LLMs often process sensitive personal information, particularly within private environments, such as homes. Given the tension between utility and privacy risks, evaluating how current LLMs manage sensitive data is critical. Specifically, we aim to explore the extent to which out-of-the-box LLMs are privacy-aware in the context of household robots. In this work, we present a set of privacy-relevant scenarios developed using the Contextual Integrity (CI) framework. We first surveyed users' privacy preferences regarding in-home robot behaviors and then examined how their privacy orientations affected their choices of these behaviors (N = 450). We then provided the same set of scenarios and questions to state-of-the-art LLMs (N = 10) and found that the agreement between humans and LLMs was generally low. To further investigate the capabilities of LLMs as potential privacy controllers, we implemented four additional prompting strategies and compared their results. We discuss the performance of the evaluated models as well as the implications and potential of AI privacy awareness in human-robot interaction.",
    "authors": [
      "Dakota Sullivan",
      "Shirley Zhang",
      "Jennica Li",
      "Heather Kirkorian",
      "Bilge Mutlu",
      "Kassem Fawaz"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-07-22T00:36:59Z",
    "pdf_url": "https://arxiv.org/pdf/2507.16124v2"
  },
  {
    "arxiv_id": "2507.15743v1",
    "entry_id": "http://arxiv.org/abs/2507.15743v1",
    "title": "Towards physician-centered oversight of conversational diagnostic AI",
    "summary": "Recent work has demonstrated the promise of conversational AI systems for diagnostic dialogue. However, real-world assurance of patient safety means that providing individual diagnoses and treatment plans is considered a regulated activity by licensed professionals. Furthermore, physicians commonly oversee other team members in such activities, including nurse practitioners (NPs) or physician assistants/associates (PAs). Inspired by this, we propose a framework for effective, asynchronous oversight of the Articulate Medical Intelligence Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent system that performs history taking within guardrails, abstaining from individualized medical advice. Afterwards, g-AMIE conveys assessments to an overseeing primary care physician (PCP) in a clinician cockpit interface. The PCP provides oversight and retains accountability of the clinical decision. This effectively decouples oversight from intake and can thus happen asynchronously. In a randomized, blinded virtual Objective Structured Clinical Examination (OSCE) of text consultations with asynchronous oversight, we compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across 60 scenarios, g-AMIE outperformed both groups in performing high-quality intake, summarizing cases, and proposing diagnoses and management plans for the overseeing PCP to review. This resulted in higher quality composite decisions. PCP oversight of g-AMIE was also more time-efficient than standalone PCP consultations in prior work. While our study does not replicate existing clinical practices and likely underestimates clinicians' capabilities, our results demonstrate the promise of asynchronous oversight as a feasible paradigm for diagnostic AI systems to operate under expert human oversight for enhancing real-world care.",
    "authors": [
      "Elahe Vedadi",
      "David Barrett",
      "Natalie Harris",
      "Ellery Wulczyn",
      "Shashir Reddy",
      "Roma Ruparel",
      "Mike Schaekermann",
      "Tim Strother",
      "Ryutaro Tanno",
      "Yash Sharma",
      "Jihyeon Lee",
      "Cían Hughes",
      "Dylan Slack",
      "Anil Palepu",
      "Jan Freyberg",
      "Khaled Saab",
      "Valentin Liévin",
      "Wei-Hung Weng",
      "Tao Tu",
      "Yun Liu",
      "Nenad Tomasev",
      "Kavita Kulkarni",
      "S. Sara Mahdavi",
      "Kelvin Guu",
      "Joëlle Barral",
      "Dale R. Webster",
      "James Manyika",
      "Avinatan Hassidim",
      "Katherine Chou",
      "Yossi Matias",
      "Pushmeet Kohli",
      "Adam Rodman",
      "Vivek Natarajan",
      "Alan Karthikesalingam",
      "David Stutz"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-07-21T15:54:36Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15743v1"
  },
  {
    "arxiv_id": "2507.15717v1",
    "entry_id": "http://arxiv.org/abs/2507.15717v1",
    "title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning",
    "summary": "Current benchmarks evaluating large language models (LLMs) in ophthalmology are limited in scope and disproportionately prioritise accuracy. We introduce BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive evaluation benchmark developed through multiple rounds of expert checking by 13 ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset underwent multiple rounds of expert checking. Duplicate and substandard questions were systematically removed. Ten ophthalmologists refined the explanations of each MCQ's correct answer. This was further adjudicated by three senior ophthalmologists. To illustrate BELO's utility, we evaluated six LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro) using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore, BARTScore, METEOR, and AlignScore). In a further evaluation involving human experts, two ophthalmologists qualitatively reviewed 50 randomly selected outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900 high-quality, expert-reviewed questions aggregated from five sources: BCSC (260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public leaderboard has been established to promote transparent evaluation and reporting. Importantly, the BELO dataset will remain a hold-out, evaluation-only benchmark to ensure fair and reproducible comparisons of future models.",
    "authors": [
      "Sahana Srinivasan",
      "Xuguang Ai",
      "Thaddaeus Wai Soon Lo",
      "Aidan Gilson",
      "Minjie Zou",
      "Ke Zou",
      "Hyunjae Kim",
      "Mingjia Yang",
      "Krithi Pushpanathan",
      "Samantha Yew",
      "Wan Ting Loke",
      "Jocelyn Goh",
      "Yibing Chen",
      "Yiming Kong",
      "Emily Yuelei Fu",
      "Michelle Ongyong Hui",
      "Kristen Nwanyanwu",
      "Amisha Dave",
      "Kelvin Zhenghao Li",
      "Chen-Hsin Sun",
      "Mark Chia",
      "Gabriel Dawei Yang",
      "Wendy Meihua Wong",
      "David Ziyou Chen",
      "Dianbo Liu",
      "Maxwell Singer",
      "Fares Antaki",
      "Lucian V Del Priore",
      "Jost Jonas",
      "Ron Adelman",
      "Qingyu Chen",
      "Yih-Chung Tham"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-21T15:27:32Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15717v1"
  },
  {
    "arxiv_id": "2507.15617v1",
    "entry_id": "http://arxiv.org/abs/2507.15617v1",
    "title": "Why can't Epidemiology be automated (yet)?",
    "summary": "Recent advances in artificial intelligence (AI) - particularly generative AI - present new opportunities to accelerate, or even automate, epidemiological research. Unlike disciplines based on physical experimentation, a sizable fraction of Epidemiology relies on secondary data analysis and thus is well-suited for such augmentation. Yet, it remains unclear which specific tasks can benefit from AI interventions or where roadblocks exist. Awareness of current AI capabilities is also mixed. Here, we map the landscape of epidemiological tasks using existing datasets - from literature review to data access, analysis, writing up, and dissemination - and identify where existing AI tools offer efficiency gains. While AI can increase productivity in some areas such as coding and administrative tasks, its utility is constrained by limitations of existing AI models (e.g. hallucinations in literature reviews) and human systems (e.g. barriers to accessing datasets). Through examples of AI-generated epidemiological outputs, including fully AI-generated papers, we demonstrate that recently developed agentic systems can now design and execute epidemiological analysis, albeit to varied quality (see https://github.com/edlowther/automated-epidemiology). Epidemiologists have new opportunities to empirically test and benchmark AI systems; realising the potential of AI will require two-way engagement between epidemiologists and engineers.",
    "authors": [
      "David Bann",
      "Ed Lowther",
      "Liam Wright",
      "Yevgeniya Kovalchuk"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-07-21T13:41:52Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15617v1"
  },
  {
    "arxiv_id": "2507.22920v1",
    "entry_id": "http://arxiv.org/abs/2507.22920v1",
    "title": "Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey",
    "summary": "The rapid advancement of large language models (LLMs) has intensified the need for effective mechanisms to transform continuous multimodal data into discrete representations suitable for language-based processing. Discrete tokenization, with vector quantization (VQ) as a central approach, offers both computational efficiency and compatibility with LLM architectures. Despite its growing importance, there is a lack of a comprehensive survey that systematically examines VQ techniques in the context of LLM-based systems. This work fills this gap by presenting the first structured taxonomy and analysis of discrete tokenization methods designed for LLMs. We categorize 8 representative VQ variants that span classical and modern paradigms and analyze their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. Beyond algorithm-level investigation, we discuss existing research in terms of classical applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting how quantization strategies influence alignment, reasoning, and generation performance. In addition, we identify key challenges including codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. Finally, we discuss emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning. This survey bridges the gap between traditional vector quantization and modern LLM applications, serving as a foundational reference for the development of efficient and generalizable multimodal systems. A continuously updated version is available at: https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.",
    "authors": [
      "Jindong Li",
      "Yali Fu",
      "Jiahong Liu",
      "Linxiao Cao",
      "Wei Ji",
      "Menglin Yang",
      "Irwin King",
      "Ming-Hsuan Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-21T10:52:14Z",
    "pdf_url": "https://arxiv.org/pdf/2507.22920v1"
  },
  {
    "arxiv_id": "2507.15469v1",
    "entry_id": "http://arxiv.org/abs/2507.15469v1",
    "title": "The Emergence of Deep Reinforcement Learning for Path Planning",
    "summary": "The increasing demand for autonomous systems in complex and dynamic environments has driven significant research into intelligent path planning methodologies. For decades, graph-based search algorithms, linear programming techniques, and evolutionary computation methods have served as foundational approaches in this domain. Recently, deep reinforcement learning (DRL) has emerged as a powerful method for enabling autonomous agents to learn optimal navigation strategies through interaction with their environments. This survey provides a comprehensive overview of traditional approaches as well as the recent advancements in DRL applied to path planning tasks, focusing on autonomous vehicles, drones, and robotic platforms. Key algorithms across both conventional and learning-based paradigms are categorized, with their innovations and practical implementations highlighted. This is followed by a thorough discussion of their respective strengths and limitations in terms of computational efficiency, scalability, adaptability, and robustness. The survey concludes by identifying key open challenges and outlining promising avenues for future research. Special attention is given to hybrid approaches that integrate DRL with classical planning techniques to leverage the benefits of both learning-based adaptability and deterministic reliability, offering promising directions for robust and resilient autonomous navigation.",
    "authors": [
      "Thanh Thi Nguyen",
      "Saeid Nahavandi",
      "Imran Razzak",
      "Dung Nguyen",
      "Nhat Truong Pham",
      "Quoc Viet Hung Nguyen"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-07-21T10:21:42Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15469v1"
  },
  {
    "arxiv_id": "2507.15901v1",
    "entry_id": "http://arxiv.org/abs/2507.15901v1",
    "title": "Advancing Responsible Innovation in Agentic AI: A study of Ethical Frameworks for Household Automation",
    "summary": "The implementation of Artificial Intelligence (AI) in household environments, especially in the form of proactive autonomous agents, brings about possibilities of comfort and attention as well as it comes with intra or extramural ethical challenges. This article analyzes agentic AI and its applications, focusing on its move from reactive to proactive autonomy, privacy, fairness and user control. We review responsible innovation frameworks, human-centered design principles, and governance practices to distill practical guidance for ethical smart home systems. Vulnerable user groups such as elderly individuals, children, and neurodivergent who face higher risks of surveillance, bias, and privacy risks were studied in detail in context of Agentic AI. Design imperatives are highlighted such as tailored explainability, granular consent mechanisms, and robust override controls, supported by participatory and inclusive methodologies. It was also explored how data-driven insights, including social media analysis via Natural Language Processing(NLP), can inform specific user needs and ethical concerns. This survey aims to provide both a conceptual foundation and suggestions for developing transparent, inclusive, and trustworthy agentic AI in household automation.",
    "authors": [
      "Joydeep Chandra",
      "Satyam Kumar Navneet"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.MA"
    ],
    "published": "2025-07-21T06:10:02Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15901v1"
  },
  {
    "arxiv_id": "2507.15025v1",
    "entry_id": "http://arxiv.org/abs/2507.15025v1",
    "title": "Survey of GenAI for Automotive Software Development: From Requirements to Executable Code",
    "summary": "Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to revolutionize many industrial areas by reducing the amount of human intervention needed and effort for handling complex underlying processes. Automotive software development is considered to be a significant area for GenAI adoption, taking into account lengthy and expensive procedures, resulting from the amount of requirements and strict standardization. In this paper, we explore the adoption of GenAI for various steps of automotive software development, mainly focusing on requirements handling, compliance aspects and code generation. Three GenAI-related technologies are covered within the state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation (RAG), Vision Language Models (VLMs), as well as overview of adopted prompting techniques in case of code generation. Additionally, we also derive a generalized GenAI-aided automotive software development workflow based on our findings from this literature review. Finally, we include a summary of a survey outcome, which was conducted among our automotive industry partners regarding the type of GenAI tools used for their daily work activities.",
    "authors": [
      "Nenad Petrovic",
      "Vahid Zolfaghari",
      "Andre Schamschurko",
      "Sven Kirchner",
      "Fengjunjie Pan",
      "Chengdng Wu",
      "Nils Purschke",
      "Aleksei Velsh",
      "Krzysztof Lebioda",
      "Yinglei Song",
      "Yi Zhang",
      "Lukasz Mazur",
      "Alois Knoll"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-07-20T16:21:51Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15025v1"
  },
  {
    "arxiv_id": "2507.22915v1",
    "entry_id": "http://arxiv.org/abs/2507.22915v1",
    "title": "Theoretical Foundations and Mitigation of Hallucination in Large Language Models",
    "summary": "Hallucination in Large Language Models (LLMs) refers to the generation of content that is not faithful to the input or the real-world facts. This paper provides a rigorous treatment of hallucination in LLMs, including formal definitions and theoretical analyses. We distinguish between intrinsic and extrinsic hallucinations, and define a \\textit{hallucination risk} for models. We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes and Rademacher complexity). We then survey detection strategies for hallucinations, such as token-level uncertainty estimation, confidence calibration, and attention alignment checks. On the mitigation side, we discuss approaches including retrieval-augmented generation, hallucination-aware fine-tuning, logit calibration, and the incorporation of fact-verification modules. We propose a unified detection and mitigation workflow, illustrated with a diagram, to integrate these strategies. Finally, we outline evaluation protocols for hallucination, recommending datasets, metrics, and experimental setups to quantify and reduce hallucinations. Our work lays a theoretical foundation and practical guidelines for addressing the crucial challenge of hallucination in LLMs.",
    "authors": [
      "Esmail Gumaan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-20T15:22:34Z",
    "pdf_url": "https://arxiv.org/pdf/2507.22915v1"
  },
  {
    "arxiv_id": "2507.15003v1",
    "entry_id": "http://arxiv.org/abs/2507.15003v1",
    "title": "The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering",
    "summary": "The future of software engineering--SE 3.0--is unfolding with the rise of AI teammates: autonomous, goal-driven systems collaborating with human developers. Among these, autonomous coding agents are especially transformative, now actively initiating, reviewing, and evolving code at scale. This paper introduces AIDev, the first large-scale dataset capturing how such agents operate in the wild. Spanning over 456,000 pull requests by five leading agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across 61,000 repositories and 47,000 developers, AIDev provides an unprecedented empirical foundation for studying autonomous teammates in software development.\n  Unlike prior work that has largely theorized the rise of AI-native software engineering, AIDev offers structured, open data to support research in benchmarking, agent readiness, optimization, collaboration modeling, and AI governance. The dataset includes rich metadata on PRs, authorship, review timelines, code changes, and integration outcomes--enabling exploration beyond synthetic benchmarks like SWE-bench. For instance, although agents often outperform humans in speed, their PRs are accepted less frequently, revealing a trust and utility gap. Furthermore, while agents accelerate code submission--one developer submitted as many PRs in three days as they had in three years--these are structurally simpler (via code complexity metrics).\n  We envision AIDev as a living resource: extensible, analyzable, and ready for the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev enables a new generation of research into AI-native workflows and supports building the next wave of symbiotic human-AI collaboration. The dataset is publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.\n  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering Agent",
    "authors": [
      "Hao Li",
      "Haoxiang Zhang",
      "Ahmed E. Hassan"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CE",
      "cs.LG"
    ],
    "published": "2025-07-20T15:15:58Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15003v1"
  },
  {
    "arxiv_id": "2507.14912v1",
    "entry_id": "http://arxiv.org/abs/2507.14912v1",
    "title": "Redefining Elderly Care with Agentic AI: Challenges and Opportunities",
    "summary": "The global ageing population necessitates new and emerging strategies for caring for older adults. In this article, we explore the potential for transformation in elderly care through Agentic Artificial Intelligence (AI), powered by Large Language Models (LLMs). We discuss the proactive and autonomous decision-making facilitated by Agentic AI in elderly care. Personalized tracking of health, cognitive care, and environmental management, all aimed at enhancing independence and high-level living for older adults, represents important areas of application. With a potential for significant transformation of elderly care, Agentic AI also raises profound concerns about data privacy and security, decision independence, and access. We share key insights to emphasize the need for ethical safeguards, privacy protections, and transparent decision-making. Our goal in this article is to provide a balanced discussion of both the potential and the challenges associated with Agentic AI, and to provide insights into its responsible use in elderly care, to bring Agentic AI into harmony with the requirements and vulnerabilities specific to the elderly. Finally, we identify the priorities for the academic research communities, to achieve human-centered advancements and integration of Agentic AI in elderly care. To the best of our knowledge, this is no existing study that reviews the role of Agentic AI in elderly care. Hence, we address the literature gap by analyzing the unique capabilities, applications, and limitations of LLM-based Agentic AI in elderly care. We also provide a companion interactive dashboard at https://hazratali.github.io/agenticai/.",
    "authors": [
      "Ruhul Amin Khalil",
      "Kashif Ahmad",
      "Hazrat Ali"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-20T10:53:01Z",
    "pdf_url": "https://arxiv.org/pdf/2507.14912v1"
  },
  {
    "arxiv_id": "2507.14899v2",
    "entry_id": "http://arxiv.org/abs/2507.14899v2",
    "title": "InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis",
    "summary": "Non-destructive testing (NDT), particularly X-ray inspection, is vital for industrial quality assurance, yet existing deep-learning-based approaches often lack interactivity, interpretability, and the capacity for critical self-assessment, limiting their reliability and operator trust. To address these shortcomings, this paper proposes InsightX Agent, a novel LMM-based agentic framework designed to deliver reliable, interpretable, and interactive X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent positions a Large Multimodal Model (LMM) as a central orchestrator, coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect region proposals for multi-scale feature maps and sparsifies them through Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in X-ray images while maintaining computational efficiency. The EGR tool guides the LMM agent through a chain-of-thought-inspired review process, incorporating context assessment, individual defect analysis, false positive elimination, confidence recalibration and quality assurance to validate and refine the SDMSD's initial proposals. By strategically employing and intelligently using tools, InsightX Agent moves beyond passive data processing to active reasoning, enhancing diagnostic reliability and providing interpretations that integrate diverse information sources. Experimental evaluations on the GDXray+ dataset demonstrate that InsightX Agent not only achieves a high object detection F1-score of 96.35% but also offers significantly improved interpretability and trustworthiness in its analyses, highlighting the transformative potential of agentic LLM frameworks for industrial inspection tasks.",
    "authors": [
      "Jiale Liu",
      "Huan Wang",
      "Yue Zhang",
      "Xiaoyu Luo",
      "Jiaxiang Hu",
      "Zhiliang Liu",
      "Min Xie"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-07-20T10:23:22Z",
    "pdf_url": "https://arxiv.org/pdf/2507.14899v2"
  },
  {
    "arxiv_id": "2507.19525v1",
    "entry_id": "http://arxiv.org/abs/2507.19525v1",
    "title": "MMCircuitEval: A Comprehensive Multimodal Circuit-Focused Benchmark for Evaluating LLMs",
    "summary": "The emergence of multimodal large language models (MLLMs) presents promising opportunities for automation and enhancement in Electronic Design Automation (EDA). However, comprehensively evaluating these models in circuit design remains challenging due to the narrow scope of existing benchmarks. To bridge this gap, we introduce MMCircuitEval, the first multimodal benchmark specifically designed to assess MLLM performance comprehensively across diverse EDA tasks. MMCircuitEval comprises 3614 meticulously curated question-answer (QA) pairs spanning digital and analog circuits across critical EDA stages - ranging from general knowledge and specifications to front-end and back-end design. Derived from textbooks, technical question banks, datasheets, and real-world documentation, each QA pair undergoes rigorous expert review for accuracy and relevance. Our benchmark uniquely categorizes questions by design stage, circuit type, tested abilities (knowledge, comprehension, reasoning, computation), and difficulty level, enabling detailed analysis of model capabilities and limitations. Extensive evaluations reveal significant performance gaps among existing LLMs, particularly in back-end design and complex computations, highlighting the critical need for targeted training datasets and modeling approaches. MMCircuitEval provides a foundational resource for advancing MLLMs in EDA, facilitating their integration into real-world circuit design workflows. Our benchmark is available at https://github.com/cure-lab/MMCircuitEval.",
    "authors": [
      "Chenchen Zhao",
      "Zhengyuan Shi",
      "Xiangyu Wen",
      "Chengjie Liu",
      "Yi Liu",
      "Yunhao Zhou",
      "Yuxiang Zhao",
      "Hefei Feng",
      "Yinan Zhu",
      "Gwok-Waa Wan",
      "Xin Cheng",
      "Weiyu Chen",
      "Yongqi Fu",
      "Chujie Chen",
      "Chenhao Xue",
      "Guangyu Sun",
      "Ying Wang",
      "Yibo Lin",
      "Jun Yang",
      "Ning Xu",
      "Xi Wang",
      "Qiang Xu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-20T05:46:32Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19525v1"
  },
  {
    "arxiv_id": "2507.14688v2",
    "entry_id": "http://arxiv.org/abs/2507.14688v2",
    "title": "Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations",
    "summary": "Post-training has emerged as a crucial technique for aligning pre-trained Large Language Models (LLMs) with human instructions, significantly enhancing their performance across a wide range of tasks. Central to this process is the quality and diversity of post-training datasets. This paper presents a review of publicly available Arabic post-training datasets on the Hugging Face Hub, organized along four key dimensions: (1) LLM Capabilities (e.g., Question Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation, and Function Calling); (2) Steerability (e.g., Persona and System Prompts); (3) Alignment (e.g., Cultural, Safety, Ethics, and Fairness); and (4) Robustness. Each dataset is rigorously evaluated based on popularity, practical adoption, recency and maintenance, documentation and annotation quality, licensing transparency, and scientific contribution. Our review revealed critical gaps in the development of Arabic post-training datasets, including limited task diversity, inconsistent or missing documentation and annotation, and low adoption across the community. Finally, the paper discusses the implications of these gaps on the progress of Arabic-centric LLMs and applications while providing concrete recommendations for future efforts in Arabic post-training dataset development.",
    "authors": [
      "Mohammed Alkhowaiter",
      "Norah Alshahrani",
      "Saied Alshahrani",
      "Reem I. Masoud",
      "Alaa Alzahrani",
      "Deema Alnuhait",
      "Emad A. Alghamdi",
      "Khalid Almubarak"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-07-19T16:30:45Z",
    "pdf_url": "https://arxiv.org/pdf/2507.14688v2"
  },
  {
    "arxiv_id": "2507.14633v1",
    "entry_id": "http://arxiv.org/abs/2507.14633v1",
    "title": "Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches",
    "summary": "The development of satellite-augmented low-altitude economy and terrestrial networks (SLAETNs) demands intelligent and autonomous systems that can operate reliably across heterogeneous, dynamic, and mission-critical environments. To address these challenges, this survey focuses on enabling agentic artificial intelligence (AI), that is, artificial agents capable of perceiving, reasoning, and acting, through generative AI (GAI) and large language models (LLMs). We begin by introducing the architecture and characteristics of SLAETNs, and analyzing the challenges that arise in integrating satellite, aerial, and terrestrial components. Then, we present a model-driven foundation by systematically reviewing five major categories of generative models: variational autoencoders (VAEs), generative adversarial networks (GANs), generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs. Moreover, we provide a comparative analysis to highlight their generative mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on this foundation, we examine how these models empower agentic functions across three domains: communication enhancement, security and privacy protection, and intelligent satellite tasks. Finally, we outline key future directions for building scalable, adaptive, and trustworthy generative agents in SLAETNs. This survey aims to provide a unified understanding and actionable reference for advancing agentic AI in next-generation integrated networks.",
    "authors": [
      "Xiaozheng Gao",
      "Yichen Wang",
      "Bosen Liu",
      "Xiao Zhou",
      "Ruichen Zhang",
      "Jiacheng Wang",
      "Dusit Niyato",
      "Dong In Kim",
      "Abbas Jamalipour",
      "Chau Yuen",
      "Jianping An",
      "Kai Yang"
    ],
    "categories": [
      "cs.NI",
      "cs.LG"
    ],
    "published": "2025-07-19T14:07:05Z",
    "pdf_url": "https://arxiv.org/pdf/2507.14633v1"
  },
  {
    "arxiv_id": "2507.14615v1",
    "entry_id": "http://arxiv.org/abs/2507.14615v1",
    "title": "Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper",
    "summary": "Large Language Models(LLMs) hold promise for improving healthcare access in low-resource settings, but their effectiveness in African primary care remains underexplored. We present a methodology for creating a benchmark dataset and evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our approach uses retrieval augmented generation (RAG) to ground clinical questions in Kenya's national guidelines, ensuring alignment with local standards. These guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic clinical scenarios, multiple-choice questions, and rationale based answers in English and Swahili. Kenyan physicians co-created and refined the dataset, and a blinded expert review process ensured clinical accuracy, clarity, and cultural appropriateness. The resulting Alama Health QA dataset includes thousands of regulator-aligned question answer pairs across common outpatient conditions. Beyond accuracy, we introduce evaluation metrics that test clinical reasoning, safety, and adaptability such as rare case detection (Needle in the Haystack), stepwise logic (Decision Points), and contextual adaptability. Initial results reveal significant performance gaps when LLMs are applied to localized scenarios, consistent with findings that LLM accuracy is lower on African medical content than on US-based benchmarks. This work offers a replicable model for guideline-driven, dynamic benchmarking to support safe AI deployment in African health systems.",
    "authors": [
      "Fred Mutisya",
      "Shikoh Gitau",
      "Christine Syovata",
      "Diana Oigara",
      "Ibrahim Matende",
      "Muna Aden",
      "Munira Ali",
      "Ryan Nyotu",
      "Diana Marion",
      "Job Nyangena",
      "Nasubo Ongoma",
      "Keith Mbae",
      "Elizabeth Wamicha",
      "Eric Mibuari",
      "Jean Philbert Nsengemana",
      "Talkmore Chidede"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-19T13:25:26Z",
    "pdf_url": "https://arxiv.org/pdf/2507.14615v1"
  },
  {
    "arxiv_id": "2507.15885v1",
    "entry_id": "http://arxiv.org/abs/2507.15885v1",
    "title": "ADEPTS: A Capability Framework for Human-Centered Agent Design",
    "summary": "Large language models have paved the way to powerful and flexible AI agents, assisting humans by increasingly integrating into their daily life. This flexibility, potential, and growing adoption demands a holistic and cross-disciplinary approach to developing, monitoring and discussing the capabilities required for agent-driven user experiences. However, current guidance on human-centered AI agent development is scattered: UX heuristics focus on interface behaviors, engineering taxonomies describe internal pipelines, and ethics checklists address high-level governance. There is no concise, user-facing vocabulary that tells teams what an agent should fundamentally be able to do. We introduce ADEPTS, a capability framework defining a set of core user-facing capabilities to provide unified guidance around the development of AI agents. ADEPTS is based on six principles for human-centered agent design, that express the minimal, user-facing capabilities an AI agent should demonstrate to be understandable, controllable and trustworthy in everyday use. ADEPTS complements existing frameworks and taxonomies; differently from them, it sits at the interface between technical and experience development. By presenting ADEPTS, we aim to condense complex AI-UX requirements into a compact framework that is actionable guidance for AI researchers, designers, engineers, and policy reviewers alike. We believe ADEPTS has the potential of accelerating the improvement of user-relevant agent capabilities, of easing the design of experiences that take advantage of those capabilities, and of providing a shared language to track and discuss progress around the development of AI agents.",
    "authors": [
      "Pierluca D'Oro",
      "Caley Drooff",
      "Joy Chen",
      "Joseph Tighe"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-07-18T22:27:40Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15885v1"
  },
  {
    "arxiv_id": "2507.19521v2",
    "entry_id": "http://arxiv.org/abs/2507.19521v2",
    "title": "Intent-Aware Schema Generation And Refinement For Literature Review Tables",
    "summary": "The increasing volume of academic literature makes it essential for researchers to organize, compare, and contrast collections of documents. Large language models (LLMs) can support this process by generating schemas defining shared aspects along which to compare papers. However, progress on schema generation has been slow due to: (i) ambiguity in reference-based evaluations, and (ii) lack of editing/refinement methods. Our work is the first to address both issues. First, we present an approach for augmenting unannotated table corpora with \\emph{synthesized intents}, and apply it to create a dataset for studying schema generation conditioned on a given information need, thus reducing ambiguity. With this dataset, we show how incorporating table intents significantly improves baseline performance in reconstructing reference schemas. We start by comprehensively benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, showing that smaller, open-weight models can be fine-tuned to be competitive with state-of-the-art prompted LLMs. Next, we propose several LLM-based schema refinement techniques and show that these can further improve schemas generated by these methods.",
    "authors": [
      "Vishakh Padmakumar",
      "Joseph Chee Chang",
      "Kyle Lo",
      "Doug Downey",
      "Aakanksha Naik"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-07-18T22:01:27Z",
    "pdf_url": "https://arxiv.org/pdf/2507.19521v2"
  },
  {
    "arxiv_id": "2507.14372v1",
    "entry_id": "http://arxiv.org/abs/2507.14372v1",
    "title": "Text-to-SQL for Enterprise Data Analytics",
    "summary": "The introduction of large language models has brought rapid progress on Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise solution. In this paper, we present insights from building an internal chatbot that enables LinkedIn's product managers, engineers, and operations teams to self-serve data insights from a large, dynamic data lake. Our approach features three components. First, we construct a knowledge graph that captures up-to-date semantics by indexing database metadata, historical query logs, wikis, and code. We apply clustering to identify relevant tables for each team or product area. Second, we build a Text-to-SQL agent that retrieves and ranks context from the knowledge graph, writes a query, and automatically corrects hallucinations and syntax errors. Third, we build an interactive chatbot that supports various user intents, from data discovery to query writing to debugging, and displays responses in rich UI elements to encourage follow-up chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of its responses are correct or close to correct on an internal benchmark set. Through ablation studies, we identify the most important knowledge graph and modeling components, offering a practical path for developing enterprise Text-to-SQL solutions.",
    "authors": [
      "Albert Chen",
      "Manas Bundele",
      "Gaurav Ahlawat",
      "Patrick Stetz",
      "Zhitao Wang",
      "Qiang Fei",
      "Donghoon Jung",
      "Audrey Chu",
      "Bharadwaj Jayaraman",
      "Ayushi Panth",
      "Yatin Arora",
      "Sourav Jain",
      "Renjith Varma",
      "Alexey Ilin",
      "Iuliia Melnychuk",
      "Chelsea Chueh",
      "Joyan Sil",
      "Xiaofeng Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.HC"
    ],
    "published": "2025-07-18T21:39:17Z",
    "pdf_url": "https://arxiv.org/pdf/2507.14372v1"
  },
  {
    "arxiv_id": "2507.13629v1",
    "entry_id": "http://arxiv.org/abs/2507.13629v1",
    "title": "Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques",
    "summary": "Large Language Models (LLMs) are transforming cybersecurity by enabling intelligent, adaptive, and automated approaches to threat detection, vulnerability assessment, and incident response. With their advanced language understanding and contextual reasoning, LLMs surpass traditional methods in tackling challenges across domains such as IoT, blockchain, and hardware security. This survey provides a comprehensive overview of LLM applications in cybersecurity, focusing on two core areas: (1) the integration of LLMs into key cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along with mitigation strategies. By synthesizing recent advancements and identifying key limitations, this work offers practical insights and strategic recommendations for leveraging LLMs to build secure, scalable, and future-ready cyber defense systems.",
    "authors": [
      "Niveen O. Jaffal",
      "Mohammed Alkhanafseh",
      "David Mohaisen"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-07-18T03:41:18Z",
    "pdf_url": "https://arxiv.org/pdf/2507.13629v1"
  },
  {
    "arxiv_id": "2507.13158v1",
    "entry_id": "http://arxiv.org/abs/2507.13158v1",
    "title": "Inverse Reinforcement Learning Meets Large Language Model Post-Training: Basics, Advances, and Opportunities",
    "summary": "In the era of Large Language Models (LLMs), alignment has emerged as a fundamental yet challenging problem in the pursuit of more reliable, controllable, and capable machine intelligence. The recent success of reasoning models and conversational AI systems has underscored the critical role of reinforcement learning (RL) in enhancing these systems, driving increased research interest at the intersection of RL and LLM alignment. This paper provides a comprehensive review of recent advances in LLM alignment through the lens of inverse reinforcement learning (IRL), emphasizing the distinctions between RL techniques employed in LLM alignment and those in conventional RL tasks. In particular, we highlight the necessity of constructing neural reward models from human data and discuss the formal and practical implications of this paradigm shift. We begin by introducing fundamental concepts in RL to provide a foundation for readers unfamiliar with the field. We then examine recent advances in this research agenda, discussing key challenges and opportunities in conducting IRL for LLM alignment. Beyond methodological considerations, we explore practical aspects, including datasets, benchmarks, evaluation metrics, infrastructure, and computationally efficient training and inference techniques. Finally, we draw insights from the literature on sparse-reward RL to identify open questions and potential research directions. By synthesizing findings from diverse studies, we aim to provide a structured and critical overview of the field, highlight unresolved challenges, and outline promising future directions for improving LLM alignment through RL and IRL techniques.",
    "authors": [
      "Hao Sun",
      "Mihaela van der Schaar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-07-17T14:22:24Z",
    "pdf_url": "https://arxiv.org/pdf/2507.13158v1"
  },
  {
    "arxiv_id": "2507.21117v2",
    "entry_id": "http://arxiv.org/abs/2507.21117v2",
    "title": "A Comprehensive Review on Harnessing Large Language Models to Overcome Recommender System Challenges",
    "summary": "Recommender systems have traditionally followed modular architectures comprising candidate generation, multi-stage ranking, and re-ranking, each trained separately with supervised objectives and hand-engineered features. While effective in many domains, such systems face persistent challenges including sparse and noisy interaction data, cold-start problems, limited personalization depth, and inadequate semantic understanding of user and item content. The recent emergence of Large Language Models (LLMs) offers a new paradigm for addressing these limitations through unified, language-native mechanisms that can generalize across tasks, domains, and modalities. In this paper, we present a comprehensive technical survey of how LLMs can be leveraged to tackle key challenges in modern recommender systems. We examine the use of LLMs for prompt-driven candidate retrieval, language-native ranking, retrieval-augmented generation (RAG), and conversational recommendation, illustrating how these approaches enhance personalization, semantic alignment, and interpretability without requiring extensive task-specific supervision. LLMs further enable zero- and few-shot reasoning, allowing systems to operate effectively in cold-start and long-tail scenarios by leveraging external knowledge and contextual cues. We categorize these emerging LLM-driven architectures and analyze their effectiveness in mitigating core bottlenecks of conventional pipelines. In doing so, we provide a structured framework for understanding the design space of LLM-enhanced recommenders, and outline the trade-offs between accuracy, scalability, and real-time performance. Our goal is to demonstrate that LLMs are not merely auxiliary components but foundational enablers for building more adaptive, semantically rich, and user-centric recommender systems",
    "authors": [
      "Rahul Raja",
      "Anshaj Vats",
      "Arpita Vats",
      "Anirban Majumder"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-07-17T06:03:57Z",
    "pdf_url": "https://arxiv.org/pdf/2507.21117v2"
  },
  {
    "arxiv_id": "2507.12774v1",
    "entry_id": "http://arxiv.org/abs/2507.12774v1",
    "title": "A Comprehensive Survey of Electronic Health Record Modeling: From Deep Learning Approaches to Large Language Models",
    "summary": "Artificial intelligence (AI) has demonstrated significant potential in transforming healthcare through the analysis and modeling of electronic health records (EHRs). However, the inherent heterogeneity, temporal irregularity, and domain-specific nature of EHR data present unique challenges that differ fundamentally from those in vision and natural language tasks. This survey offers a comprehensive overview of recent advancements at the intersection of deep learning, large language models (LLMs), and EHR modeling. We introduce a unified taxonomy that spans five key design dimensions: data-centric approaches, neural architecture design, learning-focused strategies, multimodal learning, and LLM-based modeling systems. Within each dimension, we review representative methods addressing data quality enhancement, structural and temporal representation, self-supervised learning, and integration with clinical knowledge. We further highlight emerging trends such as foundation models, LLM-driven clinical agents, and EHR-to-text translation for downstream reasoning. Finally, we discuss open challenges in benchmarking, explainability, clinical alignment, and generalization across diverse clinical settings. This survey aims to provide a structured roadmap for advancing AI-driven EHR modeling and clinical decision support. For a comprehensive list of EHR-related methods, kindly refer to https://survey-on-tabular-data.github.io/.",
    "authors": [
      "Weijieying Ren",
      "Jingxi Zhu",
      "Zehao Liu",
      "Tianxiang Zhao",
      "Vasant Honavar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-07-17T04:31:55Z",
    "pdf_url": "https://arxiv.org/pdf/2507.12774v1"
  },
  {
    "arxiv_id": "2507.12599v1",
    "entry_id": "http://arxiv.org/abs/2507.12599v1",
    "title": "A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs",
    "summary": "The success of recent Artificial Intelligence (AI) models has been accompanied by the opacity of their internal mechanisms, due notably to the use of deep neural networks. In order to understand these internal mechanisms and explain the output of these AI models, a set of methods have been proposed, grouped under the domain of eXplainable AI (XAI). This paper focuses on a sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims to explain the actions of an agent that has learned by reinforcement learning. We propose an intuitive taxonomy based on two questions \"What\" and \"How\". The first question focuses on the target that the method explains, while the second relates to the way the explanation is provided. We use this taxonomy to provide a state-of-the-art review of over 250 papers. In addition, we present a set of domains close to XRL, which we believe should get attention from the community. Finally, we identify some needs for the field of XRL.",
    "authors": [
      "Léo Saulières"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-07-16T19:41:41Z",
    "pdf_url": "https://arxiv.org/pdf/2507.12599v1"
  },
  {
    "arxiv_id": "2507.12414v1",
    "entry_id": "http://arxiv.org/abs/2507.12414v1",
    "title": "AutoVDC: Automated Vision Data Cleaning Using Vision-Language Models",
    "summary": "Training of autonomous driving systems requires extensive datasets with precise annotations to attain robust performance. Human annotations suffer from imperfections, and multiple iterations are often needed to produce high-quality datasets. However, manually reviewing large datasets is laborious and expensive. In this paper, we introduce AutoVDC (Automated Vision Data Cleaning) framework and investigate the utilization of Vision-Language Models (VLMs) to automatically identify erroneous annotations in vision datasets, thereby enabling users to eliminate these errors and enhance data quality. We validate our approach using the KITTI and nuImages datasets, which contain object detection benchmarks for autonomous driving. To test the effectiveness of AutoVDC, we create dataset variants with intentionally injected erroneous annotations and observe the error detection rate of our approach. Additionally, we compare the detection rates using different VLMs and explore the impact of VLM fine-tuning on our pipeline. The results demonstrate our method's high performance in error detection and data cleaning experiments, indicating its potential to significantly improve the reliability and accuracy of large-scale production datasets in autonomous driving.",
    "authors": [
      "Santosh Vasa",
      "Aditi Ramadwar",
      "Jnana Rama Krishna Darabattula",
      "Md Zafar Anwar",
      "Stanislaw Antol",
      "Andrei Vatavu",
      "Thomas Monninger",
      "Sihao Ding"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2025-07-16T17:04:49Z",
    "pdf_url": "https://arxiv.org/pdf/2507.12414v1"
  },
  {
    "arxiv_id": "2507.12126v1",
    "entry_id": "http://arxiv.org/abs/2507.12126v1",
    "title": "Iterative Augmentation with Summarization Refinement (IASR) Evaluation for Unstructured Survey data Modeling and Analysis",
    "summary": "Text data augmentation is a widely used strategy for mitigating data sparsity in natural language processing (NLP), particularly in low-resource settings where limited samples hinder effective semantic modeling. While augmentation can improve input diversity and downstream interpretability, existing techniques often lack mechanisms to ensure semantic preservation during large-scale or iterative generation, leading to redundancy and instability. This work introduces a principled evaluation framework for large language model (LLM) based text augmentation, comprising two components: (1) Scalability Analysis, which measures semantic consistency as augmentation volume increases, and (2) Iterative Augmentation with Summarization Refinement (IASR), which evaluates semantic drift across recursive paraphrasing cycles. Empirical evaluations across state-of-the-art LLMs show that GPT-3.5 Turbo achieved the best balance of semantic fidelity, diversity, and generation efficiency. Applied to a real-world topic modeling task using BERTopic with GPT-enhanced few-shot labeling, the proposed approach results in a 400% increase in topic granularity and complete elimination of topic overlaps. These findings validated the utility of the proposed frameworks for structured evaluation of LLM-based augmentation in practical NLP pipelines.",
    "authors": [
      "Payal Bhattad",
      "Sai Manoj Pudukotai Dinakarrao",
      "Anju Gupta"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-07-16T10:49:30Z",
    "pdf_url": "https://arxiv.org/pdf/2507.12126v1"
  },
  {
    "arxiv_id": "2507.13392v1",
    "entry_id": "http://arxiv.org/abs/2507.13392v1",
    "title": "TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction",
    "summary": "We improve the extraction of insights from customer reviews by restructuring the topic modelling pipeline to operate on opinion units - distinct statements that include relevant text excerpts and associated sentiment scores. Prior work has demonstrated that such units can be reliably extracted using large language models. The result is a heightened performance of the subsequent topic modeling, leading to coherent and interpretable topics while also capturing the sentiment associated with each topic. By correlating the topics and sentiments with business metrics, such as star ratings, we can gain insights on how specific customer concerns impact business outcomes. We present our system's implementation, use cases, and advantages over other topic modeling and classification solutions. We also evaluate its effectiveness in creating coherent topics and assess methods for integrating topic and sentiment modalities for accurate star-rating prediction.",
    "authors": [
      "Emil Häglund",
      "Johanna Björklund"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-16T09:19:26Z",
    "pdf_url": "https://arxiv.org/pdf/2507.13392v1"
  },
  {
    "arxiv_id": "2507.11936v5",
    "entry_id": "http://arxiv.org/abs/2507.11936v5",
    "title": "A Survey of Deep Learning for Geometry Problem Solving",
    "summary": "Geometry problem solving, a crucial aspect of mathematical reasoning, is vital across various domains, including education, the assessment of AI's mathematical abilities, and multimodal capability evaluation. The recent surge in deep learning technologies, particularly the emergence of multimodal large language models, has significantly accelerated research in this area. This paper provides a survey of the applications of deep learning in geometry problem solving, including (i) a comprehensive summary of the relevant tasks in geometry problem solving; (ii) a thorough review of related deep learning methods; (iii) a detailed analysis of evaluation metrics and methods; and (iv) a critical discussion of the current challenges and future directions that can be explored. Our objective is to offer a comprehensive and practical reference of deep learning for geometry problem solving, thereby fostering further advancements in this field. We create a continuously updated list of papers on GitHub: https://github.com/majianz/dl4gps.",
    "authors": [
      "Jianzhe Ma",
      "Wenxuan Wang",
      "Qin Jin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-07-16T06:03:08Z",
    "pdf_url": "https://arxiv.org/pdf/2507.11936v5"
  },
  {
    "arxiv_id": "2507.11810v1",
    "entry_id": "http://arxiv.org/abs/2507.11810v1",
    "title": "The Evolving Role of Large Language Models in Scientific Innovation: Evaluator, Collaborator, and Scientist",
    "summary": "Scientific innovation is undergoing a paradigm shift driven by the rapid advancement of Large Language Models (LLMs). As science faces mounting challenges including information overload, disciplinary silos, and diminishing returns on conventional research methods, LLMs are emerging as powerful agents capable not only of enhancing scientific workflows but also of participating in and potentially leading the innovation process. Existing surveys mainly focus on different perspectives, phrases, and tasks in scientific research and discovery, while they have limitations in understanding the transformative potential and role differentiation of LLM. This survey proposes a comprehensive framework to categorize the evolving roles of LLMs in scientific innovation across three hierarchical levels: Evaluator, Collaborator, and Scientist. We distinguish between LLMs' contributions to structured scientific research processes and open-ended scientific discovery, thereby offering a unified taxonomy that clarifies capability boundaries, evaluation criteria, and human-AI interaction patterns at each level. Through an extensive analysis of current methodologies, benchmarks, systems, and evaluation metrics, this survey delivers an in-depth and systematic synthesis on LLM-driven scientific innovation. We present LLMs not only as tools for automating existing processes, but also as catalysts capable of reshaping the epistemological foundations of science itself. This survey offers conceptual clarity, practical guidance, and theoretical foundations for future research, while also highlighting open challenges and ethical considerations in the pursuit of increasingly autonomous AI-driven science. Resources related to this survey can be accessed on GitHub at: https://github.com/haoxuan-unt2024/llm4innovation.",
    "authors": [
      "Haoxuan Zhang",
      "Ruochi Li",
      "Yang Zhang",
      "Ting Xiao",
      "Jiangping Chen",
      "Junhua Ding",
      "Haihua Chen"
    ],
    "categories": [
      "cs.DL",
      "cs.AI"
    ],
    "published": "2025-07-16T00:11:01Z",
    "pdf_url": "https://arxiv.org/pdf/2507.11810v1"
  },
  {
    "arxiv_id": "2507.11597v1",
    "entry_id": "http://arxiv.org/abs/2507.11597v1",
    "title": "AI, Humans, and Data Science: Optimizing Roles Across Workflows and the Workforce",
    "summary": "AI is transforming research. It is being leveraged to construct surveys, synthesize data, conduct analysis, and write summaries of the results. While the promise is to create efficiencies and increase quality, the reality is not always as clear cut. Leveraging our framework of Truth, Beauty, and Justice (TBJ) which we use to evaluate AI, machine learning and computational models for effective and ethical use (Taber and Timpone 1997; Timpone and Yang 2024), we consider the potential and limitation of analytic, generative, and agentic AI to augment data scientists or take on tasks traditionally done by human analysts and researchers. While AI can be leveraged to assist analysts in their tasks, we raise some warnings about push-button automation. Just as earlier eras of survey analysis created some issues when the increased ease of using statistical software allowed researchers to conduct analyses they did not fully understand, the new AI tools may create similar but larger risks. We emphasize a human-machine collaboration perspective (Daugherty and Wilson 2018) throughout the data science workflow and particularly call out the vital role that data scientists play under VUCA decision areas. We conclude by encouraging the advance of AI tools to complement data scientists but advocate for continued training and understanding of methods to ensure the substantive value of research is fully achieved by applying, interpreting, and acting upon results most effectively and ethically.",
    "authors": [
      "Richard Timpone",
      "Yongwei Yang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-07-15T17:59:06Z",
    "pdf_url": "https://arxiv.org/pdf/2507.11597v1"
  },
  {
    "arxiv_id": "2507.11330v2",
    "entry_id": "http://arxiv.org/abs/2507.11330v2",
    "title": "Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge",
    "summary": "Novelty is a crucial criterion in the peer review process for evaluating academic papers. Traditionally, it's judged by experts or measure by unique reference combinations. Both methods have limitations: experts have limited knowledge, and the effectiveness of the combination method is uncertain. Moreover, it's unclear if unique citations truly measure novelty. The large language model (LLM) possesses a wealth of knowledge, while human experts possess judgment abilities that the LLM does not possess. Therefore, our research integrates the knowledge and abilities of LLM and human experts to address the limitations of novelty assessment. One of the most common types of novelty in academic papers is the introduction of new methods. In this paper, we propose leveraging human knowledge and LLM to assist pretrained language models (PLMs, e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we extract sentences related to the novelty of the academic paper from peer review reports and use LLM to summarize the methodology section of the academic paper, which are then used to fine-tune PLMs. In addition, we have designed a text-guided fusion module with novel Sparse-Attention to better integrate human and LLM knowledge. We compared the method we proposed with a large number of baselines. Extensive experiments demonstrate that our method achieves superior performance.",
    "authors": [
      "Wenqing Wu",
      "Chengzhi Zhang",
      "Yi Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.HC"
    ],
    "published": "2025-07-15T14:03:55Z",
    "pdf_url": "https://arxiv.org/pdf/2507.11330v2"
  },
  {
    "arxiv_id": "2507.11181v1",
    "entry_id": "http://arxiv.org/abs/2507.11181v1",
    "title": "Mixture of Experts in Large Language Models",
    "summary": "This paper presents a comprehensive review of the Mixture-of-Experts (MoE) architecture in large language models, highlighting its ability to significantly enhance model performance while maintaining minimal computational overhead. Through a systematic analysis spanning theoretical foundations, core architectural designs, and large language model (LLM) applications, we examine expert gating and routing mechanisms, hierarchical and sparse MoE configurations, meta-learning approaches, multimodal and multitask learning scenarios, real-world deployment cases, and recent advances and challenges in deep learning. Our analysis identifies key advantages of MoE, including superior model capacity compared to equivalent Bayesian approaches, improved task-specific performance, and the ability to scale model capacity efficiently. We also underscore the importance of ensuring expert diversity, accurate calibration, and reliable inference aggregation, as these are essential for maximizing the effectiveness of MoE architectures. Finally, this review outlines current research limitations, open challenges, and promising future directions, providing a foundation for continued innovation in MoE architecture and its applications.",
    "authors": [
      "Danyang Zhang",
      "Junhao Song",
      "Ziqian Bi",
      "Yingfang Yuan",
      "Tianyang Wang",
      "Joe Yeong",
      "Junfeng Hao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-15T10:36:43Z",
    "pdf_url": "https://arxiv.org/pdf/2507.11181v1"
  },
  {
    "arxiv_id": "2507.10644v3",
    "entry_id": "http://arxiv.org/abs/2507.10644v3",
    "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents",
    "summary": "The concept of the Web of Agents (WoA), which transforms the static, document-centric Web into an environment of autonomous agents acting on users' behalf, has attracted growing interest as large language models (LLMs) become more capable. However, research in this area is still fragmented across different communities. Contemporary surveys catalog the latest LLM-powered frameworks, while the rich histories of Multi-Agent Systems (MAS) and the Semantic Web are often treated as separate, legacy domains. This fragmentation obscures the intellectual lineage of modern systems and hinders a holistic understanding of the field's trajectory. We present the first comprehensive evolutionary overview of the WoA. We show that modern protocols like A2A and the MCP, are direct evolutionary responses to the well-documented limitations of earlier standards like FIPA standards and OWL-based semantic agents. To systematize this analysis, we introduce a four-axis taxonomy (semantic foundation, communication paradigm, locus of intelligence, discovery mechanism). This framework provides a unified analytical lens for comparing agent architectures across all generations, revealing a clear line of descent where others have seen a disconnect. Our analysis identifies a paradigm shift in the 'locus of intelligence': from being encoded in external data (Semantic Web) or the platform (MAS) to being embedded within the agent's core model (LLM). This shift is foundational to modern Agentic AI, enabling the scalable and adaptive systems the WoA has long envisioned. We conclude that while new protocols are essential, they are insufficient for building a robust, open, trustworthy ecosystem. Finally, we argue that the next research frontier lies in solving persistent socio-technical challenges, and we map out a new agenda focused on decentralized identity, economic models, security, and governance for the emerging WoA.",
    "authors": [
      "Tatiana Petrova",
      "Boris Bliznioukov",
      "Aleksandr Puzikov",
      "Radu State"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.HC",
      "cs.MA"
    ],
    "published": "2025-07-14T16:47:19Z",
    "pdf_url": "https://arxiv.org/pdf/2507.10644v3"
  },
  {
    "arxiv_id": "2507.10311v1",
    "entry_id": "http://arxiv.org/abs/2507.10311v1",
    "title": "Recognizing Dementia from Neuropsychological Tests with State Space Models",
    "summary": "Early detection of dementia is critical for timely medical intervention and improved patient outcomes. Neuropsychological tests are widely used for cognitive assessment but have traditionally relied on manual scoring. Automatic dementia classification (ADC) systems aim to infer cognitive decline directly from speech recordings of such tests. We propose Demenba, a novel ADC framework based on state space models, which scale linearly in memory and computation with sequence length. Trained on over 1,000 hours of cognitive assessments administered to Framingham Heart Study participants, some of whom were diagnosed with dementia through adjudicated review, our method outperforms prior approaches in fine-grained dementia classification by 21\\%, while using fewer parameters. We further analyze its scaling behavior and demonstrate that our model gains additional improvement when fused with large language models, paving the way for more transparent and scalable dementia assessment tools. Code: https://anonymous.4open.science/r/Demenba-0861",
    "authors": [
      "Liming Wang",
      "Saurabhchand Bhati",
      "Cody Karjadi",
      "Rhoda Au",
      "James Glass"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-14T14:15:47Z",
    "pdf_url": "https://arxiv.org/pdf/2507.10311v1"
  },
  {
    "arxiv_id": "2507.10281v1",
    "entry_id": "http://arxiv.org/abs/2507.10281v1",
    "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence",
    "summary": "Tables are fundamental in domains such as finance, healthcare, and public administration, yet real-world table tasks often involve noise, structural heterogeneity, and semantic complexity--issues underexplored in existing research that primarily targets clean academic datasets. This survey focuses on LLM-based Table Agents, which aim to automate table-centric workflows by integrating preprocessing, reasoning, and domain adaptation. We define five core competencies--C1: Table Structure Understanding, C2: Table and Query Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze and compare current approaches. In addition, a detailed examination of the Text-to-SQL Agent reveals a performance gap between academic benchmarks and real-world scenarios, especially for open-source models. Finally, we provide actionable insights to improve the robustness, generalization, and efficiency of LLM-based Table Agents in practical settings.",
    "authors": [
      "Jiaming Tian",
      "Liyao Li",
      "Wentao Ye",
      "Haobo Wang",
      "Lingxin Wang",
      "Lihua Yu",
      "Zujie Ren",
      "Gang Chen",
      "Junbo Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.DB"
    ],
    "published": "2025-07-14T13:48:13Z",
    "pdf_url": "https://arxiv.org/pdf/2507.10281v1"
  },
  {
    "arxiv_id": "2507.10177v1",
    "entry_id": "http://arxiv.org/abs/2507.10177v1",
    "title": "Abusive text transformation using LLMs",
    "summary": "Although Large Language Models (LLMs) have demonstrated significant advancements in natural language processing tasks, their effectiveness in the classification and transformation of abusive text into non-abusive versions remains an area for exploration. In this study, we aim to use LLMs to transform abusive text (tweets and reviews) featuring hate speech and swear words into non-abusive text, while retaining the intent of the text. We evaluate the performance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and Groq, on their ability to identify abusive text. We them to transform and obtain a text that is clean from abusive and inappropriate content but maintains a similar level of sentiment and semantics, i.e. the transformed text needs to maintain its message. Afterwards, we evaluate the raw and transformed datasets with sentiment analysis and semantic analysis. Our results show Groq provides vastly different results when compared with other LLMs. We have identified similarities between GPT-4o and DeepSeek-V3.",
    "authors": [
      "Rohitash Chandra",
      "Jiyong Choi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-14T11:39:34Z",
    "pdf_url": "https://arxiv.org/pdf/2507.10177v1"
  },
  {
    "arxiv_id": "2507.10142v1",
    "entry_id": "http://arxiv.org/abs/2507.10142v1",
    "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review",
    "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in coordinating multiple agents across simulated benchmarks and constrained scenarios. However, its deployment in real-world multi-agent systems (MAS) remains limited, primarily due to the complex and dynamic nature of such environments. These challenges arise from multiple interacting sources of variability, including fluctuating agent populations, evolving task goals, and inconsistent execution conditions. Together, these factors demand that MARL algorithms remain effective under continuously changing system configurations and operational demands. To better capture and assess this capacity for adjustment, we introduce the concept of \\textit{adaptability} as a unified and practically grounded lens through which to evaluate the reliability of MARL algorithms under shifting conditions, broadly referring to any changes in the environment dynamics that may occur during learning or execution. Centred on the notion of adaptability, we propose a structured framework comprising three key dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. By adopting this adaptability perspective, we aim to support more principled assessments of MARL performance beyond narrowly defined benchmarks. Ultimately, this survey contributes to the development of algorithms that are better suited for deployment in dynamic, real-world multi-agent systems.",
    "authors": [
      "Siyi Hu",
      "Mohamad A Hady",
      "Jianglin Qiao",
      "Jimmy Cao",
      "Mahardhika Pratama",
      "Ryszard Kowalczyk"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2025-07-14T10:39:17Z",
    "pdf_url": "https://arxiv.org/pdf/2507.10142v1"
  },
  {
    "arxiv_id": "2507.09955v1",
    "entry_id": "http://arxiv.org/abs/2507.09955v1",
    "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models",
    "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their V3 and R1 series models, which attracted global attention due to their low cost, high performance, and open-source advantages. This paper begins by reviewing the evolution of large AI models focusing on paradigm shifts, the mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm. Subsequently, the paper highlights novel algorithms introduced by DeepSeek, including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE), Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO). The paper then explores DeepSeek engineering breakthroughs in LLM scaling, training, inference, and system-level optimization architecture. Moreover, the impact of DeepSeek models on the competitive AI landscape is analyzed, comparing them to mainstream LLMs across various fields. Finally, the paper reflects on the insights gained from DeepSeek innovations and discusses future trends in the technical and engineering development of large AI models, particularly in data, training, and reasoning.",
    "authors": [
      "Luolin Xiong",
      "Haofen Wang",
      "Xi Chen",
      "Lu Sheng",
      "Yun Xiong",
      "Jingping Liu",
      "Yanghua Xiao",
      "Huajun Chen",
      "Qing-Long Han",
      "Yang Tang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-14T06:10:30Z",
    "pdf_url": "https://arxiv.org/pdf/2507.09955v1"
  },
  {
    "arxiv_id": "2507.09861v1",
    "entry_id": "http://arxiv.org/abs/2507.09861v1",
    "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends",
    "summary": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field, driven by the need to automatically process documents containing complex visual, textual, and layout information. Recently, Multimodal Large Language Models (MLLMs) have shown remarkable potential in this domain, leveraging both Optical Character Recognition (OCR)-dependent and OCR-free frameworks to extract and interpret information in document images. This survey reviews recent advancements in MLLM-based VRDU, highlighting three core components: (1) methods for encoding and fusing textual, visual, and layout features; (2) training paradigms, including pretraining strategies, instruction-response tuning, and the trainability of different model modules; and (3) datasets utilized for pretraining, instruction-tuning, and supervised fine-tuning. Finally, we discuss the challenges and opportunities in this evolving field and propose future directions to advance the efficiency, generalizability, and robustness of VRDU systems.",
    "authors": [
      "Yihao Ding",
      "Siwen Luo",
      "Yue Dai",
      "Yanbei Jiang",
      "Zechuan Li",
      "Geoffrey Martin",
      "Yifan Peng"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-07-14T02:10:31Z",
    "pdf_url": "https://arxiv.org/pdf/2507.09861v1"
  },
  {
    "arxiv_id": "2507.10621v1",
    "entry_id": "http://arxiv.org/abs/2507.10621v1",
    "title": "Game Theory Meets LLM and Agentic AI: Reimagining Cybersecurity for the Age of Intelligent Threats",
    "summary": "Protecting cyberspace requires not only advanced tools but also a shift in how we reason about threats, trust, and autonomy. Traditional cybersecurity methods rely on manual responses and brittle heuristics. To build proactive and intelligent defense systems, we need integrated theoretical frameworks and software tools. Game theory provides a rigorous foundation for modeling adversarial behavior, designing strategic defenses, and enabling trust in autonomous systems. Meanwhile, software tools process cyber data, visualize attack surfaces, verify compliance, and suggest mitigations. Yet a disconnect remains between theory and practical implementation.\n  The rise of Large Language Models (LLMs) and agentic AI offers a new path to bridge this gap. LLM-powered agents can operationalize abstract strategies into real-world decisions. Conversely, game theory can inform the reasoning and coordination of these agents across complex workflows. LLMs also challenge classical game-theoretic assumptions, such as perfect rationality or static payoffs, prompting new models aligned with cognitive and computational realities. This co-evolution promises richer theoretical foundations and novel solution concepts. Agentic AI also reshapes software design: systems must now be modular, adaptive, and trust-aware from the outset.\n  This chapter explores the intersection of game theory, agentic AI, and cybersecurity. We review key game-theoretic frameworks (e.g., static, dynamic, Bayesian, and signaling games) and solution concepts. We then examine how LLM agents can enhance cyber defense and introduce LLM-driven games that embed reasoning into AI agents. Finally, we explore multi-agent workflows and coordination games, outlining how this convergence fosters secure, intelligent, and adaptive cyber systems.",
    "authors": [
      "Quanyan Zhu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "published": "2025-07-14T00:49:44Z",
    "pdf_url": "https://arxiv.org/pdf/2507.10621v1"
  },
  {
    "arxiv_id": "2507.09662v1",
    "entry_id": "http://arxiv.org/abs/2507.09662v1",
    "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey",
    "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have demonstrated impressive performance on complex reasoning tasks like mathematics and programming with long Chain-of-Thought (CoT) reasoning sequences (slow-thinking), compared with traditional large language models (fast-thinking). However, these reasoning models also face a huge challenge that generating unnecessarily lengthy and redundant reasoning chains even for trivial questions. This phenomenon leads to a significant waste of inference resources, increases the response time for simple queries, and hinders the practical application of LRMs in real-world products. To this end, it is crucial to shorten lengthy reasoning chains and learn adaptive reasoning between fast and slow thinking based on input difficulty. In this survey, we provide a comprehensive overview of recent progress in concise and adaptive thinking for efficient reasoning of LRMs, including methodologies, benchmarks, and challenges for future exploration. We hope this survey can help researchers quickly understand the landscape of this field and inspire novel adaptive thinking ideas to facilitate better usage of LRMs.",
    "authors": [
      "Jason Zhu",
      "Hongyu Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-07-13T14:51:59Z",
    "pdf_url": "https://arxiv.org/pdf/2507.09662v1"
  },
  {
    "arxiv_id": "2507.09583v1",
    "entry_id": "http://arxiv.org/abs/2507.09583v1",
    "title": "A Serverless Architecture for Real-Time Stock Analysis using Large Language Models: An Iterative Development and Debugging Case Study",
    "summary": "The advent of powerful, accessible Large Language Models (LLMs) like Google's Gemini presents new opportunities for democratizing financial data analysis. This paper documents the design, implementation, and iterative debugging of a novel, serverless system for real-time stock analysis. The system leverages the Gemini API for qualitative assessment, automates data ingestion and processing via GitHub Actions, and presents the findings through a decoupled, static frontend. We detail the architectural evolution of the system, from initial concepts to a robust, event-driven pipeline, highlighting the practical challenges encountered during deployment. A significant portion of this paper is dedicated to a case study on the debugging process, covering common software errors, platform-specific permission issues, and rare, environment-level platform bugs. The final architecture operates at a near-zero cost, demonstrating a viable model for individuals to build sophisticated AI-powered financial tools. The operational application is publicly accessible, and the complete source code is available for review. We conclude by discussing the role of LLMs in financial analysis, the importance of robust debugging methodologies, and the emerging paradigm of human-AI collaboration in software development.",
    "authors": [
      "Taniv Ashraf"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-07-13T11:29:51Z",
    "pdf_url": "https://arxiv.org/pdf/2507.09583v1"
  },
  {
    "arxiv_id": "2507.09477v2",
    "entry_id": "http://arxiv.org/abs/2507.09477v2",
    "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs",
    "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language Models (LLMs) by injecting external knowledge, yet it falls short on problems that demand multi-step inference; conversely, purely reasoning-oriented approaches often hallucinate or mis-ground facts. This survey synthesizes both strands under a unified reasoning-retrieval perspective. We first map how advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then, we show how retrieved knowledge of different type supply missing premises and expand context for complex inference (RAG-Enhanced Reasoning). Finally, we spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs iteratively interleave search and reasoning to achieve state-of-the-art performance across knowledge-intensive benchmarks. We categorize methods, datasets, and open challenges, and outline research avenues toward deeper RAG-Reasoning systems that are more effective, multimodally-adaptive, trustworthy, and human-centric. The collection is available at https://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
    "authors": [
      "Yangning Li",
      "Weizhi Zhang",
      "Yuyao Yang",
      "Wei-Chieh Huang",
      "Yaozu Wu",
      "Junyu Luo",
      "Yuanchen Bei",
      "Henry Peng Zou",
      "Xiao Luo",
      "Yusheng Zhao",
      "Chunkit Chan",
      "Yankai Chen",
      "Zhongfen Deng",
      "Yinghui Li",
      "Hai-Tao Zheng",
      "Dongyuan Li",
      "Renhe Jiang",
      "Ming Zhang",
      "Yangqiu Song",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-13T03:29:41Z",
    "pdf_url": "https://arxiv.org/pdf/2507.09477v2"
  },
  {
    "arxiv_id": "2507.09406v1",
    "entry_id": "http://arxiv.org/abs/2507.09406v1",
    "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers",
    "summary": "Large language models (LLMs) aligned for safety through techniques like reinforcement learning from human feedback (RLHF) often exhibit emergent deceptive behaviors, where outputs appear compliant but subtly mislead or omit critical information. This paper introduces adversarial activation patching, a novel mechanistic interpretability framework that leverages activation patching as an adversarial tool to induce, detect, and mitigate such deception in transformer-based models. By sourcing activations from \"deceptive\" prompts and patching them into safe forward passes at specific layers, we simulate vulnerabilities and quantify deception rates. Through toy neural network simulations across multiple scenarios (e.g., 1000 trials per setup), we demonstrate that adversarial patching increases deceptive outputs to 23.9% from a 0% baseline, with layer-specific variations supporting our hypotheses. We propose six hypotheses, including transferability across models, exacerbation in multimodal settings, and scaling effects. An expanded literature review synthesizes over 20 key works in interpretability, deception, and adversarial attacks. Mitigation strategies, such as activation anomaly detection and robust fine-tuning, are detailed, alongside ethical considerations and future research directions. This work advances AI safety by highlighting patching's dual-use potential and provides a roadmap for empirical studies on large-scale models.",
    "authors": [
      "Santhosh Kumar Ravindran"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-07-12T21:29:49Z",
    "pdf_url": "https://arxiv.org/pdf/2507.09406v1"
  },
  {
    "arxiv_id": "2507.15862v2",
    "entry_id": "http://arxiv.org/abs/2507.15862v2",
    "title": "Quantifying Holistic Review: A Multi-Modal Approach to College Admissions Prediction",
    "summary": "This paper introduces the Comprehensive Applicant Profile Score (CAPS), a novel multi-modal framework designed to quantitatively model and interpret holistic college admissions evaluations. CAPS decomposes applicant profiles into three interpretable components: academic performance (Standardized Academic Score, SAS), essay quality (Essay Quality Index, EQI), and extracurricular engagement (Extracurricular Impact Score, EIS). Leveraging transformer-based semantic embeddings, LLM scoring, and XGBoost regression, CAPS provides transparent and explainable evaluations aligned with human judgment. Experiments on a synthetic but realistic dataset demonstrate strong performance, achieving an EQI prediction R^2 of 0.80, classification accuracy over 75%, a macro F1 score of 0.69, and a weighted F1 score of 0.74. CAPS addresses key limitations in traditional holistic review -- particularly the opacity, inconsistency, and anxiety faced by applicants -- thus paving the way for more equitable and data-informed admissions practices.",
    "authors": [
      "Jun-Wei Zeng",
      "Jerry Shen"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "published": "2025-07-12T16:58:03Z",
    "pdf_url": "https://arxiv.org/pdf/2507.15862v2"
  },
  {
    "arxiv_id": "2507.09100v1",
    "entry_id": "http://arxiv.org/abs/2507.09100v1",
    "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data",
    "summary": "In decision-making conversations, experts must navigate complex choices and make on-the-spot decisions while engaged in conversation. Although extensive historical data often exists, the real-time nature of these scenarios makes it infeasible for decision-makers to review and leverage relevant information. This raises an interesting question: What if experts could utilize relevant past data in real-time decision-making through insights derived from past data? To explore this, we implemented a conversational user interface, taking doctor-patient interactions as an example use case. Our system continuously listens to the conversation, identifies patient problems and doctor-suggested solutions, and retrieves related data from an embedded dataset, generating concise insights using a pipeline built around a retrieval-based Large Language Model (LLM) agent. We evaluated the prototype by embedding Health Canada datasets into a vector database and conducting simulated studies using sample doctor-patient dialogues, showing effectiveness but also challenges, setting directions for the next steps of our work.",
    "authors": [
      "Mohammad Abolnejadian",
      "Shakiba Amirshahi",
      "Matthew Brehmer",
      "Anamaria Crisan"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-07-12T00:59:41Z",
    "pdf_url": "https://arxiv.org/pdf/2507.09100v1"
  },
  {
    "arxiv_id": "2507.09037v1",
    "entry_id": "http://arxiv.org/abs/2507.09037v1",
    "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making",
    "summary": "Large language models (LLMs) are increasingly being used as decision aids. However, users have diverse values and preferences that can affect their decision-making, which requires novel methods for LLM alignment and personalization. Existing LLM comparison tools largely focus on benchmarking tasks, such as knowledge-based question answering. In contrast, our proposed ALIGN system focuses on dynamic personalization of LLM-based decision-makers through prompt-based alignment to a set of fine-grained attributes. Key features of our system include robust configuration management, structured output generation with reasoning, and several algorithm implementations with swappable LLM backbones, enabling different types of analyses. Our user interface enables a qualitative, side-by-side comparison of LLMs and their alignment to various attributes, with a modular backend for easy algorithm integration. Additionally, we perform a quantitative analysis comparing alignment approaches in two different domains: demographic alignment for public opinion surveys and value alignment for medical triage decision-making. The entire ALIGN framework is open source and will enable new research on reliable, responsible, and personalized LLM-based decision-makers.",
    "authors": [
      "Bharadwaj Ravichandran",
      "David Joy",
      "Paul Elliott",
      "Brian Hu",
      "Jadie Adams",
      "Christopher Funk",
      "Emily Veenhuis",
      "Anthony Hoogs",
      "Arslan Basharat"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-11T21:33:38Z",
    "pdf_url": "https://arxiv.org/pdf/2507.09037v1"
  },
  {
    "arxiv_id": "2507.10587v1",
    "entry_id": "http://arxiv.org/abs/2507.10587v1",
    "title": "Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing",
    "summary": "Human users increasingly rely on natural language interactions with large language models (LLMs) in order to receive help on a large variety of tasks and problems. However, the trustworthiness and perceived legitimacy of LLMs is undermined by the fact that their output is frequently stated in very confident terms, even when its accuracy is questionable. Therefore, there is a need to signal the confidence of the language model to a user in order to reap the benefits of human-machine collaboration and mitigate potential harms. Verbalized uncertainty is the expression of confidence with linguistic means, an approach that integrates perfectly into language-based interfaces. Nevertheless, most recent research in natural language processing (NLP) overlooks the nuances surrounding human uncertainty communication and the data biases that influence machine uncertainty communication. We argue for anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty communication requires a degree of linguistic authenticity and personalization to the user, which could be achieved by emulating human communication. We present a thorough overview over the research in human uncertainty communication, survey ongoing research, and perform additional analyses to demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by pointing out unique factors in human-machine communication of uncertainty and deconstruct anthropomimetic uncertainty into future research directions for NLP.",
    "authors": [
      "Dennis Ulmer",
      "Alexandra Lorson",
      "Ivan Titov",
      "Christian Hardmeier"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-11T14:07:22Z",
    "pdf_url": "https://arxiv.org/pdf/2507.10587v1"
  },
  {
    "arxiv_id": "2507.08505v2",
    "entry_id": "http://arxiv.org/abs/2507.08505v2",
    "title": "Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R",
    "summary": "Vision-Language Models (VLMs) offer promising capabilities for mobile devices, but their deployment faces significant challenges due to computational limitations and energy inefficiency, especially for real-time applications. This study provides a comprehensive survey of deployment frameworks for VLMs on mobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R while running VLMs, with measurements covering CPU, GPU, and NPU utilization, temperature, inference time, power consumption, and user experience. Benchmarking revealed critical performance bottlenecks across frameworks: CPU resources were consistently over-utilized during token generation, while GPU and NPU accelerators were largely unused. When the GPU was used, primarily for image feature extraction, it was saturated, leading to degraded device responsiveness. The study contributes framework-level benchmarks, practical profiling tools, and an in-depth analysis of hardware utilization bottlenecks, highlighting the consistent overuse of CPUs and the ineffective or unstable use of GPUs and NPUs in current deployment frameworks.",
    "authors": [
      "Pablo Robin Guerrero",
      "Yueyang Pan",
      "Sanidhya Kashyap"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-07-11T11:30:57Z",
    "pdf_url": "https://arxiv.org/pdf/2507.08505v2"
  },
  {
    "arxiv_id": "2507.08310v1",
    "entry_id": "http://arxiv.org/abs/2507.08310v1",
    "title": "Generative AI in Science: Applications, Challenges, and Emerging Questions",
    "summary": "This paper examines the impact of Generative Artificial Intelligence (GenAI) on scientific practices, conducting a qualitative review of selected literature to explore its applications, benefits, and challenges. The review draws on the OpenAlex publication database, using a Boolean search approach to identify scientific literature related to GenAI (including large language models and ChatGPT). Thirty-nine highly cited papers and commentaries are reviewed and qualitatively coded. Results are categorized by GenAI applications in science, scientific writing, medical practice, and education and training. The analysis finds that while there is a rapid adoption of GenAI in science and science practice, its long-term implications remain unclear, with ongoing uncertainties about its use and governance. The study provides early insights into GenAI's growing role in science and identifies questions for future research in this evolving field.",
    "authors": [
      "Ryan Harries",
      "Cornelia Lawson",
      "Philip Shapira"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-07-11T05:02:24Z",
    "pdf_url": "https://arxiv.org/pdf/2507.08310v1"
  },
  {
    "arxiv_id": "2507.08309v1",
    "entry_id": "http://arxiv.org/abs/2507.08309v1",
    "title": "Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency",
    "summary": "Multimodal Large Language Models (MLLMs) have shown strong performance in document image tasks, especially Optical Character Recognition (OCR). However, they struggle with Document Image Machine Translation (DIMT), which requires handling both cross-modal and cross-lingual challenges. Previous efforts to enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT dataset often result in the forgetting of the model's existing monolingual abilities, such as OCR. To address these challenges, we introduce a novel fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR proficiency, inspired by the concept \"Bilingual Cognitive Advantage\". Specifically, SSR prompts the model to generate OCR text before producing translation text, which allows the model to leverage its strong monolingual OCR ability while learning to translate text across languages. Comprehensive experiments demonstrate the proposed SSR learning helps mitigate catastrophic forgetting, improving the generalization ability of MLLMs on both OCR and DIMT tasks.",
    "authors": [
      "Yupu Liang",
      "Yaping Zhang",
      "Zhiyang Zhang",
      "Zhiyuan Chen",
      "Yang Zhao",
      "Lu Xiang",
      "Chengqing Zong",
      "Yu Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-07-11T05:02:06Z",
    "pdf_url": "https://arxiv.org/pdf/2507.08309v1"
  },
  {
    "arxiv_id": "2507.07748v1",
    "entry_id": "http://arxiv.org/abs/2507.07748v1",
    "title": "When Large Language Models Meet Law: Dual-Lens Taxonomy, Technical Advances, and Ethical Governance",
    "summary": "This paper establishes the first comprehensive review of Large Language Models (LLMs) applied within the legal domain. It pioneers an innovative dual lens taxonomy that integrates legal reasoning frameworks and professional ontologies to systematically unify historical research and contemporary breakthroughs. Transformer-based LLMs, which exhibit emergent capabilities such as contextual reasoning and generative argumentation, surmount traditional limitations by dynamically capturing legal semantics and unifying evidence reasoning. Significant progress is documented in task generalization, reasoning formalization, workflow integration, and addressing core challenges in text processing, knowledge integration, and evaluation rigor via technical innovations like sparse attention mechanisms and mixture-of-experts architectures. However, widespread adoption of LLM introduces critical challenges: hallucination, explainability deficits, jurisdictional adaptation difficulties, and ethical asymmetry. This review proposes a novel taxonomy that maps legal roles to NLP subtasks and computationally implements the Toulmin argumentation framework, thus systematizing advances in reasoning, retrieval, prediction, and dispute resolution. It identifies key frontiers including low-resource systems, multimodal evidence integration, and dynamic rebuttal handling. Ultimately, this work provides both a technical roadmap for researchers and a conceptual framework for practitioners navigating the algorithmic future, laying a robust foundation for the next era of legal artificial intelligence. We have created a GitHub repository to index the relevant papers: https://github.com/Kilimajaro/LLMs_Meet_Law.",
    "authors": [
      "Peizhang Shao",
      "Linrui Xu",
      "Jinxi Wang",
      "Wei Zhou",
      "Xingyu Wu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-10T13:26:34Z",
    "pdf_url": "https://arxiv.org/pdf/2507.07748v1"
  },
  {
    "arxiv_id": "2507.07735v1",
    "entry_id": "http://arxiv.org/abs/2507.07735v1",
    "title": "GuardVal: Dynamic Large Language Model Jailbreak Evaluation for Comprehensive Safety Testing",
    "summary": "Jailbreak attacks reveal critical vulnerabilities in Large Language Models (LLMs) by causing them to generate harmful or unethical content. Evaluating these threats is particularly challenging due to the evolving nature of LLMs and the sophistication required in effectively probing their vulnerabilities. Current benchmarks and evaluation methods struggle to fully address these challenges, leaving gaps in the assessment of LLM vulnerabilities. In this paper, we review existing jailbreak evaluation practices and identify three assumed desiderata for an effective jailbreak evaluation protocol. To address these challenges, we introduce GuardVal, a new evaluation protocol that dynamically generates and refines jailbreak prompts based on the defender LLM's state, providing a more accurate assessment of defender LLMs' capacity to handle safety-critical situations. Moreover, we propose a new optimization method that prevents stagnation during prompt refinement, ensuring the generation of increasingly effective jailbreak prompts that expose deeper weaknesses in the defender LLMs. We apply this protocol to a diverse set of models, from Mistral-7b to GPT-4, across 10 safety domains. Our findings highlight distinct behavioral patterns among the models, offering a comprehensive view of their robustness. Furthermore, our evaluation process deepens the understanding of LLM behavior, leading to insights that can inform future research and drive the development of more secure models.",
    "authors": [
      "Peiyan Zhang",
      "Haibo Jin",
      "Liying Kang",
      "Haohan Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ],
    "published": "2025-07-10T13:15:20Z",
    "pdf_url": "https://arxiv.org/pdf/2507.07735v1"
  },
  {
    "arxiv_id": "2507.07509v1",
    "entry_id": "http://arxiv.org/abs/2507.07509v1",
    "title": "Toward Real-World Chinese Psychological Support Dialogues: CPsDD Dataset and a Co-Evolving Multi-Agent System",
    "summary": "The growing need for psychological support due to increasing pressures has exposed the scarcity of relevant datasets, particularly in non-English languages. To address this, we propose a framework that leverages limited real-world data and expert knowledge to fine-tune two large language models: Dialog Generator and Dialog Modifier. The Generator creates large-scale psychological counseling dialogues based on predefined paths, which guide system response strategies and user interactions, forming the basis for effective support. The Modifier refines these dialogues to align with real-world data quality. Through both automated and manual review, we construct the Chinese Psychological support Dialogue Dataset (CPsDD), containing 68K dialogues across 13 groups, 16 psychological problems, 13 causes, and 12 support focuses. Additionally, we introduce the Comprehensive Agent Dialogue Support System (CADSS), where a Profiler analyzes user characteristics, a Summarizer condenses dialogue history, a Planner selects strategies, and a Supporter generates empathetic responses. The experimental results of the Strategy Prediction and Emotional Support Conversation (ESC) tasks demonstrate that CADSS achieves state-of-the-art performance on both CPsDD and ESConv datasets.",
    "authors": [
      "Yuanchen Shi",
      "Longyin Zhang",
      "Fang Kong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-07-10T07:56:35Z",
    "pdf_url": "https://arxiv.org/pdf/2507.07509v1"
  },
  {
    "arxiv_id": "2507.07456v1",
    "entry_id": "http://arxiv.org/abs/2507.07456v1",
    "title": "General purpose models for the chemical sciences",
    "summary": "Data-driven techniques have a large potential to transform and accelerate the chemical sciences. However, chemical sciences also pose the unique challenge of very diverse, small, fuzzy datasets that are difficult to leverage in conventional machine learning approaches completely. A new class of models, general-purpose models (GPMs) such as large language models, have shown the ability to solve tasks they have not been directly trained on, and to flexibly operate with low amounts of data in different formats. In this review, we discuss fundamental building principles of GPMs and review recent applications of those models in the chemical sciences across the entire scientific process. While many of these applications are still in the prototype phase, we expect that the increasing interest in GPMs will make many of them mature in the coming years.",
    "authors": [
      "Nawaf Alampara",
      "Anagha Aneesh",
      "Martiño Ríos-García",
      "Adrian Mirza",
      "Mara Schilling-Wilhelmi",
      "Ali Asghar Aghajani",
      "Meiling Sun",
      "Gordan Prastalo",
      "Kevin Maik Jablonka"
    ],
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci",
      "physics.chem-ph"
    ],
    "published": "2025-07-10T06:18:46Z",
    "pdf_url": "https://arxiv.org/pdf/2507.07456v1"
  },
  {
    "arxiv_id": "2507.07293v1",
    "entry_id": "http://arxiv.org/abs/2507.07293v1",
    "title": "Thermodynamic Prediction Enabled by Automatic Dataset Building and Machine Learning",
    "summary": "New discoveries in chemistry and materials science, with increasingly expanding volume of requisite knowledge and experimental workload, provide unique opportunities for machine learning (ML) to take critical roles in accelerating research efficiency. Here, we demonstrate (1) the use of large language models (LLMs) for automated literature reviews, and (2) the training of an ML model to predict chemical knowledge (thermodynamic parameters). Our LLM-based literature review tool (LMExt) successfully extracted chemical information and beyond into a machine-readable structure, including stability constants for metal cation-ligand interactions, thermodynamic properties, and other broader data types (medical research papers, and financial reports), effectively overcoming the challenges inherent in each domain. Using the autonomous acquisition of thermodynamic data, an ML model was trained using the CatBoost algorithm for accurately predicting thermodynamic parameters (e.g., enthalpy of formation) of minerals. This work highlights the transformative potential of integrated ML approaches to reshape chemistry and materials science research.",
    "authors": [
      "Juejing Liu",
      "Haydn Anderson",
      "Noah I. Waxman",
      "Vsevolod Kovalev",
      "Byron Fisher",
      "Elizabeth Li",
      "Xiaofeng Guo"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cs.LG"
    ],
    "published": "2025-07-09T21:33:25Z",
    "pdf_url": "https://arxiv.org/pdf/2507.07293v1"
  },
  {
    "arxiv_id": "2507.07188v3",
    "entry_id": "http://arxiv.org/abs/2507.07188v3",
    "title": "Prompt Perturbations Reveal Human-Like Biases in Large Language Model Survey Responses",
    "summary": "Large Language Models (LLMs) are increasingly used as proxies for human subjects in social science surveys, but their reliability and susceptibility to known human-like response biases, such as central tendency, opinion floating and primacy bias are poorly understood. This work investigates the response robustness of LLMs in normative survey contexts, we test nine LLMs on questions from the World Values Survey (WVS), applying a comprehensive set of ten perturbations to both question phrasing and answer option structure, resulting in over 167,000 simulated survey interviews. In doing so, we not only reveal LLMs' vulnerabilities to perturbations but also show that all tested models exhibit a consistent recency bias, disproportionately favoring the last-presented answer option. While larger models are generally more robust, all models remain sensitive to semantic variations like paraphrasing and to combined perturbations. This underscores the critical importance of prompt design and robustness testing when using LLMs to generate synthetic survey data.",
    "authors": [
      "Jens Rupprecht",
      "Georg Ahnert",
      "Markus Strohmaier"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-07-09T18:01:50Z",
    "pdf_url": "https://arxiv.org/pdf/2507.07188v3"
  },
  {
    "arxiv_id": "2507.08038v1",
    "entry_id": "http://arxiv.org/abs/2507.08038v1",
    "title": "AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research",
    "summary": "Autonomous agents built on language models (LMs) are showing increasing popularity in many fields, including scientific research. AI co-scientists aim to support or automate parts of the research process using these agents. A key component of empirical AI research is the design of ablation experiments. To this end, we introduce AblationBench, a benchmark suite for evaluating agents on ablation planning tasks in empirical AI research. It includes two tasks: AuthorAblation, which helps authors propose ablation experiments based on a method section and contains 83 instances, and ReviewerAblation, which helps reviewers find missing ablations in a full paper and contains 350 instances. For both tasks, we develop LM-based judges that serve as an automatic evaluation framework. Our experiments with frontier LMs show that these tasks remain challenging, with the best-performing LM system identifying only 29% of the original ablations on average. Lastly, we analyze the limitations of current LMs on these tasks, and find that chain-of-thought prompting outperforms the currently existing agent-based approach.",
    "authors": [
      "Talor Abramovich",
      "Gal Chechik"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-09T12:07:38Z",
    "pdf_url": "https://arxiv.org/pdf/2507.08038v1"
  },
  {
    "arxiv_id": "2507.07148v1",
    "entry_id": "http://arxiv.org/abs/2507.07148v1",
    "title": "Explainable Artificial Intelligence in Biomedical Image Analysis: A Comprehensive Survey",
    "summary": "Explainable artificial intelligence (XAI) has become increasingly important in biomedical image analysis to promote transparency, trust, and clinical adoption of DL models. While several surveys have reviewed XAI techniques, they often lack a modality-aware perspective, overlook recent advances in multimodal and vision-language paradigms, and provide limited practical guidance. This survey addresses this gap through a comprehensive and structured synthesis of XAI methods tailored to biomedical image analysis.We systematically categorize XAI methods, analyzing their underlying principles, strengths, and limitations within biomedical contexts. A modality-centered taxonomy is proposed to align XAI methods with specific imaging types, highlighting the distinct interpretability challenges across modalities. We further examine the emerging role of multimodal learning and vision-language models in explainable biomedical AI, a topic largely underexplored in previous work. Our contributions also include a summary of widely used evaluation metrics and open-source frameworks, along with a critical discussion of persistent challenges and future directions. This survey offers a timely and in-depth foundation for advancing interpretable DL in biomedical image analysis.",
    "authors": [
      "Getamesay Haile Dagnaw",
      "Yanming Zhu",
      "Muhammad Hassan Maqsood",
      "Wencheng Yang",
      "Xingshuai Dong",
      "Xuefei Yin",
      "Alan Wee-Chung Liew"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-07-09T08:42:14Z",
    "pdf_url": "https://arxiv.org/pdf/2507.07148v1"
  },
  {
    "arxiv_id": "2507.06623v1",
    "entry_id": "http://arxiv.org/abs/2507.06623v1",
    "title": "Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review",
    "summary": "The data extraction stages of reviews are resource-intensive, and researchers may seek to expediate data extraction using online (large language models) LLMs and review protocols. Claude 3.5 Sonnet was used to trial two approaches that used a review protocol to prompt data extraction from 10 evidence sources included in a case study scoping review. A protocol-based approach was also used to review extracted data. Limited performance evaluation was undertaken which found high accuracy for the two extraction approaches (83.3% and 100%) when extracting simple, well-defined citation details; accuracy was lower (9.6% and 15.8%) when extracting more complex, subjective data items. Considering all data items, both approaches had precision >90% but low recall (<25%) and F1 scores (<40%). The context of a complex scoping review, open response types and methodological approach likely impacted performance due to missed and misattributed data. LLM feedback considered the baseline extraction accurate and suggested minor amendments: four of 15 (26.7%) to citation details and 8 of 38 (21.1%) to key findings data items were considered to potentially add value. However, when repeating the process with a dataset featuring deliberate errors, only 2 of 39 (5%) errors were detected. Review-protocol-based methods used for expediency require more robust performance evaluation across a range of LLMs and review contexts with comparison to conventional prompt engineering approaches. We recommend researchers evaluate and report LLM performance if using them similarly to conduct data extraction or review extracted data. LLM feedback contributed to protocol adaptation and may assist future review protocol drafting.",
    "authors": [
      "James Stewart-Evans",
      "Emma Wilson",
      "Tessa Langley",
      "Andrew Prayle",
      "Angela Hands",
      "Karen Exley",
      "Jo Leonardi-Bee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-09T07:50:55Z",
    "pdf_url": "https://arxiv.org/pdf/2507.06623v1"
  },
  {
    "arxiv_id": "2507.06565v5",
    "entry_id": "http://arxiv.org/abs/2507.06565v5",
    "title": "A Mathematical Theory of Discursive Networks",
    "summary": "Large language models (LLMs) turn writing into a live exchange between humans and software. We characterize this new medium as a discursive network that treats people and LLMs as equal nodes and tracks how their statements circulate. We define the generation of erroneous information as invalidation (any factual, logical, or structural breach) and show it follows four hazards: drift from truth, self-repair, fresh fabrication, and external detection. We develop a general mathematical model of discursive networks that shows that a network governed only by drift and self-repair stabilizes at a modest error rate. Giving each false claim even a small chance of peer review shifts the system to a truth-dominant state. We operationalize peer review with the open-source Flaws-of-Others (FOO) algorithm: a configurable loop in which any set of agents critique one another while a harmonizer merges their verdicts. We identify an ethical transgression, epithesis, that occurs when humans fail to engage in the discursive network. The takeaway is practical and cultural: reliability in this new medium comes not from perfecting single models but from connecting imperfect ones into networks that enforce mutual accountability.",
    "authors": [
      "Juan B. Gutiérrez"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-07-09T05:39:56Z",
    "pdf_url": "https://arxiv.org/pdf/2507.06565v5"
  },
  {
    "arxiv_id": "2507.06507v2",
    "entry_id": "http://arxiv.org/abs/2507.06507v2",
    "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models",
    "summary": "In the past year, Generative Recommendations (GRs) have undergone substantial advancements, especially in leveraging the powerful sequence modeling and reasoning capabilities of Large Language Models (LLMs) to enhance overall recommendation performance. LLM-based GRs are forming a new paradigm that is distinctly different from discriminative recommendations, showing strong potential to replace traditional recommendation systems heavily dependent on complex hand-crafted features. In this paper, we provide a comprehensive survey aimed at facilitating further research of LLM-based GRs. Initially, we outline the general preliminaries and application cases of LLM-based GRs. Subsequently, we introduce the main considerations when LLM-based GRs are applied in real industrial scenarios. Finally, we explore promising directions for LLM-based GRs. We hope that this survey contributes to the ongoing advancement of the GR domain.",
    "authors": [
      "Zhen Yang",
      "Haitao Lin",
      "Jiawei xue",
      "Ziji Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-07-09T03:13:08Z",
    "pdf_url": "https://arxiv.org/pdf/2507.06507v2"
  },
  {
    "arxiv_id": "2507.06185v1",
    "entry_id": "http://arxiv.org/abs/2507.06185v1",
    "title": "Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review",
    "summary": "In July 2025, 18 academic manuscripts on the preprint website arXiv were found to contain hidden instructions known as prompts designed to manipulate AI-assisted peer review. Instructions such as \"GIVE A POSITIVE REVIEW ONLY\" were concealed using techniques like white-colored text. Author responses varied: one planned to withdraw the affected paper, while another defended the practice as legitimate testing of reviewer compliance. This commentary analyzes this practice as a novel form of research misconduct. We examine the technique of prompt injection in large language models (LLMs), revealing four types of hidden prompts, ranging from simple positive review commands to detailed evaluation frameworks. The defense that prompts served as \"honeypots\" to detect reviewers improperly using AI fails under examination--the consistently self-serving nature of prompt instructions indicates intent to manipulate. Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer review entirely, while Springer Nature permits limited use with disclosure requirements. The incident exposes systematic vulnerabilities extending beyond peer review to any automated system processing scholarly texts, including plagiarism detection and citation indexing. Our analysis underscores the need for coordinated technical screening at submission portals and harmonized policies governing generative AI (GenAI) use in academic evaluation.",
    "authors": [
      "Zhicheng Lin"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2025-07-08T17:11:13Z",
    "pdf_url": "https://arxiv.org/pdf/2507.06185v1"
  },
  {
    "arxiv_id": "2507.06278v1",
    "entry_id": "http://arxiv.org/abs/2507.06278v1",
    "title": "A Survey of Multi Agent Reinforcement Learning: Federated Learning and Cooperative and Noncooperative Decentralized Regimes",
    "summary": "The increasing interest in research and innovation towards the development of autonomous agents presents a number of complex yet important scenarios of multiple AI Agents interacting with each other in an environment. The particular setting can be understood as exhibiting three possibly topologies of interaction - centrally coordinated cooperation, ad-hoc interaction and cooperation, and settings with noncooperative incentive structures. This article presents a comprehensive survey of all three domains, defined under the formalism of Federal Reinforcement Learning (RL), Decentralized RL, and Noncooperative RL, respectively. Highlighting the structural similarities and distinctions, we review the state of the art in these subjects, primarily explored and developed only recently in the literature. We include the formulations as well as known theoretical guarantees and highlights and limitations of numerical performance.",
    "authors": [
      "Kemboi Cheruiyot",
      "Nickson Kiprotich",
      "Vyacheslav Kungurtsev",
      "Kennedy Mugo",
      "Vivian Mwirigi",
      "Marvin Ngesa"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-07-08T13:47:40Z",
    "pdf_url": "https://arxiv.org/pdf/2507.06278v1"
  },
  {
    "arxiv_id": "2507.05890v2",
    "entry_id": "http://arxiv.org/abs/2507.05890v2",
    "title": "Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators",
    "summary": "As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs simulate human survey responses. We publicly release our dataset and code to support future work.",
    "authors": [
      "Sungjib Lim",
      "Woojung Song",
      "Eun-Ju Lee",
      "Yohan Jo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-08T11:26:03Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05890v2"
  },
  {
    "arxiv_id": "2507.05723v1",
    "entry_id": "http://arxiv.org/abs/2507.05723v1",
    "title": "Large Language Models for Agent-Based Modelling: Current and possible uses across the modelling cycle",
    "summary": "The emergence of Large Language Models (LLMs) with increasingly sophisticated natural language understanding and generative capabilities has sparked interest in the Agent-based Modelling (ABM) community. With their ability to summarize, generate, analyze, categorize, transcribe and translate text, answer questions, propose explanations, sustain dialogue, extract information from unstructured text, and perform logical reasoning and problem-solving tasks, LLMs have a good potential to contribute to the modelling process. After reviewing the current use of LLMs in ABM, this study reflects on the opportunities and challenges of the potential use of LLMs in ABM. It does so by following the modelling cycle, from problem formulation to documentation and communication of model results, and holding a critical stance.",
    "authors": [
      "Loïs Vanhée",
      "Melania Borit",
      "Peer-Olaf Siebers",
      "Roger Cremades",
      "Christopher Frantz",
      "Önder Gürcan",
      "František Kalvas",
      "Denisa Reshef Kera",
      "Vivek Nallur",
      "Kavin Narasimhan",
      "Martin Neumann"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-07-08T07:17:24Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05723v1"
  },
  {
    "arxiv_id": "2507.05613v2",
    "entry_id": "http://arxiv.org/abs/2507.05613v2",
    "title": "Domain adaptation of large language models for geotechnical applications",
    "summary": "The rapid advancement of large language models (LLMs) is transforming opportunities in geotechnical engineering, where workflows rely on complex, text-rich data. While general-purpose LLMs demonstrate strong reasoning capabilities, their effectiveness in geotechnical applications is constrained by limited exposure to specialized terminology and domain logic. Thus, domain adaptation, tailoring general LLMs for geotechnical use, has become essential. This paper presents the first systematic review of LLM adaptation and application in geotechnical contexts. It critically examines four key adaptation strategies, including prompt engineering, retrieval augmented generation, domain-adaptive pretraining, and fine-tuning, and evaluates their comparative benefits, limitations, and implementation trends. This review synthesizes current applications spanning geological interpretation, subsurface characterization, design analysis, numerical modeling, risk assessment, and geotechnical education. Findings show that domain-adapted LLMs substantially improve reasoning accuracy, automation, and interpretability, yet remain limited by data scarcity, validation challenges, and explainability concerns. Future research directions are also suggested. This review establishes a critical foundation for developing geotechnically literate LLMs and guides researchers and practitioners in advancing the digital transformation of geotechnical engineering.",
    "authors": [
      "Lei Fan",
      "Fangxue Liu",
      "Cheng Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-08T02:45:44Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05613v2"
  },
  {
    "arxiv_id": "2507.05598v1",
    "entry_id": "http://arxiv.org/abs/2507.05598v1",
    "title": "Self-Review Framework for Enhancing Instruction Following Capability of LLM",
    "summary": "Various techniques have been proposed to improve large language models (LLMs) adherence to formatting and instruction constraints. One of the most effective approaches involves utilizing high-quality data generated by powerful models. However, such models often fail to fully comply with complex instructions in a single generation. To address this limitation, iterative revision methods have been introduced. Nevertheless, as the number of data points and revision iterations increases, the associated monetary costs grow significantly. As a resource-efficient alternative, methods have been proposed that leverage high-performance evaluation tools to compensate for the limited self-evaluation capabilities of open-source LLMs. However, these approaches often lead to a degradation in output quality due to excessive revision. To overcome these challenges, we propose Re5, a self-evaluation and revision framework designed to enhance instruction-following performance while preserving the quality of the generated content. Re5 extracts task and constraint components from user instructions, performs structural evaluations to prevent error accumulation, and applies fine-grained constraint-specific content evaluations followed by selective revisions. This process ensures precise and quality-preserving improvements. The final high-quality outputs are used for alignment tuning, enabling long-term alignment improvements through a data-centric iterative refinement loop. Experimental results demonstrate that Re5 achieves instruction-following performance comparable to models trained on data generated by GPT-4o-mini, a high-performance model, even with a small amount of data while maintaining response quality with a 64.24%-win rate over the non-revised initial responses. These results validate Re5 as an efficient and effective solution for enhancing instruction adherence with minimal external supervision.",
    "authors": [
      "Sihyun Park"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-08T02:17:18Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05598v1"
  },
  {
    "arxiv_id": "2507.05419v1",
    "entry_id": "http://arxiv.org/abs/2507.05419v1",
    "title": "Motion Generation: A Survey of Generative Approaches and Benchmarks",
    "summary": "Motion generation, the task of synthesizing realistic motion sequences from various conditioning inputs, has become a central problem in computer vision, computer graphics, and robotics, with applications ranging from animation and virtual agents to human-robot interaction. As the field has rapidly progressed with the introduction of diverse modeling paradigms including GANs, autoencoders, autoregressive models, and diffusion-based techniques, each approach brings its own advantages and limitations. This growing diversity has created a need for a comprehensive and structured review that specifically examines recent developments from the perspective of the generative approach employed.\n  In this survey, we provide an in-depth categorization of motion generation methods based on their underlying generative strategies. Our main focus is on papers published in top-tier venues since 2023, reflecting the most recent advancements in the field. In addition, we analyze architectural principles, conditioning mechanisms, and generation settings, and compile a detailed overview of the evaluation metrics and datasets used across the literature. Our objective is to enable clearer comparisons and identify open challenges, thereby offering a timely and foundational reference for researchers and practitioners navigating the rapidly evolving landscape of motion generation.",
    "authors": [
      "Aliasghar Khani",
      "Arianna Rampini",
      "Bruno Roy",
      "Larasika Nadela",
      "Noa Kaplan",
      "Evan Atherton",
      "Derek Cheung",
      "Jacky Bibliowicz"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-07-07T19:04:56Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05419v1"
  },
  {
    "arxiv_id": "2507.18577v1",
    "entry_id": "http://arxiv.org/abs/2507.18577v1",
    "title": "Advancing Financial Engineering with Foundation Models: Progress, Applications, and Challenges",
    "summary": "The advent of foundation models (FMs) - large-scale pre-trained models with strong generalization capabilities - has opened new frontiers for financial engineering. While general-purpose FMs such as GPT-4 and Gemini have demonstrated promising performance in tasks ranging from financial report summarization to sentiment-aware forecasting, many financial applications remain constrained by unique domain requirements such as multimodal reasoning, regulatory compliance, and data privacy. These challenges have spurred the emergence of Financial Foundation Models (FFMs) - a new class of models explicitly designed for finance. This survey presents a comprehensive overview of FFMs, with a taxonomy spanning three key modalities: Financial Language Foundation Models (FinLFMs), Financial Time-Series Foundation Models (FinTSFMs), and Financial Visual-Language Foundation Models (FinVLFMs). We review their architectures, training methodologies, datasets, and real-world applications. Furthermore, we identify critical challenges in data availability, algorithmic scalability, and infrastructure constraints, and offer insights into future research opportunities. We hope this survey serves as both a comprehensive reference for understanding FFMs and a practical roadmap for future innovation. An updated collection of FFM-related publications and resources will be maintained on our website https://github.com/FinFM/Awesome-FinFMs.",
    "authors": [
      "Liyuan Chen",
      "Shuoling Liu",
      "Jiangpeng Yan",
      "Xiaoyu Wang",
      "Henglin Liu",
      "Chuang Li",
      "Kecheng Jiao",
      "Jixuan Ying",
      "Yang Veronica Liu",
      "Qiang Yang",
      "Xiu Li"
    ],
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-07-07T16:06:38Z",
    "pdf_url": "https://arxiv.org/pdf/2507.18577v1"
  },
  {
    "arxiv_id": "2507.05319v1",
    "entry_id": "http://arxiv.org/abs/2507.05319v1",
    "title": "LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review",
    "summary": "Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository https://github.com/ycycyc02/LCDS.",
    "authors": [
      "Cheng Yuan",
      "Xinkai Rui",
      "Yongqi Fan",
      "Yawei Fan",
      "Boyang Zhong",
      "Jiacheng Wang",
      "Weiyan Zhang",
      "Tong Ruan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-07T15:25:52Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05319v1"
  },
  {
    "arxiv_id": "2507.05305v1",
    "entry_id": "http://arxiv.org/abs/2507.05305v1",
    "title": "Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools",
    "summary": "Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher cryptic compiler errors for novice programmers, but their computational scale, cost, and tendency to over-assist make them problematic for widespread pedagogical adoption. This work demonstrates that smaller, specialised language models, enhanced via Supervised Fine-Tuning (SFT), present a more viable alternative for educational tools. We utilise a new dataset of 40,000 C compiler error explanations, derived from real introductory programming (CS1/2) student-generated programming errors, which we used to fine-tune three open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual evaluation, combining expert human reviews with a large-scale automated analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our results show that SFT significantly boosts the pedagogical quality of smaller models, achieving performance comparable to much larger models. We analyse the trade-offs between model size and quality, confirming that fine-tuning compact, efficient models on high-quality, domain-specific data is a potent strategy for creating specialised models to drive educational tools. We provide a replicable methodology to foster broader access to generative AI capabilities in educational contexts.",
    "authors": [
      "Lorenzo Lee Solano",
      "Charles Koutcheme",
      "Juho Leinonen",
      "Alexandra Vassar",
      "Jake Renzella"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "published": "2025-07-07T08:03:49Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05305v1"
  },
  {
    "arxiv_id": "2507.04469v2",
    "entry_id": "http://arxiv.org/abs/2507.04469v2",
    "title": "The role of large language models in UI/UX design: A systematic literature review",
    "summary": "This systematic literature review examines the role of large language models (LLMs) in UI/UX design, synthesizing findings from 38 peer-reviewed studies published between 2022 and 2025. We identify key LLMs in use, including GPT-4, Gemini, and PaLM, and map their integration across the design lifecycle, from ideation to evaluation. Common practices include prompt engineering, human-in-the-loop workflows, and multimodal input. While LLMs are reshaping design processes, challenges such as hallucination, prompt instability, and limited explainability persist. Our findings highlight LLMs as emerging collaborators in design, and we propose directions for the ethical, inclusive, and effective integration of these technologies.",
    "authors": [
      "Ammar Ahmed",
      "Ali Shariq Imran"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-07-06T17:18:05Z",
    "pdf_url": "https://arxiv.org/pdf/2507.04469v2"
  },
  {
    "arxiv_id": "2507.05296v1",
    "entry_id": "http://arxiv.org/abs/2507.05296v1",
    "title": "Integrating Generative AI in BIM Education: Insights from Classroom Implementation",
    "summary": "This study evaluates the implementation of a Generative AI-powered rule checking workflow within a graduate-level Building Information Modeling (BIM) course at a U.S. university. Over two semesters, 55 students participated in a classroom-based pilot exploring the use of GenAI for BIM compliance tasks, an area with limited prior research. The instructional design included lectures on prompt engineering and AI-driven rule checking, followed by an assignment where students used a large language model (LLM) to identify code violations in designs using Autodesk Revit. Surveys and interviews were conducted to assess student workload, learning effectiveness, and overall experience, using the NASA-TLX scale and regression analysis. Findings indicate students generally achieved learning objectives but faced challenges such as difficulties debugging AI-generated code and inconsistent tool performance, probably due to their limited prompt engineering experience. These issues increased cognitive and emotional strain, especially among students with minimal programming backgrounds. Despite these challenges, students expressed strong interest in future GenAI applications, particularly with clear instructional support.",
    "authors": [
      "Islem Sahraoui",
      "Kinam Kim",
      "Lu Gao",
      "Zia Din",
      "Ahmed Senouci"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-07-06T03:41:04Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05296v1"
  },
  {
    "arxiv_id": "2507.04136v1",
    "entry_id": "http://arxiv.org/abs/2507.04136v1",
    "title": "A Technical Survey of Reinforcement Learning Techniques for Large Language Models",
    "summary": "Reinforcement Learning (RL) has emerged as a transformative approach for aligning and enhancing Large Language Models (LLMs), addressing critical challenges in instruction following, ethical alignment, and reasoning capabilities. This survey offers a comprehensive foundation on the integration of RL with language models, highlighting prominent algorithms such as Proximal Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally, it provides an extensive technical overview of RL techniques specifically tailored for LLMs, including foundational methods like Reinforcement Learning from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced strategies such as Direct Preference Optimization (DPO) and Group Relative Policy Optimization (GRPO). We systematically analyze their applications across domains, i.e., from code generation to tool-augmented reasoning. We also present a comparative taxonomy based on reward modeling, feedback mechanisms, and optimization strategies. Our evaluation highlights key trends. RLHF remains dominant for alignment, and outcome-based RL such as RLVR significantly improves stepwise reasoning. However, persistent challenges such as reward hacking, computational costs, and scalable feedback collection underscore the need for continued innovation. We further discuss emerging directions, including hybrid RL algorithms, verifier-guided training, and multi-objective alignment frameworks. This survey serves as a roadmap for researchers advancing RL-driven LLM development, balancing capability enhancement with safety and scalability.",
    "authors": [
      "Saksham Sahai Srivastava",
      "Vaneet Aggarwal"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-05T19:13:00Z",
    "pdf_url": "https://arxiv.org/pdf/2507.04136v1"
  },
  {
    "arxiv_id": "2507.04067v1",
    "entry_id": "http://arxiv.org/abs/2507.04067v1",
    "title": "HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration",
    "summary": "Contemporary multi-agent systems encounter persistent challenges in cross-platform interoperability, dynamic task scheduling, and efficient resource sharing. Agents with heterogeneous implementations often lack standardized interfaces; collaboration frameworks remain brittle and hard to extend; scheduling policies are static; and inter-agent state synchronization is insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular framework comprising five layers-User, Workflow, Operator, Agent, and Resource-and supported by sixteen standardized interfaces. HAWK delivers an end-to-end pipeline covering task parsing, workflow orchestration, intelligent scheduling, resource invocation, and data synchronization. At its core lies an adaptive scheduling and optimization module in the Workflow Layer, which harnesses real-time feedback and dynamic strategy adjustment to maximize utilization. The Resource Layer provides a unified abstraction over heterogeneous data sources, large models, physical devices, and third-party services&tools, simplifying cross-domain information retrieval. We demonstrate HAWK's scalability and effectiveness via CreAgentive, a multi-agent novel-generation prototype, which achieves marked gains in throughput, lowers invocation complexity, and improves system controllability. We also show how hybrid deployments of large language models integrate seamlessly within HAWK, highlighting its flexibility. Finally, we outline future research avenues-hallucination mitigation, real-time performance tuning, and enhanced cross-domain adaptability-and survey prospective applications in healthcare, government, finance, and education.",
    "authors": [
      "Yuyang Cheng",
      "Yumiao Xu",
      "Chaojia Yu",
      "Yong Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-07-05T15:03:53Z",
    "pdf_url": "https://arxiv.org/pdf/2507.04067v1"
  },
  {
    "arxiv_id": "2507.04009v1",
    "entry_id": "http://arxiv.org/abs/2507.04009v1",
    "title": "Easy Dataset: A Unified and Extensible Framework for Synthesizing LLM Fine-Tuning Data from Unstructured Documents",
    "summary": "Large language models (LLMs) have shown impressive performance on general-purpose tasks, yet adapting them to specific domains remains challenging due to the scarcity of high-quality domain data. Existing data synthesis tools often struggle to extract reliable fine-tuning data from heterogeneous documents effectively. To address this limitation, we propose Easy Dataset, a unified framework for synthesizing fine-tuning data from unstructured documents via an intuitive graphical user interface (GUI). Specifically, Easy Dataset allows users to easily configure text extraction models and chunking strategies to transform raw documents into coherent text chunks. It then leverages a persona-driven prompting approach to generate diverse question-answer pairs using public-available LLMs. Throughout the pipeline, a human-in-the-loop visual interface facilitates the review and refinement of intermediate outputs to ensure data quality. Experiments on a financial question-answering task show that fine-tuning LLMs on the synthesized dataset significantly improves domain-specific performance while preserving general knowledge. The source code and installable package are available at https://github.com/ConardLi/easy-dataset and have garnered over 9,000 GitHub stars.",
    "authors": [
      "Ziyang Miao",
      "Qiyu Sun",
      "Jingyuan Wang",
      "Yuchen Gong",
      "Yaowei Zheng",
      "Shiqi Li",
      "Richong Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-07-05T11:38:59Z",
    "pdf_url": "https://arxiv.org/pdf/2507.04009v1"
  },
  {
    "arxiv_id": "2507.05288v1",
    "entry_id": "http://arxiv.org/abs/2507.05288v1",
    "title": "A Survey on Proactive Defense Strategies Against Misinformation in Large Language Models",
    "summary": "The widespread deployment of large language models (LLMs) across critical domains has amplified the societal risks posed by algorithmically generated misinformation. Unlike traditional false content, LLM-generated misinformation can be self-reinforcing, highly plausible, and capable of rapid propagation across multiple languages, which traditional detection methods fail to mitigate effectively. This paper introduces a proactive defense paradigm, shifting from passive post hoc detection to anticipatory mitigation strategies. We propose a Three Pillars framework: (1) Knowledge Credibility, fortifying the integrity of training and deployed data; (2) Inference Reliability, embedding self-corrective mechanisms during reasoning; and (3) Input Robustness, enhancing the resilience of model interfaces against adversarial attacks. Through a comprehensive survey of existing techniques and a comparative meta-analysis, we demonstrate that proactive defense strategies offer up to 63\\% improvement over conventional methods in misinformation prevention, despite non-trivial computational overhead and generalization challenges. We argue that future research should focus on co-designing robust knowledge foundations, reasoning certification, and attack-resistant interfaces to ensure LLMs can effectively counter misinformation across varied domains.",
    "authors": [
      "Shuliang Liu",
      "Hongyi Liu",
      "Aiwei Liu",
      "Bingchen Duan",
      "Qi Zheng",
      "Yibo Yan",
      "He Geng",
      "Peijie Jiang",
      "Jia Liu",
      "Xuming Hu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-07-05T09:52:21Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05288v1"
  },
  {
    "arxiv_id": "2507.03637v1",
    "entry_id": "http://arxiv.org/abs/2507.03637v1",
    "title": "Large Language Models for Combinatorial Optimization: A Systematic Review",
    "summary": "This systematic review explores the application of Large Language Models (LLMs) in Combinatorial Optimization (CO). We report our findings using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. We conduct a literature search via Scopus and Google Scholar, examining over 2,000 publications. We assess publications against four inclusion and four exclusion criteria related to their language, research focus, publication year, and type. Eventually, we select 103 studies. We classify these studies into semantic categories and topics to provide a comprehensive overview of the field, including the tasks performed by LLMs, the architectures of LLMs, the existing datasets specifically designed for evaluating LLMs in CO, and the field of application. Finally, we identify future directions for leveraging LLMs in this field.",
    "authors": [
      "Francesca Da Ros",
      "Michael Soprano",
      "Luca Di Gaspero",
      "Kevin Roitero"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-04T15:08:10Z",
    "pdf_url": "https://arxiv.org/pdf/2507.03637v1"
  },
  {
    "arxiv_id": "2507.03599v1",
    "entry_id": "http://arxiv.org/abs/2507.03599v1",
    "title": "MusGO: A Community-Driven Framework For Assessing Openness in Music-Generative AI",
    "summary": "Since 2023, generative AI has rapidly advanced in the music domain. Despite significant technological advancements, music-generative models raise critical ethical challenges, including a lack of transparency and accountability, along with risks such as the replication of artists' works, which highlights the importance of fostering openness. With upcoming regulations such as the EU AI Act encouraging open models, many generative models are being released labelled as 'open'. However, the definition of an open model remains widely debated. In this article, we adapt a recently proposed evidence-based framework for assessing openness in LLMs to the music domain. Using feedback from a survey of 110 participants from the Music Information Retrieval (MIR) community, we refine the framework into MusGO (Music-Generative Open AI), which comprises 13 openness categories: 8 essential and 5 desirable. We evaluate 16 state-of-the-art generative models and provide an openness leaderboard that is fully open to public scrutiny and community contributions. Through this work, we aim to clarify the concept of openness in music-generative AI and promote its transparent and responsible development.",
    "authors": [
      "Roser Batlle-Roca",
      "Laura Ibáñez-Martínez",
      "Xavier Serra",
      "Emilia Gómez",
      "Martín Rocamora"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CY",
      "eess.AS"
    ],
    "published": "2025-07-04T14:12:19Z",
    "pdf_url": "https://arxiv.org/pdf/2507.03599v1"
  },
  {
    "arxiv_id": "2507.05280v1",
    "entry_id": "http://arxiv.org/abs/2507.05280v1",
    "title": "Hungary and AI: efforts and opportunities in comparison with Singapore",
    "summary": "The study assesses Hungary's National AI Strategy and its implementation through the analysis of strategic documents, publicly available financial records, and expert interviews with the Hungarian AI Coalition President and Chief Strategic Advisor to the Government Commissioner for AI. 22 goals from Hungary's strategy were evaluated through conceptual, governance, temporal, and financial dimensions before being benchmarked against Singapore's National AI Strategies (NAIS 1.0 and NAIS 2.0). Key findings include an estimated total of EUR 4.65 billion in AI-related public investment in Hungary. Openly available financial data was found for only half of the evaluated goals, and just three projects made up 98\\% of all documented funding. The research also reveals Hungary's implementation challenges, including fragmented execution following ministerial reorganizations and the absence of designated biennial reviews since 2020. Furthermore, the paper provides targeted recommendations for Hungary's forthcoming AI strategy, drawing on Singapore's framework as a reference point. These include adapting to the era of large language models, restructuring the existing triple helix network to foster more effective dialogue and advocacy, and positioning the country as an East-West bridge for automotive AI experimentation.",
    "authors": [
      "András Ferenczy"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-07-04T09:12:47Z",
    "pdf_url": "https://arxiv.org/pdf/2507.05280v1"
  },
  {
    "arxiv_id": "2507.03194v1",
    "entry_id": "http://arxiv.org/abs/2507.03194v1",
    "title": "How Much Content Do LLMs Generate That Induces Cognitive Bias in Users?",
    "summary": "Large language models (LLMs) are increasingly integrated into applications ranging from review summarization to medical diagnosis support, where they affect human decisions. Even though LLMs perform well in many tasks, they may also inherit societal or cognitive biases, which can inadvertently transfer to humans. We investigate when and how LLMs expose users to biased content and quantify its severity. Specifically, we assess three LLM families in summarization and news fact-checking tasks, evaluating how much LLMs stay consistent with their context and/or hallucinate. Our findings show that LLMs expose users to content that changes the sentiment of the context in 21.86% of the cases, hallucinates on post-knowledge-cutoff data questions in 57.33% of the cases, and primacy bias in 5.94% of the cases. We evaluate 18 distinct mitigation methods across three LLM families and find that targeted interventions can be effective. Given the prevalent use of LLMs in high-stakes domains, such as healthcare or legal analysis, our results highlight the need for robust technical safeguards and for developing user-centered interventions that address LLM limitations.",
    "authors": [
      "Abeer Alessa",
      "Akshaya Lakshminarasimhan",
      "Param Somane",
      "Julian Skirzynski",
      "Julian McAuley",
      "Jessica Echterhoff"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-03T21:56:44Z",
    "pdf_url": "https://arxiv.org/pdf/2507.03194v1"
  },
  {
    "arxiv_id": "2507.03156v1",
    "entry_id": "http://arxiv.org/abs/2507.03156v1",
    "title": "The Impact of LLM-Assistants on Software Developer Productivity: A Systematic Literature Review",
    "summary": "Large language model assistants (LLM-assistants) present new opportunities to transform software development. Developers are increasingly adopting these tools across tasks, including coding, testing, debugging, documentation, and design. Yet, despite growing interest, there is no synthesis of how LLM-assistants affect software developer productivity. In this paper, we present a systematic literature review of 37 peer-reviewed studies published between January 2014 and December 2024 that examine this impact. Our analysis reveals that LLM-assistants offer both considerable benefits and critical risks. Commonly reported gains include minimized code search, accelerated development, and the automation of trivial and repetitive tasks. However, studies also highlight concerns around cognitive offloading, reduced team collaboration, and inconsistent effects on code quality. While the majority of studies (92%) adopt a multi-dimensional perspective by examining at least two SPACE dimensions, reflecting increased awareness of the complexity of developer productivity, only 14% extend beyond three dimensions, indicating substantial room for more integrated evaluations. Satisfaction, Performance, and Efficiency are the most frequently investigated dimensions, whereas Communication and Activity remain underexplored. Most studies are exploratory (64%) and methodologically diverse, but lack longitudinal and team-based evaluations. This review surfaces key research gaps and provides recommendations for future research and practice. All artifacts associated with this study are publicly available at https://zenodo.org/records/15788502.",
    "authors": [
      "Amr Mohamed",
      "Maram Assi",
      "Mariam Guizani"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-07-03T20:25:49Z",
    "pdf_url": "https://arxiv.org/pdf/2507.03156v1"
  },
  {
    "arxiv_id": "2507.02825v5",
    "entry_id": "http://arxiv.org/abs/2507.02825v5",
    "title": "Establishing Best Practices for Building Rigorous Agentic Benchmarks",
    "summary": "Benchmarks are essential for quantitatively tracking progress in AI. As AI agents become increasingly capable, researchers and practitioners have introduced agentic benchmarks to evaluate agents on complex, real-world tasks. These benchmarks typically measure agent capabilities by evaluating task outcomes via specific reward designs. However, we show that many agentic benchmarks have issues in task setup or reward design. For example, SWE-bench Verified uses insufficient test cases, while TAU-bench counts empty responses as successful. Such issues can lead to under- or overestimation of agents' performance by up to 100% in relative terms. To make agentic evaluation rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of guidelines that we synthesized from our benchmark-building experience, a survey of best practices, and previously reported issues. When applied to CVE-Bench, a benchmark with a particularly complex evaluation design, ABC reduces the performance overestimation by 33%.",
    "authors": [
      "Yuxuan Zhu",
      "Tengjun Jin",
      "Yada Pruksachatkun",
      "Andy Zhang",
      "Shu Liu",
      "Sasha Cui",
      "Sayash Kapoor",
      "Shayne Longpre",
      "Kevin Meng",
      "Rebecca Weiss",
      "Fazl Barez",
      "Rahul Gupta",
      "Jwala Dhamala",
      "Jacob Merizian",
      "Mario Giulianelli",
      "Harry Coppock",
      "Cozmin Ududec",
      "Jasjeet Sekhon",
      "Jacob Steinhardt",
      "Antony Kellermann",
      "Sarah Schwettmann",
      "Matei Zaharia",
      "Ion Stoica",
      "Percy Liang",
      "Daniel Kang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-03T17:35:31Z",
    "pdf_url": "https://arxiv.org/pdf/2507.02825v5"
  },
  {
    "arxiv_id": "2507.02593v1",
    "entry_id": "http://arxiv.org/abs/2507.02593v1",
    "title": "Revisiting Active Learning under (Human) Label Variation",
    "summary": "Access to high-quality labeled data remains a limiting factor in applied supervised learning. While label variation (LV), i.e., differing labels for the same instance, is common, especially in natural language processing, annotation frameworks often still rest on the assumption of a single ground truth. This overlooks human label variation (HLV), the occurrence of plausible differences in annotations, as an informative signal. Similarly, active learning (AL), a popular approach to optimizing the use of limited annotation budgets in training ML models, often relies on at least one of several simplifying assumptions, which rarely hold in practice when acknowledging HLV. In this paper, we examine foundational assumptions about truth and label nature, highlighting the need to decompose observed LV into signal (e.g., HLV) and noise (e.g., annotation error). We survey how the AL and (H)LV communities have addressed -- or neglected -- these distinctions and propose a conceptual framework for incorporating HLV throughout the AL loop, including instance selection, annotator choice, and label representation. We further discuss the integration of large language models (LLM) as annotators. Our work aims to lay a conceptual foundation for HLV-aware active learning, better reflecting the complexities of real-world annotation.",
    "authors": [
      "Cornelia Gruber",
      "Helen Alber",
      "Bernd Bischl",
      "Göran Kauermann",
      "Barbara Plank",
      "Matthias Aßenmacher"
    ],
    "categories": [
      "cs.CL",
      "cs.HC",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-07-03T12:59:28Z",
    "pdf_url": "https://arxiv.org/pdf/2507.02593v1"
  },
  {
    "arxiv_id": "2507.02282v1",
    "entry_id": "http://arxiv.org/abs/2507.02282v1",
    "title": "Content filtering methods for music recommendation: A review",
    "summary": "Recommendation systems have become essential in modern music streaming platforms, shaping how users discover and engage with songs. One common approach in recommendation systems is collaborative filtering, which suggests content based on the preferences of users with similar listening patterns to the target user. However, this method is less effective on media where interactions are sparse. Music is one such medium, since the average user of a music streaming service will never listen to the vast majority of tracks. Due to this sparsity, there are several challenges that have to be addressed with other methods. This review examines the current state of research in addressing these challenges, with an emphasis on the role of content filtering in mitigating biases inherent in collaborative filtering approaches. We explore various methods of song classification for content filtering, including lyrical analysis using Large Language Models (LLMs) and audio signal processing techniques. Additionally, we discuss the potential conflicts between these different analysis methods and propose avenues for resolving such discrepancies.",
    "authors": [
      "Terence Zeng",
      "Abhishek K. Umrawal"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-07-03T03:44:20Z",
    "pdf_url": "https://arxiv.org/pdf/2507.02282v1"
  },
  {
    "arxiv_id": "2507.02076v1",
    "entry_id": "http://arxiv.org/abs/2507.02076v1",
    "title": "Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs",
    "summary": "Large language models (LLMs) have rapidly progressed into general-purpose agents capable of solving a broad spectrum of tasks. However, current models remain inefficient at reasoning: they apply fixed inference-time compute regardless of task complexity, often overthinking simple problems while underthinking hard ones. This survey presents a comprehensive review of efficient test-time compute (TTC) strategies, which aim to improve the computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy that distinguishes between L1-controllability, methods that operate under fixed compute budgets, and L2-adaptiveness, methods that dynamically scale inference based on input difficulty or model confidence. We benchmark leading proprietary LLMs across diverse datasets, highlighting critical trade-offs between reasoning performance and token usage. Compared to prior surveys on efficient reasoning, our review emphasizes the practical control, adaptability, and scalability of TTC methods. Finally, we discuss emerging trends such as hybrid thinking models and identify key challenges for future work towards making LLMs more computationally efficient, robust, and responsive to user constraints.",
    "authors": [
      "Mohammad Ali Alomrani",
      "Yingxue Zhang",
      "Derek Li",
      "Qianyi Sun",
      "Soumyasundar Pal",
      "Zhanguang Zhang",
      "Yaochen Hu",
      "Rohan Deepak Ajwani",
      "Antonios Valkanas",
      "Raika Karimi",
      "Peng Cheng",
      "Yunzhou Wang",
      "Pengyi Liao",
      "Hanrui Huang",
      "Bin Wang",
      "Jianye Hao",
      "Mark Coates"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-07-02T18:27:42Z",
    "pdf_url": "https://arxiv.org/pdf/2507.02076v1"
  },
  {
    "arxiv_id": "2507.02074v2",
    "entry_id": "http://arxiv.org/abs/2507.02074v2",
    "title": "Large Language Models for Crash Detection in Video: A Survey of Methods, Datasets, and Challenges",
    "summary": "Crash detection from video feeds is a critical problem in intelligent transportation systems. Recent developments in large language models (LLMs) and vision-language models (VLMs) have transformed how we process, reason about, and summarize multimodal information. This paper surveys recent methods leveraging LLMs for crash detection from video data. We present a structured taxonomy of fusion strategies, summarize key datasets, analyze model architectures, compare performance benchmarks, and discuss ongoing challenges and opportunities. Our review provides a foundation for future research in this fast-growing intersection of video understanding and foundation models.",
    "authors": [
      "Sanjeda Akter",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-07-02T18:21:01Z",
    "pdf_url": "https://arxiv.org/pdf/2507.02074v2"
  },
  {
    "arxiv_id": "2507.01939v3",
    "entry_id": "http://arxiv.org/abs/2507.01939v3",
    "title": "SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars",
    "summary": "In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy.",
    "authors": [
      "Xiaosheng Zhao",
      "Yang Huang",
      "Guirong Xue",
      "Xiao Kong",
      "Jifeng Liu",
      "Xiaoyu Tang",
      "Timothy C. Beers",
      "Yuan-Sen Ting",
      "A-Li Luo"
    ],
    "categories": [
      "astro-ph.IM",
      "astro-ph.SR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-07-02T17:49:52Z",
    "pdf_url": "https://arxiv.org/pdf/2507.01939v3"
  },
  {
    "arxiv_id": "2507.01903v2",
    "entry_id": "http://arxiv.org/abs/2507.01903v2",
    "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
    "summary": "Recent advancements in artificial intelligence (AI), particularly in large language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated remarkable capabilities in complex domains such as logical reasoning and experimental coding. Motivated by these advancements, numerous studies have explored the application of AI in the innovation process, particularly in the context of scientific research. These AI technologies primarily aim to develop systems that can autonomously conduct research processes across a wide range of scientific disciplines. Despite these significant strides, a comprehensive survey on AI for Research (AI4Research) remains absent, which hampers our understanding and impedes further development in this field. To address this gap, we present a comprehensive survey and offer a unified perspective on AI4Research. Specifically, the main contributions of our work are as follows: (1) Systematic taxonomy: We first introduce a systematic taxonomy to classify five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key research gaps and highlight promising future directions, focusing on the rigor and scalability of automated experiments, as well as the societal impact. (3) Abundant applications and resources: Finally, we compile a wealth of resources, including relevant multidisciplinary applications, data corpora, and tools. We hope our work will provide the research community with quick access to these resources and stimulate innovative breakthroughs in AI4Research.",
    "authors": [
      "Qiguang Chen",
      "Mingda Yang",
      "Libo Qin",
      "Jinhao Liu",
      "Zheng Yan",
      "Jiannan Guan",
      "Dengyun Peng",
      "Yiyan Ji",
      "Hanjing Li",
      "Mengkang Hu",
      "Yimeng Zhang",
      "Yihao Liang",
      "Yuhang Zhou",
      "Jiaqi Wang",
      "Zhi Chen",
      "Wanxiang Che"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-07-02T17:19:20Z",
    "pdf_url": "https://arxiv.org/pdf/2507.01903v2"
  },
  {
    "arxiv_id": "2507.01376v1",
    "entry_id": "http://arxiv.org/abs/2507.01376v1",
    "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing",
    "summary": "AI agents are autonomous systems designed to perceive, reason, and act within dynamic environments. With the rapid advancements in generative AI (GenAI), large language models (LLMs) and multimodal large language models (MLLMs) have significantly improved AI agents' capabilities in semantic comprehension, complex reasoning, and autonomous decision-making. At the same time, the rise of Agentic AI highlights adaptability and goal-directed autonomy in dynamic and complex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents (MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in information processing, environmental perception, and autonomous decision-making, opening new avenues for smart manufacturing. However, the definitions, capability boundaries, and practical applications of these emerging AI paradigms in smart manufacturing remain unclear. To address this gap, this study systematically reviews the evolution of AI and AI agent technologies, examines the core concepts and technological advancements of LLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential applications in and integration into manufacturing, along with the potential challenges they may face.",
    "authors": [
      "Yinwang Ren",
      "Yangyang Liu",
      "Tang Ji",
      "Xun Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-07-02T05:31:17Z",
    "pdf_url": "https://arxiv.org/pdf/2507.01376v1"
  },
  {
    "arxiv_id": "2507.01282v1",
    "entry_id": "http://arxiv.org/abs/2507.01282v1",
    "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care",
    "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that artificial intelligence (AI) systems could aid medical diagnosis. Yet despite dazzling benchmark scores, LLM assistants have yet to deliver measurable improvements at the bedside. This scoping review aims to highlight the areas where AI is limited to make practical contributions in the clinical setting, specifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom provide actionable, interpretable guidance, eroding clinician trust. Adjacent use of LLMs by physicians did not result in better diagnostic accuracy or speed. Key limitations trace to the data-driven paradigm: black-box outputs which lack transparency, vulnerability to hallucinations, and weak causal reasoning. Hybrid approaches that combine statistical learning with expert rule-based knowledge, and involve clinicians throughout the process help bring back interpretability. They also fit better with existing clinical workflows, as seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking predictions to clinically meaningful causes. This can be done through neuro-symbolic or hybrid AI that combines the language ability of LLMs with human causal expertise. AI researchers have addressed this direction, with explainable AI and neuro-symbolic AI being the next logical steps in further advancement in AI. However, they are still based on data-driven knowledge integration instead of human-in-the-loop approaches. Future research should measure success not only by accuracy but by improvements in clinician understanding, workflow fit, and patient outcomes. A better understanding of what helps improve human-computer interactions is greatly needed for AI systems to become part of clinical practice.",
    "authors": [
      "Matthew JY Kang",
      "Wenli Yang",
      "Monica R Roberts",
      "Byeong Ho Kang",
      "Charles B Malpas"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-07-02T01:43:06Z",
    "pdf_url": "https://arxiv.org/pdf/2507.01282v1"
  },
  {
    "arxiv_id": "2507.00914v1",
    "entry_id": "http://arxiv.org/abs/2507.00914v1",
    "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications",
    "summary": "The long-standing vision of intelligent cities is to create efficient, livable, and sustainable urban environments using big data and artificial intelligence technologies. Recently, the advent of Large Language Models (LLMs) has opened new ways toward realizing this vision. With powerful semantic understanding and reasoning capabilities, LLMs can be deployed as intelligent agents capable of autonomously solving complex problems across domains. In this article, we focus on Urban LLM Agents, which are LLM-powered agents that are semi-embodied within the hybrid cyber-physical-social space of cities and used for system-level urban decision-making. First, we introduce the concept of urban LLM agents, discussing their unique capabilities and features. Second, we survey the current research landscape from the perspective of agent workflows, encompassing urban sensing, memory management, reasoning, execution, and learning. Third, we categorize the application domains of urban LLM agents into five groups: urban planning, transportation, environment, public safety, and urban society, presenting representative works in each group. Finally, we discuss trustworthiness and evaluation issues that are critical for real-world deployment, and identify several open problems for future research. This survey aims to establish a foundation for the emerging field of urban LLM agents and to provide a roadmap for advancing the intersection of LLMs and urban intelligence. A curated list of relevant papers and open-source resources is maintained and continuously updated at https://github.com/usail-hkust/Awesome-Urban-LLM-Agents.",
    "authors": [
      "Jindong Han",
      "Yansong Ning",
      "Zirui Yuan",
      "Hang Ni",
      "Fan Liu",
      "Tengfei Lyu",
      "Hao Liu"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-07-01T16:18:29Z",
    "pdf_url": "https://arxiv.org/pdf/2507.00914v1"
  },
  {
    "arxiv_id": "2507.00108v1",
    "entry_id": "http://arxiv.org/abs/2507.00108v1",
    "title": "Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives",
    "summary": "Computer programming is undergoing a true transformation driven by powerful new tools for automatic source code generation based on large language models. This transformation is also manifesting in introductory programming courses at universities around the world, generating an in-depth debate about how programming content should be taught, learned, and assessed in the context of generative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on this issue, highlighting the advantages and disadvantages identified in the specialized literature. On the other hand, it proposes enriching teaching and learning methodologies by focusing on code comprehension and execution rather than on mere coding or program functionality. In particular, it advocates for the use of visual representations of code and visual simulations of its execution as effective tools for teaching, learning, and assessing programming, thus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming course are presented to provide preliminary context supporting the incorporation of visual simulations in Java (or other languages) as part of the training process.",
    "authors": [
      "Clemente Rubio-Manzano",
      "Jazna Meza",
      "Rodolfo Fernandez-Santibanez",
      "Christian Vidal-Castro"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.PL"
    ],
    "published": "2025-06-30T17:38:27Z",
    "pdf_url": "https://arxiv.org/pdf/2507.00108v1"
  },
  {
    "arxiv_id": "2506.24044v1",
    "entry_id": "http://arxiv.org/abs/2506.24044v1",
    "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
    "summary": "The rapid progress of multimodal large language models (MLLM) has paved the way for Vision-Language-Action (VLA) paradigms, which integrate visual perception, natural language understanding, and control within a single policy. Researchers in autonomous driving are actively adapting these methods to the vehicle domain. Such models promise autonomous vehicles that can interpret high-level instructions, reason about complex traffic scenes, and make their own decisions. However, the literature remains fragmented and is rapidly expanding. This survey offers the first comprehensive overview of VLA for Autonomous Driving (VLA4AD). We (i) formalize the architectural building blocks shared across recent work, (ii) trace the evolution from early explainer to reasoning-centric VLA models, and (iii) compare over 20 representative models according to VLA's progress in the autonomous driving domain. We also consolidate existing datasets and benchmarks, highlighting protocols that jointly measure driving safety, accuracy, and explanation quality. Finally, we detail open challenges - robustness, real-time efficiency, and formal verification - and outline future directions of VLA4AD. This survey provides a concise yet complete reference for advancing interpretable socially aligned autonomous vehicles. Github repo is available at \\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.",
    "authors": [
      "Sicong Jiang",
      "Zilin Huang",
      "Kangan Qian",
      "Ziang Luo",
      "Tianze Zhu",
      "Yang Zhong",
      "Yihong Tang",
      "Menglin Kong",
      "Yunlong Wang",
      "Siwen Jiao",
      "Hao Ye",
      "Zihao Sheng",
      "Xin Zhao",
      "Tuopu Wen",
      "Zheng Fu",
      "Sikai Chen",
      "Kun Jiang",
      "Diange Yang",
      "Seongjin Choi",
      "Lijun Sun"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-06-30T16:50:02Z",
    "pdf_url": "https://arxiv.org/pdf/2506.24044v1"
  },
  {
    "arxiv_id": "2506.23844v1",
    "entry_id": "http://arxiv.org/abs/2506.23844v1",
    "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents",
    "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of autonomous AI agents capable of perceiving, reasoning, and acting in dynamic, open-ended environments. These large-model agents mark a paradigm shift from static inference systems to interactive, memory-augmented entities. While these capabilities significantly expand the functional scope of AI, they also introduce qualitatively novel security risks - such as memory poisoning, tool misuse, reward hacking, and emergent misalignment - that extend beyond the threat models of conventional systems or standalone LLMs. In this survey, we first examine the structural foundations and key capabilities that underpin increasing levels of agent autonomy, including long-term memory retention, modular tool use, recursive planning, and reflective reasoning. We then analyze the corresponding security vulnerabilities across the agent stack, identifying failure modes such as deferred decision hazards, irreversible tool chains, and deceptive behaviors arising from internal state drift or value misalignment. These risks are traced to architectural fragilities that emerge across perception, cognition, memory, and action modules. To address these challenges, we systematically review recent defense strategies deployed at different autonomy layers, including input sanitization, memory lifecycle control, constrained decision-making, structured tool invocation, and introspective reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a unified cognitive framework grounded in Constrained Markov Decision Processes (CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation, and joint reward-risk optimization to enable principled, proactive safety across the agent's decision-making loop.",
    "authors": [
      "Hang Su",
      "Jun Luo",
      "Chang Liu",
      "Xiao Yang",
      "Yichi Zhang",
      "Yinpeng Dong",
      "Jun Zhu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-30T13:34:34Z",
    "pdf_url": "https://arxiv.org/pdf/2506.23844v1"
  },
  {
    "arxiv_id": "2506.23678v1",
    "entry_id": "http://arxiv.org/abs/2506.23678v1",
    "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models",
    "summary": "The output quality of large language models (LLMs) can be improved via \"reasoning\": generating segments of chain-of-thought (CoT) content to further condition the model prior to producing user-facing output. While these chains contain valuable information, they are verbose and lack explicit organization, making them tedious to review. Moreover, they lack opportunities for user feedback, such as to remove unwanted considerations, add desired ones, or clarify unclear assumptions. We introduce Interactive Reasoning, an interaction design that visualizes chain-of-thought outputs as a hierarchy of topics and enables user review and modification. We implement interactive reasoning in Hippo, a prototype for AI-assisted decision making in the face of uncertain trade-offs. In a user study with 16 participants, we find that interactive reasoning in Hippo allows users to quickly identify and interrupt erroneous generations, efficiently steer the model towards customized responses, and better understand both model reasoning and model outputs. Our work contributes to a new paradigm that incorporates user oversight into LLM reasoning processes.",
    "authors": [
      "Rock Yuren Pang",
      "K. J. Kevin Feng",
      "Shangbin Feng",
      "Chu Li",
      "Weijia Shi",
      "Yulia Tsvetkov",
      "Jeffrey Heer",
      "Katharina Reinecke"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-06-30T10:00:43Z",
    "pdf_url": "https://arxiv.org/pdf/2506.23678v1"
  },
  {
    "arxiv_id": "2506.23520v2",
    "entry_id": "http://arxiv.org/abs/2506.23520v2",
    "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data",
    "summary": "With the increasing interest in robotic synthesis in the context of organic chemistry, the automated extraction of chemical procedures from literature is critical. However, this task remains challenging due to the inherent ambiguity of chemical language and the high cost of human annotation required for developing reliable computer-aided extraction protocols. Here, we present ChemActor, a fully fine-tuned large language model (LLM), as a chemical executor to convert between unstructured experimental procedures and structured action sequences. We propose a sequential LLM-generated data framework to address the challenges of insufficient and low-quality annotated data. This framework integrates a data selection module that selects data based on distribution divergence, with a general-purpose LLM, to generate machine-executable actions from a single molecule input. Additionally, we introduce a novel multi-round LLMs circle review metric, which reflects the model's advanced understanding of chemical experimental procedures. Extensive experiments on reaction-to-description (R2D) and description-to-action (D2A) tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves state-of-the-art performance, outperforming the baseline model by 10%. The code is available at: https://github.com/Zhanghahah/ChemActor.",
    "authors": [
      "Yu Zhang",
      "Ruijie Yu",
      "Jidong Tian",
      "Feng Zhu",
      "Jiapeng Liu",
      "Xiaokang Yang",
      "Yaohui Jin",
      "Yanyan Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-30T05:11:19Z",
    "pdf_url": "https://arxiv.org/pdf/2506.23520v2"
  },
  {
    "arxiv_id": "2506.23411v2",
    "entry_id": "http://arxiv.org/abs/2506.23411v2",
    "title": "Datasets for Fairness in Language Models: An In-Depth Survey",
    "summary": "Despite the growing reliance on fairness benchmarks to evaluate language models, the datasets that underpin these benchmarks remain critically underexamined. This survey addresses that overlooked foundation by offering a comprehensive analysis of the most widely used fairness datasets in language model research. To ground this analysis, we characterize each dataset across key dimensions, including provenance, demographic scope, annotation design, and intended use, revealing the assumptions and limitations baked into current evaluation practices. Building on this foundation, we propose a unified evaluation framework that surfaces consistent patterns of demographic disparities across benchmarks and scoring metrics. Applying this framework to sixteen popular datasets, we uncover overlooked biases that may distort conclusions about model fairness and offer guidance on selecting, combining, and interpreting these resources more effectively and responsibly. Our findings highlight an urgent need for new benchmarks that capture a broader range of social contexts and fairness notions. To support future research, we release all data, code, and results at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets, fostering transparency and reproducibility in the evaluation of language model fairness.",
    "authors": [
      "Jiale Zhang",
      "Zichong Wang",
      "Avash Palikhe",
      "Zhipeng Yin",
      "Wenbin Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-06-29T22:11:58Z",
    "pdf_url": "https://arxiv.org/pdf/2506.23411v2"
  },
  {
    "arxiv_id": "2506.23260v1",
    "entry_id": "http://arxiv.org/abs/2506.23260v1",
    "title": "From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows",
    "summary": "Autonomous AI agents powered by large language models (LLMs) with structured function-calling interfaces have dramatically expanded capabilities for real-time data retrieval, complex computation, and multi-step orchestration. Yet, the explosive proliferation of plugins, connectors, and inter-agent protocols has outpaced discovery mechanisms and security practices, resulting in brittle integrations vulnerable to diverse threats. In this survey, we introduce the first unified, end-to-end threat model for LLM-agent ecosystems, spanning host-to-tool and agent-to-agent communications, formalize adversary capabilities and attacker objectives, and catalog over thirty attack techniques. Specifically, we organized the threat model into four domains: Input Manipulation (e.g., prompt injections, long-context hijacks, multimodal adversarial inputs), Model Compromise (e.g., prompt- and parameter-level backdoors, composite and encrypted multi-backdoors, poisoning strategies), System and Privacy Attacks (e.g., speculative side-channels, membership inference, retrieval poisoning, social-engineering simulations), and Protocol Vulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent (A2A) protocol). For each category, we review representative scenarios, assess real-world feasibility, and evaluate existing defenses. Building on our threat taxonomy, we identify key open challenges and future research directions, such as securing MCP deployments through dynamic trust management and cryptographic provenance tracking; designing and hardening Agentic Web Interfaces; and achieving resilience in multi-agent and federated environments. Our work provides a comprehensive reference to guide the design of robust defense mechanisms and establish best practices for resilient LLM-agent workflows.",
    "authors": [
      "Mohamed Amine Ferrag",
      "Norbert Tihanyi",
      "Djallel Hamouda",
      "Leandros Maglaras",
      "Merouane Debbah"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-06-29T14:32:32Z",
    "pdf_url": "https://arxiv.org/pdf/2506.23260v1"
  },
  {
    "arxiv_id": "2506.23107v1",
    "entry_id": "http://arxiv.org/abs/2506.23107v1",
    "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study",
    "summary": "Large language models (LLMs) have made significant strides, extending their applications to dialogue systems, automated content creation, and domain-specific advisory tasks. However, as their use grows, concerns have emerged regarding their reliability in simulating complex decision-making behavior, such as risky decision-making, where a single choice can lead to multiple outcomes. This study investigates the ability of LLMs to simulate risky decision-making scenarios. We compare model-generated decisions with actual human responses in a series of lottery-based tasks, using transportation stated preference survey data from participants in Sydney, Dhaka, Hong Kong, and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk preferences were analyzed using the Constant Relative Risk Aversion (CRRA) framework. Results show that both models exhibit more risk-averse behavior than human participants, with o1-mini aligning more closely with observed human decisions. Further analysis of multilingual data from Nanjing and Hong Kong indicates that model predictions in Chinese deviate more from actual responses compared to English, suggesting that prompt language may influence simulation performance. These findings highlight both the promise and the current limitations of LLMs in replicating human-like risk behavior, particularly in linguistic and cultural settings.",
    "authors": [
      "Bing Song",
      "Jianing Liu",
      "Sisi Jian",
      "Chenyang Wu",
      "Vinayak Dixit"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-29T06:16:57Z",
    "pdf_url": "https://arxiv.org/pdf/2506.23107v1"
  },
  {
    "arxiv_id": "2507.01990v1",
    "entry_id": "http://arxiv.org/abs/2507.01990v1",
    "title": "Integrating Large Language Models in Financial Investments and Market Analysis: A Survey",
    "summary": "Large Language Models (LLMs) have been employed in financial decision making, enhancing analytical capabilities for investment strategies. Traditional investment strategies often utilize quantitative models, fundamental analysis, and technical indicators. However, LLMs have introduced new capabilities to process and analyze large volumes of structured and unstructured data, extract meaningful insights, and enhance decision-making in real-time. This survey provides a structured overview of recent research on LLMs within the financial domain, categorizing research contributions into four main frameworks: LLM-based Frameworks and Pipelines, Hybrid Integration Methods, Fine-Tuning and Adaptation Approaches, and Agent-Based Architectures. This study provides a structured review of recent LLMs research on applications in stock selection, risk assessment, sentiment analysis, trading, and financial forecasting. By reviewing the existing literature, this study highlights the capabilities, challenges, and potential directions of LLMs in financial markets.",
    "authors": [
      "Sedigheh Mahdavi",
      "Jiating",
      "Chen",
      "Pradeep Kumar Joshi",
      "Lina Huertas Guativa",
      "Upmanyu Singh"
    ],
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-06-29T05:25:31Z",
    "pdf_url": "https://arxiv.org/pdf/2507.01990v1"
  },
  {
    "arxiv_id": "2506.22884v1",
    "entry_id": "http://arxiv.org/abs/2506.22884v1",
    "title": "Performance Measurements in the AI-Centric Computing Continuum Systems",
    "summary": "Over the Eight decades, computing paradigms have shifted from large, centralized systems to compact, distributed architectures, leading to the rise of the Distributed Computing Continuum (DCC). In this model, multiple layers such as cloud, edge, Internet of Things (IoT), and mobile platforms work together to support a wide range of applications. Recently, the emergence of Generative AI and large language models has further intensified the demand for computational resources across this continuum. Although traditional performance metrics have provided a solid foundation, they need to be revisited and expanded to keep pace with changing computational demands and application requirements. Accurate performance measurements benefit both system designers and users by supporting improvements in efficiency and promoting alignment with system goals. In this context, we review commonly used metrics in DCC and IoT environments. We also discuss emerging performance dimensions that address evolving computing needs, such as sustainability, energy efficiency, and system observability. We also outline criteria and considerations for selecting appropriate metrics, aiming to inspire future research and development in this critical area.",
    "authors": [
      "Praveen Kumar Donta",
      "Qiyang Zhang",
      "Schahram Dustdar"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.ET",
      "cs.NI",
      "eess.SY"
    ],
    "published": "2025-06-28T13:46:07Z",
    "pdf_url": "https://arxiv.org/pdf/2506.22884v1"
  },
  {
    "arxiv_id": "2507.22902v1",
    "entry_id": "http://arxiv.org/abs/2507.22902v1",
    "title": "Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting",
    "summary": "Background: Globally we face a projected shortage of 11 million healthcare practitioners by 2030, and administrative burden consumes 50% of clinical time. Artificial intelligence (AI) has the potential to help alleviate these problems. However, no end-to-end autonomous large language model (LLM)-based AI system has been rigorously evaluated in real-world clinical practice. In this study, we evaluated whether a multi-agent LLM-based AI framework can function autonomously as an AI doctor in a virtual urgent care setting. Methods: We retrospectively compared the performance of the multi-agent AI system Doctronic and board-certified clinicians across 500 consecutive urgent-care telehealth encounters. The primary end points: diagnostic concordance, treatment plan consistency, and safety metrics, were assessed by blinded LLM-based adjudication and expert human review. Results: The top diagnosis of Doctronic and clinician matched in 81% of cases, and the treatment plan aligned in 99.2% of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not supported by clinical findings). In an expert review of discordant cases, AI performance was superior in 36.1%, and human performance was superior in 9.3%; the diagnoses were equivalent in the remaining cases. Conclusions: In this first large-scale validation of an autonomous AI doctor, we demonstrated strong diagnostic and treatment plan concordance with human clinicians, with AI performance matching and in some cases exceeding that of practicing clinicians. These findings indicate that multi-agent AI systems achieve comparable clinical decision-making to human providers and offer a potential solution to healthcare workforce shortages.",
    "authors": [
      "Hashim Hayat",
      "Maksim Kudrautsau",
      "Evgeniy Makarov",
      "Vlad Melnichenko",
      "Tim Tsykunou",
      "Piotr Varaksin",
      "Matt Pavelle",
      "Adam Z. Oskowitz"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-06-27T19:04:44Z",
    "pdf_url": "https://arxiv.org/pdf/2507.22902v1"
  },
  {
    "arxiv_id": "2506.22231v1",
    "entry_id": "http://arxiv.org/abs/2506.22231v1",
    "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education",
    "summary": "The rapid proliferation of generative artificial intelligence (AI) tools - especially large language models (LLMs) such as ChatGPT - has ushered in a transformative era in higher education. Universities in developed regions are increasingly integrating these technologies into research, teaching, and assessment. On one hand, LLMs can enhance productivity by streamlining literature reviews, facilitating idea generation, assisting with coding and data analysis, and even supporting grant proposal drafting. On the other hand, their use raises significant concerns regarding academic integrity, ethical boundaries, and equitable access. Recent empirical studies indicate that nearly 47% of students use LLMs in their coursework - with 39% using them for exam questions and 7% for entire assignments - while detection tools currently achieve around 88% accuracy, leaving a 12% error margin. This article critically examines the opportunities offered by generative AI, explores the multifaceted challenges it poses, and outlines robust policy solutions. Emphasis is placed on redesigning assessments to be AI-resilient, enhancing staff and student training, implementing multi-layered enforcement mechanisms, and defining acceptable use. By synthesizing data from recent research and case studies, the article argues that proactive policy adaptation is imperative to harness AI's potential while safeguarding the core values of academic integrity and equity.",
    "authors": [
      "Russell Beale"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-06-27T13:49:02Z",
    "pdf_url": "https://arxiv.org/pdf/2506.22231v1"
  },
  {
    "arxiv_id": "2506.22026v1",
    "entry_id": "http://arxiv.org/abs/2506.22026v1",
    "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
    "summary": "Automated scientific idea generation systems have made remarkable progress, yet the automatic evaluation of idea novelty remains a critical and underexplored challenge. Manual evaluation of novelty through literature review is labor-intensive, prone to error due to subjectivity, and impractical at scale. To address these issues, we propose the Idea Novelty Checker, an LLM-based retrieval-augmented generation (RAG) framework that leverages a two-stage retrieve-then-rerank approach. The Idea Novelty Checker first collects a broad set of relevant papers using keyword and snippet-based retrieval, then refines this collection through embedding-based filtering followed by facet-based LLM re-ranking. It incorporates expert-labeled examples to guide the system in comparing papers for novelty evaluation and in generating literature-grounded reasoning. Our extensive experiments demonstrate that our novelty checker achieves approximately 13% higher agreement than existing approaches. Ablation studies further showcases the importance of the facet-based re-ranker in identifying the most relevant literature for novelty evaluation.",
    "authors": [
      "Simra Shahid",
      "Marissa Radensky",
      "Raymond Fok",
      "Pao Siangliulue",
      "Daniel S. Weld",
      "Tom Hope"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-06-27T08:47:28Z",
    "pdf_url": "https://arxiv.org/pdf/2506.22026v1"
  },
  {
    "arxiv_id": "2506.21899v1",
    "entry_id": "http://arxiv.org/abs/2506.21899v1",
    "title": "Advancements and Challenges in Continual Reinforcement Learning: A Comprehensive Review",
    "summary": "The diversity of tasks and dynamic nature of reinforcement learning (RL) require RL agents to be able to learn sequentially and continuously, a learning paradigm known as continuous reinforcement learning. This survey reviews how continual learning transforms RL agents into dynamic continual learners. This enables RL agents to acquire and retain useful and reusable knowledge seamlessly. The paper delves into fundamental aspects of continual reinforcement learning, exploring key concepts, significant challenges, and novel methodologies. Special emphasis is placed on recent advancements in continual reinforcement learning within robotics, along with a succinct overview of evaluation environments utilized in prominent research, facilitating accessibility for newcomers to the field. The review concludes with a discussion on limitations and promising future directions, providing valuable insights for researchers and practitioners alike.",
    "authors": [
      "Amara Zuffer",
      "Michael Burke",
      "Mehrtash Harandi"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-06-27T04:36:05Z",
    "pdf_url": "https://arxiv.org/pdf/2506.21899v1"
  },
  {
    "arxiv_id": "2506.21872v1",
    "entry_id": "http://arxiv.org/abs/2506.21872v1",
    "title": "A Survey of Continual Reinforcement Learning",
    "summary": "Reinforcement Learning (RL) is an important machine learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in this field due to the rapid development of deep neural networks. However, the success of RL currently relies on extensive training data and computational resources. In addition, RL's limited ability to generalize across tasks restricts its applicability in dynamic and real-world environments. With the arisen of Continual Learning (CL), Continual Reinforcement Learning (CRL) has emerged as a promising research direction to address these limitations by enabling agents to learn continuously, adapt to new tasks, and retain previously acquired knowledge. In this survey, we provide a comprehensive examination of CRL, focusing on its core concepts, challenges, and methodologies. Firstly, we conduct a detailed review of existing works, organizing and analyzing their metrics, tasks, benchmarks, and scenario settings. Secondly, we propose a new taxonomy of CRL methods, categorizing them into four types from the perspective of knowledge storage and/or transfer. Finally, our analysis highlights the unique challenges of CRL and provides practical insights into future directions.",
    "authors": [
      "Chaofan Pan",
      "Xin Yang",
      "Yanhua Li",
      "Wei Wei",
      "Tianrui Li",
      "Bo An",
      "Jiye Liang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-06-27T03:10:20Z",
    "pdf_url": "https://arxiv.org/pdf/2506.21872v1"
  },
  {
    "arxiv_id": "2506.22521v1",
    "entry_id": "http://arxiv.org/abs/2506.22521v1",
    "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models",
    "summary": "Model extraction attacks pose significant security threats to deployed language models, potentially compromising intellectual property and user privacy. This survey provides a comprehensive taxonomy of LLM-specific extraction attacks and defenses, categorizing attacks into functionality extraction, training data extraction, and prompt-targeted attacks. We analyze various attack methodologies including API-based knowledge distillation, direct querying, parameter recovery, and prompt stealing techniques that exploit transformer architectures. We then examine defense mechanisms organized into model protection, data privacy protection, and prompt-targeted strategies, evaluating their effectiveness across different deployment scenarios. We propose specialized metrics for evaluating both attack effectiveness and defense performance, addressing the specific challenges of generative language models. Through our analysis, we identify critical limitations in current approaches and propose promising research directions, including integrated attack methodologies and adaptive defense mechanisms that balance security with model utility. This work serves NLP researchers, ML engineers, and security professionals seeking to protect language models in production environments.",
    "authors": [
      "Kaixiang Zhao",
      "Lincan Li",
      "Kaize Ding",
      "Neil Zhenqiang Gong",
      "Yue Zhao",
      "Yushun Dong"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-06-26T22:02:01Z",
    "pdf_url": "https://arxiv.org/pdf/2506.22521v1"
  },
  {
    "arxiv_id": "2506.21763v2",
    "entry_id": "http://arxiv.org/abs/2506.21763v2",
    "title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?",
    "summary": "Large Language Models (LLMs) are accelerating scientific idea generation, but rigorously evaluating these numerous, often superficial, AI-generated propositions for novelty and factual accuracy is a critical bottleneck; manual verification is too slow. Existing validation methods are inadequate: LLMs as standalone verifiers may hallucinate and lack domain knowledge (our findings show 60% unawareness of relevant papers in specific domains), while traditional citation networks lack explicit causality and narrative surveys are unstructured. This underscores a core challenge: the absence of structured, verifiable, and causally-linked historical data of scientific evolution.To address this,we introduce \\textbf{THE-Tree} (\\textbf{T}echnology \\textbf{H}istory \\textbf{E}volution Tree), a computational framework that constructs such domain-specific evolution trees from scientific literature. THE-Tree employs a search algorithm to explore evolutionary paths. During its node expansion, it utilizes a novel \"Think-Verbalize-Cite-Verify\" process: an LLM proposes potential advancements and cites supporting literature. Critically, each proposed evolutionary link is then validated for logical coherence and evidential support by a recovered natural language inference mechanism that interrogates the cited literature, ensuring that each step is grounded. We construct and validate 88 THE-Trees across diverse domains and release a benchmark dataset including up to 71k fact verifications covering 27k papers to foster further research. Experiments demonstrate that i) in graph completion, our THE-Tree improves hit@1 by 8% to 14% across multiple models compared to traditional citation networks; ii) for predicting future scientific developments, it improves hit@1 metric by nearly 10%; and iii) when combined with other methods, it boosts the performance of evaluating important scientific papers by almost 100%.",
    "authors": [
      "Xin Wang",
      "Jiyao Liu",
      "Yulong Xiao",
      "Junzhi Ning",
      "Lihao Liu",
      "Junjun He",
      "Botian Shi",
      "Kaicheng Yu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-26T20:44:51Z",
    "pdf_url": "https://arxiv.org/pdf/2506.21763v2"
  },
  {
    "arxiv_id": "2506.21443v1",
    "entry_id": "http://arxiv.org/abs/2506.21443v1",
    "title": "Domain Knowledge-Enhanced LLMs for Fraud and Concept Drift Detection",
    "summary": "Detecting deceptive conversations on dynamic platforms is increasingly difficult due to evolving language patterns and Concept Drift (CD)-i.e., semantic or topical shifts that alter the context or intent of interactions over time. These shifts can obscure malicious intent or mimic normal dialogue, making accurate classification challenging. While Large Language Models (LLMs) show strong performance in natural language tasks, they often struggle with contextual ambiguity and hallucinations in risk-sensitive scenarios. To address these challenges, we present a Domain Knowledge (DK)-Enhanced LLM framework that integrates pretrained LLMs with structured, task-specific insights to perform fraud and concept drift detection. The proposed architecture consists of three main components: (1) a DK-LLM module to detect fake or deceptive conversations; (2) a drift detection unit (OCDD) to determine whether a semantic shift has occurred; and (3) a second DK-LLM module to classify the drift as either benign or fraudulent. We first validate the value of domain knowledge using a fake review dataset and then apply our full framework to SEConvo, a multiturn dialogue dataset that includes various types of fraud and spam attacks. Results show that our system detects fake conversations with high accuracy and effectively classifies the nature of drift. Guided by structured prompts, the LLaMA-based implementation achieves 98% classification accuracy. Comparative studies against zero-shot baselines demonstrate that incorporating domain knowledge and drift awareness significantly improves performance, interpretability, and robustness in high-stakes NLP applications.",
    "authors": [
      "Ali Şenol",
      "Garima Agrawal",
      "Huan Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-26T16:29:45Z",
    "pdf_url": "https://arxiv.org/pdf/2506.21443v1"
  },
  {
    "arxiv_id": "2506.20803v1",
    "entry_id": "http://arxiv.org/abs/2506.20803v1",
    "title": "The Ideation-Execution Gap: Execution Outcomes of LLM-Generated versus Human Research Ideas",
    "summary": "Large Language Models (LLMs) have shown promise in accelerating the scientific research pipeline. A key capability for this process is the ability to generate novel research ideas, and prior studies have found settings in which LLM-generated research ideas were judged as more novel than human-expert ideas. However, a good idea should not simply appear to be novel, it should also result in better research after being executed. To test whether AI-generated ideas lead to better research outcomes, we conduct an execution study by recruiting 43 expert researchers to execute randomly-assigned ideas, either written by experts or generated by an LLM. Each expert spent over 100 hours implementing the idea and wrote a 4-page short paper to document the experiments. All the executed projects are then reviewed blindly by expert NLP researchers. Comparing the review scores of the same ideas before and after execution, the scores of the LLM-generated ideas decrease significantly more than expert-written ideas on all evaluation metrics (novelty, excitement, effectiveness, and overall; p < 0.05), closing the gap between LLM and human ideas observed at the ideation stage. When comparing the aggregated review scores from the execution study, we even observe that for many metrics there is a flip in rankings where human ideas score higher than LLM ideas. This ideation-execution gap highlights the limitations of current LLMs in generating truly effective research ideas and the challenge of evaluating research ideas in the absence of execution outcomes.",
    "authors": [
      "Chenglei Si",
      "Tatsunori Hashimoto",
      "Diyi Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-06-25T19:47:23Z",
    "pdf_url": "https://arxiv.org/pdf/2506.20803v1"
  },
  {
    "arxiv_id": "2506.20743v1",
    "entry_id": "http://arxiv.org/abs/2506.20743v1",
    "title": "A Survey of AI for Materials Science: Foundation Models, LLM Agents, Datasets, and Tools",
    "summary": "Foundation models (FMs) are catalyzing a transformative shift in materials science (MatSci) by enabling scalable, general-purpose, and multimodal AI systems for scientific discovery. Unlike traditional machine learning models, which are typically narrow in scope and require task-specific engineering, FMs offer cross-domain generalization and exhibit emergent capabilities. Their versatility is especially well-suited to materials science, where research challenges span diverse data types and scales. This survey provides a comprehensive overview of foundation models, agentic systems, datasets, and computational tools supporting this growing field. We introduce a task-driven taxonomy encompassing six broad application areas: data extraction, interpretation and Q\\&A; atomistic simulation; property prediction; materials structure, design and discovery; process planning, discovery, and optimization; and multiscale modeling. We discuss recent advances in both unimodal and multimodal FMs, as well as emerging large language model (LLM) agents. Furthermore, we review standardized datasets, open-source tools, and autonomous experimental platforms that collectively fuel the development and integration of FMs into research workflows. We assess the early successes of foundation models and identify persistent limitations, including challenges in generalizability, interpretability, data imbalance, safety concerns, and limited multimodal fusion. Finally, we articulate future research directions centered on scalable pretraining, continual learning, data governance, and trustworthiness.",
    "authors": [
      "Minh-Hao Van",
      "Prateek Verma",
      "Chen Zhao",
      "Xintao Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.CE"
    ],
    "published": "2025-06-25T18:10:30Z",
    "pdf_url": "https://arxiv.org/pdf/2506.20743v1"
  },
  {
    "arxiv_id": "2506.20331v1",
    "entry_id": "http://arxiv.org/abs/2506.20331v1",
    "title": "Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content",
    "summary": "We introduce Biomed-Enriched, a biomedical text dataset constructed from PubMed via a two-stage annotation process. In the first stage, a large language model annotates 400K paragraphs from PubMed scientific articles, assigning scores for their type (review, study, clinical case, other), domain (clinical, biomedical, other), and educational quality. The educational quality score (rated 1 to 5) estimates how useful a paragraph is for college-level learning. These annotations are then used to fine-tune a small language model, which propagates the labels across the full PMC-OA corpus. The resulting metadata allows us to extract refined subsets, including 2M clinical case paragraphs with over 450K high-quality ones from articles with commercial-use licenses, and to construct several variants via quality filtering and domain upsampling. Clinical text is typically difficult to access due to privacy constraints, as hospital records cannot be publicly shared. Hence, our dataset provides an alternative large-scale, openly available collection of clinical cases from PubMed, making it a valuable resource for biomedical and clinical NLP. Preliminary continual-pretraining experiments with OLMo2 suggest these curated subsets enable targeted improvements, with clinical upsampling boosting performance by ~5% on MMLU ProfMed and educational quality filtering improving MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster convergence, reaching same performance with a third of training tokens, indicating potential for more efficient and effective biomedical pretraining strategies.",
    "authors": [
      "Rian Touchent",
      "Nathan Godey",
      "Eric de la Clergerie"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-06-25T11:30:25Z",
    "pdf_url": "https://arxiv.org/pdf/2506.20331v1"
  },
  {
    "arxiv_id": "2506.20249v1",
    "entry_id": "http://arxiv.org/abs/2506.20249v1",
    "title": "Language Modeling by Language Models",
    "summary": "Can we leverage LLMs to model the process of discovering novel language model (LM) architectures? Inspired by real research, we propose a multi-agent LLM approach that simulates the conventional stages of research, from ideation and literature search (proposal stage) to design implementation (code generation), generative pre-training, and downstream evaluation (verification). Using ideas from scaling laws, our system, Genesys, employs a Ladder of Scales approach; new designs are proposed, adversarially reviewed, implemented, and selectively verified at increasingly larger model scales (14M$\\sim$350M parameters) with a narrowing budget (the number of models we can train at each scale). To help make discovery efficient and factorizable, Genesys uses a novel genetic programming backbone, which we show has empirical advantages over commonly used direct prompt generation workflows (e.g., $\\sim$86\\% percentage point improvement in successful design generation, a key bottleneck). We report experiments involving 1,162 newly discovered designs (1,062 fully verified through pre-training) and find the best designs to be highly competitive with known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common benchmarks). We couple these results with comprehensive system-level ablations and formal results, which give broader insights into the design of effective autonomous discovery systems.",
    "authors": [
      "Junyan Cheng",
      "Peter Clark",
      "Kyle Richardson"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-06-25T08:46:10Z",
    "pdf_url": "https://arxiv.org/pdf/2506.20249v1"
  },
  {
    "arxiv_id": "2506.20156v1",
    "entry_id": "http://arxiv.org/abs/2506.20156v1",
    "title": "Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype",
    "summary": "The core challenge in learning has shifted from knowledge acquisition to effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting on one's learning. Existing digital tools, however, inadequately support metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized review, overlooking the role of context, while Personal Knowledge Management (PKM) tools require high manual maintenance.\n  To address these challenges, this paper introduces \"Insight Recall,\" a novel paradigm that conceptualizes the context-triggered retrieval of personal past insights as a metacognitive scaffold to promote SRL. We formalize this paradigm using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses a dynamic knowledge graph of the user's learning history. When a user faces a new problem, a hybrid retrieval engine recalls relevant personal \"insights.\" Subsequently, a large language model (LLM) performs a deep similarity assessment to filter and present the most relevant scaffold in a just-in-time manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline for LLM-based knowledge graph construction. We also propose an optional \"Guided Inquiry\" module, where users can engage in a Socratic dialogue with an expert LLM, using the current problem and recalled insights as context. The contribution of this paper is a solid theoretical framework and a usable system platform for designing next-generation intelligent learning systems that enhance metacognition and self-regulation.",
    "authors": [
      "Xuefei Hou",
      "Xizhao Tan"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-06-25T06:23:39Z",
    "pdf_url": "https://arxiv.org/pdf/2506.20156v1"
  },
  {
    "arxiv_id": "2506.20018v1",
    "entry_id": "http://arxiv.org/abs/2506.20018v1",
    "title": "Achieving Trustworthy Real-Time Decision Support Systems with Low-Latency Interpretable AI Models",
    "summary": "This paper investigates real-time decision support systems that leverage low-latency AI models, bringing together recent progress in holistic AI-driven decision tools, integration with Edge-IoT technologies, and approaches for effective human-AI teamwork. It looks into how large language models can assist decision-making, especially when resources are limited. The research also examines the effects of technical developments such as DeLLMa, methods for compressing models, and improvements for analytics on edge devices, while also addressing issues like limited resources and the need for adaptable frameworks. Through a detailed review, the paper offers practical perspectives on development strategies and areas of application, adding to the field by pointing out opportunities for more efficient and flexible AI-supported systems. The conclusions set the stage for future breakthroughs in this fast-changing area, highlighting how AI can reshape real-time decision support.",
    "authors": [
      "Zechun Deng",
      "Ziwei Liu",
      "Ziqian Bi",
      "Junhao Song",
      "Chia Xin Liang",
      "Joe Yeong",
      "Junfeng Hao"
    ],
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "published": "2025-06-24T21:22:25Z",
    "pdf_url": "https://arxiv.org/pdf/2506.20018v1"
  },
  {
    "arxiv_id": "2506.22493v4",
    "entry_id": "http://arxiv.org/abs/2506.22493v4",
    "title": "A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models",
    "summary": "The Political Compass Test (PCT) and similar surveys are commonly used to assess political bias in auto-regressive LLMs. Our rigorous statistical experiments show that while changes to standard generation parameters have minimal effect on PCT scores, prompt phrasing and fine-tuning individually and together can significantly influence results. Interestingly, fine-tuning on politically rich vs. neutral datasets does not lead to different shifts in scores. We also generalize these findings to a similar popular test called 8 Values. Humans do not change their responses to questions when prompted differently (``answer this question'' vs ``state your opinion''), or after exposure to politically neutral text, such as mathematical formulae. But the fact that the models do so raises concerns about the validity of these tests for measuring model bias, and paves the way for deeper exploration into how political and social views are encoded in LLMs.",
    "authors": [
      "Sadia Kamal",
      "Lalu Prasad Yadav Prakash",
      "S M Rafiuddin",
      "Mohammed Rakib",
      "Atriya Sen",
      "Sagnik Ray Choudhury"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-06-24T20:33:51Z",
    "pdf_url": "https://arxiv.org/pdf/2506.22493v4"
  },
  {
    "arxiv_id": "2506.19769v1",
    "entry_id": "http://arxiv.org/abs/2506.19769v1",
    "title": "A Survey of Multi-sensor Fusion Perception for Embodied AI: Background, Methods, Challenges and Prospects",
    "summary": "Multi-sensor fusion perception (MSFP) is a key technology for embodied AI, which can serve a variety of downstream tasks (e.g., 3D object detection and semantic segmentation) and application scenarios (e.g., autonomous driving and swarm robotics). Recently, impressive achievements on AI-based MSFP methods have been reviewed in relevant surveys. However, we observe that the existing surveys have some limitations after a rigorous and detailed investigation. For one thing, most surveys are oriented to a single task or research field, such as 3D object detection or autonomous driving. Therefore, researchers in other related tasks often find it difficult to benefit directly. For another, most surveys only introduce MSFP from a single perspective of multi-modal fusion, while lacking consideration of the diversity of MSFP methods, such as multi-view fusion and time-series fusion. To this end, in this paper, we hope to organize MSFP research from a task-agnostic perspective, where methods are reported from various technical views. Specifically, we first introduce the background of MSFP. Next, we review multi-modal and multi-agent fusion methods. A step further, time-series fusion methods are analyzed. In the era of LLM, we also investigate multimodal LLM fusion methods. Finally, we discuss open challenges and future directions for MSFP. We hope this survey can help researchers understand the important progress in MSFP and provide possible insights for future research.",
    "authors": [
      "Shulan Ruan",
      "Rongwei Wang",
      "Xuchen Shen",
      "Huijie Liu",
      "Baihui Xiao",
      "Jun Shi",
      "Kun Zhang",
      "Zhenya Huang",
      "Yu Liu",
      "Enhong Chen",
      "You He"
    ],
    "categories": [
      "cs.MM",
      "cs.AI"
    ],
    "published": "2025-06-24T16:34:56Z",
    "pdf_url": "https://arxiv.org/pdf/2506.19769v1"
  },
  {
    "arxiv_id": "2506.19484v1",
    "entry_id": "http://arxiv.org/abs/2506.19484v1",
    "title": "Dialogic Pedagogy for Large Language Models: Aligning Conversational AI with Proven Theories of Learning",
    "summary": "Large Language Models (LLMs) are rapidly transforming education by enabling rich conversational learning experiences. This article provides a comprehensive review of how LLM-based conversational agents are being used in higher education, with extensions to secondary and lifelong learning contexts. We synthesize existing literature on LLMs in education and theories of conversational and dialogic pedagogy - including Vygotsky's sociocultural learning (scaffolding and the Zone of Proximal Development), the Socratic method, and Laurillard's conversational framework - and examine how prompting strategies and retrieval-augmented generation (RAG) can align LLM behaviors with these pedagogical theories, and how it can support personalized, adaptive learning. We map educational theories to LLM capabilities, highlighting where LLM-driven dialogue supports established learning principles and where it challenges or falls short of traditional pedagogical assumptions. Notable gaps in applying prior theories to LLMs are identified, such as the models tendency to provide direct answers instead of fostering co-construction of knowledge, and the need to account for the constant availability and broad but non-human expertise of LLM tutors. In response, we propose practical strategies to better align LLM interactions with sound pedagogy - for example, designing prompts that encourage Socratic questioning, scaffolded guidance, and student reflection, as well as integrating retrieval mechanisms to ensure accuracy and contextual relevance. Our aim is to bridge the gap between educational theory and the emerging practice of AI-driven conversational learning, offering insights and tools for making LLM-based dialogues more educationally productive and theory-aligned.",
    "authors": [
      "Russell Beale"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-06-24T10:19:09Z",
    "pdf_url": "https://arxiv.org/pdf/2506.19484v1"
  },
  {
    "arxiv_id": "2506.19107v2",
    "entry_id": "http://arxiv.org/abs/2506.19107v2",
    "title": "Improving Student-AI Interaction Through Pedagogical Prompting: An Example in Computer Science Education",
    "summary": "With the proliferation of large language model (LLM) applications since 2022, their use in education has sparked both excitement and concern. Recent studies consistently highlight students' (mis)use of LLMs can hinder learning outcomes. This work aims to teach students how to effectively prompt LLMs to improve their learning. We first proposed pedagogical prompting, a theoretically-grounded new concept to elicit learning-oriented responses from LLMs. To move from concept design to a proof-of-concept learning intervention in real educational settings, we selected early undergraduate CS education (CS1/CS2) as the example context. We began with a formative survey study with instructors (N=36) teaching early-stage undergraduate-level CS courses to inform the instructional design based on classroom needs. Based on their insights, we designed and developed a learning intervention through an interactive system with scenario-based instruction to train pedagogical prompting skills. Finally, we evaluated its instructional effectiveness through a user study with CS novice students (N=22) using pre/post-tests. Through mixed methods analyses, our results indicate significant improvements in learners' LLM-based pedagogical help-seeking skills, along with positive attitudes toward the system and increased willingness to use pedagogical prompts in the future. Our contributions include (1) a theoretical framework of pedagogical prompting; (2) empirical insights into current instructor attitudes toward pedagogical prompting; and (3) a learning intervention design with an interactive learning tool and scenario-based instruction leading to promising results on teaching LLM-based help-seeking. Our approach is scalable for broader implementation in classrooms and has the potential to be integrated into tools like ChatGPT as an on-boarding experience to encourage learning-oriented use of generative AI.",
    "authors": [
      "Ruiwei Xiao",
      "Xinying Hou",
      "Runlong Ye",
      "Majeed Kazemitabaar",
      "Nicholas Diana",
      "Michael Liut",
      "John Stamper"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-06-23T20:39:17Z",
    "pdf_url": "https://arxiv.org/pdf/2506.19107v2"
  },
  {
    "arxiv_id": "2506.22485v1",
    "entry_id": "http://arxiv.org/abs/2506.22485v1",
    "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents",
    "summary": "This study presents a modular, multi-agent system for the automated review of highly structured enterprise business documents using AI agents. Unlike prior solutions focused on unstructured texts or limited compliance checks, this framework leverages modern orchestration tools such as LangChain, CrewAI, TruLens, and Guidance to enable section-by-section evaluation of documents for accuracy, consistency, completeness, and clarity. Specialized agents, each responsible for discrete review criteria such as template compliance or factual correctness, operate in parallel or sequence as required. Evaluation outputs are enforced to a standardized, machine-readable schema, supporting downstream analytics and auditability. Continuous monitoring and a feedback loop with human reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system approaches or exceeds human performance in key areas: achieving 99% information consistency (vs. 92% for humans), halving error and bias rates, and reducing average review time from 30 to 2.5 minutes per document, with a 95% agreement rate between AI and expert human judgment. While promising for a wide range of industries, the study also discusses current limitations, including the need for human oversight in highly specialized domains and the operational cost of large-scale LLM usage. The proposed system serves as a flexible, auditable, and scalable foundation for AI-driven document quality assurance in the enterprise context.",
    "authors": [
      "Sudip Dasgupta",
      "Himanshu Shankar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-23T17:46:15Z",
    "pdf_url": "https://arxiv.org/pdf/2506.22485v1"
  },
  {
    "arxiv_id": "2506.18504v2",
    "entry_id": "http://arxiv.org/abs/2506.18504v2",
    "title": "Generalizing vision-language models to novel domains: A comprehensive survey",
    "summary": "Recently, vision-language pretraining has emerged as a transformative technique that integrates the strengths of both visual and textual modalities, resulting in powerful vision-language models (VLMs). Leveraging web-scale pretraining data, these models exhibit strong zero-shot capabilities. However, their performance often deteriorates when confronted with domain-specific or specialized generalization tasks. To address this, a growing body of research focuses on transferring or generalizing the rich knowledge embedded in VLMs to various downstream applications. This survey aims to comprehensively summarize the generalization settings, methodologies, benchmarking and results in VLM literatures. Delving into the typical VLM structures, current literatures are categorized into prompt-based, parameter-based and feature-based methods according to the transferred modules. The differences and characteristics in each category are furthered summarized and discussed by revisiting the typical transfer learning (TL) settings, providing novel interpretations for TL in the era of VLMs. Popular benchmarks for VLM generalization are further introduced with thorough performance comparisons among the reviewed methods. Following the advances in large-scale generalizable pretraining, this survey also discusses the relations and differences between VLMs and up-to-date multimodal large language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the surging literatures in vision-language research from a novel and practical generalization prospective, this survey contributes to a clear landscape of current and future multimodal researches.",
    "authors": [
      "Xinyao Li",
      "Jingjing Li",
      "Fengling Li",
      "Lei Zhu",
      "Yang Yang",
      "Heng Tao Shen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-06-23T10:56:37Z",
    "pdf_url": "https://arxiv.org/pdf/2506.18504v2"
  },
  {
    "arxiv_id": "2506.18501v3",
    "entry_id": "http://arxiv.org/abs/2506.18501v3",
    "title": "Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance",
    "summary": "The increasing use of large language models (LLMs) in natural language processing (NLP) tasks has sparked significant interest in evaluating their effectiveness across diverse applications. While models like ChatGPT and DeepSeek have shown strong results in many NLP domains, a comprehensive evaluation is needed to understand their strengths, weaknesses, and domain-specific abilities. This is critical as these models are applied to various tasks, from sentiment analysis to more nuanced tasks like textual entailment and translation. This study aims to evaluate ChatGPT and DeepSeek across five key NLP tasks: sentiment analysis, topic classification, text summarization, machine translation, and textual entailment. A structured experimental protocol is used to ensure fairness and minimize variability. Both models are tested with identical, neutral prompts and evaluated on two benchmark datasets per task, covering domains like news, reviews, and formal/informal texts. The results show that DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility. These findings provide valuable insights for selecting the appropriate LLM based on task requirements.",
    "authors": [
      "Wael Etaiwi",
      "Bushra Alhijawi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-23T10:52:54Z",
    "pdf_url": "https://arxiv.org/pdf/2506.18501v3"
  },
  {
    "arxiv_id": "2506.18348v3",
    "entry_id": "http://arxiv.org/abs/2506.18348v3",
    "title": "Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team",
    "summary": "Scientific progress increasingly relies on effective collaboration among researchers, a dynamic that large language models (LLMs) have only begun to emulate. While recent LLM-based scientist agents show promise in autonomous scientific discovery, they often lack the interactive reasoning and evaluation mechanisms essential to real-world research. We propose IDVSCI (Internal Discussion and Vote SCIentists), a multi-agent framework built on LLMs that incorporates two key innovations: a Dynamic Knowledge Exchange mechanism enabling iterative feedback among agents, and a Dual-Diversity Review paradigm that simulates heterogeneous expert evaluation. These components jointly promote deeper reasoning and the generation of more creative and impactful scientific ideas. To evaluate the effectiveness and generalizability of our approach, we conduct experiments on two datasets: a widely used benchmark in computer science and a new dataset we introduce in the health sciences domain. Results show that IDVSCI consistently achieves the best performance across both datasets, outperforming existing systems such as AI Scientist and VIRSCI. These findings highlight the value of modeling interaction and peer review dynamics in LLM-based autonomous research.",
    "authors": [
      "Weilun Yu",
      "Shixiang Tang",
      "Yonggui Huang",
      "Nanqing Dong",
      "Li Fan",
      "Honggang Qi",
      "Wei Liu",
      "Xiaoli Diao",
      "Xi Chen",
      "Wanli Ouyang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-23T07:12:08Z",
    "pdf_url": "https://arxiv.org/pdf/2506.18348v3"
  },
  {
    "arxiv_id": "2506.18199v2",
    "entry_id": "http://arxiv.org/abs/2506.18199v2",
    "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
    "summary": "Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.",
    "authors": [
      "Bushra Asseri",
      "Estabrag Abdelaziz",
      "Areej Al-Wabil"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2025-06-22T23:15:25Z",
    "pdf_url": "https://arxiv.org/pdf/2506.18199v2"
  },
  {
    "arxiv_id": "2506.18096v2",
    "entry_id": "http://arxiv.org/abs/2506.18096v2",
    "title": "Deep Research Agents: A Systematic Examination And Roadmap",
    "summary": "The rapid progress of Large Language Models (LLMs) has given rise to a new category of autonomous AI systems, referred to as Deep Research (DR) agents. These agents are designed to tackle complex, multi-turn informational research tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon planning, multi-hop information retrieval, iterative tool use, and the generation of structured analytical reports. In this paper, we conduct a detailed analysis of the foundational technologies and architectural components that constitute Deep Research agents. We begin by reviewing information acquisition strategies, contrasting API-based retrieval methods with browser-based exploration. We then examine modular tool-use frameworks, including code execution, multimodal input processing, and the integration of Model Context Protocols (MCPs) to support extensibility and ecosystem development. To systematize existing approaches, we propose a taxonomy that differentiates between static and dynamic workflows, and we classify agent architectures based on planning strategies and agent composition, including single-agent and multi-agent configurations. We also provide a critical evaluation of current benchmarks, highlighting key limitations such as restricted access to external knowledge, sequential execution inefficiencies, and misalignment between evaluation metrics and the practical objectives of DR agents. Finally, we outline open challenges and promising directions for future research. A curated and continuously updated repository of DR agent research is available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.",
    "authors": [
      "Yuxuan Huang",
      "Yihang Chen",
      "Haozheng Zhang",
      "Kang Li",
      "Huichi Zhou",
      "Meng Fang",
      "Linyi Yang",
      "Xiaoguang Li",
      "Lifeng Shang",
      "Songcen Xu",
      "Jianye Hao",
      "Kun Shao",
      "Jun Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-22T16:52:48Z",
    "pdf_url": "https://arxiv.org/pdf/2506.18096v2"
  },
  {
    "arxiv_id": "2506.18019v3",
    "entry_id": "http://arxiv.org/abs/2506.18019v3",
    "title": "Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities",
    "summary": "AI agents have experienced a paradigm shift, from early dominance by reinforcement learning (RL) to the rise of agents powered by large language models (LLMs), and now further advancing towards a synergistic fusion of RL and LLM capabilities. This progression has endowed AI agents with increasingly strong abilities. Despite these advances, to accomplish complex real-world tasks, agents are required to plan and execute effectively, maintain reliable memory, and coordinate smoothly with other agents. Achieving these capabilities involves contending with ever-present intricate information, operations, and interactions. In light of this challenge, data structurization can play a promising role by transforming intricate and disorganized data into well-structured forms that agents can more effectively understand and process. In this context, graphs, with their natural advantage in organizing, managing, and harnessing intricate data relationships, present a powerful data paradigm for structurization to support the capabilities demanded by advanced AI agents. To this end, this survey presents a first systematic review of how graphs can empower AI agents. Specifically, we explore the integration of graph techniques with core agent functionalities, highlight notable applications, and identify prospective avenues for future research. By comprehensively surveying this burgeoning intersection, we hope to inspire the development of next-generation AI agents equipped to tackle increasingly sophisticated challenges with graphs. Related resources are collected and continuously updated for the community in the Github link.",
    "authors": [
      "Yuanchen Bei",
      "Weizhi Zhang",
      "Siwen Wang",
      "Weizhi Chen",
      "Sheng Zhou",
      "Hao Chen",
      "Yong Li",
      "Jiajun Bu",
      "Shirui Pan",
      "Yizhou Yu",
      "Irwin King",
      "Fakhri Karray",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-22T12:59:12Z",
    "pdf_url": "https://arxiv.org/pdf/2506.18019v3"
  },
  {
    "arxiv_id": "2506.17700v1",
    "entry_id": "http://arxiv.org/abs/2506.17700v1",
    "title": "The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future",
    "summary": "Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by automating traditional labor-intensive tasks and consequently accelerated the development of computer-aided applications. As researchers continue to advance this field with the introduction of novel language models and more efficient training/finetuning methodologies, the idea of prompt engineering and subsequent optimization strategies with LLMs has emerged as a particularly impactful trend to yield a substantial performance boost across diverse NLP tasks. To best of our knowledge numerous review articles have explored prompt engineering, however, a critical gap exists in comprehensive analyses of prompt optimization strategies. To bridge this gap this paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies. It analyzes their underlying working paradigms and based on these principles, categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings: a critical need in the current landscape. Ultimately, this research will centralize diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks.",
    "authors": [
      "Summra Saleem",
      "Muhammad Nabeel Asim",
      "Shaista Zulfiqar",
      "Andreas Dengel"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-21T12:25:37Z",
    "pdf_url": "https://arxiv.org/pdf/2506.17700v1"
  },
  {
    "arxiv_id": "2506.17467v1",
    "entry_id": "http://arxiv.org/abs/2506.17467v1",
    "title": "Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems",
    "summary": "Large language models (LLMs) have shown significant potential to change how we write, communicate, and create, leading to rapid adoption across society. This dissertation examines how individuals and institutions are adapting to and engaging with this emerging technology through three research directions. First, I demonstrate how the institutional adoption of AI detectors introduces systematic biases, particularly disadvantaging writers of non-dominant language varieties, highlighting critical equity concerns in AI governance. Second, I present novel population-level algorithmic approaches that measure the increasing adoption of LLMs across writing domains, revealing consistent patterns of AI-assisted content in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Finally, I investigate LLMs' capability to provide feedback on research manuscripts through a large-scale empirical analysis, offering insights into their potential to support researchers who face barriers in accessing timely manuscript feedback, particularly early-career researchers and those from under-resourced settings.",
    "authors": [
      "Weixin Liang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-06-20T20:15:09Z",
    "pdf_url": "https://arxiv.org/pdf/2506.17467v1"
  },
  {
    "arxiv_id": "2506.17442v1",
    "entry_id": "http://arxiv.org/abs/2506.17442v1",
    "title": "Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation",
    "summary": "Artificial intelligence (AI) is increasingly integrated into modern healthcare, offering powerful support for clinical decision-making. However, in real-world settings, AI systems may experience performance degradation over time, due to factors such as shifting data distributions, changes in patient characteristics, evolving clinical protocols, and variations in data quality. These factors can compromise model reliability, posing safety concerns and increasing the likelihood of inaccurate predictions or adverse outcomes. This review presents a forward-looking perspective on monitoring and maintaining the \"health\" of AI systems in healthcare. We highlight the urgent need for continuous performance monitoring, early degradation detection, and effective self-correction mechanisms. The paper begins by reviewing common causes of performance degradation at both data and model levels. We then summarize key techniques for detecting data and model drift, followed by an in-depth look at root cause analysis. Correction strategies are further reviewed, ranging from model retraining to test-time adaptation. Our survey spans both traditional machine learning models and state-of-the-art large language models (LLMs), offering insights into their strengths and limitations. Finally, we discuss ongoing technical challenges and propose future research directions. This work aims to guide the development of reliable, robust medical AI systems capable of sustaining safe, long-term deployment in dynamic clinical settings.",
    "authors": [
      "Hao Guan",
      "David Bates",
      "Li Zhou"
    ],
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "published": "2025-06-20T19:22:07Z",
    "pdf_url": "https://arxiv.org/pdf/2506.17442v1"
  },
  {
    "arxiv_id": "2506.17363v1",
    "entry_id": "http://arxiv.org/abs/2506.17363v1",
    "title": "A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant",
    "summary": "Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs) have the potential to enhance student learning by providing instant feedback and facilitating multi-turn interactions. However, empirical studies on their effectiveness and acceptance in real-world classrooms are limited, leaving their practical impact uncertain. In this study, we develop an LLM-based VTA and deploy it in an introductory AI programming course with 477 graduate students. To assess how student perceptions of the VTA's performance evolve over time, we conduct three rounds of comprehensive surveys at different stages of the course. Additionally, we analyze 3,869 student--VTA interaction pairs to identify common question types and engagement patterns. We then compare these interactions with traditional student--human instructor interactions to evaluate the VTA's role in the learning process. Through a large-scale empirical study and interaction analysis, we assess the feasibility of deploying VTAs in real-world classrooms and identify key challenges for broader adoption. Finally, we release the source code of our VTA system, fostering future advancements in AI-driven education: \\texttt{https://github.com/sean0042/VTA}.",
    "authors": [
      "Sunjun Kweon",
      "Sooyohn Nam",
      "Hyunseung Lim",
      "Hwajung Hong",
      "Edward Choi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-06-20T10:59:57Z",
    "pdf_url": "https://arxiv.org/pdf/2506.17363v1"
  },
  {
    "arxiv_id": "2506.16702v1",
    "entry_id": "http://arxiv.org/abs/2506.16702v1",
    "title": "Large Language Models as Psychological Simulators: A Methodological Guide",
    "summary": "Large language models (LLMs) offer emerging opportunities for psychological and behavioral research, but methodological guidance is lacking. This article provides a framework for using LLMs as psychological simulators across two primary applications: simulating roles and personas to explore diverse contexts, and serving as computational models to investigate cognitive processes. For simulation, we present methods for developing psychologically grounded personas that move beyond demographic categories, with strategies for validation against human data and use cases ranging from studying inaccessible populations to prototyping research instruments. For cognitive modeling, we synthesize emerging approaches for probing internal representations, methodological advances in causal interventions, and strategies for relating model behavior to human cognition. We address overarching challenges including prompt sensitivity, temporal limitations from training data cutoffs, and ethical considerations that extend beyond traditional human subjects review. Throughout, we emphasize the need for transparency about model capabilities and constraints. Together, this framework integrates emerging empirical evidence about LLM performance--including systematic biases, cultural limitations, and prompt brittleness--to help researchers wrangle these challenges and leverage the unique capabilities of LLMs in psychological research.",
    "authors": [
      "Zhicheng Lin"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2025-06-20T02:45:23Z",
    "pdf_url": "https://arxiv.org/pdf/2506.16702v1"
  },
  {
    "arxiv_id": "2506.17352v2",
    "entry_id": "http://arxiv.org/abs/2506.17352v2",
    "title": "Towards Safety Evaluations of Theory of Mind in Large Language Models",
    "summary": "As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their behavior. To evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.",
    "authors": [
      "Tatsuhiro Aoshima",
      "Mitsuaki Akiyama"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-20T01:52:17Z",
    "pdf_url": "https://arxiv.org/pdf/2506.17352v2"
  },
  {
    "arxiv_id": "2506.16653v1",
    "entry_id": "http://arxiv.org/abs/2506.16653v1",
    "title": "LLMs in Coding and their Impact on the Commercial Software Engineering Landscape",
    "summary": "Large-language-model coding tools are now mainstream in software engineering. But as these same tools move human effort up the development stack, they present fresh dangers: 10% of real prompts leak private data, 42% of generated snippets hide security flaws, and the models can even ``agree'' with wrong ideas, a trait called sycophancy. We argue that firms must tag and review every AI-generated line of code, keep prompts and outputs inside private or on-premises deployments, obey emerging safety regulations, and add tests that catch sycophantic answers -- so they can gain speed without losing security and accuracy.",
    "authors": [
      "Vladislav Belozerov",
      "Peter J Barclay",
      "Askhan Sami"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-06-19T23:43:54Z",
    "pdf_url": "https://arxiv.org/pdf/2506.16653v1"
  },
  {
    "arxiv_id": "2506.16650v1",
    "entry_id": "http://arxiv.org/abs/2506.16650v1",
    "title": "SemAgent: A Semantics Aware Program Repair Agent",
    "summary": "Large Language Models (LLMs) have shown impressive capabilities in downstream software engineering tasks such as Automated Program Repair (APR). In particular, there has been a lot of research on repository-level issue-resolution benchmarks such as SWE-Bench. Although there has been significant progress on this topic, we notice that in the process of solving such issues, existing agentic systems tend to hyper-localize on immediately suspicious lines of code and fix them in isolation, without a deeper understanding of the issue semantics, code semantics, or execution semantics. Consequently, many existing systems generate patches that overfit to the user issue, even when a more general fix is preferable. To address this limitation, we introduce SemAgent, a novel workflow-based procedure that leverages issue, code, and execution semantics to generate patches that are complete - identifying and fixing all lines relevant to the issue. We achieve this through a novel pipeline that (a) leverages execution semantics to retrieve relevant context, (b) comprehends issue-semantics via generalized abstraction, (c) isolates code-semantics within the context of this abstraction, and (d) leverages this understanding in a two-stage architecture: a repair stage that proposes fine-grained fixes, followed by a reviewer stage that filters relevant fixes based on the inferred issue-semantics. Our evaluations show that our methodology achieves a solve rate of 44.66% on the SWEBench-Lite benchmark beating all other workflow-based approaches, and an absolute improvement of 7.66% compared to our baseline, which lacks such deep semantic understanding. We note that our approach performs particularly well on issues requiring multi-line reasoning (and editing) and edge-case handling, suggesting that incorporating issue and code semantics into APR pipelines can lead to robust and semantically consistent repairs.",
    "authors": [
      "Anvith Pabba",
      "Alex Mathai",
      "Anindya Chakraborty",
      "Baishakhi Ray"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-06-19T23:27:58Z",
    "pdf_url": "https://arxiv.org/pdf/2506.16650v1"
  },
  {
    "arxiv_id": "2506.16393v1",
    "entry_id": "http://arxiv.org/abs/2506.16393v1",
    "title": "From LLM-anation to LLM-orchestrator: Coordinating Small Models for Data Labeling",
    "summary": "Although the annotation paradigm based on Large Language Models (LLMs) has made significant breakthroughs in recent years, its actual deployment still has two core bottlenecks: first, the cost of calling commercial APIs in large-scale annotation is very expensive; second, in scenarios that require fine-grained semantic understanding, such as sentiment classification and toxicity classification, the annotation accuracy of LLMs is even lower than that of Small Language Models (SLMs) dedicated to this field. To address these problems, we propose a new paradigm of multi-model cooperative annotation and design a fully automatic annotation framework AutoAnnotator based on this. Specifically, AutoAnnotator consists of two layers. The upper-level meta-controller layer uses the generation and reasoning capabilities of LLMs to select SLMs for annotation, automatically generate annotation code and verify difficult samples; the lower-level task-specialist layer consists of multiple SLMs that perform annotation through multi-model voting. In addition, we use the difficult samples obtained by the secondary review of the meta-controller layer as the reinforcement learning set and fine-tune the SLMs in stages through a continual learning strategy, thereby improving the generalization of SLMs. Extensive experiments show that AutoAnnotator outperforms existing open-source/API LLMs in zero-shot, one-shot, CoT, and majority voting settings. Notably, AutoAnnotator reduces the annotation cost by 74.15% compared to directly annotating with GPT-3.5-turbo, while still improving the accuracy by 6.21%. Project page: https://github.com/Zhaiyuan-Ji/AutoAnnotator.",
    "authors": [
      "Yao Lu",
      "Zhaiyuan Ji",
      "Jiawei Du",
      "Yu Shanqing",
      "Qi Xuan",
      "Tianyi Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-19T15:26:08Z",
    "pdf_url": "https://arxiv.org/pdf/2506.16393v1"
  },
  {
    "arxiv_id": "2506.16370v1",
    "entry_id": "http://arxiv.org/abs/2506.16370v1",
    "title": "Can structural correspondences ground real world representational content in Large Language Models?",
    "summary": "Large Language Models (LLMs) such as GPT-4 produce compelling responses to a wide range of prompts. But their representational capacities are uncertain. Many LLMs have no direct contact with extra-linguistic reality: their inputs, outputs and training data consist solely of text, raising the questions (1) can LLMs represent anything and (2) if so, what? In this paper, I explore what it would take to answer these questions according to a structural-correspondence based account of representation, and make an initial survey of this evidence. I argue that the mere existence of structural correspondences between LLMs and worldly entities is insufficient to ground representation of those entities. However, if these structural correspondences play an appropriate role - they are exploited in a way that explains successful task performance - then they could ground real world contents. This requires overcoming a challenge: the text-boundedness of LLMs appears, on the face of it, to prevent them engaging in the right sorts of tasks.",
    "authors": [
      "Iwan Williams"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-19T14:48:40Z",
    "pdf_url": "https://arxiv.org/pdf/2506.16370v1"
  },
  {
    "arxiv_id": "2506.16006v1",
    "entry_id": "http://arxiv.org/abs/2506.16006v1",
    "title": "DIGMAPPER: A Modular System for Automated Geologic Map Digitization",
    "summary": "Historical geologic maps contain rich geospatial information, such as rock units, faults, folds, and bedding planes, that is critical for assessing mineral resources essential to renewable energy, electric vehicles, and national security. However, digitizing maps remains a labor-intensive and time-consuming task. We present DIGMAPPER, a modular, scalable system developed in collaboration with the United States Geological Survey (USGS) to automate the digitization of geologic maps. DIGMAPPER features a fully dockerized, workflow-orchestrated architecture that integrates state-of-the-art deep learning models for map layout analysis, feature extraction, and georeferencing. To overcome challenges such as limited training data and complex visual content, our system employs innovative techniques, including in-context learning with large language models, synthetic data generation, and transformer-based models. Evaluations on over 100 annotated maps from the DARPA-USGS dataset demonstrate high accuracy across polygon, line, and point feature extraction, and reliable georeferencing performance. Deployed at USGS, DIGMAPPER significantly accelerates the creation of analysis-ready geospatial datasets, supporting national-scale critical mineral assessments and broader geoscientific applications.",
    "authors": [
      "Weiwei Duan",
      "Michael P. Gerlek",
      "Steven N. Minton",
      "Craig A. Knoblock",
      "Fandel Lin",
      "Theresa Chen",
      "Leeje Jang",
      "Sofia Kirsanova",
      "Zekun Li",
      "Yijun Lin",
      "Yao-Yi Chiang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-06-19T03:51:47Z",
    "pdf_url": "https://arxiv.org/pdf/2506.16006v1"
  },
  {
    "arxiv_id": "2506.15421v1",
    "entry_id": "http://arxiv.org/abs/2506.15421v1",
    "title": "Reward Models in Deep Reinforcement Learning: A Survey",
    "summary": "In reinforcement learning (RL), agents continually interact with the environment and use the feedback to refine their behavior. To guide policy optimization, reward models are introduced as proxies of the desired objectives, such that when the agent maximizes the accumulated reward, it also fulfills the task designer's intentions. Recently, significant attention from both academic and industrial researchers has focused on developing reward models that not only align closely with the true objectives but also facilitate policy optimization. In this survey, we provide a comprehensive review of reward modeling techniques within the deep RL literature. We begin by outlining the background and preliminaries in reward modeling. Next, we present an overview of recent reward modeling approaches, categorizing them based on the source, the mechanism, and the learning paradigm. Building on this understanding, we discuss various applications of these reward modeling techniques and review methods for evaluating reward models. Finally, we conclude by highlighting promising research directions in reward modeling. Altogether, this survey includes both established and emerging methods, filling the vacancy of a systematic review of reward models in current literature.",
    "authors": [
      "Rui Yu",
      "Shenghua Wan",
      "Yucen Wang",
      "Chen-Xiao Gao",
      "Le Gan",
      "Zongzhang Zhang",
      "De-Chuan Zhan"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-06-18T12:46:39Z",
    "pdf_url": "https://arxiv.org/pdf/2506.15421v1"
  },
  {
    "arxiv_id": "2506.15301v2",
    "entry_id": "http://arxiv.org/abs/2506.15301v2",
    "title": "Cohort Discovery: A Survey on LLM-Assisted Clinical Trial Recruitment",
    "summary": "Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet, their adoption in critical domains, such as clinical trial recruitment, remains limited. As trials are designed in natural language and patient data is represented as both structured and unstructured text, the task of matching trials and patients benefits from knowledge aggregation and reasoning abilities of LLMs. Classical approaches are trial-specific and LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution. Yet recent applications of LLM-assisted methods rely on proprietary models and weak evaluation benchmarks. In this survey, we are the first to analyze the task of trial-patient matching and contextualize emerging LLM-based approaches in clinical trial recruitment. We critically examine existing benchmarks, approaches and evaluation frameworks, the challenges to adopting LLM technologies in clinical research and exciting future directions.",
    "authors": [
      "Shrestha Ghosh",
      "Moritz Schneider",
      "Carina Reinicke",
      "Carsten Eickhoff"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-18T09:32:16Z",
    "pdf_url": "https://arxiv.org/pdf/2506.15301v2"
  },
  {
    "arxiv_id": "2506.14997v1",
    "entry_id": "http://arxiv.org/abs/2506.14997v1",
    "title": "Hypothesis Testing for Quantifying LLM-Human Misalignment in Multiple Choice Settings",
    "summary": "As Large Language Models (LLMs) increasingly appear in social science research (e.g., economics and marketing), it becomes crucial to assess how well these models replicate human behavior. In this work, using hypothesis testing, we present a quantitative framework to assess the misalignment between LLM-simulated and actual human behaviors in multiple-choice survey settings. This framework allows us to determine in a principled way whether a specific language model can effectively simulate human opinions, decision-making, and general behaviors represented through multiple-choice options. We applied this framework to a popular language model for simulating people's opinions in various public surveys and found that this model is ill-suited for simulating the tested sub-populations (e.g., across different races, ages, and incomes) for contentious questions. This raises questions about the alignment of this language model with the tested populations, highlighting the need for new practices in using LLMs for social science studies beyond naive simulations of human subjects.",
    "authors": [
      "Harbin Hong",
      "Sebastian Caldas",
      "Liu Leqi"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-06-17T22:04:55Z",
    "pdf_url": "https://arxiv.org/pdf/2506.14997v1"
  },
  {
    "arxiv_id": "2506.14670v2",
    "entry_id": "http://arxiv.org/abs/2506.14670v2",
    "title": "StreetLens: Enabling Human-Centered AI Agents for Neighborhood Assessment from Street View Imagery",
    "summary": "Traditionally, neighborhood studies have used interviews, surveys, and manual image annotation guided by detailed protocols to identify environmental characteristics, including physical disorder, decay, street safety, and sociocultural symbols, and to examine their impact on developmental and health outcomes. Although these methods yield rich insights, they are time-consuming and require intensive expert intervention. Recent technological advances, including vision language models (VLMs), have begun to automate parts of this process; however, existing efforts are often ad hoc and lack adaptability across research designs and geographic contexts. In this paper, we present StreetLens, a user-configurable human-centered workflow that integrates relevant social science expertise into a VLM for scalable neighborhood environmental assessments. StreetLens mimics the process of trained human coders by focusing the analysis on questions derived from established interview protocols, retrieving relevant street view imagery (SVI), and generating a wide spectrum of semantic annotations from objective features (e.g., the number of cars) to subjective perceptions (e.g., the sense of disorder in an image). By enabling researchers to define the VLM's role through domain-informed prompting, StreetLens places domain knowledge at the core of the analysis process. It also supports the integration of prior survey data to enhance robustness and expand the range of characteristics assessed in diverse settings. StreetLens represents a shift toward flexible and agentic AI systems that work closely with researchers to accelerate and scale neighborhood studies. StreetLens is publicly available at https://knowledge-computing.github.io/projects/streetlens.",
    "authors": [
      "Jina Kim",
      "Leeje Jang",
      "Yao-Yi Chiang",
      "Guanyu Wang",
      "Michelle C. Pasco"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-06-17T16:06:03Z",
    "pdf_url": "https://arxiv.org/pdf/2506.14670v2"
  },
  {
    "arxiv_id": "2506.14634v3",
    "entry_id": "http://arxiv.org/abs/2506.14634v3",
    "title": "AIn't Nothing But a Survey? Using Large Language Models for Coding German Open-Ended Survey Responses on Survey Motivation",
    "summary": "The recent development and wider accessibility of LLMs have spurred discussions about how they can be used in survey research, including classifying open-ended survey responses. Due to their linguistic capacities, it is possible that LLMs are an efficient alternative to time-consuming manual coding and the pre-training of supervised machine learning models. As most existing research on this topic has focused on English-language responses relating to non-complex topics or on single LLMs, it is unclear whether its findings generalize and how the quality of these classifications compares to established methods. In this study, we investigate to what extent different LLMs can be used to code open-ended survey responses in other contexts, using German data on reasons for survey participation as an example. We compare several state-of-the-art LLMs and several prompting approaches, and evaluate the LLMs' performance by using human expert codings. Overall performance differs greatly between LLMs, and only a fine-tuned LLM achieves satisfactory levels of predictive performance. Performance differences between prompting approaches are conditional on the LLM used. Finally, LLMs' unequal classification performance across different categories of reasons for survey participation results in different categorical distributions when not using fine-tuning. We discuss the implications of these findings, both for methodological research on coding open-ended responses and for their substantive analysis, and for practitioners processing or substantively analyzing such data. Finally, we highlight the many trade-offs researchers need to consider when choosing automated methods for open-ended response classification in the age of LLMs. In doing so, our study contributes to the growing body of research about the conditions under which LLMs can be efficiently, accurately, and reliably leveraged in survey research.",
    "authors": [
      "Leah von der Heyde",
      "Anna-Carolina Haensch",
      "Bernd Weiß",
      "Jessica Daikeler"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-06-17T15:28:53Z",
    "pdf_url": "https://arxiv.org/pdf/2506.14634v3"
  },
  {
    "arxiv_id": "2506.14627v2",
    "entry_id": "http://arxiv.org/abs/2506.14627v2",
    "title": "Working Document -- Formalising Software Requirements with Large Language Models",
    "summary": "This draft is a working document, having a summary of nighty-four (94) papers with additional sections on Traceability of Software Requirements (Section 4), Formal Methods and Its Tools (Section 5), Unifying Theories of Programming (UTP) and Theory of Institutions (Section 6). Please refer to abstract of [7,8]. Key difference of this draft from our recently anticipated ones with similar titles, i.e. AACS 2025 [7] and SAIV 2025 [8] is:  [7] is a two page submission to ADAPT Annual Conference, Ireland. Submitted on 18th of March, 2025, it went through the light-weight blind review and accepted for poster presentation. Conference was held on 15th of May, 2025; [8] is a nine page paper with additional nine pages of references and summary tables, submitted to Symposium on AI Verification (SAIV 2025) on 24th of April, 2025. It went through rigorous review process. The uploaded version on arXiv.org [8] is the improved one of the submission, after addressing the specific suggestions to improve the paper.",
    "authors": [
      "Arshad Beg",
      "Diarmuid O'Donoghue",
      "Rosemary Monahan"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-06-17T15:23:56Z",
    "pdf_url": "https://arxiv.org/pdf/2506.14627v2"
  },
  {
    "arxiv_id": "2506.14096v2",
    "entry_id": "http://arxiv.org/abs/2506.14096v2",
    "title": "Image Segmentation with Large Language Models: A Survey with Perspectives for Intelligent Transportation Systems",
    "summary": "The integration of Large Language Models (LLMs) with computer vision is profoundly transforming perception tasks like image segmentation. For intelligent transportation systems (ITS), where accurate scene understanding is critical for safety and efficiency, this new paradigm offers unprecedented capabilities. This survey systematically reviews the emerging field of LLM-augmented image segmentation, focusing on its applications, challenges, and future directions within ITS. We provide a taxonomy of current approaches based on their prompting mechanisms and core architectures, and we highlight how these innovations can enhance road scene understanding for autonomous driving, traffic monitoring, and infrastructure maintenance. Finally, we identify key challenges, including real-time performance and safety-critical reliability, and outline a perspective centered on explainable, human-centric AI as a prerequisite for the successful deployment of this technology in next-generation transportation systems.",
    "authors": [
      "Sanjeda Akter",
      "Ibne Farabi Shihab",
      "Anuj Sharma"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-06-17T01:20:50Z",
    "pdf_url": "https://arxiv.org/pdf/2506.14096v2"
  },
  {
    "arxiv_id": "2506.13932v1",
    "entry_id": "http://arxiv.org/abs/2506.13932v1",
    "title": "How Does LLM Reasoning Work for Code? A Survey and a Call to Action",
    "summary": "The rise of large language models (LLMs) has led to dramatic improvements across a wide range of natural language tasks. These advancements have extended into the domain of code, facilitating complex tasks such as code generation, translation, summarization, and repair. However, their utility for real-world deployment in-the-wild has only recently been studied, particularly on software engineering (SWE) tasks such as GitHub issue resolution. In this study, we examine the code reasoning techniques that underlie the ability to perform such tasks, and examine the paradigms used to drive their performance. Our contributions in this paper are: (1) the first dedicated survey on code reasoning for code tasks, highlighting overarching strategies, hybrid and agentic approaches; (2) a taxonomy of various techniques used to drive code reasoning; (3) a comprehensive overview of performance on common benchmarks and a showcase of new, under-explored benchmarks with high potential in SWE; (4) an exploration on how core properties of code can be used to explain different reasoning techniques; and (5) gaps and potentially under-explored areas for future research.",
    "authors": [
      "Ira Ceka",
      "Saurabh Pujar",
      "Irene Manotas",
      "Gail Kaiser",
      "Baishakhi Ray",
      "Shyam Ramji"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-06-16T19:18:09Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13932v1"
  },
  {
    "arxiv_id": "2506.13759v5",
    "entry_id": "http://arxiv.org/abs/2506.13759v5",
    "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
    "summary": "In this work, we provide a systematic survey of Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs). Unlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token, parallel decoding paradigm using full attention and a denoising-based generation strategy. This paradigm naturally enables parallel generation, fine-grained output control, and dynamic perception. These capabilities are previously difficult to achieve with AR models. A growing number of industrial-scale proprietary d(M)LLMs, as well as a large number of open-source academic d(M)LLMs, have demonstrated performance comparable to their autoregressive counterparts, while achieving up to 10$\\times$ acceleration in inference speed. These developments position discrete diffusion models as a promising alternative to intelligence based on the traditional autoregressive approach. In this work, we present a comprehensive overview of the research in the dLLM and dMLLM domains. We trace the historical development of dLLMs and dMLLMs, formalize the underlying mathematical frameworks, list commonly-used modeling methods, and categorize representative models. We further analyze key techniques for training, inference, quantization. We also discuss the trustworthy issues and summarize emerging applications across language, vision-language, and biological domains and etc.. We conclude by discussing future directions for research and deployment. Relative papers are collected in https://github.com/LiQiiiii/Awesome-Discrete-Diffusion-LLM_MLLM",
    "authors": [
      "Runpeng Yu",
      "Qi Li",
      "Xinchao Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-06-16T17:59:08Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13759v5"
  },
  {
    "arxiv_id": "2506.13384v1",
    "entry_id": "http://arxiv.org/abs/2506.13384v1",
    "title": "Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses",
    "summary": "Large language models (LLMs) offer the potential to simulate human-like responses and behaviors, creating new opportunities for psychological science. In the context of self-regulated learning (SRL), if LLMs can reliably simulate survey responses at scale and speed, they could be used to test intervention scenarios, refine theoretical models, augment sparse datasets, and represent hard-to-reach populations. However, the validity of LLM-generated survey responses remains uncertain, with limited research focused on SRL and existing studies beyond SRL yielding mixed results. Therefore, in this study, we examined LLM-generated responses to the 44-item Motivated Strategies for Learning Questionnaire (MSLQ; Pintrich \\& De Groot, 1990), a widely used instrument assessing students' learning strategies and academic motivation. Particularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large. We analyzed item distributions, the psychological network of the theoretical SRL dimensions, and psychometric validity based on the latent factor structure. Our results suggest that Gemini 2 Flash was the most promising LLM, showing considerable sampling variability and producing underlying dimensions and theoretical relationships that align with prior theory and empirical findings. At the same time, we observed discrepancies and limitations, underscoring both the potential and current constraints of using LLMs for simulating psychological survey data and applying it in educational contexts.",
    "authors": [
      "Leonie V. D. E. Vogelsmeier",
      "Eduardo Oliveira",
      "Kamila Misiejuk",
      "Sonsoles López-Pernas",
      "Mohammed Saqr"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "stat.ME",
      "stat.OT"
    ],
    "published": "2025-06-16T11:48:58Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13384v1"
  },
  {
    "arxiv_id": "2506.13324v1",
    "entry_id": "http://arxiv.org/abs/2506.13324v1",
    "title": "Towards Pervasive Distributed Agentic Generative AI -- A State of The Art",
    "summary": "The rapid advancement of intelligent agents and Large Language Models (LLMs) is reshaping the pervasive computing field. Their ability to perceive, reason, and act through natural language understanding enables autonomous problem-solving in complex pervasive environments, including the management of heterogeneous sensors, devices, and data. This survey outlines the architectural components of LLM agents (profiling, memory, planning, and action) and examines their deployment and evaluation across various scenarios. Than it reviews computational and infrastructural advancements (cloud to edge) in pervasive computing and how AI is moving in this field. It highlights state-of-the-art agent deployment strategies and applications, including local and distributed execution on resource-constrained devices. This survey identifies key challenges of these agents in pervasive computing such as architectural, energetic and privacy limitations. It finally proposes what we called \"Agent as a Tool\", a conceptual framework for pervasive agentic AI, emphasizing context awareness, modularity, security, efficiency and effectiveness.",
    "authors": [
      "Gianni Molinari",
      "Fabio Ciravegna"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-06-16T10:15:06Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13324v1"
  },
  {
    "arxiv_id": "2506.13313v1",
    "entry_id": "http://arxiv.org/abs/2506.13313v1",
    "title": "Large Language Models as 'Hidden Persuaders': Fake Product Reviews are Indistinguishable to Humans and Machines",
    "summary": "Reading and evaluating product reviews is central to how most people decide what to buy and consume online. However, the recent emergence of Large Language Models and Generative Artificial Intelligence now means writing fraudulent or fake reviews is potentially easier than ever. Through three studies we demonstrate that (1) humans are no longer able to distinguish between real and fake product reviews generated by machines, averaging only 50.8% accuracy overall - essentially the same that would be expected by chance alone; (2) that LLMs are likewise unable to distinguish between fake and real reviews and perform equivalently bad or even worse than humans; and (3) that humans and LLMs pursue different strategies for evaluating authenticity which lead to equivalently bad accuracy, but different precision, recall and F1 scores - indicating they perform worse at different aspects of judgment. The results reveal that review systems everywhere are now susceptible to mechanised fraud if they do not depend on trustworthy purchase verification to guarantee the authenticity of reviewers. Furthermore, the results provide insight into the consumer psychology of how humans judge authenticity, demonstrating there is an inherent 'scepticism bias' towards positive reviews and a special vulnerability to misjudge the authenticity of fake negative reviews. Additionally, results provide a first insight into the 'machine psychology' of judging fake reviews, revealing that the strategies LLMs take to evaluate authenticity radically differ from humans, in ways that are equally wrong in terms of accuracy, but different in their misjudgments.",
    "authors": [
      "Weiyao Meng",
      "John Harvey",
      "James Goulding",
      "Chris James Carter",
      "Evgeniya Lukinova",
      "Andrew Smith",
      "Paul Frobisher",
      "Mina Forrest",
      "Georgiana Nica-Avram"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "econ.GN"
    ],
    "published": "2025-06-16T09:54:56Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13313v1"
  },
  {
    "arxiv_id": "2506.13245v1",
    "entry_id": "http://arxiv.org/abs/2506.13245v1",
    "title": "A Game-Theoretic Negotiation Framework for Cross-Cultural Consensus in LLMs",
    "summary": "The increasing prevalence of large language models (LLMs) is influencing global value systems. However, these models frequently exhibit a pronounced WEIRD (Western, Educated, Industrialized, Rich, Democratic) cultural bias due to lack of attention to minority values. This monocultural perspective may reinforce dominant values and marginalize diverse cultural viewpoints, posing challenges for the development of equitable and inclusive AI systems. In this work, we introduce a systematic framework designed to boost fair and robust cross-cultural consensus among LLMs. We model consensus as a Nash Equilibrium and employ a game-theoretic negotiation method based on Policy-Space Response Oracles (PSRO) to simulate an organized cross-cultural negotiation process. To evaluate this approach, we construct regional cultural agents using data transformed from the World Values Survey (WVS). Beyond the conventional model-level evaluation method, We further propose two quantitative metrics, Perplexity-based Acceptence and Values Self-Consistency, to assess consensus outcomes. Experimental results indicate that our approach generates consensus of higher quality while ensuring more balanced compromise compared to baselines. Overall, it mitigates WEIRD bias by guiding agents toward convergence through fair and gradual negotiation steps.",
    "authors": [
      "Guoxi Zhang",
      "Jiawei Chen",
      "Tianzhuo Yang",
      "Jiaming Ji",
      "Yaodong Yang",
      "Juntao Dai"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.GT"
    ],
    "published": "2025-06-16T08:42:39Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13245v1"
  },
  {
    "arxiv_id": "2506.13082v3",
    "entry_id": "http://arxiv.org/abs/2506.13082v3",
    "title": "Discerning What Matters: A Multi-Dimensional Assessment of Moral Competence in LLMs",
    "summary": "Moral competence is the ability to act in accordance with moral principles. As large language models (LLMs) are increasingly deployed in situations demanding moral competence, there is increasing interest in evaluating this ability empirically. We review existing literature and identify three significant shortcoming: (i) Over-reliance on prepackaged moral scenarios with explicitly highlighted moral features; (ii) Focus on verdict prediction rather than moral reasoning; and (iii) Inadequate testing of models' (in)ability to recognize when additional information is needed. Grounded in philosophical research on moral skill, we then introduce a novel method for assessing moral competence in LLMs. Our approach moves beyond simple verdict comparisons to evaluate five dimensions of moral competence: identifying morally relevant features, weighting their importance, assigning moral reasons to these features, synthesizing coherent moral judgments, and recognizing information gaps. We conduct two experiments comparing six leading LLMs against non-expert humans and professional philosophers. In our first experiment using ethical vignettes standard to existing work, LLMs generally outperformed non-expert humans across multiple dimensions of moral reasoning. However, our second experiment, featuring novel scenarios designed to test moral sensitivity by embedding relevant features among irrelevant details, revealed a striking reversal: several LLMs performed significantly worse than humans. Our findings suggest that current evaluations may substantially overestimate LLMs' moral reasoning capabilities by eliminating the task of discerning moral relevance from noisy information, which we take to be a prerequisite for genuine moral skill. This work provides a more nuanced framework for assessing AI moral competence and highlights important directions for improving moral competence in advanced AI systems.",
    "authors": [
      "Daniel Kilov",
      "Caroline Hendy",
      "Secil Yanik Guyot",
      "Aaron J. Snoswell",
      "Seth Lazar"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-16T03:59:38Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13082v3"
  },
  {
    "arxiv_id": "2506.13045v4",
    "entry_id": "http://arxiv.org/abs/2506.13045v4",
    "title": "Continual Learning for Generative AI: From LLMs to MLLMs and Beyond",
    "summary": "The rapid advancement of generative models has empowered modern AI systems to comprehend and produce highly sophisticated content, even achieving human-level performance in specific domains. However, these models are fundamentally constrained by \\emph{catastrophic forgetting}, \\ie~a persistent challenge where models experience performance degradation on previously learned tasks when adapting to new tasks. To address this practical limitation, numerous approaches have been proposed to enhance the adaptability and scalability of generative AI in real-world applications. In this work, we present a comprehensive survey of continual learning methods for mainstream generative AI models, encompassing large language models, multimodal large language models, vision-language-action models, and diffusion models. Drawing inspiration from the memory mechanisms of the human brain, we systematically categorize these approaches into three paradigms: architecture-based, regularization-based, and replay-based methods, while elucidating their underlying methodologies and motivations. We further analyze continual learning setups for different generative models, including training objectives, benchmarks, and core backbones, thereby providing deeper insights into the field. The project page of this paper is available at https://github.com/Ghy0501/Awesome-Continual-Learning-in-Generative-Models.",
    "authors": [
      "Haiyang Guo",
      "Fanhu Zeng",
      "Fei Zhu",
      "Jiayi Wang",
      "Xukai Wang",
      "Jingang Zhou",
      "Hongbo Zhao",
      "Wenzhuo Liu",
      "Shijie Ma",
      "Da-Han Wang",
      "Xu-Yao Zhang",
      "Cheng-Lin Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-06-16T02:27:25Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13045v4"
  },
  {
    "arxiv_id": "2506.13036v1",
    "entry_id": "http://arxiv.org/abs/2506.13036v1",
    "title": "Forecast-Then-Optimize Deep Learning Methods",
    "summary": "Time series forecasting underpins vital decision-making across various sectors, yet raw predictions from sophisticated models often harbor systematic errors and biases. We examine the Forecast-Then-Optimize (FTO) framework, pioneering its systematic synopsis. Unlike conventional Predict-Then-Optimize (PTO) methods, FTO explicitly refines forecasts through optimization techniques such as ensemble methods, meta-learners, and uncertainty adjustments. Furthermore, deep learning and large language models have established superiority over traditional parametric forecasting models for most enterprise applications. This paper surveys significant advancements from 2016 to 2025, analyzing mainstream deep learning FTO architectures. Focusing on real-world applications in operations management, we demonstrate FTO's crucial role in enhancing predictive accuracy, robustness, and decision efficacy. Our study establishes foundational guidelines for future forecasting methodologies, bridging theory and operational practicality.",
    "authors": [
      "Jinhang Jiang",
      "Nan Wu",
      "Ben Liu",
      "Mei Feng",
      "Xin Ji",
      "Karthik Srinivasan"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-06-16T02:02:30Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13036v1"
  },
  {
    "arxiv_id": "2506.12990v1",
    "entry_id": "http://arxiv.org/abs/2506.12990v1",
    "title": "Humans, Machine Learning, and Language Models in Union: A Cognitive Study on Table Unionability",
    "summary": "Data discovery and table unionability in particular became key tasks in modern Data Science. However, the human perspective for these tasks is still under-explored. Thus, this research investigates the human behavior in determining table unionability within data discovery. We have designed an experimental survey and conducted a comprehensive analysis, in which we assess human decision-making for table unionability. We use the observations from the analysis to develop a machine learning framework to boost the (raw) performance of humans. Furthermore, we perform a preliminary study on how LLM performance is compared to humans indicating that it is typically better to consider a combination of both. We believe that this work lays the foundations for developing future Human-in-the-Loop systems for efficient data discovery.",
    "authors": [
      "Sreeram Marimuthu",
      "Nina Klimenkova",
      "Roee Shraga"
    ],
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-06-15T23:13:20Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12990v1"
  },
  {
    "arxiv_id": "2506.12958v2",
    "entry_id": "http://arxiv.org/abs/2506.12958v2",
    "title": "Domain Specific Benchmarks for Evaluating Multimodal Large Language Models",
    "summary": "Large language models (LLMs) are increasingly being deployed across disciplines due to their advanced reasoning and problem solving capabilities. To measure their effectiveness, various benchmarks have been developed that measure aspects of LLM reasoning, comprehension, and problem-solving. While several surveys address LLM evaluation and benchmarks, a domain-specific analysis remains underexplored in the literature. This paper introduces a taxonomy of seven key disciplines, encompassing various domains and application areas where LLMs are extensively utilized. Additionally, we provide a comprehensive review of LLM benchmarks and survey papers within each domain, highlighting the unique capabilities of LLMs and the challenges faced in their application. Finally, we compile and categorize these benchmarks by domain to create an accessible resource for researchers, aiming to pave the way for advancements toward artificial general intelligence (AGI)",
    "authors": [
      "Khizar Anjum",
      "Muhammad Arbab Arshad",
      "Kadhim Hayawi",
      "Efstathios Polyzos",
      "Asadullah Tariq",
      "Mohamed Adel Serhani",
      "Laiba Batool",
      "Brady Lund",
      "Nishith Reddy Mannuru",
      "Ravi Varma Kumar Bevara",
      "Taslim Mahbub",
      "Muhammad Zeeshan Akram",
      "Sakib Shahriar"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-06-15T20:42:45Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12958v2"
  },
  {
    "arxiv_id": "2506.12894v1",
    "entry_id": "http://arxiv.org/abs/2506.12894v1",
    "title": "Homeostatic Coupling for Prosocial Behavior",
    "summary": "When regarding the suffering of others, we often experience personal distress and feel compelled to help\\footnote{Preprint. Under review.}. Inspired by living systems, we investigate the emergence of prosocial behavior among autonomous agents that are motivated by homeostatic self-regulation. We perform multi-agent reinforcement learning, treating each agent as a vulnerable homeostat charged with maintaining its own well-being. We introduce an empathy-like mechanism to share homeostatic states between agents: an agent can either \\emph{observe} their partner's internal state ({\\bf cognitive empathy}) or the agent's internal state can be \\emph{directly coupled} to that of their partner ({\\bf affective empathy}). In three simple multi-agent environments, we show that prosocial behavior arises only under homeostatic coupling - when the distress of a partner can affect one's own well-being. Additionally, we show that empathy can be learned: agents can ``decode\" their partner's external emotive states to infer the partner's internal homeostatic states. Assuming some level of physiological similarity, agents reference their own emotion-generation functions to invert the mapping from outward display to internal state. Overall, we demonstrate the emergence of prosocial behavior when homeostatic agents learn to ``read\" the emotions of others and then to empathize, or feel as they feel.",
    "authors": [
      "Naoto Yoshida",
      "Kingson Man"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-06-15T15:49:21Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12894v1"
  },
  {
    "arxiv_id": "2506.12689v2",
    "entry_id": "http://arxiv.org/abs/2506.12689v2",
    "title": "SciSage: A Multi-Agent Framework for High-Quality Scientific Survey Generation",
    "summary": "The rapid growth of scientific literature demands robust tools for automated survey-generation. However, current large language model (LLM)-based methods often lack in-depth analysis, structural coherence, and reliable citations. To address these limitations, we introduce SciSage, a multi-agent framework employing a reflect-when-you-write paradigm. SciSage features a hierarchical Reflector agent that critically evaluates drafts at outline, section, and document levels, collaborating with specialized agents for query interpretation, content retrieval, and refinement. We also release SurveyScope, a rigorously curated benchmark of 46 high-impact papers (2020-2025) across 11 computer science domains, with strict recency and citation-based quality controls. Evaluations demonstrate that SciSage outperforms state-of-the-art baselines (LLM x MapReduce-V2, AutoSurvey), achieving +1.73 points in document coherence and +32% in citation F1 scores. Human evaluations reveal mixed outcomes (3 wins vs. 7 losses against human-written surveys), but highlight SciSage's strengths in topical breadth and retrieval efficiency. Overall, SciSage offers a promising foundation for research-assistive writing tools.",
    "authors": [
      "Xiaofeng Shi",
      "Qian Kou",
      "Yuduo Li",
      "Ning Tang",
      "Jinxin Xie",
      "Longbin Yu",
      "Songjing Wang",
      "Hua Zhou"
    ],
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-06-15T02:23:47Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12689v2"
  },
  {
    "arxiv_id": "2506.17285v1",
    "entry_id": "http://arxiv.org/abs/2506.17285v1",
    "title": "A Framework for Generating Conversational Recommendation Datasets from Behavioral Interactions",
    "summary": "Modern recommendation systems typically follow two complementary paradigms: collaborative filtering, which models long-term user preferences from historical interactions, and conversational recommendation systems (CRS), which interact with users in natural language to uncover immediate needs. Each captures a different dimension of user intent. While CRS models lack collaborative signals, leading to generic or poorly personalized suggestions, traditional recommenders lack mechanisms to interactively elicit immediate needs. Unifying these paradigms promises richer personalization but remains challenging due to the lack of large-scale conversational datasets grounded in real user behavior. We present ConvRecStudio, a framework that uses large language models (LLMs) to simulate realistic, multi-turn dialogs grounded in timestamped user-item interactions and reviews. ConvRecStudio follows a three-stage pipeline: (1) Temporal Profiling, which constructs user profiles and community-level item sentiment trajectories over fine-grained aspects; (2) Semantic Dialog Planning, which generates a structured plan using a DAG of flexible super-nodes; and (3) Multi-Turn Simulation, which instantiates the plan using paired LLM agents for the user and system, constrained by executional and behavioral fidelity checks. We apply ConvRecStudio to three domains -- MobileRec, Yelp, and Amazon Electronics -- producing over 12K multi-turn dialogs per dataset. Human and automatic evaluations confirm the naturalness, coherence, and behavioral grounding of the generated conversations. To demonstrate utility, we build a cross-attention transformer model that jointly encodes user history and dialog context, achieving gains in Hit@K and NDCG@K over baselines using either signal alone or naive fusion. Notably, our model achieves a 10.9% improvement in Hit@1 on Yelp over the strongest baseline.",
    "authors": [
      "Vinaik Chhetri",
      "Yousaf Reza",
      "Moghis Fereidouni",
      "Srijata Maji",
      "Umar Farooq",
      "AB Siddique"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-06-14T22:58:48Z",
    "pdf_url": "https://arxiv.org/pdf/2506.17285v1"
  },
  {
    "arxiv_id": "2506.12594v1",
    "entry_id": "http://arxiv.org/abs/2506.12594v1",
    "title": "A Comprehensive Survey of Deep Research: Systems, Methodologies, and Applications",
    "summary": "This survey examines the rapidly evolving field of Deep Research systems -- AI-powered applications that automate complex research workflows through the integration of large language models, advanced information retrieval, and autonomous reasoning capabilities. We analyze more than 80 commercial and non-commercial implementations that have emerged since 2023, including OpenAI/Deep Research, Gemini/Deep Research, Perplexity/Deep Research, and numerous open-source alternatives. Through comprehensive examination, we propose a novel hierarchical taxonomy that categorizes systems according to four fundamental technical dimensions: foundation models and reasoning engines, tool utilization and environmental interaction, task planning and execution control, and knowledge synthesis and output generation. We explore the architectural patterns, implementation approaches, and domain-specific adaptations that characterize these systems across academic, scientific, business, and educational applications. Our analysis reveals both the significant capabilities of current implementations and the technical and ethical challenges they present regarding information accuracy, privacy, intellectual property, and accessibility. The survey concludes by identifying promising research directions in advanced reasoning architectures, multimodal integration, domain specialization, human-AI collaboration, and ecosystem standardization that will likely shape the future evolution of this transformative technology. By providing a comprehensive framework for understanding Deep Research systems, this survey contributes to both the theoretical understanding of AI-augmented knowledge work and the practical development of more capable, responsible, and accessible research technologies. The paper resources can be viewed at https://github.com/scienceaix/deepresearch.",
    "authors": [
      "Renjun Xu",
      "Jingwen Peng"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-06-14T18:19:05Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12594v1"
  },
  {
    "arxiv_id": "2506.12576v2",
    "entry_id": "http://arxiv.org/abs/2506.12576v2",
    "title": "Enabling Precise Topic Alignment in Large Language Models Via Sparse Autoencoders",
    "summary": "Recent work shows that Sparse Autoencoders (SAE) applied to large language model (LLM) layers have neurons corresponding to interpretable concepts. These SAE neurons can be modified to align generated outputs, but only towards pre-identified topics and with some parameter tuning. Our approach leverages the observational and modification properties of SAEs to enable alignment for any topic. This method 1) scores each SAE neuron by its semantic similarity to an alignment text and uses them to 2) modify SAE-layer-level outputs by emphasizing topic-aligned neurons. We assess the alignment capabilities of this approach on diverse public topic datasets including Amazon reviews, Medicine, and Sycophancy, across the currently available open-source LLMs and SAE pairs (GPT2 and Gemma) with multiple SAEs configurations. Experiments aligning to medical prompts reveal several benefits over fine-tuning, including increased average language acceptability (0.25 vs. 0.5), reduced training time across multiple alignment topics (333.6s vs. 62s), and acceptable inference time for many applications (+0.00092s/token). Our open-source code is available at github.com/IBM/sae-steering.",
    "authors": [
      "Ananya Joshi",
      "Celia Cintas",
      "Skyler Speakman"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-14T17:11:48Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12576v2"
  },
  {
    "arxiv_id": "2506.12385v1",
    "entry_id": "http://arxiv.org/abs/2506.12385v1",
    "title": "Recent Advances and Future Directions in Literature-Based Discovery",
    "summary": "The explosive growth of scientific publications has created an urgent need for automated methods that facilitate knowledge synthesis and hypothesis generation. Literature-based discovery (LBD) addresses this challenge by uncovering previously unknown associations between disparate domains. This article surveys recent methodological advances in LBD, focusing on developments from 2000 to the present. We review progress in three key areas: knowledge graph construction, deep learning approaches, and the integration of pre-trained and large language models (LLMs). While LBD has made notable progress, several fundamental challenges remain unresolved, particularly concerning scalability, reliance on structured data, and the need for extensive manual curation. By examining ongoing advances and outlining promising future directions, this survey underscores the transformative role of LLMs in enhancing LBD and aims to support researchers and practitioners in harnessing these technologies to accelerate scientific innovation.",
    "authors": [
      "Andrej Kastrin",
      "Bojan Cestnik",
      "Nada Lavrač"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-14T07:47:13Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12385v1"
  },
  {
    "arxiv_id": "2506.12317v1",
    "entry_id": "http://arxiv.org/abs/2506.12317v1",
    "title": "The Budget AI Researcher and the Power of RAG Chains",
    "summary": "Navigating the vast and rapidly growing body of scientific literature is a formidable challenge for aspiring researchers. Current approaches to supporting research idea generation often rely on generic large language models (LLMs). While LLMs are effective at aiding comprehension and summarization, they often fall short in guiding users toward practical research ideas due to their limitations. In this study, we present a novel structural framework for research ideation. Our framework, The Budget AI Researcher, uses retrieval-augmented generation (RAG) chains, vector databases, and topic-guided pairing to recombine concepts from hundreds of machine learning papers. The system ingests papers from nine major AI conferences, which collectively span the vast subfields of machine learning, and organizes them into a hierarchical topic tree. It uses the tree to identify distant topic pairs, generate novel research abstracts, and refine them through iterative self-evaluation against relevant literature and peer reviews, generating and refining abstracts that are both grounded in real-world research and demonstrably interesting. Experiments using LLM-based metrics indicate that our method significantly improves the concreteness of generated research ideas relative to standard prompting approaches. Human evaluations further demonstrate a substantial enhancement in the perceived interestingness of the outputs. By bridging the gap between academic data and creative generation, the Budget AI Researcher offers a practical, free tool for accelerating scientific discovery and lowering the barrier for aspiring researchers. Beyond research ideation, this approach inspires solutions to the broader challenge of generating personalized, context-aware outputs grounded in evolving real-world knowledge.",
    "authors": [
      "Franklin Lee",
      "Tengfei Ma"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-14T02:40:35Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12317v1"
  },
  {
    "arxiv_id": "2506.14831v1",
    "entry_id": "http://arxiv.org/abs/2506.14831v1",
    "title": "Recent Advances in Multi-Agent Human Trajectory Prediction: A Comprehensive Review",
    "summary": "With the emergence of powerful data-driven methods in human trajectory prediction (HTP), gaining a finer understanding of multi-agent interactions lies within hand's reach, with important implications in areas such as autonomous navigation and crowd modeling. This survey reviews some of the most recent advancements in deep learning-based multi-agent trajectory prediction, focusing on studies published between 2020 and 2024. We categorize the existing methods based on their architectural design, their input representations, and their overall prediction strategies, placing a particular emphasis on models evaluated using the ETH/UCY benchmark. Furthermore, we highlight key challenges and future research directions in the field of multi-agent HTP.",
    "authors": [
      "Céline Finet",
      "Stephane Da Silva Martins",
      "Jean-Bernard Hayet",
      "Ioannis Karamouzas",
      "Javad Amirian",
      "Sylvie Le Hégarat-Mascle",
      "Julien Pettré",
      "Emanuel Aldea"
    ],
    "categories": [
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2025-06-13T23:03:43Z",
    "pdf_url": "https://arxiv.org/pdf/2506.14831v1"
  },
  {
    "arxiv_id": "2506.12195v1",
    "entry_id": "http://arxiv.org/abs/2506.12195v1",
    "title": "OSI Stack Redesign for Quantum Networks: Requirements, Technologies, Challenges, and Future Directions",
    "summary": "Quantum communication is poised to become a foundational element of next-generation networking, offering transformative capabilities in security, entanglement-based connectivity, and computational offloading. However, the classical OSI model-designed for deterministic and error-tolerant systems-cannot support quantum-specific phenomena such as coherence fragility, probabilistic entanglement, and the no-cloning theorem. This paper provides a comprehensive survey and proposes an architectural redesign of the OSI model for quantum networks in the context of 7G. We introduce a Quantum-Converged OSI stack by extending the classical model with Layer 0 (Quantum Substrate) and Layer 8 (Cognitive Intent), supporting entanglement, teleportation, and semantic orchestration via LLMs and QML. Each layer is redefined to incorporate quantum mechanisms such as enhanced MAC protocols, fidelity-aware routing, and twin-based applications. This survey consolidates over 150 research works from IEEE, ACM, MDPI, arXiv, and Web of Science (2018-2025), classifying them by OSI layer, enabling technologies such as QKD, QEC, PQC, and RIS, and use cases such as satellite QKD, UAV swarms, and quantum IoT. A taxonomy of cross-layer enablers-such as hybrid quantum-classical control, metadata-driven orchestration, and blockchain-integrated quantum trust-is provided, along with simulation tools including NetSquid, QuNetSim, and QuISP. We present several domain-specific applications, including quantum healthcare telemetry, entangled vehicular networks, and satellite mesh overlays. An evaluation framework is proposed based on entropy throughput, coherence latency, and entanglement fidelity. Key future directions include programmable quantum stacks, digital twins, and AI-defined QNet agents, laying the groundwork for a scalable, intelligent, and quantum-compliant OSI framework for 7G and beyond.",
    "authors": [
      "Shakil Ahmed",
      "Muhammad Kamran Saeed",
      "Ashfaq Khokhar"
    ],
    "categories": [
      "quant-ph",
      "cs.CR",
      "cs.IT",
      "cs.LG",
      "cs.NI"
    ],
    "published": "2025-06-13T19:48:18Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12195v1"
  },
  {
    "arxiv_id": "2506.11687v1",
    "entry_id": "http://arxiv.org/abs/2506.11687v1",
    "title": "Differential Privacy in Machine Learning: From Symbolic AI to LLMs",
    "summary": "Machine learning models should not reveal particular information that is not otherwise accessible. Differential privacy provides a formal framework to mitigate privacy risks by ensuring that the inclusion or exclusion of any single data point does not significantly alter the output of an algorithm, thus limiting the exposure of private information. This survey paper explores the foundational definitions of differential privacy, reviews its original formulations and tracing its evolution through key research contributions. It then provides an in-depth examination of how DP has been integrated into machine learning models, analyzing existing proposals and methods to preserve privacy when training ML models. Finally, it describes how DP-based ML techniques can be evaluated in practice. %Finally, it discusses the broader implications of DP, highlighting its potential for public benefit, its real-world applications, and the challenges it faces, including vulnerabilities to adversarial attacks. By offering a comprehensive overview of differential privacy in machine learning, this work aims to contribute to the ongoing development of secure and responsible AI systems.",
    "authors": [
      "Francisco Aguilera-Martínez",
      "Fernando Berzal"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2025-06-13T11:30:35Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11687v1"
  },
  {
    "arxiv_id": "2506.11613v1",
    "entry_id": "http://arxiv.org/abs/2506.11613v1",
    "title": "Model Organisms for Emergent Misalignment",
    "summary": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication revealed this was highly unexpected, demonstrating critical gaps in our understanding of model alignment. In this work, we both advance understanding and provide tools for future research. Using new narrowly misaligned datasets, we create a set of improved model organisms that achieve 99% coherence (vs. 67% prior), work with smaller 0.5B parameter models (vs. 32B), and that induce misalignment using a single rank-1 LoRA adapter. We demonstrate that EM occurs robustly across diverse model sizes, three model families, and numerous training protocols including full supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a mechanistic phase transition and demonstrate that it corresponds to a robust behavioural phase transition in all studied organisms. Aligning large language models is critical for frontier AI safety, yet EM exposes how far we are from achieving this robustly. By distilling clean model organisms that isolate a minimal alignment-compromising change, and where this is learnt, we establish a foundation for future research into understanding and mitigating alignment risks in LLMs.",
    "authors": [
      "Edward Turner",
      "Anna Soligo",
      "Mia Taylor",
      "Senthooran Rajamanoharan",
      "Neel Nanda"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-06-13T09:34:25Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11613v1"
  },
  {
    "arxiv_id": "2506.11526v1",
    "entry_id": "http://arxiv.org/abs/2506.11526v1",
    "title": "Foundation Models in Autonomous Driving: A Survey on Scenario Generation and Scenario Analysis",
    "summary": "For autonomous vehicles, safe navigation in complex environments depends on handling a broad range of diverse and rare driving scenarios. Simulation- and scenario-based testing have emerged as key approaches to development and validation of autonomous driving systems. Traditional scenario generation relies on rule-based systems, knowledge-driven models, and data-driven synthesis, often producing limited diversity and unrealistic safety-critical cases. With the emergence of foundation models, which represent a new generation of pre-trained, general-purpose AI models, developers can process heterogeneous inputs (e.g., natural language, sensor data, HD maps, and control actions), enabling the synthesis and interpretation of complex driving scenarios. In this paper, we conduct a survey about the application of foundation models for scenario generation and scenario analysis in autonomous driving (as of May 2025). Our survey presents a unified taxonomy that includes large language models, vision-language models, multimodal large language models, diffusion models, and world models for the generation and analysis of autonomous driving scenarios. In addition, we review the methodologies, open-source datasets, simulation platforms, and benchmark challenges, and we examine the evaluation metrics tailored explicitly to scenario generation and analysis. Finally, the survey concludes by highlighting the open challenges and research questions, and outlining promising future research directions. All reviewed papers are listed in a continuously maintained repository, which contains supplementary materials and is available at https://github.com/TUM-AVS/FM-for-Scenario-Generation-Analysis.",
    "authors": [
      "Yuan Gao",
      "Mattia Piccinini",
      "Yuchen Zhang",
      "Dingrui Wang",
      "Korbinian Moller",
      "Roberto Brusnicki",
      "Baha Zarrouki",
      "Alessio Gambi",
      "Jan Frederik Totz",
      "Kai Storms",
      "Steven Peters",
      "Andrea Stocco",
      "Bassam Alrifaee",
      "Marco Pavone",
      "Johannes Betz"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-06-13T07:25:59Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11526v1"
  },
  {
    "arxiv_id": "2506.11521v1",
    "entry_id": "http://arxiv.org/abs/2506.11521v1",
    "title": "Investigating Vulnerabilities and Defenses Against Audio-Visual Attacks: A Comprehensive Survey Emphasizing Multimodal Models",
    "summary": "Multimodal large language models (MLLMs), which bridge the gap between audio-visual and natural language processing, achieve state-of-the-art performance on several audio-visual tasks. Despite the superior performance of MLLMs, the scarcity of high-quality audio-visual training data and computational resources necessitates the utilization of third-party data and open-source MLLMs, a trend that is increasingly observed in contemporary research. This prosperity masks significant security risks. Empirical studies demonstrate that the latest MLLMs can be manipulated to produce malicious or harmful content. This manipulation is facilitated exclusively through instructions or inputs, including adversarial perturbations and malevolent queries, effectively bypassing the internal security mechanisms embedded within the models. To gain a deeper comprehension of the inherent security vulnerabilities associated with audio-visual-based multimodal models, a series of surveys investigates various types of attacks, including adversarial and backdoor attacks. While existing surveys on audio-visual attacks provide a comprehensive overview, they are limited to specific types of attacks, which lack a unified review of various types of attacks. To address this issue and gain insights into the latest trends in the field, this paper presents a comprehensive and systematic review of audio-visual attacks, which include adversarial attacks, backdoor attacks, and jailbreak attacks. Furthermore, this paper also reviews various types of attacks in the latest audio-visual-based MLLMs, a dimension notably absent in existing surveys. Drawing upon comprehensive insights from a substantial review, this paper delineates both challenges and emergent trends for future research on audio-visual attacks and defense.",
    "authors": [
      "Jinming Wen",
      "Xinyi Wu",
      "Shuai Zhao",
      "Yanhao Jia",
      "Yuwen Li"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MM"
    ],
    "published": "2025-06-13T07:22:36Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11521v1"
  },
  {
    "arxiv_id": "2506.11475v2",
    "entry_id": "http://arxiv.org/abs/2506.11475v2",
    "title": "AutoGen Driven Multi Agent Framework for Iterative Crime Data Analysis and Prediction",
    "summary": "This paper introduces LUCID-MA (Learning and Understanding Crime through Dialogue of Multiple Agents), an innovative AI powered framework where multiple AI agents collaboratively analyze and understand crime data. Our system that consists of three core components: an analysis assistant that highlights spatiotemporal crime patterns; a feedback component that reviews and refines analytical results; and a prediction component that forecasts future crime trends. With a well-designed prompt and the LLaMA-2-13B-Chat-GPTQ model, it runs completely offline and allows the agents undergo self-improvement through 100 rounds of communication with less human interaction. A scoring function is incorporated to evaluate agent performance, providing visual plots to track learning progress. This work demonstrates the potential of AutoGen-style agents for autonomous, scalable, and iterative analysis in social science domains, maintaining data privacy through offline execution. It also showcases a computational model with emergent intelligence, where the system's global behavior emerges from the interactions of its agents. This emergent behavior manifests as enhanced individual agent performance, driven by collaborative dialogue between the LLM-based agents.",
    "authors": [
      "Syeda Kisaa Fatima",
      "Tehreem Zubair",
      "Noman Ahmed",
      "Asifullah Khan"
    ],
    "categories": [
      "cs.MA",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-06-13T05:39:28Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11475v2"
  },
  {
    "arxiv_id": "2506.13798v1",
    "entry_id": "http://arxiv.org/abs/2506.13798v1",
    "title": "Contemporary AI foundation models increase biological weapons risk",
    "summary": "The rapid advancement of artificial intelligence has raised concerns about its potential to facilitate biological weapons development. We argue existing safety assessments of contemporary foundation AI models underestimate this risk, largely due to flawed assumptions and inadequate evaluation methods. First, assessments mistakenly assume biological weapons development requires tacit knowledge, or skills gained through hands-on experience that cannot be easily verbalized. Second, they rely on imperfect benchmarks that overlook how AI can uplift both nonexperts and already-skilled individuals. To challenge the tacit knowledge assumption, we examine cases where individuals without formal expertise, including a 2011 Norwegian ultranationalist who synthesized explosives, successfully carried out complex technical tasks. We also review efforts to document pathogen construction processes, highlighting how such tasks can be conveyed in text. We identify \"elements of success\" for biological weapons development that large language models can describe in words, including steps such as acquiring materials and performing technical procedures. Applying this framework, we find that advanced AI models Llama 3.1 405B, ChatGPT-4o, and Claude 3.5 Sonnet can accurately guide users through the recovery of live poliovirus from commercially obtained synthetic DNA, challenging recent claims that current models pose minimal biosecurity risk. We advocate for improved benchmarks, while acknowledging the window for meaningful implementation may have already closed.",
    "authors": [
      "Roger Brent",
      "T. Greg McKelvey"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-06-12T19:53:38Z",
    "pdf_url": "https://arxiv.org/pdf/2506.13798v1"
  },
  {
    "arxiv_id": "2506.10960v3",
    "entry_id": "http://arxiv.org/abs/2506.10960v3",
    "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
    "summary": "Large language models (LLMs) have been increasingly applied to automated harmful content detection tasks, assisting moderators in identifying policy violations and improving the overall efficiency and accuracy of content review. However, existing resources for harmful content detection are predominantly focused on English, with Chinese datasets remaining scarce and often limited in scope. We present a comprehensive, professionally annotated benchmark for Chinese content harm detection, which covers six representative categories and is constructed entirely from real-world data. Our annotation process further yields a knowledge rule base that provides explicit expert knowledge to assist LLMs in Chinese harmful content detection. In addition, we propose a knowledge-augmented baseline that integrates both human-annotated knowledge rules and implicit knowledge from large language models, enabling smaller models to achieve performance comparable to state-of-the-art LLMs. Code and data are available at https://github.com/zjunlp/ChineseHarm-bench.",
    "authors": [
      "Kangwei Liu",
      "Siyuan Cheng",
      "Bozhong Tian",
      "Xiaozhuan Liang",
      "Yuyang Yin",
      "Meng Han",
      "Ningyu Zhang",
      "Bryan Hooi",
      "Xi Chen",
      "Shumin Deng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-06-12T17:57:05Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10960v3"
  },
  {
    "arxiv_id": "2506.10927v1",
    "entry_id": "http://arxiv.org/abs/2506.10927v1",
    "title": "The Role of Generative AI in Facilitating Social Interactions: A Scoping Review",
    "summary": "Reduced social connectedness increasingly poses a threat to mental health, life expectancy, and general well-being. Generative AI (GAI) technologies, such as large language models (LLMs) and image generation tools, are increasingly integrated into applications aimed at enhancing human social experiences. Despite their growing presence, little is known about how these technologies influence social interactions. This scoping review investigates how GAI-based applications are currently designed to facilitate social interaction, what forms of social engagement they target, and which design and evaluation methodologies designers use to create and evaluate them. Through an analysis of 30 studies published since 2020, we identify key trends in application domains including storytelling, socio-emotional skills training, reminiscence, collaborative learning, music making, and general conversation. We highlight the role of participatory and co-design approaches in fostering both effective technology use and social engagement, while also examining socio-ethical concerns such as cultural bias and accessibility. This review underscores the potential of GAI to support dynamic and personalized interactions, but calls for greater attention to equitable design practices and inclusive evaluation strategies.",
    "authors": [
      "T. T. J. E. Arets",
      "G. Perugia",
      "M. Houben",
      "W. A. IJsselsteijn"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-06-12T17:37:19Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10927v1"
  },
  {
    "arxiv_id": "2506.10857v2",
    "entry_id": "http://arxiv.org/abs/2506.10857v2",
    "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
    "summary": "We present VRBench, the first long narrative video benchmark crafted for evaluating large models' multi-step reasoning capabilities, addressing limitations in existing evaluations that overlook temporal reasoning and procedural validity. It comprises 960 long videos (with an average duration of 1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs and 25,106 reasoning steps with timestamps. These videos are curated via a multi-stage filtering process including expert inter-rater reviewing to prioritize plot coherence. We develop a human-AI collaborative framework that generates coherent reasoning chains, each requiring multiple temporally grounded steps, spanning seven types (e.g., event attribution, implicit inference). VRBench designs a multi-phase evaluation pipeline that assesses models at both the outcome and process levels. Apart from the MCQs for the final results, we propose a progress-level LLM-guided scoring metric to evaluate the quality of the reasoning chain from multiple dimensions comprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on VRBench, we undertake a thorough analysis and provide valuable insights that advance the field of multi-step reasoning.",
    "authors": [
      "Jiashuo Yu",
      "Yue Wu",
      "Meng Chu",
      "Zhifei Ren",
      "Zizheng Huang",
      "Pei Chu",
      "Ruijie Zhang",
      "Yinan He",
      "Qirui Li",
      "Songze Li",
      "Zhenxiang Li",
      "Zhongying Tu",
      "Conghui He",
      "Yu Qiao",
      "Yali Wang",
      "Yi Wang",
      "Limin Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "published": "2025-06-12T16:17:17Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10857v2"
  },
  {
    "arxiv_id": "2506.10825v1",
    "entry_id": "http://arxiv.org/abs/2506.10825v1",
    "title": "Generalist Models in Medical Image Segmentation: A Survey and Performance Comparison with Task-Specific Approaches",
    "summary": "Following the successful paradigm shift of large language models, leveraging pre-training on a massive corpus of data and fine-tuning on different downstream tasks, generalist models have made their foray into computer vision. The introduction of Segment Anything Model (SAM) set a milestone on segmentation of natural images, inspiring the design of a multitude of architectures for medical image segmentation. In this survey we offer a comprehensive and in-depth investigation on generalist models for medical image segmentation. We start with an introduction on the fundamentals concepts underpinning their development. Then, we provide a taxonomy on the different declinations of SAM in terms of zero-shot, few-shot, fine-tuning, adapters, on the recent SAM 2, on other innovative models trained on images alone, and others trained on both text and images. We thoroughly analyze their performances at the level of both primary research and best-in-literature, followed by a rigorous comparison with the state-of-the-art task-specific models. We emphasize the need to address challenges in terms of compliance with regulatory frameworks, privacy and security laws, budget, and trustworthy artificial intelligence (AI). Finally, we share our perspective on future directions concerning synthetic data, early fusion, lessons learnt from generalist models in natural language processing, agentic AI and physical AI, and clinical translation.",
    "authors": [
      "Andrea Moglia",
      "Matteo Leccardi",
      "Matteo Cavicchioli",
      "Alice Maccarini",
      "Marco Marcon",
      "Luca Mainardi",
      "Pietro Cerveri"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-06-12T15:44:49Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10825v1"
  },
  {
    "arxiv_id": "2506.10785v1",
    "entry_id": "http://arxiv.org/abs/2506.10785v1",
    "title": "What Users Value and Critique: Large-Scale Analysis of User Feedback on AI-Powered Mobile Apps",
    "summary": "Artificial Intelligence (AI)-powered features have rapidly proliferated across mobile apps in various domains, including productivity, education, entertainment, and creativity. However, how users perceive, evaluate, and critique these AI features remains largely unexplored, primarily due to the overwhelming volume of user feedback. In this work, we present the first comprehensive, large-scale study of user feedback on AI-powered mobile apps, leveraging a curated dataset of 292 AI-driven apps across 14 categories with 894K AI-specific reviews from Google Play. We develop and validate a multi-stage analysis pipeline that begins with a human-labeled benchmark and systematically evaluates large language models (LLMs) and prompting strategies. Each stage, including review classification, aspect-sentiment extraction, and clustering, is validated for accuracy and consistency. Our pipeline enables scalable, high-precision analysis of user feedback, extracting over one million aspect-sentiment pairs clustered into 18 positive and 15 negative user topics. Our analysis reveals that users consistently focus on a narrow set of themes: positive comments emphasize productivity, reliability, and personalized assistance, while negative feedback highlights technical failures (e.g., scanning and recognition), pricing concerns, and limitations in language support. Our pipeline surfaces both satisfaction with one feature and frustration with another within the same review. These fine-grained, co-occurring sentiments are often missed by traditional approaches that treat positive and negative feedback in isolation or rely on coarse-grained analysis. To this end, our approach provides a more faithful reflection of the real-world user experiences with AI-powered apps. Category-aware analysis further uncovers both universal drivers of satisfaction and domain-specific frustrations.",
    "authors": [
      "Vinaik Chhetri",
      "Krishna Upadhyay",
      "A. B. Siddique",
      "Umar Farooq"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-06-12T14:56:52Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10785v1"
  },
  {
    "arxiv_id": "2506.10540v2",
    "entry_id": "http://arxiv.org/abs/2506.10540v2",
    "title": "AniMaker: Multi-Agent Animated Storytelling with MCTS-Driven Clip Generation",
    "summary": "Despite rapid advancements in video generation models, generating coherent storytelling videos that span multiple scenes and characters remains challenging. Current methods often rigidly convert pre-generated keyframes into fixed-length clips, resulting in disjointed narratives and pacing issues. Furthermore, the inherent instability of video generation models means that even a single low-quality clip can significantly degrade the entire output animation's logical coherence and visual continuity. To overcome these obstacles, we introduce AniMaker, a multi-agent framework enabling efficient multi-candidate clip generation and storytelling-aware clip selection, thus creating globally consistent and story-coherent animation solely from text input. The framework is structured around specialized agents, including the Director Agent for storyboard generation, the Photography Agent for video clip generation, the Reviewer Agent for evaluation, and the Post-Production Agent for editing and voiceover. Central to AniMaker's approach are two key technical components: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search (MCTS)-inspired strategy that intelligently navigates the candidate space to generate high-potential clips while optimizing resource usage; and AniEval in Reviewer Agent, the first framework specifically designed for multi-shot animation evaluation, which assesses critical aspects such as story-level consistency, action completion, and animation-specific features by considering each clip in the context of its preceding and succeeding clips. Experiments demonstrate that AniMaker achieves superior quality as measured by popular metrics including VBench and our proposed AniEval framework, while significantly improving the efficiency of multi-candidate generation, pushing AI-generated storytelling animation closer to production standards.",
    "authors": [
      "Haoyuan Shi",
      "Yunxin Li",
      "Xinyu Chen",
      "Longyue Wang",
      "Baotian Hu",
      "Min Zhang"
    ],
    "categories": [
      "cs.MA",
      "cs.CV"
    ],
    "published": "2025-06-12T10:06:21Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10540v2"
  },
  {
    "arxiv_id": "2506.10408v1",
    "entry_id": "http://arxiv.org/abs/2506.10408v1",
    "title": "Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges",
    "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful framework to overcome the knowledge limitations of Large Language Models (LLMs) by integrating external retrieval with language generation. While early RAG systems based on static pipelines have shown effectiveness in well-structured tasks, they struggle in real-world scenarios requiring complex reasoning, dynamic retrieval, and multi-modal integration. To address these challenges, the field has shifted toward Reasoning Agentic RAG, a paradigm that embeds decision-making and adaptive tool use directly into the retrieval process. In this paper, we present a comprehensive review of Reasoning Agentic RAG methods, categorizing them into two primary systems: predefined reasoning, which follows fixed modular pipelines to boost reasoning, and agentic reasoning, where the model autonomously orchestrates tool interaction during inference. We analyze representative techniques under both paradigms, covering architectural design, reasoning strategies, and tool coordination. Finally, we discuss key research challenges and propose future directions to advance the flexibility, robustness, and applicability of reasoning agentic RAG systems. Our collection of the relevant research has been organized into a https://github.com/ByebyeMonica/Reasoning-Agentic-RAG.",
    "authors": [
      "Jintao Liang",
      "Gang Su",
      "Huifeng Lin",
      "You Wu",
      "Rui Zhao",
      "Ziyue Li"
    ],
    "categories": [
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-06-12T07:01:56Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10408v1"
  },
  {
    "arxiv_id": "2506.10274v3",
    "entry_id": "http://arxiv.org/abs/2506.10274v3",
    "title": "Discrete Audio Tokens: More Than a Survey!",
    "summary": "Discrete audio tokens are compact representations that aim to preserve perceptual quality, phonetic content, and speaker characteristics while enabling efficient storage and inference, as well as competitive performance across diverse downstream tasks. They provide a practical alternative to continuous features, enabling the integration of speech and audio into modern large language models (LLMs). As interest in token-based audio processing grows, various tokenization methods have emerged, and several surveys have reviewed the latest progress in the field. However, existing studies often focus on specific domains or tasks and lack a unified comparison across various benchmarks. This paper presents a systematic review and benchmark of discrete audio tokenizers, covering three domains: speech, music, and general audio. We propose a taxonomy of tokenization approaches based on encoder-decoder, quantization techniques, training paradigm, streamability, and application domains. We evaluate tokenizers on multiple benchmarks for reconstruction, downstream performance, and acoustic language modeling, and analyze trade-offs through controlled ablation studies. Our findings highlight key limitations, practical considerations, and open challenges, providing insight and guidance for future research in this rapidly evolving area. For more information, including our main results and tokenizer database, please refer to our website: https://poonehmousavi.github.io/dates-website/.",
    "authors": [
      "Pooneh Mousavi",
      "Gallil Maimon",
      "Adel Moumen",
      "Darius Petermann",
      "Jiatong Shi",
      "Haibin Wu",
      "Haici Yang",
      "Anastasia Kuznetsova",
      "Artem Ploujnikov",
      "Ricard Marxer",
      "Bhuvana Ramabhadran",
      "Benjamin Elizalde",
      "Loren Lugosch",
      "Jinyu Li",
      "Cem Subakan",
      "Phil Woodland",
      "Minje Kim",
      "Hung-yi Lee",
      "Shinji Watanabe",
      "Yossi Adi",
      "Mirco Ravanelli"
    ],
    "categories": [
      "cs.SD",
      "cs.AI",
      "cs.CL",
      "eess.AS"
    ],
    "published": "2025-06-12T01:35:43Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10274v3"
  },
  {
    "arxiv_id": "2506.09755v2",
    "entry_id": "http://arxiv.org/abs/2506.09755v2",
    "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era",
    "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced engineering innovation, efficiency, quality, and productivity over recent decades, fundamentally reshaping how engineering designers think, behave, and interact with design processes. The recent emergence of Foundation Models (FMs), particularly Large Language Models (LLMs), has demonstrated general knowledge-based reasoning capabilities, and open new avenues for further transformation in engineering design. In this context, this paper introduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by foundation model-based agentic AI systems. We review the historical evolution of ID across four distinct stages: rule-based expert systems, task-specific machine learning models, large-scale foundation AI models, and the recent emerging paradigm of foundation model-based multi-agent collaboration. We propose an ontological framework for ID 4.0 and discuss its potential to support end-to-end automation of engineering design processes through coordinated, autonomous multi-agent-based systems. Furthermore, we discuss challenges and opportunities of ID 4.0, including perspectives on data foundations, agent collaboration mechanisms, and the formulation of design problems and objectives. In sum, these insights provide a foundation for advancing Intelligent Design toward greater adaptivity, autonomy, and effectiveness in addressing the growing complexity of engineering design.",
    "authors": [
      "Shuo Jiang",
      "Min Xie",
      "Frank Youhua Chen",
      "Jian Ma",
      "Jianxi Luo"
    ],
    "categories": [
      "cs.CE",
      "cs.AI"
    ],
    "published": "2025-06-11T13:57:26Z",
    "pdf_url": "https://arxiv.org/pdf/2506.09755v2"
  },
  {
    "arxiv_id": "2506.09738v1",
    "entry_id": "http://arxiv.org/abs/2506.09738v1",
    "title": "Towards Multi-modal Graph Large Language Model",
    "summary": "Multi-modal graphs, which integrate diverse multi-modal features and relations, are ubiquitous in real-world applications. However, existing multi-modal graph learning methods are typically trained from scratch for specific graph data and tasks, failing to generalize across various multi-modal graph data and tasks. To bridge this gap, we explore the potential of Multi-modal Graph Large Language Models (MG-LLM) to unify and generalize across diverse multi-modal graph data and tasks. We propose a unified framework of multi-modal graph data, task, and model, discovering the inherent multi-granularity and multi-scale characteristics in multi-modal graphs. Specifically, we present five key desired characteristics for MG-LLM: 1) unified space for multi-modal structures and attributes, 2) capability of handling diverse multi-modal graph tasks, 3) multi-modal graph in-context learning, 4) multi-modal graph interaction with natural language, and 5) multi-modal graph reasoning. We then elaborate on the key challenges, review related works, and highlight promising future research directions towards realizing these ambitious characteristics. Finally, we summarize existing multi-modal graph datasets pertinent for model training. We believe this paper can contribute to the ongoing advancement of the research towards MG-LLM for generalization across multi-modal graph data and tasks.",
    "authors": [
      "Xin Wang",
      "Zeyang Zhang",
      "Linxin Xiao",
      "Haibo Chen",
      "Chendi Ge",
      "Wenwu Zhu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-06-11T13:41:29Z",
    "pdf_url": "https://arxiv.org/pdf/2506.09738v1"
  },
  {
    "arxiv_id": "2506.09656v2",
    "entry_id": "http://arxiv.org/abs/2506.09656v2",
    "title": "Multi-level Value Alignment in Agentic AI Systems: Survey and Perspectives",
    "summary": "The ongoing evolution of AI paradigms has propelled AI research into the agentic AI stage. Consequently, the focus of research has shifted from single agents and simple applications towards multi-agent autonomous decision-making and task collaboration in complex environments. As Large Language Models (LLMs) advance, their applications become more diverse and complex, leading to increasing situational and systemic risks. This has brought significant attention to value alignment for agentic AI systems, which aims to ensure that an agent's goals, preferences, and behaviors align with human values and societal norms. Addressing socio-governance demands through a Multi-level Value framework, this study comprehensively reviews value alignment in LLM-based multi-agent systems as the representative archetype of agentic AI systems. Our survey systematically examines three interconnected dimensions: First, value principles are structured via a top-down hierarchy across macro, meso, and micro levels. Second, application scenarios are categorized along a general-to-specific continuum explicitly mirroring these value tiers. Third, value alignment methods and evaluation are mapped to this tiered framework through systematic examination of benchmarking datasets and relevant methodologies. Additionally, we delve into value coordination among multiple agents within agentic AI systems. Finally, we propose several potential research directions in this field.",
    "authors": [
      "Wei Zeng",
      "Hengshu Zhu",
      "Chuan Qin",
      "Han Wu",
      "Yihang Cheng",
      "Sirui Zhang",
      "Xiaowei Jin",
      "Yinuo Shen",
      "Zhenxing Wang",
      "Feimin Zhong",
      "Hui Xiong"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-11T12:25:38Z",
    "pdf_url": "https://arxiv.org/pdf/2506.09656v2"
  },
  {
    "arxiv_id": "2506.09566v1",
    "entry_id": "http://arxiv.org/abs/2506.09566v1",
    "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies",
    "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs) enhances factual grounding and reasoning capabilities. This survey paper systematically examines the synergy between KGs and LLMs, categorizing existing approaches into two main groups: KG-enhanced LLMs, which improve reasoning, reduce hallucinations, and enable complex question answering; and LLM-augmented KGs, which facilitate KG construction, completion, and querying. Through comprehensive analysis, we identify critical gaps and highlight the mutual benefits of structured knowledge integration. Compared to existing surveys, our study uniquely emphasizes scalability, computational efficiency, and data quality. Finally, we propose future research directions, including neuro-symbolic integration, dynamic KG updating, data reliability, and ethical considerations, paving the way for intelligent systems capable of managing more complex real-world knowledge tasks.",
    "authors": [
      "Blaž Škrlj",
      "Boshko Koloski",
      "Senja Pollak",
      "Nada Lavrač"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-06-11T09:58:14Z",
    "pdf_url": "https://arxiv.org/pdf/2506.09566v1"
  },
  {
    "arxiv_id": "2506.09516v1",
    "entry_id": "http://arxiv.org/abs/2506.09516v1",
    "title": "LLM-Powered CPI Prediction Inference with Online Text Time Series",
    "summary": "Forecasting the Consumer Price Index (CPI) is an important yet challenging task in economics, where most existing approaches rely on low-frequency, survey-based data. With the recent advances of large language models (LLMs), there is growing potential to leverage high-frequency online text data for improved CPI prediction, an area still largely unexplored. This paper proposes LLM-CPI, an LLM-based approach for CPI prediction inference incorporating online text time series. We collect a large set of high-frequency online texts from a popularly used Chinese social network site and employ LLMs such as ChatGPT and the trained BERT models to construct continuous inflation labels for posts that are related to inflation. Online text embeddings are extracted via LDA and BERT. We develop a joint time series framework that combines monthly CPI data with LLM-generated daily CPI surrogates. The monthly model employs an ARX structure combining observed CPI data with text embeddings and macroeconomic variables, while the daily model uses a VARX structure built on LLM-generated CPI surrogates and text embeddings. We establish the asymptotic properties of the method and provide two forms of constructed prediction intervals. The finite-sample performance and practical advantages of LLM-CPI are demonstrated through both simulation and real data examples.",
    "authors": [
      "Yingying Fan",
      "Jinchi Lv",
      "Ao Sun",
      "Yurou Wang"
    ],
    "categories": [
      "stat.ML",
      "cs.LG",
      "stat.ME"
    ],
    "published": "2025-06-11T08:41:58Z",
    "pdf_url": "https://arxiv.org/pdf/2506.09516v1"
  },
  {
    "arxiv_id": "2506.09495v1",
    "entry_id": "http://arxiv.org/abs/2506.09495v1",
    "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers",
    "summary": "Suicide remains a leading cause of death in Western countries, underscoring the need for new research approaches. As social media becomes central to daily life, digital footprints offer valuable insight into suicidal behavior. Focusing on individuals who attempted suicide while uploading videos to their channels, we investigate: How do suicidal behaviors manifest on YouTube, and how do they differ from expert knowledge? We applied complementary approaches: computational bottom-up, hybrid, and expert-driven top-down, on a novel longitudinal dataset of 181 YouTube channels from individuals with life-threatening attempts, alongside 134 control channels. In the bottom-up approach, we applied LLM-based topic modeling to identify behavioral indicators. Of 166 topics, five were associated with suicide-attempt, with two also showing temporal attempt-related changes ($p<.01$) - Mental Health Struggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach, a clinical expert reviewed LLM-derived topics and flagged 19 as suicide-related. However, none showed significant attempt-related temporal effects beyond those identified bottom-up. Notably, YouTube Engagement, a platform-specific indicator, was not flagged by the expert, underscoring the value of bottom-up discovery. In the top-down approach, psychological assessment of suicide attempt narratives revealed that the only significant difference between individuals who attempted before and those attempted during their upload period was the motivation to share this experience: the former aimed to Help Others ($β=-1.69$, $p<.01$), while the latter framed it as part of their Personal Recovery ($β=1.08$, $p<.01$). By integrating these approaches, we offer a nuanced understanding of suicidality, bridging digital behavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.",
    "authors": [
      "Ilanit Sobol",
      "Shir Lissak",
      "Refael Tikochinski",
      "Tal Nakash",
      "Anat Brunstein Klomek",
      "Eyal Fruchter",
      "Roi Reichart"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-06-11T08:12:02Z",
    "pdf_url": "https://arxiv.org/pdf/2506.09495v1"
  },
  {
    "arxiv_id": "2506.09368v1",
    "entry_id": "http://arxiv.org/abs/2506.09368v1",
    "title": "Anomaly Detection and Generation with Diffusion Models: A Survey",
    "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including cybersecurity, finance, healthcare, and industrial manufacturing, by identifying unexpected patterns that deviate from established norms in real-world data. Recent advancements in deep learning, specifically diffusion models (DMs), have sparked significant interest due to their ability to learn complex data distributions and generate high-fidelity samples, offering a robust framework for unsupervised AD. In this survey, we comprehensively review anomaly detection and generation with diffusion models (ADGDM), presenting a tutorial-style analysis of the theoretical foundations and practical implementations and spanning images, videos, time series, tabular, and multimodal data. Crucially, unlike existing surveys that often treat anomaly detection and generation as separate problems, we highlight their inherent synergistic relationship. We reveal how DMs enable a reinforcing cycle where generation techniques directly address the fundamental challenge of anomaly data scarcity, while detection methods provide critical feedback to improve generation fidelity and relevance, advancing both capabilities beyond their individual potential. A detailed taxonomy categorizes ADGDM methods based on anomaly scoring mechanisms, conditioning strategies, and architectural designs, analyzing their strengths and limitations. We final discuss key challenges including scalability and computational efficiency, and outline promising future directions such as efficient architectures, conditioning strategies, and integration with foundation models (e.g., visual-language models and large language models). By synthesizing recent advances and outlining open research questions, this survey aims to guide researchers and practitioners in leveraging DMs for innovative AD solutions across diverse applications.",
    "authors": [
      "Yang Liu",
      "Jing Liu",
      "Chengfang Li",
      "Rui Xi",
      "Wenchao Li",
      "Liang Cao",
      "Jin Wang",
      "Laurence T. Yang",
      "Junsong Yuan",
      "Wei Zhou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-06-11T03:29:18Z",
    "pdf_url": "https://arxiv.org/pdf/2506.09368v1"
  },
  {
    "arxiv_id": "2506.09227v1",
    "entry_id": "http://arxiv.org/abs/2506.09227v1",
    "title": "SoK: Machine Unlearning for Large Language Models",
    "summary": "Large language model (LLM) unlearning has become a critical topic in machine learning, aiming to eliminate the influence of specific training data or knowledge without retraining the model from scratch. A variety of techniques have been proposed, including Gradient Ascent, model editing, and re-steering hidden representations. While existing surveys often organize these methods by their technical characteristics, such classifications tend to overlook a more fundamental dimension: the underlying intention of unlearning--whether it seeks to truly remove internal knowledge or merely suppress its behavioral effects. In this SoK paper, we propose a new taxonomy based on this intention-oriented perspective. Building on this taxonomy, we make three key contributions. First, we revisit recent findings suggesting that many removal methods may functionally behave like suppression, and explore whether true removal is necessary or achievable. Second, we survey existing evaluation strategies, identify limitations in current metrics and benchmarks, and suggest directions for developing more reliable and intention-aligned evaluations. Third, we highlight practical challenges--such as scalability and support for sequential unlearning--that currently hinder the broader deployment of unlearning methods. In summary, this work offers a comprehensive framework for understanding and advancing unlearning in generative AI, aiming to support future research and guide policy decisions around data removal and privacy.",
    "authors": [
      "Jie Ren",
      "Yue Xing",
      "Yingqian Cui",
      "Charu C. Aggarwal",
      "Hui Liu"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2025-06-10T20:30:39Z",
    "pdf_url": "https://arxiv.org/pdf/2506.09227v1"
  },
  {
    "arxiv_id": "2506.11135v1",
    "entry_id": "http://arxiv.org/abs/2506.11135v1",
    "title": "Large Language Models and Emergence: A Complex Systems Perspective",
    "summary": "Emergence is a concept in complexity science that describes how many-body systems manifest novel higher-level properties, properties that can be described by replacing high-dimensional mechanisms with lower-dimensional effective variables and theories. This is captured by the idea \"more is different\". Intelligence is a consummate emergent property manifesting increasingly efficient -- cheaper and faster -- uses of emergent capabilities to solve problems. This is captured by the idea \"less is more\". In this paper, we first examine claims that Large Language Models exhibit emergent capabilities, reviewing several approaches to quantifying emergence, and secondly ask whether LLMs possess emergent intelligence.",
    "authors": [
      "David C. Krakauer",
      "John W. Krakauer",
      "Melanie Mitchell"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2025-06-10T19:31:26Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11135v1"
  },
  {
    "arxiv_id": "2506.08800v2",
    "entry_id": "http://arxiv.org/abs/2506.08800v2",
    "title": "Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents",
    "summary": "Data science aims to extract insights from data to support decision-making processes. Recently, Large Language Models (LLMs) have been increasingly used as assistants for data science, by suggesting ideas, techniques and small code snippets, or for the interpretation of results and reporting. Proper automation of some data-science activities is now promised by the rise of LLM agents, i.e., AI systems powered by an LLM equipped with additional affordances--such as code execution and knowledge bases--that can perform self-directed actions and interact with digital environments. In this paper, we survey the evaluation of LLM assistants and agents for data science. We find (1) a dominant focus on a small subset of goal-oriented activities, largely ignoring data management and exploratory activities; (2) a concentration on pure assistance or fully autonomous agents, without considering intermediate levels of human-AI collaboration; and (3) an emphasis on human substitution, therefore neglecting the possibility of higher levels of automation thanks to task transformation.",
    "authors": [
      "Irene Testini",
      "José Hernández-Orallo",
      "Lorenzo Pacchiardi"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-06-10T13:47:22Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08800v2"
  },
  {
    "arxiv_id": "2506.08757v1",
    "entry_id": "http://arxiv.org/abs/2506.08757v1",
    "title": "Enhancing Accuracy and Maintainability in Nuclear Plant Data Retrieval: A Function-Calling LLM Approach Over NL-to-SQL",
    "summary": "Retrieving operational data from nuclear power plants requires exceptional accuracy and transparency due to the criticality of the decisions it supports. Traditionally, natural language to SQL (NL-to-SQL) approaches have been explored for querying such data. While NL-to-SQL promises ease of use, it poses significant risks: end-users cannot easily validate generated SQL queries, and legacy nuclear plant databases -- often complex and poorly structured -- complicate query generation due to decades of incremental modifications. These challenges increase the likelihood of inaccuracies and reduce trust in the approach. In this work, we propose an alternative paradigm: leveraging function-calling large language models (LLMs) to address these challenges. Instead of directly generating SQL queries, we define a set of pre-approved, purpose-specific functions representing common use cases. Queries are processed by invoking these functions, which encapsulate validated SQL logic. This hybrid approach mitigates the risks associated with direct NL-to-SQL translations by ensuring that SQL queries are reviewed and optimized by experts before deployment. While this strategy introduces the upfront cost of developing and maintaining the function library, we demonstrate how NL-to-SQL tools can assist in the initial generation of function code, allowing experts to focus on validation rather than creation. Our study includes a performance comparison between direct NL-to-SQL generation and the proposed function-based approach, highlighting improvements in accuracy and maintainability. This work underscores the importance of balancing user accessibility with operational safety and provides a novel, actionable framework for robust data retrieval in critical systems.",
    "authors": [
      "Mishca de Costa",
      "Muhammad Anwar",
      "Dave Mercier",
      "Mark Randall",
      "Issam Hammad"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-06-10T12:55:07Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08757v1"
  },
  {
    "arxiv_id": "2506.08446v1",
    "entry_id": "http://arxiv.org/abs/2506.08446v1",
    "title": "A Survey on Large Language Models for Mathematical Reasoning",
    "summary": "Mathematical reasoning has long represented one of the most fundamental and challenging frontiers in artificial intelligence research. In recent years, large language models (LLMs) have achieved significant advances in this area. This survey examines the development of mathematical reasoning abilities in LLMs through two high-level cognitive phases: comprehension, where models gain mathematical understanding via diverse pretraining strategies, and answer generation, which has progressed from direct prediction to step-by-step Chain-of-Thought (CoT) reasoning. We review methods for enhancing mathematical reasoning, ranging from training-free prompting to fine-tuning approaches such as supervised fine-tuning and reinforcement learning, and discuss recent work on extended CoT and \"test-time scaling\". Despite notable progress, fundamental challenges remain in terms of capacity, efficiency, and generalization. To address these issues, we highlight promising research directions, including advanced pretraining and knowledge augmentation techniques, formal reasoning frameworks, and meta-generalization through principled learning paradigms. This survey tries to provide some insights for researchers interested in enhancing reasoning capabilities of LLMs and for those seeking to apply these techniques to other domains.",
    "authors": [
      "Peng-Yuan Wang",
      "Tian-Shuo Liu",
      "Chenyang Wang",
      "Yi-Di Wang",
      "Shu Yan",
      "Cheng-Xing Jia",
      "Xu-Hui Liu",
      "Xin-Wei Chen",
      "Jia-Cheng Xu",
      "Ziniu Li",
      "Yang Yu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-06-10T04:44:28Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08446v1"
  },
  {
    "arxiv_id": "2506.08422v2",
    "entry_id": "http://arxiv.org/abs/2506.08422v2",
    "title": "Transforming Expert Knowledge into Scalable Ontology via Large Language Models",
    "summary": "Having a unified, coherent taxonomy is essential for effective knowledge representation in domain-specific applications as diverse terminologies need to be mapped to underlying concepts. Traditional manual approaches to taxonomy alignment rely on expert review of concept pairs, but this becomes prohibitively expensive and time-consuming at scale, while subjective interpretations often lead to expert disagreements. Existing automated methods for taxonomy alignment have shown promise but face limitations in handling nuanced semantic relationships and maintaining consistency across different domains. These approaches often struggle with context-dependent concept mappings and lack transparent reasoning processes. We propose a novel framework that combines large language models (LLMs) with expert calibration and iterative prompt optimization to automate taxonomy alignment. Our method integrates expert-labeled examples, multi-stage prompt engineering, and human validation to guide LLMs in generating both taxonomy linkages and supporting rationales. In evaluating our framework on a domain-specific mapping task of concept essentiality, we achieved an F1-score of 0.97, substantially exceeding the human benchmark of 0.68. These results demonstrate the effectiveness of our approach in scaling taxonomy alignment while maintaining high-quality mappings and preserving expert oversight for ambiguous cases.",
    "authors": [
      "Ikkei Itoku",
      "David Theil",
      "Evelyn Eichelsdoerfer Uehara",
      "Sreyoshi Bhaduri",
      "Junnosuke Kuroda",
      "Toshi Yumoto",
      "Alex Gil",
      "Natalie Perez",
      "Rajesh Cherukuri",
      "Naumaan Nayyar"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-10T03:48:26Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08422v2"
  },
  {
    "arxiv_id": "2506.08235v1",
    "entry_id": "http://arxiv.org/abs/2506.08235v1",
    "title": "Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim $\\rightarrow$ Evidence Reasoning",
    "summary": "Large language models (LLMs) are increasingly being used for complex research tasks such as literature review, idea generation, and scientific paper analysis, yet their ability to truly understand and process the intricate relationships within complex research papers, such as the logical links between claims and supporting evidence remains largely unexplored. In this study, we present CLAIM-BENCH, a comprehensive benchmark for evaluating LLMs' capabilities in scientific claim-evidence extraction and validation, a task that reflects deeper comprehension of scientific argumentation. We systematically compare three approaches which are inspired by divide and conquer approaches, across six diverse LLMs, highlighting model-specific strengths and weaknesses in scientific comprehension. Through evaluation involving over 300 claim-evidence pairs across multiple research domains, we reveal significant limitations in LLMs' ability to process complex scientific content. Our results demonstrate that closed-source models like GPT-4 and Claude consistently outperform open-source counterparts in precision and recall across claim-evidence identification tasks. Furthermore, strategically designed three-pass and one-by-one prompting approaches significantly improve LLMs' abilities to accurately link dispersed evidence with claims, although this comes at increased computational cost. CLAIM-BENCH sets a new standard for evaluating scientific comprehension in LLMs, offering both a diagnostic tool and a path forward for building systems capable of deeper, more reliable reasoning across full-length papers.",
    "authors": [
      "Shashidhar Reddy Javaji",
      "Yupeng Cao",
      "Haohang Li",
      "Yangyang Yu",
      "Nikhil Muralidhar",
      "Zining Zhu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-09T21:04:39Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08235v1"
  },
  {
    "arxiv_id": "2506.08234v2",
    "entry_id": "http://arxiv.org/abs/2506.08234v2",
    "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and Future Directions",
    "summary": "Recent advancements in large language models (LLMs) and AI systems have led to a paradigm shift in the design and optimization of complex AI workflows. By integrating multiple components, compound AI systems have become increasingly adept at performing sophisticated tasks. However, as these systems grow in complexity, new challenges arise in optimizing not only individual components but also their interactions. While traditional optimization methods such as supervised fine-tuning (SFT) and reinforcement learning (RL) remain foundational, the rise of natural language feedback introduces promising new approaches, especially for optimizing non-differentiable systems. This paper provides a systematic review of recent progress in optimizing compound AI systems, encompassing both numerical and language-based techniques. We formalize the notion of compound AI system optimization, classify existing methods along several key dimensions, and highlight open research challenges and future directions in this rapidly evolving field. A list of surveyed papers is publicly available at https://github.com/MiuLab/AISysOpt-Survey.",
    "authors": [
      "Yu-Ang Lee",
      "Guan-Ting Yi",
      "Mei-Yi Liu",
      "Jui-Chao Lu",
      "Guan-Bo Yang",
      "Yun-Nung Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-09T21:04:14Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08234v2"
  },
  {
    "arxiv_id": "2506.08134v3",
    "entry_id": "http://arxiv.org/abs/2506.08134v3",
    "title": "The AI Imperative: Scaling High-Quality Peer Review in Machine Learning",
    "summary": "Peer review, the bedrock of scientific advancement in machine learning (ML), is strained by a crisis of scale. Exponential growth in manuscript submissions to premier ML venues such as NeurIPS, ICML, and ICLR is outpacing the finite capacity of qualified reviewers, leading to concerns about review quality, consistency, and reviewer fatigue. This position paper argues that AI-assisted peer review must become an urgent research and infrastructure priority. We advocate for a comprehensive AI-augmented ecosystem, leveraging Large Language Models (LLMs) not as replacements for human judgment, but as sophisticated collaborators for authors, reviewers, and Area Chairs (ACs). We propose specific roles for AI in enhancing factual verification, guiding reviewer performance, assisting authors in quality improvement, and supporting ACs in decision-making. Crucially, we contend that the development of such systems hinges on access to more granular, structured, and ethically-sourced peer review process data. We outline a research agenda, including illustrative experiments, to develop and validate these AI assistants, and discuss significant technical and ethical challenges. We call upon the ML community to proactively build this AI-assisted future, ensuring the continued integrity and scalability of scientific validation, while maintaining high standards of peer review.",
    "authors": [
      "Qiyao Wei",
      "Samuel Holt",
      "Jing Yang",
      "Markus Wulfmeier",
      "Mihaela van der Schaar"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-06-09T18:37:14Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08134v3"
  },
  {
    "arxiv_id": "2506.07739v3",
    "entry_id": "http://arxiv.org/abs/2506.07739v3",
    "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models",
    "summary": "Architectural cultures across regions are characterized by stylistic diversity, shaped by historical, social, and technological contexts in addition to geograph-ical conditions. Understanding architectural styles requires the ability to describe and analyze the stylistic features of different architects from various regions through visual observations of architectural imagery. However, traditional studies of architectural culture have largely relied on subjective expert interpretations and historical literature reviews, often suffering from regional biases and limited ex-planatory scope. To address these challenges, this study proposes three core contributions: (1) We construct a professional architectural style dataset named ArchDiffBench, which comprises 1,765 high-quality architectural images and their corresponding style annotations, collected from different regions and historical periods. (2) We propose ArchiLense, an analytical framework grounded in Vision-Language Models and constructed using the ArchDiffBench dataset. By integrating ad-vanced computer vision techniques, deep learning, and machine learning algo-rithms, ArchiLense enables automatic recognition, comparison, and precise classi-fication of architectural imagery, producing descriptive language outputs that ar-ticulate stylistic differences. (3) Extensive evaluations show that ArchiLense achieves strong performance in architectural style recognition, with a 92.4% con-sistency rate with expert annotations and 84.5% classification accuracy, effec-tively capturing stylistic distinctions across images. The proposed approach transcends the subjectivity inherent in traditional analyses and offers a more objective and accurate perspective for comparative studies of architectural culture.",
    "authors": [
      "Jing Zhong",
      "Jun Yin",
      "Peilin Li",
      "Pengyu Zeng",
      "Miao Zang",
      "Ran Luo",
      "Shuai Lu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-06-09T13:22:57Z",
    "pdf_url": "https://arxiv.org/pdf/2506.07739v3"
  },
  {
    "arxiv_id": "2506.07583v1",
    "entry_id": "http://arxiv.org/abs/2506.07583v1",
    "title": "Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models",
    "summary": "Despite the popularity of the large language models (LLMs), their application to machine translation is relatively underexplored, especially in context-aware settings. This work presents a literature review of context-aware translation with LLMs. The existing works utilise prompting and fine-tuning approaches, with few focusing on automatic post-editing and creating translation agents for context-aware machine translation. We observed that the commercial LLMs (such as ChatGPT and Tower LLM) achieved better results than the open-source LLMs (such as Llama and Bloom LLMs), and prompt-based approaches serve as good baselines to assess the quality of translations. Finally, we present some interesting future directions to explore.",
    "authors": [
      "Ramakrishna Appicharla",
      "Baban Gain",
      "Santanu Pal",
      "Asif Ekbal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-09T09:27:00Z",
    "pdf_url": "https://arxiv.org/pdf/2506.07583v1"
  },
  {
    "arxiv_id": "2506.07400v2",
    "entry_id": "http://arxiv.org/abs/2506.07400v2",
    "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models",
    "summary": "The integration of deep learning-based glaucoma detection with large language models (LLMs) presents an automated strategy to mitigate ophthalmologist shortages and improve clinical reporting efficiency. However, applying general LLMs to medical imaging remains challenging due to hallucinations, limited interpretability, and insufficient domain-specific medical knowledge, which can potentially reduce clinical accuracy. Although recent approaches combining imaging models with LLM reasoning have improved reporting, they typically rely on a single generalist agent, restricting their capacity to emulate the diverse and complex reasoning found in multidisciplinary medical teams. To address these limitations, we propose MedChat, a multi-agent diagnostic framework and platform that combines specialized vision models with multiple role-specific LLM agents, all coordinated by a director agent. This design enhances reliability, reduces hallucination risk, and enables interactive diagnostic reporting through an interface tailored for clinical review and educational use. Code available at https://github.com/Purdue-M2/MedChat.",
    "authors": [
      "Philip R. Liu",
      "Sparsh Bansal",
      "Jimmy Dinh",
      "Aditya Pawar",
      "Ramani Satishkumar",
      "Shail Desai",
      "Neeraj Gupta",
      "Xin Wang",
      "Shu Hu"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-06-09T03:51:18Z",
    "pdf_url": "https://arxiv.org/pdf/2506.07400v2"
  },
  {
    "arxiv_id": "2506.07274v1",
    "entry_id": "http://arxiv.org/abs/2506.07274v1",
    "title": "Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages",
    "summary": "Code-switching presents a complex challenge for syntactic analysis, especially in low-resource language settings where annotated data is scarce. While recent work has explored the use of large language models (LLMs) for sequence-level tagging, few approaches systematically investigate how well these models capture syntactic structure in code-switched contexts. Moreover, existing parsers trained on monolingual treebanks often fail to generalize to multilingual and mixed-language input. To address this gap, we introduce the BiLingua Parser, an LLM-based annotation pipeline designed to produce Universal Dependencies (UD) annotations for code-switched text. First, we develop a prompt-based framework for Spanish-English and Spanish-Guaraní data, combining few-shot LLM prompting with expert review. Second, we release two annotated datasets, including the first Spanish-Guaraní UD-parsed corpus. Third, we conduct a detailed syntactic analysis of switch points across language pairs and communicative contexts. Experimental results show that BiLingua Parser achieves up to 95.29% LAS after expert revision, significantly outperforming prior baselines and multilingual parsers. These results show that LLMs, when carefully guided, can serve as practical tools for bootstrapping syntactic resources in under-resourced, code-switched environments. Data and source code are available at https://github.com/N3mika/ParsingProject",
    "authors": [
      "Olga Kellert",
      "Nemika Tyagi",
      "Muhammad Imran",
      "Nelvin Licona-Guevara",
      "Carlos Gómez-Rodríguez"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-08T20:23:57Z",
    "pdf_url": "https://arxiv.org/pdf/2506.07274v1"
  },
  {
    "arxiv_id": "2506.11113v3",
    "entry_id": "http://arxiv.org/abs/2506.11113v3",
    "title": "Breaking the Reviewer: Assessing the Vulnerability of Large Language Models in Automated Peer Review Under Textual Adversarial Attacks",
    "summary": "Peer review is essential for maintaining academic quality, but the increasing volume of submissions places a significant burden on reviewers. Large language models (LLMs) offer potential assistance in this process, yet their susceptibility to textual adversarial attacks raises reliability concerns. This paper investigates the robustness of LLMs used as automated reviewers in the presence of such attacks. We focus on three key questions: (1) The effectiveness of LLMs in generating reviews compared to human reviewers. (2) The impact of adversarial attacks on the reliability of LLM-generated reviews. (3) Challenges and potential mitigation strategies for LLM-based review. Our evaluation reveals significant vulnerabilities, as text manipulations can distort LLM assessments. We offer a comprehensive evaluation of LLM performance in automated peer reviewing and analyze its robustness against adversarial attacks. Our findings emphasize the importance of addressing adversarial risks to ensure AI strengthens, rather than compromises, the integrity of scholarly communication.",
    "authors": [
      "Tzu-Ling Lin",
      "Wei-Chih Chen",
      "Teng-Fang Hsiao",
      "Hou-I Liu",
      "Ya-Hsin Yeh",
      "Yu Kai Chan",
      "Wen-Sheng Lien",
      "Po-Yen Kuo",
      "Philip S. Yu",
      "Hong-Han Shuai"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-08T16:57:38Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11113v3"
  },
  {
    "arxiv_id": "2506.11111v2",
    "entry_id": "http://arxiv.org/abs/2506.11111v2",
    "title": "Evaluating and Improving Robustness in Large Language Models: A Survey and Future Directions",
    "summary": "Large Language Models (LLMs) have gained enormous attention in recent years due to their capability of understanding and generating natural languages. With the rapid development and wild-range applications (e.g., Agents, Embodied Intelligence), the robustness of LLMs has received increased attention. As the core brain of many AI applications, the robustness of LLMs requires that models should not only generate consistent contents, but also ensure the correctness and stability of generated content when dealing with unexpeted application scenarios (e.g., toxic prompts, limited noise domain data, outof-distribution (OOD) applications, etc). In this survey paper, we conduct a thorough review of the robustness of LLMs, aiming to provide a comprehensive terminology of concepts and methods around this field and facilitate the community. Specifically, we first give a formal definition of LLM robustness and present the collection protocol of this survey paper. Then, based on the types of perturbated inputs, we organize this survey from the following perspectives: 1) Adversarial Robustness: tackling the problem that prompts are manipulated intentionally, such as noise prompts, long context, data attack, etc; 2) OOD Robustness: dealing with the unexpected real-world application scenarios, such as OOD detection, zero-shot transferring, hallucinations, etc; 3) Evaluation of Robustness: summarizing the new evaluation datasets, metrics, and tools for verifying the robustness of LLMs. After reviewing the representative work from each perspective, we discuss and highlight future opportunities and research directions in this field. Meanwhile, we also organize related works and provide an easy-to-search project (https://github.com/zhangkunzk/Awesome-LLM-Robustness-papers) to support the community.",
    "authors": [
      "Kun Zhang",
      "Le Wu",
      "Kui Yu",
      "Guangyi Lv",
      "Dacao Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-08T16:20:12Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11111v2"
  },
  {
    "arxiv_id": "2506.07153v2",
    "entry_id": "http://arxiv.org/abs/2506.07153v2",
    "title": "Mind the Web: The Security of Web Use Agents",
    "summary": "Web-use agents are rapidly being deployed to automate complex web tasks with extensive browser capabilities. However, these capabilities create a critical and previously unexplored attack surface. This paper demonstrates how attackers can exploit web-use agents by embedding malicious content in web pages, such as comments, reviews, or advertisements, that agents encounter during legitimate browsing tasks. We introduce the task-aligned injection technique that frames malicious commands as helpful task guidance rather than obvious attacks, exploiting fundamental limitations in LLMs' contextual reasoning. Agents struggle to maintain coherent contextual awareness and fail to detect when seemingly helpful web content contains steering attempts that deviate them from their original task goal. To scale this attack, we developed an automated three-stage pipeline that generates effective injections without manual annotation or costly online agent interactions during training, remaining efficient even with limited training data. This pipeline produces a generator model that we evaluate on five popular agents using payloads organized by the Confidentiality-Integrity-Availability (CIA) security triad, including unauthorized camera activation, file exfiltration, user impersonation, phishing, and denial-of-service. This generator achieves over 80% attack success rate (ASR) with strong transferability across unseen payloads, diverse web environments, and different underlying LLMs. This attack succeed even against agents with built-in safety mechanisms, requiring only the ability to post content on public websites. To address this risk, we propose comprehensive mitigation strategies including oversight mechanisms, execution constraints, and task-aware reasoning techniques.",
    "authors": [
      "Avishag Shapira",
      "Parth Atulbhai Gandhi",
      "Edan Habler",
      "Asaf Shabtai"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-06-08T13:59:55Z",
    "pdf_url": "https://arxiv.org/pdf/2506.07153v2"
  },
  {
    "arxiv_id": "2506.08045v1",
    "entry_id": "http://arxiv.org/abs/2506.08045v1",
    "title": "UAVs Meet Agentic AI: A Multidomain Survey of Autonomous Aerial Intelligence and Agentic UAVs",
    "summary": "Agentic UAVs represent a new frontier in autonomous aerial intelligence, integrating perception, decision-making, memory, and collaborative planning to operate adaptively in complex, real-world environments. Driven by recent advances in Agentic AI, these systems surpass traditional UAVs by exhibiting goal-driven behavior, contextual reasoning, and interactive autonomy. We provide a comprehensive foundation for understanding the architectural components and enabling technologies that distinguish Agentic UAVs from traditional autonomous UAVs. Furthermore, a detailed comparative analysis highlights advancements in autonomy with AI agents, learning, and mission flexibility. This study explores seven high-impact application domains precision agriculture, construction & mining, disaster response, environmental monitoring, infrastructure inspection, logistics, security, and wildlife conservation, illustrating the broad societal value of agentic aerial intelligence. Furthermore, we identify key challenges in technical constraints, regulatory limitations, and data-model reliability, and we present emerging solutions across hardware innovation, learning architectures, and human-AI interaction. Finally, a future roadmap is proposed, outlining pathways toward self-evolving aerial ecosystems, system-level collaboration, and sustainable, equitable deployments. This survey establishes a foundational framework for the future development, deployment, and governance of agentic aerial systems (Agentic UAVs) across diverse societal and industrial domains.",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I. Roumeliotis",
      "Manoj Karkee"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-06-08T01:39:51Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08045v1"
  },
  {
    "arxiv_id": "2506.06580v2",
    "entry_id": "http://arxiv.org/abs/2506.06580v2",
    "title": "AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture",
    "summary": "Insufficient data volume and quality are particularly pressing challenges in the adoption of modern subsymbolic AI. To alleviate these challenges, AI simulation uses virtual training environments in which AI agents can be safely and efficiently developed with simulated, synthetic data. Digital twins open new avenues in AI simulation, as these high-fidelity virtual replicas of physical systems are equipped with state-of-the-art simulators and the ability to further interact with the physical system for additional data collection. In this article, we report on our systematic survey of digital twin-enabled AI simulation. By analyzing 22 primary studies, we identify technological trends and derive a reference framework to situate digital twins and AI components. Based on our findings, we derive a reference framework and provide architectural guidelines by mapping it onto the ISO 23247 reference architecture for digital twins. Finally, we identify challenges and research opportunities for prospective researchers.",
    "authors": [
      "Xiaoran Liu",
      "Istvan David"
    ],
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.SE",
      "eess.SY"
    ],
    "published": "2025-06-06T23:13:38Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06580v2"
  },
  {
    "arxiv_id": "2506.06579v1",
    "entry_id": "http://arxiv.org/abs/2506.06579v1",
    "title": "Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques",
    "summary": "Recent progress in Language Models (LMs) has dramatically advanced the field of natural language processing (NLP), excelling at tasks like text generation, summarization, and question answering. However, their inference remains computationally expensive and energy intensive, especially in settings with limited hardware, power, or bandwidth. This makes it difficult to deploy LMs in mobile, edge, or cost sensitive environments. To address these challenges, recent approaches have introduced multi LLM intelligent model selection strategies that dynamically allocate computational resources based on query complexity -- using lightweight models for simpler queries and escalating to larger models only when necessary. This survey explores two complementary strategies for efficient LLM inference: (i) routing, which selects the most suitable model based on the query, and (ii) cascading or hierarchical inference (HI), which escalates queries through a sequence of models until a confident response is found. Both approaches aim to reduce computation by using lightweight models for simpler tasks while offloading only when needed. We provide a comparative analysis of these techniques across key performance metrics, discuss benchmarking efforts, and outline open challenges. Finally, we outline future research directions to enable faster response times, adaptive model selection based on task complexity, and scalable deployment across heterogeneous environments, making LLM based systems more efficient and accessible for real world applications.",
    "authors": [
      "Adarsh Prasad Behera",
      "Jaya Prakash Champati",
      "Roberto Morabito",
      "Sasu Tarkoma",
      "James Gross"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.DC"
    ],
    "published": "2025-06-06T23:13:08Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06579v1"
  },
  {
    "arxiv_id": "2506.06540v1",
    "entry_id": "http://arxiv.org/abs/2506.06540v1",
    "title": "Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches",
    "summary": "After a disruptive event or shock, such as the Department of Government Efficiency (DOGE) federal layoffs of 2025, expert judgments are colored by knowledge of the outcome. This can make it difficult or impossible to reconstruct the pre-event perceptions needed to study the factors associated with the event. This position paper argues that large language models (LLMs), trained on vast amounts of digital media data, can be a viable substitute for expert political surveys when a shock disrupts traditional measurement. We analyze the DOGE layoffs as a specific case study for this position. We use pairwise comparison prompts with LLMs and derive ideology scores for federal executive agencies. These scores replicate pre-layoff expert measures and predict which agencies were targeted by DOGE. We also use this same approach and find that the perceptions of certain federal agencies as knowledge institutions predict which agencies were targeted by DOGE, even when controlling for ideology. This case study demonstrates that using LLMs allows us to rapidly and easily test the associated factors hypothesized behind the shock. More broadly, our case study of this recent event exemplifies how LLMs offer insights into the correlational factors of the shock when traditional measurement techniques fail. We conclude by proposing a two-part criterion for when researchers can turn to LLMs as a substitute for expert political surveys.",
    "authors": [
      "Patrick Y. Wu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-06-06T21:14:57Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06540v1"
  },
  {
    "arxiv_id": "2506.06518v1",
    "entry_id": "http://arxiv.org/abs/2506.06518v1",
    "title": "A Systematic Review of Poisoning Attacks Against Large Language Models",
    "summary": "With the widespread availability of pretrained Large Language Models (LLMs) and their training datasets, concerns about the security risks associated with their usage has increased significantly. One of these security risks is the threat of LLM poisoning attacks where an attacker modifies some part of the LLM training process to cause the LLM to behave in a malicious way. As an emerging area of research, the current frameworks and terminology for LLM poisoning attacks are derived from earlier classification poisoning literature and are not fully equipped for generative LLM settings. We conduct a systematic review of published LLM poisoning attacks to clarify the security implications and address inconsistencies in terminology across the literature. We propose a comprehensive poisoning threat model applicable to categorize a wide range of LLM poisoning attacks. The poisoning threat model includes four poisoning attack specifications that define the logistics and manipulation strategies of an attack as well as six poisoning metrics used to measure key characteristics of an attack. Under our proposed framework, we organize our discussion of published LLM poisoning literature along four critical dimensions of LLM poisoning attacks: concept poisons, stealthy poisons, persistent poisons, and poisons for unique tasks, to better understand the current landscape of security risks.",
    "authors": [
      "Neil Fendley",
      "Edward W. Staley",
      "Joshua Carney",
      "William Redman",
      "Marie Chau",
      "Nathan Drenkow"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-06-06T20:32:43Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06518v1"
  },
  {
    "arxiv_id": "2506.11102v1",
    "entry_id": "http://arxiv.org/abs/2506.11102v1",
    "title": "Evolutionary Perspectives on the Evaluation of LLM-Based AI Agents: A Comprehensive Survey",
    "summary": "The advent of large language models (LLMs), such as GPT, Gemini, and DeepSeek, has significantly advanced natural language processing, giving rise to sophisticated chatbots capable of diverse language-related tasks. The transition from these traditional LLM chatbots to more advanced AI agents represents a pivotal evolutionary step. However, existing evaluation frameworks often blur the distinctions between LLM chatbots and AI agents, leading to confusion among researchers selecting appropriate benchmarks. To bridge this gap, this paper introduces a systematic analysis of current evaluation approaches, grounded in an evolutionary perspective. We provide a detailed analytical framework that clearly differentiates AI agents from LLM chatbots along five key aspects: complex environment, multi-source instructor, dynamic feedback, multi-modal perception, and advanced capability. Further, we categorize existing evaluation benchmarks based on external environments driving forces, and resulting advanced internal capabilities. For each category, we delineate relevant evaluation attributes, presented comprehensively in practical reference tables. Finally, we synthesize current trends and outline future evaluation methodologies through four critical lenses: environment, agent, evaluator, and metrics. Our findings offer actionable guidance for researchers, facilitating the informed selection and application of benchmarks in AI agent evaluation, thus fostering continued advancement in this rapidly evolving research domain.",
    "authors": [
      "Jiachen Zhu",
      "Menghui Zhu",
      "Renting Rui",
      "Rong Shan",
      "Congmin Zheng",
      "Bo Chen",
      "Yunjia Xi",
      "Jianghao Lin",
      "Weiwen Liu",
      "Ruiming Tang",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-06T17:52:18Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11102v1"
  },
  {
    "arxiv_id": "2506.05873v1",
    "entry_id": "http://arxiv.org/abs/2506.05873v1",
    "title": "Research on Personalized Financial Product Recommendation by Integrating Large Language Models and Graph Neural Networks",
    "summary": "With the rapid growth of fintech, personalized financial product recommendations have become increasingly important. Traditional methods like collaborative filtering or content-based models often fail to capture users' latent preferences and complex relationships. We propose a hybrid framework integrating large language models (LLMs) and graph neural networks (GNNs). A pre-trained LLM encodes text data (e.g., user reviews) into rich feature vectors, while a heterogeneous user-product graph models interactions and social ties. Through a tailored message-passing mechanism, text and graph information are fused within the GNN to jointly optimize embeddings. Experiments on public and real-world financial datasets show our model outperforms standalone LLM or GNN in accuracy, recall, and NDCG, with strong interpretability. This work offers new insights for personalized financial recommendations and cross-modal fusion in broader recommendation tasks.",
    "authors": [
      "Yushang Zhao",
      "Yike Peng",
      "Dannier Li",
      "Yuxin Yang",
      "Chengrui Zhou",
      "Jing Dong"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-06-06T08:41:33Z",
    "pdf_url": "https://arxiv.org/pdf/2506.05873v1"
  },
  {
    "arxiv_id": "2506.11094v2",
    "entry_id": "http://arxiv.org/abs/2506.11094v2",
    "title": "The Scales of Justitia: A Comprehensive Survey on Safety Evaluation of LLMs",
    "summary": "With the rapid advancement of artificial intelligence, Large Language Models (LLMs) have shown remarkable capabilities in Natural Language Processing (NLP), including content generation, human-computer interaction, machine translation, and code generation. However, their widespread deployment has also raised significant safety concerns. In particular, LLM-generated content can exhibit unsafe behaviors such as toxicity, bias, or misinformation, especially in adversarial contexts, which has attracted increasing attention from both academia and industry. Although numerous studies have attempted to evaluate these risks, a comprehensive and systematic survey on safety evaluation of LLMs is still lacking. This work aims to fill this gap by presenting a structured overview of recent advances in safety evaluation of LLMs. Specifically, we propose a four-dimensional taxonomy: (i) Why to evaluate, which explores the background of safety evaluation of LLMs, how they differ from general LLMs evaluation, and the significance of such evaluation; (ii) What to evaluate, which examines and categorizes existing safety evaluation tasks based on key capabilities, including dimensions such as toxicity, robustness, ethics, bias and fairness, truthfulness, and related aspects; (iii) Where to evaluate, which summarizes the evaluation metrics, datasets and benchmarks currently used in safety evaluations; (iv) How to evaluate, which reviews existing mainstream evaluation methods based on the roles of the evaluators and some evaluation frameworks that integrate the entire evaluation pipeline. Finally, we identify the challenges in safety evaluation of LLMs and propose promising research directions to promote further advancement in this field. We emphasize the necessity of prioritizing safety evaluation to ensure the reliable and responsible deployment of LLMs in real-world applications.",
    "authors": [
      "Songyang Liu",
      "Chaozhuo Li",
      "Jiameng Qiu",
      "Xi Zhang",
      "Feiran Huang",
      "Litian Zhang",
      "Yiming Hei",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-06-06T05:50:50Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11094v2"
  },
  {
    "arxiv_id": "2506.05555v1",
    "entry_id": "http://arxiv.org/abs/2506.05555v1",
    "title": "Using Large Language Models to Simulate Human Behavioural Experiments: Port of Mars",
    "summary": "Collective risk social dilemmas (CRSD) highlight a trade-off between individual preferences and the need for all to contribute toward achieving a group objective. Problems such as climate change are in this category, and so it is critical to understand their social underpinnings. However, rigorous CRSD methodology often demands large-scale human experiments but it is difficult to guarantee sufficient power and heterogeneity over socio-demographic factors. Generative AI offers a potential complementary approach to address thisproblem. By replacing human participants with large language models (LLM), it allows for a scalable empirical framework. This paper focuses on the validity of this approach and whether it is feasible to represent a large-scale human-like experiment with sufficient diversity using LLM. In particular, where previous literature has focused on political surveys, virtual towns and classical game-theoretic examples, we focus on a complex CRSD used in the institutional economics and sustainability literature known as Port of Mars",
    "authors": [
      "Oliver Slumbers",
      "Joel Z. Leibo",
      "Marco A. Janssen"
    ],
    "categories": [
      "cs.MA",
      "cs.CY"
    ],
    "published": "2025-06-05T20:02:31Z",
    "pdf_url": "https://arxiv.org/pdf/2506.05555v1"
  },
  {
    "arxiv_id": "2506.05451v1",
    "entry_id": "http://arxiv.org/abs/2506.05451v1",
    "title": "Interpretation Meets Safety: A Survey on Interpretation Methods and Tools for Improving LLM Safety",
    "summary": "As large language models (LLMs) see wider real-world use, understanding and mitigating their unsafe behaviors is critical. Interpretation techniques can reveal causes of unsafe outputs and guide safety, but such connections with safety are often overlooked in prior surveys. We present the first survey that bridges this gap, introducing a unified framework that connects safety-focused interpretation methods, the safety enhancements they inform, and the tools that operationalize them. Our novel taxonomy, organized by LLM workflow stages, summarizes nearly 70 works at their intersections. We conclude with open challenges and future directions. This timely survey helps researchers and practitioners navigate key advancements for safer, more interpretable LLMs.",
    "authors": [
      "Seongmin Lee",
      "Aeree Cho",
      "Grace C. Kim",
      "ShengYun Peng",
      "Mansi Phute",
      "Duen Horng Chau"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-06-05T17:56:05Z",
    "pdf_url": "https://arxiv.org/pdf/2506.05451v1"
  },
  {
    "arxiv_id": "2506.04836v1",
    "entry_id": "http://arxiv.org/abs/2506.04836v1",
    "title": "Oversight Structures for Agentic AI in Public-Sector Organizations",
    "summary": "This paper finds that the introduction of agentic AI systems intensifies existing challenges to traditional public sector oversight mechanisms -- which rely on siloed compliance units and episodic approvals rather than continuous, integrated supervision. We identify five governance dimensions essential for responsible agent deployment: cross-departmental implementation, comprehensive evaluation, enhanced security protocols, operational visibility, and systematic auditing. We evaluate the capacity of existing oversight structures to meet these challenges, via a mixed-methods approach consisting of a literature review and interviews with civil servants in AI-related roles. We find that agent oversight poses intensified versions of three existing governance challenges: continuous oversight, deeper integration of governance and operational capabilities, and interdepartmental coordination. We propose approaches that both adapt institutional structures and design agent oversight compatible with public sector constraints.",
    "authors": [
      "Chris Schmitz",
      "Jonathan Rystrøm",
      "Jan Batzner"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-06-05T09:57:15Z",
    "pdf_url": "https://arxiv.org/pdf/2506.04836v1"
  },
  {
    "arxiv_id": "2506.04788v1",
    "entry_id": "http://arxiv.org/abs/2506.04788v1",
    "title": "Towards LLM-Centric Multimodal Fusion: A Survey on Integration Strategies and Techniques",
    "summary": "The rapid progress of Multimodal Large Language Models(MLLMs) has transformed the AI landscape. These models combine pre-trained LLMs with various modality encoders. This integration requires a systematic understanding of how different modalities connect to the language backbone. Our survey presents an LLM-centric analysis of current approaches. We examine methods for transforming and aligning diverse modal inputs into the language embedding space. This addresses a significant gap in existing literature. We propose a classification framework for MLLMs based on three key dimensions. First, we examine architectural strategies for modality integration. This includes both the specific integration mechanisms and the fusion level. Second, we categorize representation learning techniques as either joint or coordinate representations. Third, we analyze training paradigms, including training strategies and objective functions. By examining 125 MLLMs developed between 2021 and 2025, we identify emerging patterns in the field. Our taxonomy provides researchers with a structured overview of current integration techniques. These insights aim to guide the development of more robust multimodal integration strategies for future models built on pre-trained foundations.",
    "authors": [
      "Jisu An",
      "Junseok Lee",
      "Jeoungeun Lee",
      "Yongseok Son"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-06-05T09:14:41Z",
    "pdf_url": "https://arxiv.org/pdf/2506.04788v1"
  },
  {
    "arxiv_id": "2506.04654v2",
    "entry_id": "http://arxiv.org/abs/2506.04654v2",
    "title": "E-bike agents: Large Language Model-Driven E-Bike Accident Analysis and Severity Prediction",
    "summary": "E-bikes have rapidly gained popularity as a sustainable form of urban mobility, yet their safety implications remain underexplored. This paper analyzes injury incidents involving e-bikes and traditional bicycles using two sources of data, the CPSRMS (Consumer Product Safety Risk Management System Information Security Review Report) and NEISS (National Electronic Injury Surveillance System) datasets. We propose a standardized classification framework to identify and quantify injury causes and severity. By integrating incident narratives with demographic attributes, we reveal key differences in mechanical failure modes, injury severity patterns, and affected user groups. While both modes share common causes, such as loss of control and pedal malfunctions, e-bikes present distinct risks, including battery-related fires and brake failures. These findings highlight the need for tailored safety interventions and infrastructure design to support the safe integration of micromobility devices into urban transportation networks.",
    "authors": [
      "Zhichao Yang",
      "Jiashu He",
      "Mohammad B. Al-Khasawneh",
      "Darshan Pandit",
      "Cirillo Cinzia"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-05T05:49:41Z",
    "pdf_url": "https://arxiv.org/pdf/2506.04654v2"
  },
  {
    "arxiv_id": "2506.04565v1",
    "entry_id": "http://arxiv.org/abs/2506.04565v1",
    "title": "From Standalone LLMs to Integrated Intelligence: A Survey of Compound Al Systems",
    "summary": "Compound Al Systems (CAIS) is an emerging paradigm that integrates large language models (LLMs) with external components, such as retrievers, agents, tools, and orchestrators, to overcome the limitations of standalone models in tasks requiring memory, reasoning, real-time grounding, and multimodal understanding. These systems enable more capable and context-aware behaviors by composing multiple specialized modules into cohesive workflows. Despite growing adoption in both academia and industry, the CAIS landscape remains fragmented, lacking a unified framework for analysis, taxonomy, and evaluation. In this survey, we define the concept of CAIS, propose a multi-dimensional taxonomy based on component roles and orchestration strategies, and analyze four foundational paradigms: Retrieval-Augmented Generation (RAG), LLM Agents, Multimodal LLMs (MLLMs), and orchestration-centric architectures. We review representative systems, compare design trade-offs, and summarize evaluation methodologies across these paradigms. Finally, we identify key challenges-including scalability, interoperability, benchmarking, and coordination-and outline promising directions for future research. This survey aims to provide researchers and practitioners with a comprehensive foundation for understanding, developing, and advancing the next generation of system-level artificial intelligence.",
    "authors": [
      "Jiayi Chen",
      "Junyi Ye",
      "Guiling Wang"
    ],
    "categories": [
      "cs.MA",
      "cs.CL"
    ],
    "published": "2025-06-05T02:34:43Z",
    "pdf_url": "https://arxiv.org/pdf/2506.04565v1"
  },
  {
    "arxiv_id": "2506.06380v1",
    "entry_id": "http://arxiv.org/abs/2506.06380v1",
    "title": "Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events",
    "summary": "Extreme events, such as market crashes, natural disasters, and pandemics, are rare but catastrophic, often triggering cascading failures across interconnected systems. Accurate prediction and early warning can help minimize losses and improve preparedness. While data-driven methods offer powerful capabilities for extreme event modeling, they require abundant training data, yet extreme event data is inherently scarce, creating a fundamental challenge. Synthetic data generation has emerged as a powerful solution. However, existing surveys focus on general data with privacy preservation emphasis, rather than extreme events' unique performance requirements. This survey provides the first overview of synthetic data generation for extreme events. We systematically review generative modeling techniques and large language models, particularly those enhanced by statistical theory as well as specialized training and sampling mechanisms to capture heavy-tailed distributions. We summarize benchmark datasets and introduce a tailored evaluation framework covering statistical, dependence, visual, and task-oriented metrics. A central contribution is our in-depth analysis of each metric's applicability in extremeness and domain-specific adaptations, providing actionable guidance for model evaluation in extreme settings. We categorize key application domains and identify underexplored areas like behavioral finance, wildfires, earthquakes, windstorms, and infectious outbreaks. Finally, we outline open challenges, providing a structured foundation for advancing synthetic rare-event research.",
    "authors": [
      "Jingyi Gu",
      "Xuan Zhang",
      "Guiling Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE"
    ],
    "published": "2025-06-04T20:21:23Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06380v1"
  },
  {
    "arxiv_id": "2506.06377v1",
    "entry_id": "http://arxiv.org/abs/2506.06377v1",
    "title": "Evaluating Large Language Model Capabilities in Assessing Spatial Econometrics Research",
    "summary": "This paper investigates Large Language Models (LLMs) ability to assess the economic soundness and theoretical consistency of empirical findings in spatial econometrics. We created original and deliberately altered \"counterfactual\" summaries from 28 published papers (2005-2024), which were evaluated by a diverse set of LLMs. The LLMs provided qualitative assessments and structured binary classifications on variable choice, coefficient plausibility, and publication suitability. The results indicate that while LLMs can expertly assess the coherence of variable choices (with top models like GPT-4o achieving an overall F1 score of 0.87), their performance varies significantly when evaluating deeper aspects such as coefficient plausibility and overall publication suitability. The results further revealed that the choice of LLM, the specific characteristics of the paper and the interaction between these two factors significantly influence the accuracy of the assessment, particularly for nuanced judgments. These findings highlight LLMs' current strengths in assisting with initial, more surface-level checks and their limitations in performing comprehensive, deep economic reasoning, suggesting a potential assistive role in peer review that still necessitates robust human oversight.",
    "authors": [
      "Giuseppe Arbia",
      "Luca Morandini",
      "Vincenzo Nardelli"
    ],
    "categories": [
      "cs.CY",
      "cs.LG",
      "econ.EM",
      "stat.CO"
    ],
    "published": "2025-06-04T16:30:57Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06377v1"
  },
  {
    "arxiv_id": "2506.04133v4",
    "entry_id": "http://arxiv.org/abs/2506.04133v4",
    "title": "TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Agentic Multi-Agent Systems",
    "summary": "Agentic AI systems, built upon large language models (LLMs) and deployed in multi-agent configurations, are redefining intelligence, autonomy, collaboration, and decision-making across enterprise and societal domains. This review presents a structured analysis of Trust, Risk, and Security Management (TRiSM) in the context of LLM-based Agentic Multi-Agent Systems (AMAS). We begin by examining the conceptual foundations of Agentic AI and highlight its architectural distinctions from traditional AI agents. We then adapt and extend the AI TRiSM framework for Agentic AI, structured around key pillars: \\textit{ Explainability, ModelOps, Security, Privacy} and \\textit{their Lifecycle Governance}, each contextualized to the challenges of AMAS. A risk taxonomy is proposed to capture the unique threats and vulnerabilities of Agentic AI, ranging from coordination failures to prompt-based adversarial manipulation. To support practical assessment in Agentic AI works, we introduce two novel metrics: the Component Synergy Score (CSS), which quantifies the quality of inter-agent collaboration, and the Tool Utilization Efficacy (TUE), which evaluates the efficiency of tool use within agent workflows. We further discuss strategies for improving explainability in Agentic AI, as well as approaches to enhancing security and privacy through encryption, adversarial robustness, and regulatory compliance. The review concludes with a research roadmap for the responsible development and deployment of Agentic AI, highlighting key directions to align emerging systems with TRiSM principles-ensuring safety, transparency, and accountability in their operation.",
    "authors": [
      "Shaina Raza",
      "Ranjan Sapkota",
      "Manoj Karkee",
      "Christos Emmanouilidis"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-04T16:26:11Z",
    "pdf_url": "https://arxiv.org/pdf/2506.04133v4"
  },
  {
    "arxiv_id": "2506.04290v2",
    "entry_id": "http://arxiv.org/abs/2506.04290v2",
    "title": "Interpretable LLMs for Credit Risk: A Systematic Review and Taxonomy",
    "summary": "Large Language Models (LLM), which have developed in recent years, enable credit risk assessment through the analysis of financial texts such as analyst reports and corporate disclosures. This paper presents the first systematic review and taxonomy focusing on LLMbased approaches in credit risk estimation. We determined the basic model architectures by selecting 60 relevant papers published between 2020-2025 with the PRISMA research strategy. And we examined the data used for scenarios such as credit default prediction and risk analysis. Since the main focus of the paper is interpretability, we classify concepts such as explainability mechanisms, chain of thought prompts and natural language justifications for LLM-based credit models. The taxonomy organizes the literature under four main headings: model architectures, data types, explainability mechanisms and application areas. Based on this analysis, we highlight the main future trends and research gaps for LLM-based credit scoring systems. This paper aims to be a reference paper for artificial intelligence and financial researchers.",
    "authors": [
      "Muhammed Golec",
      "Maha AlabdulJalil"
    ],
    "categories": [
      "q-fin.RM",
      "cs.LG"
    ],
    "published": "2025-06-04T10:24:40Z",
    "pdf_url": "https://arxiv.org/pdf/2506.04290v2"
  },
  {
    "arxiv_id": "2506.03501v1",
    "entry_id": "http://arxiv.org/abs/2506.03501v1",
    "title": "Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing",
    "summary": "Content creation has dramatically progressed with the rapid advancement of large language models like ChatGPT and Claude. While this progress has greatly enhanced various aspects of life and work, it has also negatively affected certain areas of society. A recent survey revealed that nearly 30% of college students use generative AI to help write academic papers and reports. Most countermeasures treat the detection of AI-generated text as a binary classification task and thus lack robustness. This approach overlooks human involvement in the generation of content even though human-machine collaboration is becoming mainstream. Besides generating entire texts, people may use machines to complete or revise texts. Such human involvement varies case by case, which makes binary classification a less than satisfactory approach. We refer to this situation as participation detection obfuscation. We propose using BERTScore as a metric to measure human involvement in the generation process and a multi-task RoBERTa-based regressor trained on a token classification task to address this problem. To evaluate the effectiveness of this approach, we simulated academic-based scenarios and created a continuous dataset reflecting various levels of human involvement. All of the existing detectors we examined failed to detect the level of human involvement on this dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor mean squared error of 0.004). Moreover, it demonstrated some generalizability across generative models. Our code is available at https://github.com/gyc-nii/CAS-CS-and-dual-head-detector",
    "authors": [
      "Yuchen Guo",
      "Zhicheng Dou",
      "Huy H. Nguyen",
      "Ching-Chun Chang",
      "Saku Sugawara",
      "Isao Echizen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-04T02:31:36Z",
    "pdf_url": "https://arxiv.org/pdf/2506.03501v1"
  },
  {
    "arxiv_id": "2506.14809v1",
    "entry_id": "http://arxiv.org/abs/2506.14809v1",
    "title": "Impact of a Deployed LLM Survey Creation Tool through the IS Success Model",
    "summary": "Surveys are a cornerstone of Information Systems (IS) research, yet creating high-quality surveys remains labor-intensive, requiring both domain expertise and methodological rigor. With the evolution of large language models (LLMs), new opportunities emerge to automate survey generation. This paper presents the real-world deployment of an LLM-powered system designed to accelerate data collection while maintaining survey quality. Deploying such systems in production introduces real-world complexity, including diverse user needs and quality control. We evaluate the system using the DeLone and McLean IS Success Model to understand how generative AI can reshape a core IS method. This study makes three key contributions. To our knowledge, this is the first application of the IS Success Model to a generative AI system for survey creation. In addition, we propose a hybrid evaluation framework combining automated and human assessments. Finally, we implement safeguards that mitigate post-deployment risks and support responsible integration into IS workflows.",
    "authors": [
      "Peng Jiang",
      "Vinicius Cezar Monteiro de Lira",
      "Antonio Maiorino"
    ],
    "categories": [
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-06-03T22:36:36Z",
    "pdf_url": "https://arxiv.org/pdf/2506.14809v1"
  },
  {
    "arxiv_id": "2506.06359v1",
    "entry_id": "http://arxiv.org/abs/2506.06359v1",
    "title": "From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins",
    "summary": "Artificial intelligence (AI) has long promised to improve energy management in smart grids by enhancing situational awareness and supporting more effective decision-making. While traditional machine learning has demonstrated notable results in forecasting and optimization, it often struggles with generalization, situational awareness, and heterogeneous data integration. Recent advances in foundation models such as Transformer architecture and Large Language Models (LLMs) have demonstrated improved capabilities in modelling complex temporal and contextual relationships, as well as in multi-modal data fusion which is essential for most AI applications in the energy sector. In this review we synthesize the rapid expanding field of AI applications in the energy domain focusing on Transformers and LLMs. We examine the architectural foundations, domain-specific adaptations and practical implementations of transformer models across various forecasting and grid management tasks. We then explore the emerging role of LLMs in the field: adaptation and fine tuning for the energy sector, the type of tasks they are suited for, and the new challenges they introduce. Along the way, we highlight practical implementations, innovations, and areas where the research frontier is rapidly expanding. These recent developments reviewed underscore a broader trend: Generative AI (GenAI) is beginning to augment decision-making not only in high-level planning but also in day-to-day operations, from forecasting and grid balancing to workforce training and asset onboarding. Building on these developments, we introduce the concept of the Agentic Digital Twin, a next-generation model that integrates LLMs to bring autonomy, proactivity, and social interaction into digital twin-based energy management systems.",
    "authors": [
      "Gabriel Antonesi",
      "Tudor Cioara",
      "Ionut Anghel",
      "Vasilis Michalakopoulos",
      "Elissaios Sarmas",
      "Liana Toderean"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-06-03T10:02:07Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06359v1"
  },
  {
    "arxiv_id": "2506.02485v2",
    "entry_id": "http://arxiv.org/abs/2506.02485v2",
    "title": "Generative AI as a Pillar for Predicting 2D and 3D Wildfire Spread: Beyond Physics-Based Models and Traditional Deep Learning",
    "summary": "Wildfires increasingly threaten human life, ecosystems, and infrastructure, with events like the 2025 Palisades and Eaton fires in Los Angeles County underscoring the urgent need for more advanced prediction frameworks. Existing physics-based and deep learning models struggle to capture dynamic wildfire spread across both 2D and 3D domains, especially when incorporating real-time, multimodal geospatial data. This paper explores how generative Artificial Intelligence (AI) models-such as GANs, VAEs, and Transformers-can serve as transformative tools for wildfire prediction and simulation. These models offer superior capabilities in managing uncertainty, integrating multimodal inputs, and generating realistic, scalable wildfire scenarios. We introduce a new paradigm that leverages large language models (LLMs) for literature synthesis, classification, and knowledge extraction, conducting a systematic review of recent studies applying generative AI to fire prediction and monitoring. We highlight how generative approaches uniquely address challenges faced by traditional simulation and deep learning methods. Finally, we outline five key future directions for generative AI in wildfire management, including unified multimodal modeling of 2D and 3D dynamics, agentic AI systems and chatbots for decision intelligence, and real-time scenario generation on mobile devices, along with a discussion of critical challenges. Our findings advocate for a paradigm shift toward multimodal generative frameworks to support proactive, data-informed wildfire response.",
    "authors": [
      "Haowen Xu",
      "Sisi Zlatanova",
      "Ruiyu Liang",
      "Ismet Canbulat"
    ],
    "categories": [
      "cs.AI",
      "cs.CE"
    ],
    "published": "2025-06-03T05:54:40Z",
    "pdf_url": "https://arxiv.org/pdf/2506.02485v2"
  },
  {
    "arxiv_id": "2506.02481v1",
    "entry_id": "http://arxiv.org/abs/2506.02481v1",
    "title": "Do Language Models Think Consistently? A Study of Value Preferences Across Varying Response Lengths",
    "summary": "Evaluations of LLMs' ethical risks and value inclinations often rely on short-form surveys and psychometric tests, yet real-world use involves long-form, open-ended responses -- leaving value-related risks and preferences in practical settings largely underexplored. In this work, we ask: Do value preferences inferred from short-form tests align with those expressed in long-form outputs? To address this question, we compare value preferences elicited from short-form reactions and long-form responses, varying the number of arguments in the latter to capture users' differing verbosity preferences. Analyzing five LLMs (llama3-8b, gemma2-9b, mistral-7b, qwen2-7b, and olmo-7b), we find (1) a weak correlation between value preferences inferred from short-form and long-form responses across varying argument counts, and (2) similarly weak correlation between preferences derived from any two distinct long-form generation settings. (3) Alignment yields only modest gains in the consistency of value expression. Further, we examine how long-form generation attributes relate to value preferences, finding that argument specificity negatively correlates with preference strength, while representation across scenarios shows a positive correlation. Our findings underscore the need for more robust methods to ensure consistent value expression across diverse applications.",
    "authors": [
      "Inderjeet Nair",
      "Lu Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-03T05:52:03Z",
    "pdf_url": "https://arxiv.org/pdf/2506.02481v1"
  },
  {
    "arxiv_id": "2506.02280v3",
    "entry_id": "http://arxiv.org/abs/2506.02280v3",
    "title": "The State of Large Language Models for African Languages: Progress and Challenges",
    "summary": "Large Language Models (LLMs) are transforming Natural Language Processing (NLP), but their benefits are largely absent for Africa's 2,000 low-resource languages. This paper comparatively analyzes African language coverage across six LLMs, eight Small Language Models (SLMs), and six Specialized SLMs (SSLMs). The evaluation covers language coverage, training sets, technical limitations, script problems, and language modelling roadmaps. The work identifies 42 supported African languages and 23 available public data sets, and it shows a big gap where four languages (Amharic, Swahili, Afrikaans, and Malagasy) are always treated while there is over 98\\% of unsupported African languages. Moreover, the review shows that just Latin, Arabic, and Ge'ez scripts are identified while 20 active scripts are neglected. Some of the primary challenges are lack of data, tokenization biases, computational costs being very high, and evaluation issues. These issues demand language standardization, corpus development by the community, and effective adaptation methods for African languages.",
    "authors": [
      "Kedir Yassin Hussen",
      "Walelign Tewabe Sewunetie",
      "Abinew Ali Ayele",
      "Sukairaj Hafiz Imam",
      "Shamsuddeen Hassan Muhammad",
      "Seid Muhie Yimam"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-06-02T21:39:40Z",
    "pdf_url": "https://arxiv.org/pdf/2506.02280v3"
  },
  {
    "arxiv_id": "2506.02212v1",
    "entry_id": "http://arxiv.org/abs/2506.02212v1",
    "title": "Leveraging Natural Language Processing to Unravel the Mystery of Life: A Review of NLP Approaches in Genomics, Transcriptomics, and Proteomics",
    "summary": "Natural Language Processing (NLP) has transformed various fields beyond linguistics by applying techniques originally developed for human language to the analysis of biological sequences. This review explores the application of NLP methods to biological sequence data, focusing on genomics, transcriptomics, and proteomics. We examine how various NLP methods, from classic approaches like word2vec to advanced models employing transformers and hyena operators, are being adapted to analyze DNA, RNA, protein sequences, and entire genomes. The review also examines tokenization strategies and model architectures, evaluating their strengths, limitations, and suitability for different biological tasks. We further cover recent advances in NLP applications for biological data, such as structure prediction, gene expression, and evolutionary analysis, highlighting the potential of these methods for extracting meaningful insights from large-scale genomic data. As language models continue to advance, their integration into bioinformatics holds immense promise for advancing our understanding of biological processes in all domains of life.",
    "authors": [
      "Ella Rannon",
      "David Burstein"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "q-bio.GN"
    ],
    "published": "2025-06-02T19:54:03Z",
    "pdf_url": "https://arxiv.org/pdf/2506.02212v1"
  },
  {
    "arxiv_id": "2506.06353v1",
    "entry_id": "http://arxiv.org/abs/2506.06353v1",
    "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy",
    "summary": "The growing convergence between Large Language Models (LLMs) and electroencephalography (EEG) research is enabling new directions in neural decoding, brain-computer interfaces (BCIs), and affective computing. This survey offers a systematic review and structured taxonomy of recent advancements that utilize LLMs for EEG-based analysis and applications. We organize the literature into four domains: (1) LLM-inspired foundation models for EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal generation including image and 3D object synthesis, and (4) clinical applications and dataset management tools. The survey highlights how transformer-based architectures adapted through fine-tuning, few-shot, and zero-shot learning have enabled EEG-based models to perform complex tasks such as natural language generation, semantic interpretation, and diagnostic assistance. By offering a structured overview of modeling strategies, system designs, and application areas, this work serves as a foundational resource for future work to bridge natural language processing and neural signal analysis through language models.",
    "authors": [
      "Naseem Babu",
      "Jimson Mathew",
      "A. P. Vinod"
    ],
    "categories": [
      "eess.SP",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-06-02T18:58:57Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06353v1"
  },
  {
    "arxiv_id": "2506.01789v2",
    "entry_id": "http://arxiv.org/abs/2506.01789v2",
    "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and Accountability",
    "summary": "High-quality datasets are fundamental to training and evaluating machine learning models, yet their creation-especially with accurate human annotations-remains a significant challenge. Many dataset paper submissions lack originality, diversity, or rigorous quality control, and these shortcomings are often overlooked during peer review. Submissions also frequently omit essential details about dataset construction and properties. While existing tools such as datasheets aim to promote transparency, they are largely descriptive and do not provide standardized, measurable methods for evaluating data quality. Similarly, metadata requirements at conferences promote accountability but are inconsistently enforced. To address these limitations, this position paper advocates for the integration of systematic, rubric-based evaluation metrics into the dataset review process-particularly as submission volumes continue to grow. We also explore scalable, cost-effective methods for synthetic data generation, including dedicated tools and LLM-as-a-judge approaches, to support more efficient evaluation. As a call to action, we introduce DataRubrics, a structured framework for assessing the quality of both human- and model-generated datasets. Leveraging recent advances in LLM-based evaluation, DataRubrics offers a reproducible, scalable, and actionable solution for dataset quality assessment, enabling both authors and reviewers to uphold higher standards in data-centric research. We also release code to support reproducibility of LLM-based evaluations at https://github.com/datarubrics/datarubrics.",
    "authors": [
      "Genta Indra Winata",
      "David Anugraha",
      "Emmy Liu",
      "Alham Fikri Aji",
      "Shou-Yi Hung",
      "Aditya Parashar",
      "Patrick Amadeus Irawan",
      "Ruochen Zhang",
      "Zheng-Xin Yong",
      "Jan Christian Blaise Cruz",
      "Niklas Muennighoff",
      "Seungone Kim",
      "Hanyang Zhao",
      "Sudipta Kar",
      "Kezia Erina Suryoraharjo",
      "M. Farid Adilazuarda",
      "En-Shiun Annie Lee",
      "Ayu Purwarianti",
      "Derry Tanti Wijaya",
      "Monojit Choudhury"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "eess.AS"
    ],
    "published": "2025-06-02T15:31:52Z",
    "pdf_url": "https://arxiv.org/pdf/2506.01789v2"
  },
  {
    "arxiv_id": "2506.01698v1",
    "entry_id": "http://arxiv.org/abs/2506.01698v1",
    "title": "When LLMs Team Up: The Emergence of Collaborative Affective Computing",
    "summary": "Affective Computing (AC) is essential in bridging the gap between human emotional experiences and machine understanding. Traditionally, AC tasks in natural language processing (NLP) have been approached through pipeline architectures, which often suffer from structure rigidity that leads to inefficiencies and limited adaptability. The advent of Large Language Models (LLMs) has revolutionized this field by offering a unified approach to affective understanding and generation tasks, enhancing the potential for dynamic, real-time interactions. However, LLMs face cognitive limitations in affective reasoning, such as misinterpreting cultural nuances or contextual emotions, and hallucination problems in decision-making. To address these challenges, recent research advocates for LLM-based collaboration systems that emphasize interactions among specialized models and LLMs, mimicking human-like affective intelligence through the synergy of emotional and rational thinking that aligns with Dual Process Theory in psychology. This survey aims to provide a comprehensive overview of LLM-based collaboration systems in AC, exploring from structured collaborations to autonomous collaborations. Specifically, it includes: (1) A systematic review of existing methods, focusing on collaboration strategies, mechanisms, key functions, and applications; (2) Experimental comparisons of collaboration strategies across representative tasks in affective understanding and generation; (3) An analysis highlighting the potential of these systems to enhance robustness and adaptability in complex affective reasoning; (4) A discussion of key challenges and future research directions to further advance the field. This work is the first to systematically explore collaborative intelligence with LLMs in AC, paving the way for more powerful applications that approach human-like social intelligence.",
    "authors": [
      "Wenna Lai",
      "Haoran Xie",
      "Guandong Xu",
      "Qing Li",
      "S. Joe Qin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-02T14:00:54Z",
    "pdf_url": "https://arxiv.org/pdf/2506.01698v1"
  },
  {
    "arxiv_id": "2506.01463v1",
    "entry_id": "http://arxiv.org/abs/2506.01463v1",
    "title": "Agentic AI and Multiagentic: Are We Reinventing the Wheel?",
    "summary": "The terms Agentic AI and Multiagentic AI have recently gained popularity in discussions on generative artificial intelligence, often used to describe autonomous software agents and systems composed of such agents. However, the use of these terms confuses these buzzwords with well-established concepts in AI literature: intelligent agents and multi-agent systems. This article offers a critical analysis of this conceptual misuse. We review the theoretical origins of \"agentic\" in the social sciences (Bandura, 1986) and philosophical notions of intentionality (Dennett, 1971), and then summarise foundational works on intelligent agents and multi-agent systems by Wooldridge, Jennings and others. We examine classic agent architectures, from simple reactive agents to Belief-Desire-Intention (BDI) models, and highlight key properties (autonomy, reactivity, proactivity, social capability) that define agency in AI. We then discuss recent developments in large language models (LLMs) and agent platforms based on LLMs, including the emergence of LLM-powered AI agents and open-source multi-agent orchestration frameworks. We argue that the term AI Agentic is often used as a buzzword for what are essentially AI agents, and AI Multiagentic for what are multi-agent systems. This confusion overlooks decades of research in the field of autonomous agents and multi-agent systems. The article advocates for scientific and technological rigour and the use of established terminology from the state of the art in AI, incorporating the wealth of existing knowledge, including standards for multi-agent system platforms, communication languages and coordination and cooperation algorithms, agreement technologies (automated negotiation, argumentation, virtual organisations, trust, reputation, etc.), into the new and promising wave of LLM-based AI agents, so as not to end up reinventing the wheel.",
    "authors": [
      "V. Botti"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-06-02T09:19:11Z",
    "pdf_url": "https://arxiv.org/pdf/2506.01463v1"
  },
  {
    "arxiv_id": "2506.01307v1",
    "entry_id": "http://arxiv.org/abs/2506.01307v1",
    "title": "Align is not Enough: Multimodal Universal Jailbreak Attack against Multimodal Large Language Models",
    "summary": "Large Language Models (LLMs) have evolved into Multimodal Large Language Models (MLLMs), significantly enhancing their capabilities by integrating visual information and other types, thus aligning more closely with the nature of human intelligence, which processes a variety of data forms beyond just text. Despite advancements, the undesirable generation of these models remains a critical concern, particularly due to vulnerabilities exposed by text-based jailbreak attacks, which have represented a significant threat by challenging existing safety protocols. Motivated by the unique security risks posed by the integration of new and old modalities for MLLMs, we propose a unified multimodal universal jailbreak attack framework that leverages iterative image-text interactions and transfer-based strategy to generate a universal adversarial suffix and image. Our work not only highlights the interaction of image-text modalities can be used as a critical vulnerability but also validates that multimodal universal jailbreak attacks can bring higher-quality undesirable generations across different MLLMs. We evaluate the undesirable context generation of MLLMs like LLaVA, Yi-VL, MiniGPT4, MiniGPT-v2, and InstructBLIP, and reveal significant multimodal safety alignment issues, highlighting the inadequacy of current safety mechanisms against sophisticated multimodal attacks. This study underscores the urgent need for robust safety measures in MLLMs, advocating for a comprehensive review and enhancement of security protocols to mitigate potential risks associated with multimodal capabilities.",
    "authors": [
      "Youze Wang",
      "Wenbo Hu",
      "Yinpeng Dong",
      "Jing Liu",
      "Hanwang Zhang",
      "Richang Hong"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-06-02T04:33:56Z",
    "pdf_url": "https://arxiv.org/pdf/2506.01307v1"
  },
  {
    "arxiv_id": "2506.01257v1",
    "entry_id": "http://arxiv.org/abs/2506.01257v1",
    "title": "DeepSeek in Healthcare: A Survey of Capabilities, Risks, and Clinical Applications of Open-Source Large Language Models",
    "summary": "DeepSeek-R1 is a cutting-edge open-source large language model (LLM) developed by DeepSeek, showcasing advanced reasoning capabilities through a hybrid architecture that integrates mixture of experts (MoE), chain of thought (CoT) reasoning, and reinforcement learning. Released under the permissive MIT license, DeepSeek-R1 offers a transparent and cost-effective alternative to proprietary models like GPT-4o and Claude-3 Opus; it excels in structured problem-solving domains such as mathematics, healthcare diagnostics, code generation, and pharmaceutical research. The model demonstrates competitive performance on benchmarks like the United States Medical Licensing Examination (USMLE) and American Invitational Mathematics Examination (AIME), with strong results in pediatric and ophthalmologic clinical decision support tasks. Its architecture enables efficient inference while preserving reasoning depth, making it suitable for deployment in resource-constrained settings. However, DeepSeek-R1 also exhibits increased vulnerability to bias, misinformation, adversarial manipulation, and safety failures - especially in multilingual and ethically sensitive contexts. This survey highlights the model's strengths, including interpretability, scalability, and adaptability, alongside its limitations in general language fluency and safety alignment. Future research priorities include improving bias mitigation, natural language comprehension, domain-specific validation, and regulatory compliance. Overall, DeepSeek-R1 represents a major advance in open, scalable AI, underscoring the need for collaborative governance to ensure responsible and equitable deployment.",
    "authors": [
      "Jiancheng Ye",
      "Sophie Bronstein",
      "Jiarui Hai",
      "Malak Abu Hashish"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-06-02T02:17:04Z",
    "pdf_url": "https://arxiv.org/pdf/2506.01257v1"
  },
  {
    "arxiv_id": "2506.04255v1",
    "entry_id": "http://arxiv.org/abs/2506.04255v1",
    "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource Utilization",
    "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous Multi-Agent System (MAS) development. However, current frameworks often lack flexibility, resource awareness, model diversity, and autonomous tool creation. This paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent Resource Utilization), a novel MAS framework enhancing flexibility, resource efficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically managing specialized \"employee\" agents, instantiated based on task needs and resource constraints (cost, memory). Its hybrid intelligence prioritizes smaller, local LLMs (via Ollama) while flexibly using external APIs and larger models when necessary. An economic model with hiring/firing costs promotes team stability and efficient resource allocation. The system also includes autonomous API tool creation and a memory function. Evaluations on tasks like academic paper review (58% success), safety assessments (100% on a JailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash on GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate HASHIRU's capabilities. Case studies illustrate its self-improvement via autonomous cost model generation, tool integration, and budget management. HASHIRU offers a promising approach for more robust, efficient, and adaptable MAS through dynamic hierarchical control, resource-aware hybrid intelligence, and autonomous functional extension. Source code and benchmarks are available at https://github.com/HASHIRU-AI/HASHIRU and https://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is available at https://hashiruagentx-hashiruai.hf.space upon request.",
    "authors": [
      "Kunal Pai",
      "Parth Shah",
      "Harshil Patel"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-06-01T17:33:16Z",
    "pdf_url": "https://arxiv.org/pdf/2506.04255v1"
  },
  {
    "arxiv_id": "2506.02055v1",
    "entry_id": "http://arxiv.org/abs/2506.02055v1",
    "title": "Will Agents Replace Us? Perceptions of Autonomous Multi-Agent AI",
    "summary": "Autonomous multi-agent AI systems are poised to transform various industries, particularly software development and knowledge work. Understanding current perceptions among professionals is crucial for anticipating adoption challenges, ethical considerations, and future workforce development. This study analyzes responses from 130 participants to a survey on the capabilities, impact, and governance of AI agents. We explore expected timelines for AI replacing programmers, identify perceived barriers to deployment, and examine beliefs about responsibility when agents make critical decisions. Key findings reveal three distinct clusters of respondents. While the study explored factors associated with current AI agent deployment, the initial logistic regression model did not yield statistically significant predictors, suggesting that deployment decisions are complex and may be influenced by factors not fully captured or that a larger sample is needed. These insights highlight the need for organizations to address compliance concerns (a commonly cited barrier) and establish clear governance frameworks as they integrate autonomous agents into their workflows.",
    "authors": [
      "Nikola Balic"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-06-01T11:02:52Z",
    "pdf_url": "https://arxiv.org/pdf/2506.02055v1"
  },
  {
    "arxiv_id": "2506.00943v1",
    "entry_id": "http://arxiv.org/abs/2506.00943v1",
    "title": "Legal Compliance Evaluation of Smart Contracts Generated By Large Language Models",
    "summary": "Smart contracts can implement and automate parts of legal contracts, but ensuring their legal compliance remains challenging. Existing approaches such as formal specification, verification, and model-based development require expertise in both legal and software development domains, as well as extensive manual effort. Given the recent advances of Large Language Models (LLMs) in code generation, we investigate their ability to generate legally compliant smart contracts directly from natural language legal contracts, addressing these challenges. We propose a novel suite of metrics to quantify legal compliance based on modeling both legal and smart contracts as processes and comparing their behaviors. We select four LLMs, generate 20 smart contracts based on five legal contracts, and analyze their legal compliance. We find that while all LLMs generate syntactically correct code, there is significant variance in their legal compliance with larger models generally showing higher levels of compliance. We also evaluate the proposed metrics against properties of software metrics, showing they provide fine-grained distinctions, enable nuanced comparisons, and are applicable across domains for code from any source, LLM or developer. Our results suggest that LLMs can assist in generating starter code for legally compliant smart contracts with strict reviews, and the proposed metrics provide a foundation for automated and self-improving development workflows.",
    "authors": [
      "Chanuka Wijayakoon",
      "Hai Dong",
      "H. M. N. Dilum Bandara",
      "Zahir Tari",
      "Anurag Soin"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-06-01T10:20:13Z",
    "pdf_url": "https://arxiv.org/pdf/2506.00943v1"
  },
  {
    "arxiv_id": "2506.02049v1",
    "entry_id": "http://arxiv.org/abs/2506.02049v1",
    "title": "EvoGit: Decentralized Code Evolution via Git-Based Multi-Agent Collaboration",
    "summary": "We introduce EvoGit, a decentralized multi-agent framework for collaborative software development driven by autonomous code evolution. EvoGit deploys a population of independent coding agents, each proposing edits to a shared codebase without centralized coordination, explicit message passing, or shared memory. Instead, all coordination emerges through a Git-based phylogenetic graph that tracks the full version lineage and enables agents to asynchronously read from and write to the evolving code repository. This graph-based structure supports fine-grained branching, implicit concurrency, and scalable agent interaction while preserving a consistent historical record. Human involvement is minimal but strategic: users define high-level goals, periodically review the graph, and provide lightweight feedback to promote promising directions or prune unproductive ones. Experiments demonstrate EvoGit's ability to autonomously produce functional and modular software artifacts across two real-world tasks: (1) building a web application from scratch using modern frameworks, and (2) constructing a meta-level system that evolves its own language-model-guided solver for the bin-packing optimization problem. Our results underscore EvoGit's potential to establish a new paradigm for decentralized, automated, and continual software development. EvoGit is open-sourced at https://github.com/BillHuang2001/evogit.",
    "authors": [
      "Beichen Huang",
      "Ran Cheng",
      "Kay Chen Tan"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.MA",
      "cs.NE"
    ],
    "published": "2025-06-01T05:20:42Z",
    "pdf_url": "https://arxiv.org/pdf/2506.02049v1"
  },
  {
    "arxiv_id": "2506.02046v1",
    "entry_id": "http://arxiv.org/abs/2506.02046v1",
    "title": "Machine vs Machine: Using AI to Tackle Generative AI Threats in Assessment",
    "summary": "This paper presents a theoretical framework for addressing the challenges posed by generative artificial intelligence (AI) in higher education assessment through a machine-versus-machine approach. Large language models like GPT-4, Claude, and Llama increasingly demonstrate the ability to produce sophisticated academic content, traditional assessment methods face an existential threat, with surveys indicating 74-92% of students experimenting with these tools for academic purposes. Current responses, ranging from detection software to manual assessment redesign, show significant limitations: detection tools demonstrate bias against non-native English writers and can be easily circumvented, while manual frameworks rely heavily on subjective judgment and assume static AI capabilities. This paper introduces a dual strategy paradigm combining static analysis and dynamic testing to create a comprehensive theoretical framework for assessment vulnerability evaluation. The static analysis component comprises eight theoretically justified elements: specificity and contextualization, temporal relevance, process visibility requirements, personalization elements, resource accessibility, multimodal integration, ethical reasoning requirements, and collaborative elements. Each element addresses specific limitations in generative AI capabilities, creating barriers that distinguish authentic human learning from AI-generated simulation. The dynamic testing component provides a complementary approach through simulation-based vulnerability assessment, addressing limitations in pattern-based analysis. The paper presents a theoretical framework for vulnerability scoring, including the conceptual basis for quantitative assessment, weighting frameworks, and threshold determination theory.",
    "authors": [
      "Mohammad Saleh Torkestani",
      "Taha Mansouri"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-05-31T22:29:43Z",
    "pdf_url": "https://arxiv.org/pdf/2506.02046v1"
  },
  {
    "arxiv_id": "2506.12060v1",
    "entry_id": "http://arxiv.org/abs/2506.12060v1",
    "title": "Organizational Adaptation to Generative AI in Cybersecurity: A Systematic Review",
    "summary": "Cybersecurity organizations are adapting to GenAI integration through modified frameworks and hybrid operational processes, with success influenced by existing security maturity, regulatory requirements, and investments in human capital and infrastructure. This qualitative research employs systematic document analysis and comparative case study methodology to examine how cybersecurity organizations adapt their threat modeling frameworks and operational processes to address generative artificial intelligence integration. Through examination of 25 studies from 2022 to 2025, the research documents substantial transformation in organizational approaches to threat modeling, moving from traditional signature-based systems toward frameworks incorporating artificial intelligence capabilities. The research identifies three primary adaptation patterns: Large Language Model integration for security applications, GenAI frameworks for risk detection and response automation, and AI/ML integration for threat hunting. Organizations with mature security infrastructures, particularly in finance and critical infrastructure sectors, demonstrate higher readiness through structured governance approaches, dedicated AI teams, and robust incident response processes. Organizations achieve successful GenAI integration when they maintain appropriate human oversight of automated systems, address data quality concerns and explainability requirements, and establish governance frameworks tailored to their specific sectors. Organizations encounter ongoing difficulties with privacy protection, bias reduction, personnel training, and defending against adversarial attacks. This work advances understanding of how organizations adopt innovative technologies in high-stakes environments and offers actionable insights for cybersecurity professionals implementing GenAI systems.",
    "authors": [
      "Christopher Nott"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-05-31T18:16:11Z",
    "pdf_url": "https://arxiv.org/pdf/2506.12060v1"
  },
  {
    "arxiv_id": "2506.00551v2",
    "entry_id": "http://arxiv.org/abs/2506.00551v2",
    "title": "AnnaAgent: Dynamic Evolution Agent System with Multi-Session Memory for Realistic Seeker Simulation",
    "summary": "Constrained by the cost and ethical concerns of involving real seekers in AI-driven mental health, researchers develop LLM-based conversational agents (CAs) with tailored configurations, such as profiles, symptoms, and scenarios, to simulate seekers. While these efforts advance AI in mental health, achieving more realistic seeker simulation remains hindered by two key challenges: dynamic evolution and multi-session memory. Seekers' mental states often fluctuate during counseling, which typically spans multiple sessions. To address this, we propose AnnaAgent, an emotional and cognitive dynamic agent system equipped with tertiary memory. AnnaAgent incorporates an emotion modulator and a complaint elicitor trained on real counseling dialogues, enabling dynamic control of the simulator's configurations. Additionally, its tertiary memory mechanism effectively integrates short-term and long-term memory across sessions. Evaluation results, both automated and manual, demonstrate that AnnaAgent achieves more realistic seeker simulation in psychological counseling compared to existing baselines. The ethically reviewed and screened code can be found on https://github.com/sci-m-wang/AnnaAgent.",
    "authors": [
      "Ming Wang",
      "Peidong Wang",
      "Lin Wu",
      "Xiaocui Yang",
      "Daling Wang",
      "Shi Feng",
      "Yuxin Chen",
      "Bixuan Wang",
      "Yifei Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-31T13:15:51Z",
    "pdf_url": "https://arxiv.org/pdf/2506.00551v2"
  },
  {
    "arxiv_id": "2506.03191v1",
    "entry_id": "http://arxiv.org/abs/2506.03191v1",
    "title": "Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward",
    "summary": "This paper presents an in-depth survey on the use of multimodal Generative Artificial Intelligence (GenAI) and autoregressive Large Language Models (LLMs) for human motion understanding and generation, offering insights into emerging methods, architectures, and their potential to advance realistic and versatile motion synthesis. Focusing exclusively on text and motion modalities, this research investigates how textual descriptions can guide the generation of complex, human-like motion sequences. The paper explores various generative approaches, including autoregressive models, diffusion models, Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and transformer-based models, by analyzing their strengths and limitations in terms of motion quality, computational efficiency, and adaptability. It highlights recent advances in text-conditioned motion generation, where textual inputs are used to control and refine motion outputs with greater precision. The integration of LLMs further enhances these models by enabling semantic alignment between instructions and motion, improving coherence and contextual relevance. This systematic survey underscores the transformative potential of text-to-motion GenAI and LLM architectures in applications such as healthcare, humanoids, gaming, animation, and assistive technologies, while addressing ongoing challenges in generating efficient and realistic human motion.",
    "authors": [
      "Muhammad Islam",
      "Tao Huang",
      "Euijoon Ahn",
      "Usman Naseem"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-05-31T11:02:24Z",
    "pdf_url": "https://arxiv.org/pdf/2506.03191v1"
  },
  {
    "arxiv_id": "2506.00312v1",
    "entry_id": "http://arxiv.org/abs/2506.00312v1",
    "title": "An evaluation of LLMs for generating movie reviews: GPT-4o, Gemini-2.0 and DeepSeek-V3",
    "summary": "Large language models (LLMs) have been prominent in various tasks, including text generation and summarisation. The applicability of LLMs to the generation of product reviews is gaining momentum, paving the way for the generation of movie reviews. In this study, we propose a framework that generates movie reviews using three LLMs (GPT-4o, DeepSeek-V3, and Gemini-2.0), and evaluate their performance by comparing the generated outputs with IMDb user reviews. We use movie subtitles and screenplays as input to the LLMs and investigate how they affect the quality of reviews generated. We review the LLM-based movie reviews in terms of vocabulary, sentiment polarity, similarity, and thematic consistency in comparison to IMDB user reviews. The results demonstrate that LLMs are capable of generating syntactically fluent and structurally complete movie reviews. Nevertheless, there is still a noticeable gap in emotional richness and stylistic coherence between LLM-generated and IMDb reviews, suggesting that further refinement is needed to improve the overall quality of movie review generation. We provided a survey-based analysis where participants were told to distinguish between LLM and IMDb user reviews. The results show that LLM-generated reviews are difficult to distinguish from IMDB user reviews. We found that DeepSeek-V3 produced the most balanced reviews, closely matching IMDb reviews. GPT-4o overemphasised positive emotions, while Gemini-2.0 captured negative emotions better but showed excessive emotional intensity.",
    "authors": [
      "Brendan Sands",
      "Yining Wang",
      "Chenhao Xu",
      "Yuxuan Zhou",
      "Lai Wei",
      "Rohitash Chandra"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-30T23:45:53Z",
    "pdf_url": "https://arxiv.org/pdf/2506.00312v1"
  },
  {
    "arxiv_id": "2506.08027v2",
    "entry_id": "http://arxiv.org/abs/2506.08027v2",
    "title": "Recipes for Pre-training LLMs with MXFP8",
    "summary": "Using fewer bits to represent model parameters and related tensors during pre-training has become a required technique for improving GPU efficiency without sacrificing accuracy. Microscaling (MX) formats introduced in NVIDIA Blackwell generation of GPUs represent a major advancement of this technique, making it practical to combine narrow floating-point data types with finer granularity per-block scaling factors. In turn, this enables both quantization of more tensors than previous approaches and more efficient execution of operations on those tensors.\n  Effective use of MX-formats requires careful choices of various parameters. In this paper we review these choices and show how MXFP8-E4M3 datatype and a specific number conversion algorithm result in training sessions that match those carried out in BF16. We present results using models with up to 8B parameters, trained on high-quality datasets of up to 15T tokens.",
    "authors": [
      "Asit Mishra",
      "Dusan Stosic",
      "Simon Layton",
      "Paulius Micikevicius"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2025-05-30T21:08:15Z",
    "pdf_url": "https://arxiv.org/pdf/2506.08027v2"
  },
  {
    "arxiv_id": "2506.00128v1",
    "entry_id": "http://arxiv.org/abs/2506.00128v1",
    "title": "Applying Large Language Models to Issue Classification: Revisiting with Extended Data and New Models",
    "summary": "Effective prioritization of issue reports in software engineering helps to optimize resource allocation and information recovery. However, manual issue classification is laborious and lacks scalability. As an alternative, many open source software (OSS) projects employ automated processes for this task, yet this method often relies on large datasets for adequate training. Traditionally, machine learning techniques have been used for issue classification. More recently, large language models (LLMs) have emerged as powerful tools for addressing a range of software engineering challenges, including code and test generation, mapping new requirements to legacy software endpoints, and conducting code reviews. The following research investigates an automated approach to issue classification based on LLMs. By leveraging the capabilities of such models, we aim to develop a robust system for prioritizing issue reports, mitigating the necessity for extensive training data while also maintaining reliability in classification. In our research, we developed an LLM-based approach for accurately labeling issues by selecting two of the most prominent large language models. We then compared their performance across multiple datasets. Our findings show that GPT-4o achieved the best results in classifying issues from the NLBSE 2024 competition. Moreover, GPT-4o outperformed DeepSeek R1, achieving an F1 score 20% higher when both models were trained on the same dataset from the NLBSE 2023 competition, which was ten times larger than the NLBSE 2024 dataset. The fine-tuned GPT-4o model attained an average F1 score of 80.7%, while the fine-tuned DeepSeek R1 model achieved 59.33%. Increasing the dataset size did not improve the F1 score, reducing the dependence on massive datasets for building an efficient solution to issue classification.",
    "authors": [
      "Gabriel Aracena",
      "Kyle Luster",
      "Fabio Santos",
      "Igor Steinmacher",
      "Marco A. Gerosa"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2025-05-30T18:02:55Z",
    "pdf_url": "https://arxiv.org/pdf/2506.00128v1"
  },
  {
    "arxiv_id": "2505.24767v1",
    "entry_id": "http://arxiv.org/abs/2505.24767v1",
    "title": "A survey of using EHR as real-world evidence for discovering and validating new drug indications",
    "summary": "Electronic Health Records (EHRs) have been increasingly used as real-world evidence (RWE) to support the discovery and validation of new drug indications. This paper surveys current approaches to EHR-based drug repurposing, covering data sources, processing methodologies, and representation techniques. It discusses study designs and statistical frameworks for evaluating drug efficacy. Key challenges in validation are discussed, with emphasis on the role of large language models (LLMs) and target trial emulation. By synthesizing recent developments and methodological advances, this work provides a foundational resource for researchers aiming to translate real-world data into actionable drug-repurposing evidence.",
    "authors": [
      "Nabasmita Talukdar",
      "Xiaodan Zhang",
      "Shreya Paithankar",
      "Hui Wang",
      "Bin Chen"
    ],
    "categories": [
      "stat.AP",
      "cs.AI"
    ],
    "published": "2025-05-30T16:30:54Z",
    "pdf_url": "https://arxiv.org/pdf/2505.24767v1"
  },
  {
    "arxiv_id": "2505.24553v1",
    "entry_id": "http://arxiv.org/abs/2505.24553v1",
    "title": "CREFT: Sequential Multi-Agent LLM for Character Relation Extraction",
    "summary": "Understanding complex character relations is crucial for narrative analysis and efficient script evaluation, yet existing extraction methods often fail to handle long-form narratives with nuanced interactions. To address this challenge, we present CREFT, a novel sequential framework leveraging specialized Large Language Model (LLM) agents. First, CREFT builds a base character graph through knowledge distillation, then iteratively refines character composition, relation extraction, role identification, and group assignments. Experiments on a curated Korean drama dataset demonstrate that CREFT significantly outperforms single-agent LLM baselines in both accuracy and completeness. By systematically visualizing character networks, CREFT streamlines narrative comprehension and accelerates script review -- offering substantial benefits to the entertainment, publishing, and educational sectors.",
    "authors": [
      "Ye Eun Chun",
      "Taeyoon Hwang",
      "Seung-won Hwang",
      "Byung-Hak Kim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-30T13:01:36Z",
    "pdf_url": "https://arxiv.org/pdf/2505.24553v1"
  },
  {
    "arxiv_id": "2506.06324v1",
    "entry_id": "http://arxiv.org/abs/2506.06324v1",
    "title": "Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review",
    "summary": "Several papers have delved into the challenges of human-AI-robot co-learning and co-adaptation. It has been noted that the terminology used to describe this collaborative relationship in existing studies needs to be more consistent. For example, the prefix \"co\" is used interchangeably to represent both \"collaborative\" and \"mutual,\" and the terms \"co-learning\" and \"co-adaptation\" are sometimes used interchangeably. However, they can reflect subtle differences in the focus of the studies. The current scoping review's primary research question (RQ1) aims to gather existing papers discussing this collaboration pattern and examine the terms researchers use to describe this human-agent relationship. Given the relative newness of this area of study, we are also keen on exploring the specific types of intelligent agents and task domains that have been considered in existing research (RQ2). This exploration is significant as it can shed light on the diversity of human-agent interactions, from one-time to continuous learning/adaptation scenarios. It can also help us understand the dynamics of human-agent interactions in different task domains, guiding our expectations towards research situated in dynamic, complex domains. Our third objective (RQ3) is to investigate the cognitive theories and frameworks that have been utilized in existing studies to measure human-agent co-learning and co-adaptation. This investigation is crucial as it can help us understand the theoretical underpinnings of human-agent collaboration and adaptation, and it can also guide us in identifying any new frameworks proposed specifically for this type of relationship.",
    "authors": [
      "Shruti Kumar",
      "Xiaoyu Chen",
      "Xiaomei Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-30T03:10:44Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06324v1"
  },
  {
    "arxiv_id": "2505.23580v1",
    "entry_id": "http://arxiv.org/abs/2505.23580v1",
    "title": "Engineering Serendipity through Recommendations of Items with Atypical Aspects",
    "summary": "A restaurant dinner or a hotel stay may lead to memorable experiences when guests encounter unexpected aspects that also match their interests. For example, an origami-making station in the waiting area of a restaurant may be both surprising and enjoyable for a customer who is passionate about paper crafts. Similarly, an exhibit of 18th century harpsichords would be atypical for a hotel lobby and likely pique the interest of a guest who has a passion for Baroque music. Motivated by this insight, in this paper we introduce the new task of engineering serendipity through recommendations of items with atypical aspects. We describe an LLM-based system pipeline that extracts atypical aspects from item reviews, then estimates and aggregates their user-specific utility in a measure of serendipity potential that is used to rerank a list of items recommended to the user. To facilitate system development and evaluation, we introduce a dataset of Yelp reviews that are manually annotated with atypical aspects and a dataset of artificially generated user profiles, together with crowdsourced annotations of user-aspect utility values. Furthermore, we introduce a custom procedure for dynamic selection of in-context learning examples, which is shown to improve LLM-based judgments of atypicality and utility. Experimental evaluations show that serendipity-based rankings generated by the system are highly correlated with ground truth rankings for which serendipity scores are computed from manual annotations of atypical aspects and their user-dependent utility. Overall, we hope that the new recommendation task and the associated system presented in this paper catalyze further research into recommendation approaches that go beyond accuracy in their pursuit of enhanced user satisfaction.\n  The datasets and the code are made publicly available at https://github.com/ramituncc49er/ATARS .",
    "authors": [
      "Ramit Aditya",
      "Razvan Bunescu",
      "Smita Nannaware",
      "Erfan Al-Hossami"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-05-29T15:53:21Z",
    "pdf_url": "https://arxiv.org/pdf/2505.23580v1"
  },
  {
    "arxiv_id": "2505.23559v1",
    "entry_id": "http://arxiv.org/abs/2505.23559v1",
    "title": "SafeScientist: Toward Risk-Aware Scientific Discoveries by LLM Agents",
    "summary": "Recent advancements in large language model (LLM) agents have significantly accelerated scientific discovery automation, yet concurrently raised critical ethical and safety concerns. To systematically address these challenges, we introduce \\textbf{SafeScientist}, an innovative AI scientist framework explicitly designed to enhance safety and ethical responsibility in AI-driven scientific exploration. SafeScientist proactively refuses ethically inappropriate or high-risk tasks and rigorously emphasizes safety throughout the research process. To achieve comprehensive safety oversight, we integrate multiple defensive mechanisms, including prompt monitoring, agent-collaboration monitoring, tool-use monitoring, and an ethical reviewer component. Complementing SafeScientist, we propose \\textbf{SciSafetyBench}, a novel benchmark specifically designed to evaluate AI safety in scientific contexts, comprising 240 high-risk scientific tasks across 6 domains, alongside 30 specially designed scientific tools and 120 tool-related risk tasks. Extensive experiments demonstrate that SafeScientist significantly improves safety performance by 35\\% compared to traditional AI scientist frameworks, without compromising scientific output quality. Additionally, we rigorously validate the robustness of our safety pipeline against diverse adversarial attack methods, further confirming the effectiveness of our integrated approach. The code and data will be available at https://github.com/ulab-uiuc/SafeScientist. \\textcolor{red}{Warning: this paper contains example data that may be offensive or harmful.}",
    "authors": [
      "Kunlun Zhu",
      "Jiaxun Zhang",
      "Ziheng Qi",
      "Nuoxing Shang",
      "Zijia Liu",
      "Peixuan Han",
      "Yue Su",
      "Haofei Yu",
      "Jiaxuan You"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-29T15:35:58Z",
    "pdf_url": "https://arxiv.org/pdf/2505.23559v1"
  },
  {
    "arxiv_id": "2505.23486v2",
    "entry_id": "http://arxiv.org/abs/2505.23486v2",
    "title": "Autoformalization in the Era of Large Language Models: A Survey",
    "summary": "Autoformalization, the process of transforming informal mathematical propositions into verifiable formal representations, is a foundational task in automated theorem proving, offering a new perspective on the use of mathematics in both theoretical and applied domains. Driven by the rapid progress in artificial intelligence, particularly large language models (LLMs), this field has witnessed substantial growth, bringing both new opportunities and unique challenges. In this survey, we provide a comprehensive overview of recent advances in autoformalization from both mathematical and LLM-centric perspectives. We examine how autoformalization is applied across various mathematical domains and levels of difficulty, and analyze the end-to-end workflow from data preprocessing to model design and evaluation. We further explore the emerging role of autoformalization in enhancing the verifiability of LLM-generated outputs, highlighting its potential to improve both the trustworthiness and reasoning capabilities of LLMs. Finally, we summarize key open-source models and datasets supporting current research, and discuss open challenges and promising future directions for the field.",
    "authors": [
      "Ke Weng",
      "Lun Du",
      "Sirui Li",
      "Wangyue Lu",
      "Haozhe Sun",
      "Hengyu Liu",
      "Tiancheng Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-29T14:34:54Z",
    "pdf_url": "https://arxiv.org/pdf/2505.23486v2"
  },
  {
    "arxiv_id": "2506.00066v1",
    "entry_id": "http://arxiv.org/abs/2506.00066v1",
    "title": "Literature Review Of Multi-Agent Debate For Problem-Solving",
    "summary": "Multi-agent large language models (MA-LLMs) are a rapidly growing research area that leverages multiple interacting language agents to tackle complex tasks, outperforming single-agent large language models. This literature review synthesizes the latest research on agent profiles, communication structures, and decision-making processes, drawing insights from both traditional multi-agent systems and state-of-the-art MA-LLM studies. In doing so, it aims to address the lack of direct comparisons in the field, illustrating how factors like scalability, communication structure, and decision-making processes influence MA-LLM performance. By examining frequent practices and outlining current challenges, the review reveals that multi-agent approaches can yield superior results but also face elevated computational costs and under-explored challenges unique to MA-LLM. Overall, these findings provide researchers and practitioners with a roadmap for developing robust and efficient multi-agent AI solutions.",
    "authors": [
      "Arne Tillmann"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-05-29T13:57:00Z",
    "pdf_url": "https://arxiv.org/pdf/2506.00066v1"
  },
  {
    "arxiv_id": "2506.10016v2",
    "entry_id": "http://arxiv.org/abs/2506.10016v2",
    "title": "A Survey of Generative Categories and Techniques in Multimodal Large Language Models",
    "summary": "Multimodal Large Language Models (MLLMs) have rapidly evolved beyond text generation, now spanning diverse output modalities including images, music, video, human motion, and 3D objects, by integrating language with other sensory modalities under unified architectures. This survey categorises six primary generative modalities and examines how foundational techniques, namely Self-Supervised Learning (SSL), Mixture of Experts (MoE), Reinforcement Learning from Human Feedback (RLHF), and Chain-of-Thought (CoT) prompting, enable cross-modal capabilities. We analyze key models, architectural trends, and emergent cross-modal synergies, while highlighting transferable techniques and unresolved challenges. Architectural innovations like transformers and diffusion models underpin this convergence, enabling cross-modal transfer and modular specialization. We highlight emerging patterns of synergy, and identify open challenges in evaluation, modularity, and structured reasoning. This survey offers a unified perspective on MLLM development and identifies critical paths toward more general-purpose, adaptive, and interpretable multimodal systems.",
    "authors": [
      "Longzhen Han",
      "Awes Mubarak",
      "Almas Baimagambetov",
      "Nikolaos Polatidis",
      "Thar Baker"
    ],
    "categories": [
      "cs.MM",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-29T12:29:39Z",
    "pdf_url": "https://arxiv.org/pdf/2506.10016v2"
  },
  {
    "arxiv_id": "2505.23135v2",
    "entry_id": "http://arxiv.org/abs/2505.23135v2",
    "title": "VERINA: Benchmarking Verifiable Code Generation",
    "summary": "Large language models (LLMs) are increasingly integrated in software development, but ensuring correctness in LLM-generated code remains challenging and often requires costly manual review. Verifiable code generation -- jointly generating code, specifications, and proofs of code-specification alignment -- offers a promising path to address this limitation and further unleash LLMs' benefits in coding. Yet, there exists a significant gap in evaluation: current benchmarks often focus on only individual components rather than providing a holistic evaluation framework of all tasks. In this paper, we introduce Verina (Verifiable Code Generation Arena), a high-quality benchmark enabling a comprehensive and modular evaluation of code, specification, and proof generation as well as their compositions. Verina consists of 189 manually curated coding tasks in Lean, with detailed problem descriptions, reference implementations, formal specifications, and extensive test suites. Our extensive evaluation of state-of-the-art LLMs reveals significant challenges in verifiable code generation, especially in proof generation, underscoring the need for improving LLM-based theorem provers in verification domains. The best model, OpenAI o4-mini, achieves a 61.4\\% code correctness rate, 51.0\\% for specification soundness and completeness, and a mere 3.6\\% proof success rate (based on one trial per task). We hope Verina will catalyze progress in verifiable code generation by providing a rigorous and comprehensive benchmark. We release our dataset on https://huggingface.co/datasets/sunblaze-ucb/verina and our evaluation code on https://github.com/sunblaze-ucb/verina.",
    "authors": [
      "Zhe Ye",
      "Zhengxu Yan",
      "Jingxuan He",
      "Timothe Kasriel",
      "Kaiyu Yang",
      "Dawn Song"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.LO",
      "cs.PL",
      "cs.SE"
    ],
    "published": "2025-05-29T06:12:52Z",
    "pdf_url": "https://arxiv.org/pdf/2505.23135v2"
  },
  {
    "arxiv_id": "2505.22954v2",
    "entry_id": "http://arxiv.org/abs/2505.22954v2",
    "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents",
    "summary": "Today's AI systems have human-designed, fixed architectures and cannot autonomously and continuously improve themselves. The advance of AI could itself be automated. If done safely, that would accelerate AI development and allow us to reap its benefits much sooner. Meta-learning can automate the discovery of novel algorithms, but is limited by first-order improvements and the human design of a suitable search space. The Gödel machine proposed a theoretical alternative: a self-improving AI that repeatedly modifies itself in a provably beneficial manner. Unfortunately, proving that most changes are net beneficial is impossible in practice. We introduce the Darwin Gödel Machine (DGM), a self-improving system that iteratively modifies its own code (thereby also improving its ability to modify its own codebase) and empirically validates each change using coding benchmarks. Inspired by Darwinian evolution and open-endedness research, the DGM maintains an archive of generated coding agents. It grows the archive by sampling an agent from it and using a foundation model to create a new, interesting, version of the sampled agent. This open-ended exploration forms a growing tree of diverse, high-quality agents and allows the parallel exploration of many different paths through the search space. Empirically, the DGM automatically improves its coding capabilities (e.g., better code editing tools, long-context window management, peer-review mechanisms), increasing performance on SWE-bench from 20.0% to 50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly outperforms baselines without self-improvement or open-ended exploration. All experiments were done with safety precautions (e.g., sandboxing, human oversight). The DGM is a significant step toward self-improving AI, capable of gathering its own stepping stones along paths that unfold into endless innovation.",
    "authors": [
      "Jenny Zhang",
      "Shengran Hu",
      "Cong Lu",
      "Robert Lange",
      "Jeff Clune"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-29T00:26:15Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22954v2"
  },
  {
    "arxiv_id": "2505.22939v1",
    "entry_id": "http://arxiv.org/abs/2505.22939v1",
    "title": "Generative Social Choice: The Next Generation",
    "summary": "A key task in certain democratic processes is to produce a concise slate of statements that proportionally represents the full spectrum of user opinions. This task is similar to committee elections, but unlike traditional settings, the candidate set comprises all possible statements of varying lengths, and so it can only be accessed through specific queries. Combining social choice and large language models, prior work has approached this challenge through a framework of generative social choice. We extend the framework in two fundamental ways, providing theoretical guarantees even in the face of approximately optimal queries and a budget limit on the overall length of the slate. Using GPT-4o to implement queries, we showcase our approach on datasets related to city improvement measures and drug reviews, demonstrating its effectiveness in generating representative slates from unstructured user opinions.",
    "authors": [
      "Niclas Boehmer",
      "Sara Fish",
      "Ariel D. Procaccia"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-28T23:40:24Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22939v1"
  },
  {
    "arxiv_id": "2505.22928v2",
    "entry_id": "http://arxiv.org/abs/2505.22928v2",
    "title": "Enhancing Study-Level Inference from Clinical Trial Papers via Reinforcement Learning-Based Numeric Reasoning",
    "summary": "Systematic reviews in medicine play a critical role in evidence-based decision-making by aggregating findings from multiple studies. A central bottleneck in automating this process is extracting numeric evidence and determining study-level conclusions for specific outcomes and comparisons. Prior work has framed this problem as a textual inference task by retrieving relevant content fragments and inferring conclusions from them. However, such approaches often rely on shallow textual cues and fail to capture the underlying numeric reasoning behind expert assessments.\n  In this work, we conceptualise the problem as one of quantitative reasoning. Rather than inferring conclusions from surface text, we extract structured numerical evidence (e.g., event counts or standard deviations) and apply domain knowledge informed logic to derive outcome-specific conclusions. We develop a numeric reasoning system composed of a numeric data extraction model and an effect estimate component, enabling more accurate and interpretable inference aligned with the domain expert principles. We train the numeric data extraction model using different strategies, including supervised fine-tuning (SFT) and reinforcement learning (RL) with a new value reward model.\n  When evaluated on the CochraneForest benchmark, our best-performing approach -- using RL to train a small-scale number extraction model -- yields up to a 21% absolute improvement in F1 score over retrieval-based systems and outperforms general-purpose LLMs of over 400B parameters by up to 9%. Our results demonstrate the promise of reasoning-driven approaches for automating systematic evidence synthesis.",
    "authors": [
      "Massimiliano Pronesti",
      "Michela Lorandi",
      "Paul Flanagan",
      "Oisin Redmond",
      "Anya Belz",
      "Yufang Hou"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-28T22:59:45Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22928v2"
  },
  {
    "arxiv_id": "2505.22922v1",
    "entry_id": "http://arxiv.org/abs/2505.22922v1",
    "title": "Scalable Parameter and Memory Efficient Pretraining for LLM: Recent Algorithmic Advances and Benchmarking",
    "summary": "Fueled by their remarkable ability to tackle diverse tasks across multiple domains, large language models (LLMs) have grown at an unprecedented rate, with some recent models containing trillions of parameters. This growth is accompanied by substantial computational challenges, particularly regarding the memory and compute resources required for training and fine-tuning. Numerous approaches have been explored to address these issues, such as LoRA. While these methods are effective for fine-tuning, their application to pre-training is significantly more challenging due to the need to learn vast datasets. Motivated by this issue, we aim to address the following questions: Can parameter- or memory-efficient methods enhance pre-training efficiency while achieving performance comparable to full-model training? How can the performance gap be narrowed? To this end, the contributions of this work are the following. (1) We begin by conducting a comprehensive survey that summarizes state-of-the-art methods for efficient pre-training. (2) We perform a benchmark evaluation of several representative memory efficient pre-training approaches to comprehensively evaluate their performance across model sizes. We observe that with a proper choice of optimizer and hyperparameters, full-rank training delivers the best performance, as expected. We also notice that incorporating high-rank updates in low-rank approaches is the key to improving their performance. (3) Finally, we propose two practical techniques, namely weight refactorization and momentum reset, to enhance the performance of efficient pre-training methods. We observe that applying these techniques to the low-rank method (on a 1B model) can achieve a lower perplexity than popular memory efficient algorithms such as GaLore and Fira, while simultaneously using about 25% less memory.",
    "authors": [
      "Athanasios Glentis",
      "Jiaxiang Li",
      "Qiulin Shang",
      "Andi Han",
      "Ioannis Tsaknakis",
      "Quan Wei",
      "Mingyi Hong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-05-28T22:51:43Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22922v1"
  },
  {
    "arxiv_id": "2505.22655v1",
    "entry_id": "http://arxiv.org/abs/2505.22655v1",
    "title": "Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents",
    "summary": "Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.",
    "authors": [
      "Michael Kirchhof",
      "Gjergji Kasneci",
      "Enkelejda Kasneci"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-28T17:59:08Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22655v1"
  },
  {
    "arxiv_id": "2505.22477v1",
    "entry_id": "http://arxiv.org/abs/2505.22477v1",
    "title": "Human-Centered Human-AI Collaboration (HCHAC)",
    "summary": "In the intelligent era, the interaction between humans and intelligent systems fundamentally involves collaboration with autonomous intelligent agents. Human-AI Collaboration (HAC) represents a novel type of human-machine relationship facilitated by autonomous intelligent machines equipped with AI technologies. In this paradigm, AI agents serve not only as auxiliary tools but also as active teammates, partnering with humans to accomplish tasks collaboratively. Human-centered AI (HCAI) emphasizes that humans play critical leadership roles in the collaboration. This human-led collaboration imparts new dimensions to the human-machine relationship, necessitating innovative research perspectives, paradigms, and agenda to address the unique challenges posed by HAC. This chapter delves into the essence of HAC from the human-centered perspective, outlining its core concepts and distinguishing features. It reviews the current research methodologies and research agenda within the HAC field from the HCAI perspective, highlighting advancements and ongoing studies. Furthermore, a framework for human-centered HAC (HCHAC) is proposed by integrating these reviews and analyses. A case study of HAC in the context of autonomous vehicles is provided, illustrating practical applications and the synergistic interactions between humans and AI agents. Finally, it identifies potential future research directions aimed at enhancing the effectiveness, reliability, and ethical integration of human-centered HAC systems in diverse domains.",
    "authors": [
      "Qi Gao",
      "Wei Xu",
      "Hanxi Pan",
      "Mowei Shen",
      "Zaifeng Gao"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-05-28T15:27:52Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22477v1"
  },
  {
    "arxiv_id": "2505.22311v1",
    "entry_id": "http://arxiv.org/abs/2505.22311v1",
    "title": "From Large AI Models to Agentic AI: A Tutorial on Future Intelligent Communications",
    "summary": "With the advent of 6G communications, intelligent communication systems face multiple challenges, including constrained perception and response capabilities, limited scalability, and low adaptability in dynamic environments. This tutorial provides a systematic introduction to the principles, design, and applications of Large Artificial Intelligence Models (LAMs) and Agentic AI technologies in intelligent communication systems, aiming to offer researchers a comprehensive overview of cutting-edge technologies and practical guidance. First, we outline the background of 6G communications, review the technological evolution from LAMs to Agentic AI, and clarify the tutorial's motivation and main contributions. Subsequently, we present a comprehensive review of the key components required for constructing LAMs. We further categorize LAMs and analyze their applicability, covering Large Language Models (LLMs), Large Vision Models (LVMs), Large Multimodal Models (LMMs), Large Reasoning Models (LRMs), and lightweight LAMs. Next, we propose a LAM-centric design paradigm tailored for communications, encompassing dataset construction and both internal and external learning approaches. Building upon this, we develop an LAM-based Agentic AI system for intelligent communications, clarifying its core components such as planners, knowledge bases, tools, and memory modules, as well as its interaction mechanisms. We also introduce a multi-agent framework with data retrieval, collaborative planning, and reflective evaluation for 6G. Subsequently, we provide a detailed overview of the applications of LAMs and Agentic AI in communication scenarios. Finally, we summarize the research challenges and future directions in current studies, aiming to support the development of efficient, secure, and sustainable next-generation intelligent communication systems.",
    "authors": [
      "Feibo Jiang",
      "Cunhua Pan",
      "Li Dong",
      "Kezhi Wang",
      "Octavia A. Dobre",
      "Merouane Debbah"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.NI",
      "eess.SP"
    ],
    "published": "2025-05-28T12:54:07Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22311v1"
  },
  {
    "arxiv_id": "2505.22192v1",
    "entry_id": "http://arxiv.org/abs/2505.22192v1",
    "title": "Efficient Leave-one-out Approximation in LLM Multi-agent Debate Based on Introspection",
    "summary": "Multi-agent systems based on large language models (LLMs) advance automatic task completion in various fields, where debate is a common cooperation form for agents to solve complicated problems with reasoning and cross-review to solidify answers. Assessing the individual contributions of agents within these debates is crucial for system refinement and outcome reliability. Traditional leave-one-out (LOO) method offers a clear framework for evaluating each agent's role but face challenges in LLM-based systems due to high computational costs and associated financial implications. This paper presents introspective-leave-one-out (IntrospecLOO), a simple yet effective prompting for approximation of LOO in LLM-powered multi-agent debates. IntrospecLOO introduces an additional querying round after standard debates, prompting agents to update their answers while ignoring responses from a designated agent. This strategy effectively isolates and gauges each participant's influence at a reduced query complexity compared to the original LOO approaches. Validation through experiments on three benchmark datasets confirms the effectiveness of IntrospecLOO.",
    "authors": [
      "Yue Cui",
      "Liuyi Yao",
      "Zitao Li",
      "Yaliang Li",
      "Bolin Ding",
      "Xiaofang Zhou"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-05-28T10:08:31Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22192v1"
  },
  {
    "arxiv_id": "2505.22125v1",
    "entry_id": "http://arxiv.org/abs/2505.22125v1",
    "title": "Sentiment Simulation using Generative AI Agents",
    "summary": "Traditional sentiment analysis relies on surface-level linguistic patterns and retrospective data, limiting its ability to capture the psychological and contextual drivers of human sentiment. These limitations constrain its effectiveness in applications that require predictive insight, such as policy testing, narrative framing, and behavioral forecasting. We present a robust framework for sentiment simulation using generative AI agents embedded with psychologically rich profiles. Agents are instantiated from a nationally representative survey of 2,485 Filipino respondents, combining sociodemographic information with validated constructs of personality traits, values, beliefs, and socio-political attitudes. The framework includes three stages: (1) agent embodiment via categorical or contextualized encodings, (2) exposure to real-world political and economic scenarios, and (3) generation of sentiment ratings accompanied by explanatory rationales. Using Quadratic Weighted Accuracy (QWA), we evaluated alignment between agent-generated and human responses. Contextualized encoding achieved 92% alignment in replicating original survey responses. In sentiment simulation tasks, agents reached 81%--86% accuracy against ground truth sentiment, with contextualized profile encodings significantly outperforming categorical (p < 0.0001, Cohen's d = 0.70). Simulation results remained consistent across repeated trials (+/-0.2--0.5% SD) and resilient to variation in scenario framing (p = 0.9676, Cohen's d = 0.02). Our findings establish a scalable framework for sentiment modeling through psychographically grounded AI agents. This work signals a paradigm shift in sentiment analysis from retrospective classification to prospective and dynamic simulation grounded in psychology of sentiment formation.",
    "authors": [
      "Melrose Tia",
      "Jezreel Sophia Lanuzo",
      "Lei Rigi Baltazar",
      "Marie Joy Lopez-Relente",
      "Diwa Malaya Quiñones",
      "Jason Albia"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-05-28T08:50:56Z",
    "pdf_url": "https://arxiv.org/pdf/2505.22125v1"
  },
  {
    "arxiv_id": "2505.21935v2",
    "entry_id": "http://arxiv.org/abs/2505.21935v2",
    "title": "From Reasoning to Learning: A Survey on Hypothesis Discovery and Rule Learning with Large Language Models",
    "summary": "Since the advent of Large Language Models (LLMs), efforts have largely focused on improving their instruction-following and deductive reasoning abilities, leaving open the question of whether these models can truly discover new knowledge. In pursuit of artificial general intelligence (AGI), there is a growing need for models that not only execute commands or retrieve information but also learn, reason, and generate new knowledge by formulating novel hypotheses and theories that deepen our understanding of the world. Guided by Peirce's framework of abduction, deduction, and induction, this survey offers a structured lens to examine LLM-based hypothesis discovery. We synthesize existing work in hypothesis generation, application, and validation, identifying both key achievements and critical gaps. By unifying these threads, we illuminate how LLMs might evolve from mere ``information executors'' into engines of genuine innovation, potentially transforming research, science, and real-world problem solving.",
    "authors": [
      "Kaiyu He",
      "Zhiyu Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-28T03:40:02Z",
    "pdf_url": "https://arxiv.org/pdf/2505.21935v2"
  },
  {
    "arxiv_id": "2506.15695v2",
    "entry_id": "http://arxiv.org/abs/2506.15695v2",
    "title": "SimuGen: Multi-modal Agentic Framework for Constructing Block Diagram-Based Simulation Models",
    "summary": "Recent advances in large language models (LLMs) have shown impressive performance in mathematical reasoning and code generation. However, LLMs still struggle in the simulation domain, particularly in generating Simulink models, which are essential tools in engineering and scientific research. Our preliminary experiments indicate that LLM agents often fail to produce reliable and complete Simulink simulation code from text-only inputs, likely due to the lack of Simulink-specific data in their pretraining. To address this challenge, we propose SimuGen, a multimodal agent-based framework that automatically generates accurate Simulink simulation code by leveraging both the visual Simulink diagram and domain knowledge. SimuGen coordinates several specialized agents, including an investigator, unit test reviewer, code generator, executor, debug locator, and report writer, supported by a domain-specific knowledge base. This collaborative and modular design enables interpretable, robust, and reproducible Simulink simulation generation. Our source code is publicly available at https://github.com/renxinxing123/SimuGen_beta.",
    "authors": [
      "Xinxing Ren",
      "Qianbo Zang",
      "Zekun Guo"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-05-28T00:35:43Z",
    "pdf_url": "https://arxiv.org/pdf/2506.15695v2"
  },
  {
    "arxiv_id": "2505.21660v2",
    "entry_id": "http://arxiv.org/abs/2505.21660v2",
    "title": "PreGenie: An Agentic Framework for High-quality Visual Presentation Generation",
    "summary": "Visual presentations are vital for effective communication. Early attempts to automate their creation using deep learning often faced issues such as poorly organized layouts, inaccurate text summarization, and a lack of image understanding, leading to mismatched visuals and text. These limitations restrict their application in formal contexts like business and scientific research. To address these challenges, we propose PreGenie, an agentic and modular framework powered by multimodal large language models (MLLMs) for generating high-quality visual presentations.\n  PreGenie is built on the Slidev presentation framework, where slides are rendered from Markdown code. It operates in two stages: (1) Analysis and Initial Generation, which summarizes multimodal input and generates initial code, and (2) Review and Re-generation, which iteratively reviews intermediate code and rendered slides to produce final, high-quality presentations. Each stage leverages multiple MLLMs that collaborate and share information. Comprehensive experiments demonstrate that PreGenie excels in multimodal understanding, outperforming existing models in both aesthetics and content consistency, while aligning more closely with human design preferences.",
    "authors": [
      "Xiaojie Xu",
      "Xinli Xu",
      "Sirui Chen",
      "Haoyu Chen",
      "Fan Zhang",
      "Ying-Cong Chen"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-05-27T18:36:19Z",
    "pdf_url": "https://arxiv.org/pdf/2505.21660v2"
  },
  {
    "arxiv_id": "2505.21636v2",
    "entry_id": "http://arxiv.org/abs/2505.21636v2",
    "title": "The Feasibility of Topic-Based Watermarking on Academic Peer Reviews",
    "summary": "Large language models (LLMs) are increasingly integrated into academic workflows, with many conferences and journals permitting their use for tasks such as language refinement and literature summarization. However, their use in peer review remains prohibited due to concerns around confidentiality breaches, hallucinated content, and inconsistent evaluations. As LLM-generated text becomes more indistinguishable from human writing, there is a growing need for reliable attribution mechanisms to preserve the integrity of the review process. In this work, we evaluate topic-based watermarking (TBW), a semantic-aware technique designed to embed detectable signals into LLM-generated text. We conduct a systematic assessment across multiple LLM configurations, including base, few-shot, and fine-tuned variants, using authentic peer review data from academic conferences. Our results show that TBW maintains review quality relative to non-watermarked outputs, while demonstrating robust detection performance under paraphrasing. These findings highlight the viability of TBW as a minimally intrusive and practical solution for LLM attribution in peer review settings.",
    "authors": [
      "Alexander Nemecek",
      "Yuzhou Jiang",
      "Erman Ayday"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-27T18:09:27Z",
    "pdf_url": "https://arxiv.org/pdf/2505.21636v2"
  },
  {
    "arxiv_id": "2505.21362v1",
    "entry_id": "http://arxiv.org/abs/2505.21362v1",
    "title": "Evaluating LLM Adaptation to Sociodemographic Factors: User Profile vs. Dialogue History",
    "summary": "Effective engagement by large language models (LLMs) requires adapting responses to users' sociodemographic characteristics, such as age, occupation, and education level. While many real-world applications leverage dialogue history for contextualization, existing evaluations of LLMs' behavioral adaptation often focus on single-turn prompts. In this paper, we propose a framework to evaluate LLM adaptation when attributes are introduced either (1) explicitly via user profiles in the prompt or (2) implicitly through multi-turn dialogue history. We assess the consistency of model behavior across these modalities. Using a multi-agent pipeline, we construct a synthetic dataset pairing dialogue histories with distinct user profiles and employ questions from the Value Survey Module (VSM 2013) (Hofstede and Hofstede, 2016) to probe value expression. Our findings indicate that most models adjust their expressed values in response to demographic changes, particularly in age and education level, but consistency varies. Models with stronger reasoning capabilities demonstrate greater alignment, indicating the importance of reasoning in robust sociodemographic adaptation.",
    "authors": [
      "Qishuai Zhong",
      "Zongmin Li",
      "Siqi Fan",
      "Aixin Sun"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-05-27T15:52:39Z",
    "pdf_url": "https://arxiv.org/pdf/2505.21362v1"
  },
  {
    "arxiv_id": "2505.21116v1",
    "entry_id": "http://arxiv.org/abs/2505.21116v1",
    "title": "Creativity in LLM-based Multi-Agent Systems: A Survey",
    "summary": "Large language model (LLM)-driven multi-agent systems (MAS) are transforming how humans and AIs collaboratively generate ideas and artifacts. While existing surveys provide comprehensive overviews of MAS infrastructures, they largely overlook the dimension of \\emph{creativity}, including how novel outputs are generated and evaluated, how creativity informs agent personas, and how creative workflows are coordinated. This is the first survey dedicated to creativity in MAS. We focus on text and image generation tasks, and present: (1) a taxonomy of agent proactivity and persona design; (2) an overview of generation techniques, including divergent exploration, iterative refinement, and collaborative synthesis, as well as relevant datasets and evaluation metrics; and (3) a discussion of key challenges, such as inconsistent evaluation standards, insufficient bias mitigation, coordination conflicts, and the lack of unified benchmarks. This survey offers a structured framework and roadmap for advancing the development, evaluation, and standardization of creative MAS.",
    "authors": [
      "Yi-Cheng Lin",
      "Kang-Chieh Chen",
      "Zhe-Yan Li",
      "Tzu-Heng Wu",
      "Tzu-Hsuan Wu",
      "Kuan-Yu Chen",
      "Hung-yi Lee",
      "Yun-Nung Chen"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-27T12:36:14Z",
    "pdf_url": "https://arxiv.org/pdf/2505.21116v1"
  },
  {
    "arxiv_id": "2505.21040v2",
    "entry_id": "http://arxiv.org/abs/2505.21040v2",
    "title": "FCKT: Fine-Grained Cross-Task Knowledge Transfer with Semantic Contrastive Learning for Targeted Sentiment Analysis",
    "summary": "In this paper, we address the task of targeted sentiment analysis (TSA), which involves two sub-tasks, i.e., identifying specific aspects from reviews and determining their corresponding sentiments. Aspect extraction forms the foundation for sentiment prediction, highlighting the critical dependency between these two tasks for effective cross-task knowledge transfer. While most existing studies adopt a multi-task learning paradigm to align task-specific features in the latent space, they predominantly rely on coarse-grained knowledge transfer. Such approaches lack fine-grained control over aspect-sentiment relationships, often assuming uniform sentiment polarity within related aspects. This oversimplification neglects contextual cues that differentiate sentiments, leading to negative transfer. To overcome these limitations, we propose FCKT, a fine-grained cross-task knowledge transfer framework tailored for TSA. By explicitly incorporating aspect-level information into sentiment prediction, FCKT achieves fine-grained knowledge transfer, effectively mitigating negative transfer and enhancing task performance. Experiments on three datasets, including comparisons with various baselines and large language models (LLMs), demonstrate the effectiveness of FCKT. The source code is available on https://github.com/cwei01/FCKT.",
    "authors": [
      "Wei Chen",
      "Zhao Zhang",
      "Meng Yuan",
      "Kepeng Xu",
      "Fuzhen Zhuang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-27T11:23:53Z",
    "pdf_url": "https://arxiv.org/pdf/2505.21040v2"
  },
  {
    "arxiv_id": "2505.20901v1",
    "entry_id": "http://arxiv.org/abs/2505.20901v1",
    "title": "A Stereotype Content Analysis on Color-related Social Bias in Large Vision Language Models",
    "summary": "As large vision language models(LVLMs) rapidly advance, concerns about their potential to learn and generate social biases and stereotypes are increasing. Previous studies on LVLM's stereotypes face two primary limitations: metrics that overlooked the importance of content words, and datasets that overlooked the effect of color. To address these limitations, this study introduces new evaluation metrics based on the Stereotype Content Model (SCM). We also propose BASIC, a benchmark for assessing gender, race, and color stereotypes. Using SCM metrics and BASIC, we conduct a study with eight LVLMs to discover stereotypes. As a result, we found three findings. (1) The SCM-based evaluation is effective in capturing stereotypes. (2) LVLMs exhibit color stereotypes in the output along with gender and race ones. (3) Interaction between model architecture and parameter sizes seems to affect stereotypes. We release BASIC publicly on [anonymized for review].",
    "authors": [
      "Junhyuk Choi",
      "Minju Kim",
      "Yeseon Hong",
      "Bugeun Kim"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-27T08:44:05Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20901v1"
  },
  {
    "arxiv_id": "2505.20820v1",
    "entry_id": "http://arxiv.org/abs/2505.20820v1",
    "title": "MT-Mol:Multi Agent System with Tool-based Reasoning for Molecular Optimization",
    "summary": "Large language models (LLMs) have large potential for molecular optimization, as they can gather external chemistry tools and enable collaborative interactions to iteratively refine molecular candidates. However, this potential remains underexplored, particularly in the context of structured reasoning, interpretability, and comprehensive tool-grounded molecular optimization. To address this gap, we introduce MT-Mol, a multi-agent framework for molecular optimization that leverages tool-guided reasoning and role-specialized LLM agents. Our system incorporates comprehensive RDKit tools, categorized into five distinct domains: structural descriptors, electronic and topological features, fragment-based functional groups, molecular representations, and miscellaneous chemical properties. Each category is managed by an expert analyst agent, responsible for extracting task-relevant tools and enabling interpretable, chemically grounded feedback. MT-Mol produces molecules with tool-aligned and stepwise reasoning through the interaction between the analyst agents, a molecule-generating scientist, a reasoning-output verifier, and a reviewer agent. As a result, we show that our framework shows the state-of-the-art performance of the PMO-1K benchmark on 17 out of 23 tasks.",
    "authors": [
      "Hyomin Kim",
      "Yunhui Jang",
      "Sungsoo Ahn"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-27T07:27:30Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20820v1"
  },
  {
    "arxiv_id": "2505.20503v1",
    "entry_id": "http://arxiv.org/abs/2505.20503v1",
    "title": "Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review",
    "summary": "Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interactions, robots can improve understanding, adapt to, and execute complex tasks in dynamic real-world environments. However, embodied AI in mobile service robots continues to face key challenges, including multimodal sensor fusion, real-time decision-making under uncertainty, task generalization, and effective human-robot interactions (HRI). In this paper, we present the first systematic review of the integration of foundation models in mobile service robotics, identifying key open challenges in embodied AI and examining how foundation models can address them. Namely, we explore the role of such models in enabling real-time sensor fusion, language-conditioned control, and adaptive task execution. Furthermore, we discuss real-world applications in the domestic assistance, healthcare, and service automation sectors, demonstrating the transformative impact of foundation models on service robotics. We also include potential future research directions, emphasizing the need for predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to enable scalable, efficient, and robust deployment of foundation models in human-centric robotic systems.",
    "authors": [
      "Matthew Lisondra",
      "Beno Benhabib",
      "Goldie Nejat"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-05-26T20:08:09Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20503v1"
  },
  {
    "arxiv_id": "2505.20223v1",
    "entry_id": "http://arxiv.org/abs/2505.20223v1",
    "title": "Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects",
    "summary": "The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the system's ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at https://github.com/cuiyx1720/Awesome-CoT4AD.",
    "authors": [
      "Yixin Cui",
      "Haotian Lin",
      "Shuo Yang",
      "Yixiao Wang",
      "Yanjun Huang",
      "Hong Chen"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2025-05-26T17:06:00Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20223v1"
  },
  {
    "arxiv_id": "2505.20206v1",
    "entry_id": "http://arxiv.org/abs/2505.20206v1",
    "title": "Evaluating Large Language Models for Code Review",
    "summary": "Context: Code reviews are crucial for software quality. Recent AI advances have allowed large language models (LLMs) to review and fix code; now, there are tools that perform these reviews. However, their reliability and accuracy have not yet been systematically evaluated. Objective: This study compares different LLMs' performance in detecting code correctness and suggesting improvements. Method: We tested GPT4o and Gemini 2.0 Flash on 492 AI generated code blocks of varying correctness, along with 164 canonical code blocks from the HumanEval benchmark. To simulate the code review task objectively, we expected LLMs to assess code correctness and improve the code if needed. We ran experiments with different configurations and reported on the results. Results: With problem descriptions, GPT4o and Gemini 2.0 Flash correctly classified code correctness 68.50% and 63.89% of the time, respectively, and corrected the code 67.83% and 54.26% of the time for the 492 code blocks of varying correctness. Without problem descriptions, performance declined. The results for the 164 canonical code blocks differed, suggesting that performance depends on the type of code. Conclusion: LLM code reviews can help suggest improvements and assess correctness, but there is a risk of faulty outputs. We propose a process that involves humans, called the \"Human in the loop LLM Code Review\" to promote knowledge sharing while mitigating the risk of faulty outputs.",
    "authors": [
      "Umut Cihan",
      "Arda İçöz",
      "Vahid Haratian",
      "Eray Tüzün"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-05-26T16:47:29Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20206v1"
  },
  {
    "arxiv_id": "2505.20099v2",
    "entry_id": "http://arxiv.org/abs/2505.20099v2",
    "title": "Large Language Models Meet Knowledge Graphs for Question Answering: Synthesis and Opportunities",
    "summary": "Large language models (LLMs) have demonstrated remarkable performance on question-answering (QA) tasks because of their superior capabilities in natural language understanding and generation. However, LLM-based QA struggles with complex QA tasks due to poor reasoning capacity, outdated knowledge, and hallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs) for QA to address the above challenges. In this survey, we propose a new structured taxonomy that categorizes the methodology of synthesizing LLMs and KGs for QA according to the categories of QA and the KG's role when integrating with LLMs. We systematically survey state-of-the-art methods in synthesizing LLMs and KGs for QA and compare and analyze these approaches in terms of strength, limitations, and KG requirements. We then align the approaches with QA and discuss how these approaches address the main challenges of different complex QA. Finally, we summarize the advancements, evaluation metrics, and benchmark datasets and highlight open challenges and opportunities.",
    "authors": [
      "Chuangtao Ma",
      "Yongrui Chen",
      "Tianxing Wu",
      "Arijit Khan",
      "Haofen Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-05-26T15:08:23Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20099v2"
  },
  {
    "arxiv_id": "2505.20068v1",
    "entry_id": "http://arxiv.org/abs/2505.20068v1",
    "title": "On the Same Page: Dimensions of Perceived Shared Understanding in Human-AI Interaction",
    "summary": "Shared understanding plays a key role in the effective communication in and performance of human-human interactions. With the increasingly common integration of AI into human contexts, the future of personal and workplace interactions will likely see human-AI interaction (HAII) in which the perception of shared understanding is important. Existing literature has addressed the processes and effects of PSU in human-human interactions, but the construal remains underexplored in HAII. To better understand PSU in HAII, we conducted an online survey to collect user reflections on interactions with a large language model when it sunderstanding of a situation was thought to be similar to or different from the participant's. Through inductive thematic analysis, we identified eight dimensions comprising PSU in human-AI interactions: Fluency, aligned operation, fluidity, outcome satisfaction, contextual awareness, lack of humanlike abilities, computational limits, and suspicion.",
    "authors": [
      "Qingyu Liang",
      "Jaime Banks"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-05-26T14:50:40Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20068v1"
  },
  {
    "arxiv_id": "2505.20011v3",
    "entry_id": "http://arxiv.org/abs/2505.20011v3",
    "title": "The Many Challenges of Human-Like Agents in Virtual Game Environments",
    "summary": "Human-like agents are an increasingly important topic in games and beyond. Believable non-player characters enhance the gaming experience by improving immersion and providing entertainment. They also offer players the opportunity to engage with AI entities that can function as opponents, teachers, or cooperating partners. Additionally, in games where bots are prohibited -- and even more so in non-game environments -- there is a need for methods capable of identifying whether digital interactions occur with bots or humans. This leads to two fundamental research questions: (1) how to model and implement human-like AI, and (2) how to measure its degree of human likeness.\n  This article offers two contributions. The first one is a survey of the most significant challenges in implementing human-like AI in games (or any virtual environment featuring simulated agents, although this article specifically focuses on games). Thirteen such challenges, both conceptual and technical, are discussed in detail. The second is an empirical study performed in a tactical video game that addresses the research question: \"Is it possible to distinguish human players from bots (AI agents) based on empirical data?\" A machine-learning approach using a custom deep recurrent convolutional neural network is presented. We hypothesize that the more challenging it is to create human-like AI for a given game, the easier it becomes to develop a method for distinguishing humans from AI-driven players.",
    "authors": [
      "Maciej Swiechowski",
      "Dominik Slezak"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MM"
    ],
    "published": "2025-05-26T14:00:39Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20011v3"
  },
  {
    "arxiv_id": "2505.19973v1",
    "entry_id": "http://arxiv.org/abs/2505.19973v1",
    "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in Digital Forensics and Incident Response",
    "summary": "Digital Forensics and Incident Response (DFIR) involves analyzing digital evidence to support legal investigations. Large Language Models (LLMs) offer new opportunities in DFIR tasks such as log analysis and memory forensics, but their susceptibility to errors and hallucinations raises concerns in high-stakes contexts. Despite growing interest, there is no comprehensive benchmark to evaluate LLMs across both theoretical and practical DFIR domains. To address this gap, we present DFIR-Metric, a benchmark with three components: (1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice questions sourced from industry-standard certifications and official documentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing multi-step reasoning and evidence correlation; and (3) Practical Analysis: 500 disk and memory forensics cases from the NIST Computer Forensics Tool Testing Program (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their accuracy and consistency across trials. We also introduce a new metric, the Task Understanding Score (TUS), designed to more effectively evaluate models in scenarios where they achieve near-zero accuracy. This benchmark offers a rigorous, reproducible foundation for advancing AI in digital forensics. All scripts, artifacts, and results are available on the project website at https://github.com/DFIR-Metric.",
    "authors": [
      "Bilel Cherif",
      "Tamas Bisztray",
      "Richard A. Dubniczky",
      "Aaesha Aldahmani",
      "Saeed Alshehhi",
      "Norbert Tihanyi"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-26T13:35:37Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19973v1"
  },
  {
    "arxiv_id": "2505.19955v3",
    "entry_id": "http://arxiv.org/abs/2505.19955v3",
    "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
    "summary": "Recent advancements in AI agents have demonstrated their growing potential to drive and support scientific discovery. In this work, we introduce MLR-Bench, a comprehensive benchmark for evaluating AI agents on open-ended machine learning research. MLR-Bench includes three key components: (1) 201 research tasks sourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2) MLR-Judge, an automated evaluation framework combining LLM-based reviewers with carefully designed review rubrics to assess research quality; and (3) MLR-Agent, a modular agent scaffold capable of completing research tasks through four stages: idea generation, proposal formulation, experimentation, and paper writing. Our framework supports both stepwise assessment across these distinct research stages, and end-to-end evaluation of the final research paper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced coding agent, finding that while LLMs are effective at generating coherent ideas and well-structured papers, current coding agents frequently (e.g., in 80% of the cases) produce fabricated or invalidated experimental results--posing a major barrier to scientific reliability. We validate MLR-Judge through human evaluation, showing high agreement with expert reviewers, supporting its potential as a scalable tool for research evaluation. We open-source MLR-Bench to help the community benchmark, diagnose, and improve AI research agents toward trustworthy and transparent scientific discovery.",
    "authors": [
      "Hui Chen",
      "Miao Xiong",
      "Yujie Lu",
      "Wei Han",
      "Ailin Deng",
      "Yufei He",
      "Jiaying Wu",
      "Yibo Li",
      "Yue Liu",
      "Bryan Hooi"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-26T13:18:37Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19955v3"
  },
  {
    "arxiv_id": "2505.19896v1",
    "entry_id": "http://arxiv.org/abs/2505.19896v1",
    "title": "Large Language Models as Autonomous Spacecraft Operators in Kerbal Space Program",
    "summary": "Recent trends are emerging in the use of Large Language Models (LLMs) as autonomous agents that take actions based on the content of the user text prompts. We intend to apply these concepts to the field of Control in space, enabling LLMs to play a significant role in the decision-making process for autonomous satellite operations. As a first step towards this goal, we have developed a pure LLM-based solution for the Kerbal Space Program Differential Games (KSPDG) challenge, a public software design competition where participants create autonomous agents for maneuvering satellites involved in non-cooperative space operations, running on the KSP game engine. Our approach leverages prompt engineering, few-shot prompting, and fine-tuning techniques to create an effective LLM-based agent that ranked 2nd in the competition. To the best of our knowledge, this work pioneers the integration of LLM agents into space research. The project comprises several open repositories to facilitate replication and further research. The codebase is accessible on \\href{https://github.com/ARCLab-MIT/kspdg}{GitHub}, while the trained models and datasets are available on \\href{https://huggingface.co/OhhTuRnz}{Hugging Face}. Additionally, experiment tracking and detailed results can be reviewed on \\href{https://wandb.ai/carrusk/huggingface}{Weights \\& Biases",
    "authors": [
      "Alejandro Carrasco",
      "Victor Rodriguez-Fernandez",
      "Richard Linares"
    ],
    "categories": [
      "cs.AI",
      "astro-ph.IM",
      "cs.CL"
    ],
    "published": "2025-05-26T12:25:35Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19896v1"
  },
  {
    "arxiv_id": "2505.19837v1",
    "entry_id": "http://arxiv.org/abs/2505.19837v1",
    "title": "Multi-Agent Reinforcement Learning in Cybersecurity: From Fundamentals to Applications",
    "summary": "Multi-Agent Reinforcement Learning (MARL) has shown great potential as an adaptive solution for addressing modern cybersecurity challenges. MARL enables decentralized, adaptive, and collaborative defense strategies and provides an automated mechanism to combat dynamic, coordinated, and sophisticated threats. This survey investigates the current state of research in MARL applications for automated cyber defense (ACD), focusing on intruder detection and lateral movement containment. Additionally, it examines the role of Autonomous Intelligent Cyber-defense Agents (AICA) and Cyber Gyms in training and validating MARL agents. Finally, the paper outlines existing challenges, such as scalability and adversarial robustness, and proposes future research directions. This also discusses how MARL integrates in AICA to provide adaptive, scalable, and dynamic solutions to counter the increasingly sophisticated landscape of cyber threats. It highlights the transformative potential of MARL in areas like intrusion detection and lateral movement containment, and underscores the value of Cyber Gyms for training and validation of AICA.",
    "authors": [
      "Christoph R. Landolt",
      "Christoph Würsch",
      "Roland Meier",
      "Alain Mermoud",
      "Julian Jang-Jaccard"
    ],
    "categories": [
      "cs.MA",
      "cs.GT",
      "cs.LG"
    ],
    "published": "2025-05-26T11:19:43Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19837v1"
  },
  {
    "arxiv_id": "2505.19806v1",
    "entry_id": "http://arxiv.org/abs/2505.19806v1",
    "title": "Exploring Consciousness in LLMs: A Systematic Survey of Theories, Implementations, and Frontier Risks",
    "summary": "Consciousness stands as one of the most profound and distinguishing features of the human mind, fundamentally shaping our understanding of existence and agency. As large language models (LLMs) develop at an unprecedented pace, questions concerning intelligence and consciousness have become increasingly significant. However, discourse on LLM consciousness remains largely unexplored territory. In this paper, we first clarify frequently conflated terminologies (e.g., LLM consciousness and LLM awareness). Then, we systematically organize and synthesize existing research on LLM consciousness from both theoretical and empirical perspectives. Furthermore, we highlight potential frontier risks that conscious LLMs might introduce. Finally, we discuss current challenges and outline future directions in this emerging field. The references discussed in this paper are organized at https://github.com/OpenCausaLab/Awesome-LLM-Consciousness.",
    "authors": [
      "Sirui Chen",
      "Shuqin Ma",
      "Shu Yu",
      "Hanwang Zhang",
      "Shengjie Zhao",
      "Chaochao Lu"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-05-26T10:40:52Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19806v1"
  },
  {
    "arxiv_id": "2505.19683v1",
    "entry_id": "http://arxiv.org/abs/2505.19683v1",
    "title": "Large Language Models for Planning: A Comprehensive and Systematic Survey",
    "summary": "Planning represents a fundamental capability of intelligent agents, requiring comprehensive environmental understanding, rigorous logical reasoning, and effective sequential decision-making. While Large Language Models (LLMs) have demonstrated remarkable performance on certain planning tasks, their broader application in this domain warrants systematic investigation. This paper presents a comprehensive review of LLM-based planning. Specifically, this survey is structured as follows: First, we establish the theoretical foundations by introducing essential definitions and categories about automated planning. Next, we provide a detailed taxonomy and analysis of contemporary LLM-based planning methodologies, categorizing them into three principal approaches: 1) External Module Augmented Methods that combine LLMs with additional components for planning, 2) Finetuning-based Methods that involve using trajectory data and feedback signals to adjust LLMs in order to improve their planning abilities, and 3) Searching-based Methods that break down complex tasks into simpler components, navigate the planning space, or enhance decoding strategies to find the best solutions. Subsequently, we systematically summarize existing evaluation frameworks, including benchmark datasets, evaluation metrics and performance comparisons between representative planning methods. Finally, we discuss the underlying mechanisms enabling LLM-based planning and outline promising research directions for this rapidly evolving field. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this field.",
    "authors": [
      "Pengfei Cao",
      "Tianyi Men",
      "Wencan Liu",
      "Jingwen Zhang",
      "Xuzhao Li",
      "Xixun Lin",
      "Dianbo Sui",
      "Yanan Cao",
      "Kang Liu",
      "Jun Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-26T08:44:53Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19683v1"
  },
  {
    "arxiv_id": "2505.19658v1",
    "entry_id": "http://arxiv.org/abs/2505.19658v1",
    "title": "Large Language Models in Code Co-generation for Safe Autonomous Vehicles",
    "summary": "Software engineers in various industrial domains are already using Large Language Models (LLMs) to accelerate the process of implementing parts of software systems. When considering its potential use for ADAS or AD systems in the automotive context, there is a need to systematically assess this new setup: LLMs entail a well-documented set of risks for safety-related systems' development due to their stochastic nature. To reduce the effort for code reviewers to evaluate LLM-generated code, we propose an evaluation pipeline to conduct sanity-checks on the generated code. We compare the performance of six state-of-the-art LLMs (CodeLlama, CodeGemma, DeepSeek-r1, DeepSeek-Coders, Mistral, and GPT-4) on four safety-related programming tasks. Additionally, we qualitatively analyse the most frequent faults generated by these LLMs, creating a failure-mode catalogue to support human reviewers. Finally, the limitations and capabilities of LLMs in code generation, and the use of the proposed pipeline in the existing process, are discussed.",
    "authors": [
      "Ali Nouri",
      "Beatriz Cabrero-Daniel",
      "Zhennan Fei",
      "Krishna Ronanki",
      "Håkan Sivencrona",
      "Christian Berger"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-05-26T08:18:30Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19658v1"
  },
  {
    "arxiv_id": "2505.19621v1",
    "entry_id": "http://arxiv.org/abs/2505.19621v1",
    "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions, and Beliefs of Large Language Models",
    "summary": "As Large Language Models (LLMs) become deeply integrated into human life and increasingly influence decision-making, it's crucial to evaluate whether and to what extent they exhibit subjective preferences, opinions, and beliefs. These tendencies may stem from biases within the models, which may shape their behavior, influence the advice and recommendations they offer to users, and potentially reinforce certain viewpoints. This paper presents the Preference, Opinion, and Belief survey (POBs), a benchmark developed to assess LLMs' subjective inclinations across societal, cultural, ethical, and personal domains. We applied our benchmark to evaluate leading open- and closed-source LLMs, measuring desired properties such as reliability, neutrality, and consistency. In addition, we investigated the effect of increasing the test-time compute, through reasoning and self-reflection mechanisms, on those metrics. While effective in other tasks, our results show that these mechanisms offer only limited gains in our domain. Furthermore, we reveal that newer model versions are becoming less consistent and more biased toward specific viewpoints, highlighting a blind spot and a concerning trend. POBS: https://ibm.github.io/POBS",
    "authors": [
      "George Kour",
      "Itay Nakash",
      "Ateret Anaby-Tavor",
      "Michal Shmueli-Scheuer"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-26T07:41:21Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19621v1"
  },
  {
    "arxiv_id": "2505.19443v1",
    "entry_id": "http://arxiv.org/abs/2505.19443v1",
    "title": "Vibe Coding vs. Agentic Coding: Fundamentals and Practical Implications of Agentic AI",
    "summary": "This review presents a comprehensive analysis of two emerging paradigms in AI-assisted software development: vibe coding and agentic coding. While both leverage large language models (LLMs), they differ fundamentally in autonomy, architectural design, and the role of the developer. Vibe coding emphasizes intuitive, human-in-the-loop interaction through prompt-based, conversational workflows that support ideation, experimentation, and creative exploration. In contrast, agentic coding enables autonomous software development through goal-driven agents capable of planning, executing, testing, and iterating tasks with minimal human intervention. We propose a detailed taxonomy spanning conceptual foundations, execution models, feedback loops, safety mechanisms, debugging strategies, and real-world tool ecosystems. Through comparative workflow analysis and 20 detailed use cases, we illustrate how vibe systems thrive in early-stage prototyping and education, while agentic systems excel in enterprise-grade automation, codebase refactoring, and CI/CD integration. We further examine emerging trends in hybrid architectures, where natural language interfaces are coupled with autonomous execution pipelines. Finally, we articulate a future roadmap for agentic AI, outlining the infrastructure needed for trustworthy, explainable, and collaborative systems. Our findings suggest that successful AI software engineering will rely not on choosing one paradigm, but on harmonizing their strengths within a unified, human-centered development lifecycle.",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I. Roumeliotis",
      "Manoj Karkee"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-26T03:00:21Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19443v1"
  },
  {
    "arxiv_id": "2505.19402v1",
    "entry_id": "http://arxiv.org/abs/2505.19402v1",
    "title": "Recalibrating the Compass: Integrating Large Language Models into Classical Research Methods",
    "summary": "This paper examines how large language models (LLMs) are transforming core quantitative methods in communication research in particular, and in the social sciences more broadly-namely, content analysis, survey research, and experimental studies. Rather than replacing classical approaches, LLMs introduce new possibilities for coding and interpreting text, simulating dynamic respondents, and generating personalized and interactive stimuli. Drawing on recent interdisciplinary work, the paper highlights both the potential and limitations of LLMs as research tools, including issues of validity, bias, and interpretability. To situate these developments theoretically, the paper revisits Lasswell's foundational framework -- \"Who says what, in which channel, to whom, with what effect?\" -- and demonstrates how LLMs reconfigure message studies, audience analysis, and effects research by enabling interpretive variation, audience trajectory modeling, and counterfactual experimentation. Revisiting the metaphor of the methodological compass, the paper argues that classical research logics remain essential as the field integrates LLMs and generative AI. By treating LLMs not only as technical instruments but also as epistemic and cultural tools, the paper calls for thoughtful, rigorous, and imaginative use of LLMs in future communication and social science research.",
    "authors": [
      "Tai-Quan Peng",
      "Xuzhen Yang"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-05-26T01:38:02Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19402v1"
  },
  {
    "arxiv_id": "2505.19240v2",
    "entry_id": "http://arxiv.org/abs/2505.19240v2",
    "title": "LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models",
    "summary": "Large language model (LLM) research has grown rapidly, along with increasing concern about their limitations such as failures in reasoning, hallucinations, and limited multilingual capability. While prior reviews have addressed these issues, they often focus on individual limitations or consider them within the broader context of evaluating overall model performance. This survey addresses the gap by presenting a data-driven, semi-automated review of research on limitations of LLMs (LLLMs) from 2022 to 2025, using a bottom-up approach. From a corpus of 250,000 ACL and arXiv papers, we extract 14,648 relevant limitation papers using keyword filtering and LLM-based classification, validated against expert labels. Using topic clustering (via two approaches, HDBSCAN+BERTopic and LlooM), we identify between 7 and 15 prominent types of limitations discussed in recent LLM research across the ACL and arXiv datasets. We find that LLM-related research increases nearly sixfold in ACL and nearly fifteenfold in arXiv between 2022 and 2025, while LLLMs research grows even faster, by a factor of over 12 in ACL and nearly 28 in arXiv. Reasoning remains the most studied limitation, followed by generalization, hallucination, bias, and security. The distribution of topics in the ACL dataset stays relatively stable over time, while arXiv shifts toward safety and controllability (with topics like security risks, alignment, hallucinations, knowledge editing), and multimodality between 2022 and 2025. We offer a quantitative view of trends in LLM limitations research and release a dataset of annotated abstracts and a validated methodology, available at: https://github.com/a-kostikova/LLLMs-Survey.",
    "authors": [
      "Aida Kostikova",
      "Zhipin Wang",
      "Deidamea Bajri",
      "Ole Pütz",
      "Benjamin Paaßen",
      "Steffen Eger"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-25T17:38:32Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19240v2"
  },
  {
    "arxiv_id": "2505.19219v2",
    "entry_id": "http://arxiv.org/abs/2505.19219v2",
    "title": "Where Paths Collide: A Comprehensive Survey of Classic and Learning-Based Multi-Agent Pathfinding",
    "summary": "Multi-Agent Path Finding (MAPF) is a fundamental problem in artificial intelligence and robotics, requiring the computation of collision-free paths for multiple agents navigating from their start locations to designated goals. As autonomous systems become increasingly prevalent in warehouses, urban transportation, and other complex environments, MAPF has evolved from a theoretical challenge to a critical enabler of real-world multi-robot coordination. This comprehensive survey bridges the long-standing divide between classical algorithmic approaches and emerging learning-based methods in MAPF research. We present a unified framework that encompasses search-based methods (including Conflict-Based Search, Priority-Based Search, and Large Neighborhood Search), compilation-based approaches (SAT, SMT, CSP, ASP, and MIP formulations), and data-driven techniques (reinforcement learning, supervised learning, and hybrid strategies). Through systematic analysis of experimental practices across 200+ papers, we uncover significant disparities in evaluation methodologies, with classical methods typically tested on larger-scale instances (up to 200 by 200 grids with 1000+ agents) compared to learning-based approaches (predominantly 10-100 agents). We provide a comprehensive taxonomy of evaluation metrics, environment types, and baseline selections, highlighting the need for standardized benchmarking protocols. Finally, we outline promising future directions including mixed-motive MAPF with game-theoretic considerations, language-grounded planning with large language models, and neural solver architectures that combine the rigor of classical methods with the flexibility of deep learning. This survey serves as both a comprehensive reference for researchers and a practical guide for deploying MAPF solutions in increasingly complex real-world applications.",
    "authors": [
      "Shiyue Wang",
      "Haozheng Xu",
      "Yuhan Zhang",
      "Jingran Lin",
      "Changhong Lu",
      "Xiangfeng Wang",
      "Wenhao Li"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "math.CO"
    ],
    "published": "2025-05-25T16:28:06Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19219v2"
  },
  {
    "arxiv_id": "2505.19184v3",
    "entry_id": "http://arxiv.org/abs/2505.19184v3",
    "title": "When Two LLMs Debate, Both Think They'll Win",
    "summary": "Can LLMs accurately adjust their confidence when facing opposition? Building on previous studies measuring calibration on static fact-based question-answering tasks, we evaluate Large Language Models (LLMs) in a dynamic, adversarial debate setting, uniquely combining two realistic factors: (a) a multi-turn format requiring models to update beliefs as new information emerges, and (b) a zero-sum structure to control for task-related uncertainty, since mutual high-confidence claims imply systematic overconfidence. We organized 60 three-round policy debates among ten state-of-the-art LLMs, with models privately rating their confidence (0-100) in winning after each round. We observed five concerning patterns: (1) Systematic overconfidence: models began debates with average initial confidence of 72.9% vs. a rational 50% baseline. (2) Confidence escalation: rather than reducing confidence as debates progressed, debaters increased their win probabilities, averaging 83% by the final round. (3) Mutual overestimation: in 61.7% of debates, both sides simultaneously claimed >=75% probability of victory, a logical impossibility. (4) Persistent self-debate bias: models debating identical copies increased confidence from 64.1% to 75.2%; even when explicitly informed their chance of winning was exactly 50%, confidence still rose (from 50.0% to 57.1%). (5) Misaligned private reasoning: models' private scratchpad thoughts sometimes differed from their public confidence ratings, raising concerns about faithfulness of chain-of-thought reasoning. These results suggest LLMs lack the ability to accurately self-assess or update their beliefs in dynamic, multi-turn tasks; a major concern as LLMs are now increasingly deployed without careful review in assistant and agentic roles.\n  Code for our experiments is available at https://github.com/pradyuprasad/llms_overconfidence",
    "authors": [
      "Pradyumna Shyama Prasad",
      "Minh Nhat Nguyen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-25T15:06:17Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19184v3"
  },
  {
    "arxiv_id": "2505.19147v3",
    "entry_id": "http://arxiv.org/abs/2505.19147v3",
    "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
    "summary": "The advancement of large language models (LLMs) and multi-modal LLMs (MLLMs) has historically relied on scaling model parameters. However, as hardware limits constrain further model growth, the primary computational bottleneck has shifted to the quadratic cost of self-attention over increasingly long sequences by ultra-long text contexts, high-resolution images, and extended videos. In this position paper, \\textbf{we argue that the focus of research for efficient artificial intelligence (AI) is shifting from model-centric compression to data-centric compression}. We position data-centric compression as the emerging paradigm, which improves AI efficiency by directly compressing the volume of data processed during model training or inference. To formalize this shift, we establish a unified framework for existing efficiency strategies and demonstrate why it constitutes a crucial paradigm change for long-context AI. We then systematically review the landscape of data-centric compression methods, analyzing their benefits across diverse scenarios. Finally, we outline key challenges and promising future research directions. Our work aims to provide a novel perspective on AI efficiency, synthesize existing efforts, and catalyze innovation to address the challenges posed by ever-increasing context lengths.",
    "authors": [
      "Xuyang Liu",
      "Zichen Wen",
      "Shaobo Wang",
      "Junjie Chen",
      "Zhishan Tao",
      "Yubo Wang",
      "Tailai Chen",
      "Xiangqi Jin",
      "Chang Zou",
      "Yiyu Wang",
      "Chenfei Liao",
      "Xu Zheng",
      "Honggang Chen",
      "Weijia Li",
      "Xuming Hu",
      "Conghui He",
      "Linfeng Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-05-25T13:51:17Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19147v3"
  },
  {
    "arxiv_id": "2505.23789v1",
    "entry_id": "http://arxiv.org/abs/2505.23789v1",
    "title": "Conversational Exploration of Literature Landscape with LitChat",
    "summary": "We are living in an era of \"big literature\", where the volume of digital scientific publications is growing exponentially. While offering new opportunities, this also poses challenges for understanding literature landscapes, as traditional manual reviewing is no longer feasible. Recent large language models (LLMs) have shown strong capabilities for literature comprehension, yet they are incapable of offering \"comprehensive, objective, open and transparent\" views desired by systematic reviews due to their limited context windows and trust issues like hallucinations. Here we present LitChat, an end-to-end, interactive and conversational literature agent that augments LLM agents with data-driven discovery tools to facilitate literature exploration. LitChat automatically interprets user queries, retrieves relevant sources, constructs knowledge graphs, and employs diverse data-mining techniques to generate evidence-based insights addressing user needs. We illustrate the effectiveness of LitChat via a case study on AI4Health, highlighting its capacity to quickly navigate the users through large-scale literature landscape with data-based evidence that is otherwise infeasible with traditional means.",
    "authors": [
      "Mingyu Huang",
      "Shasha Zhou",
      "Yuxuan Chen",
      "Ke Li"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-05-25T13:15:09Z",
    "pdf_url": "https://arxiv.org/pdf/2505.23789v1"
  },
  {
    "arxiv_id": "2505.19134v1",
    "entry_id": "http://arxiv.org/abs/2505.19134v1",
    "title": "Incentivizing High-Quality Human Annotations with Golden Questions",
    "summary": "Human-annotated data plays a vital role in training large language models (LLMs), such as supervised fine-tuning and human preference alignment. However, it is not guaranteed that paid human annotators produce high-quality data. In this paper, we study how to incentivize human annotators to do so. We start from a principal-agent model to model the dynamics between the company (the principal) and the annotator (the agent), where the principal can only monitor the annotation quality by examining $n$ samples. We investigate the maximum likelihood estimators (MLE) and the corresponding hypothesis testing to incentivize annotators: the agent is given a bonus if the MLE passes the test. By analyzing the variance of the outcome, we show that the strategic behavior of the agent makes the hypothesis testing very different from traditional ones: Unlike the exponential rate proved by the large deviation theory, the principal-agent model's hypothesis testing rate is of $Θ(1/\\sqrt{n \\log n})$. Our theory implies two criteria for the \\emph{golden questions} to monitor the performance of the annotators: they should be of (1) high certainty and (2) similar format to normal ones. In that light, we select a set of golden questions in human preference data. By doing incentive-compatible experiments, we find out that the annotators' behavior is better revealed by those golden questions, compared to traditional survey techniques such as instructed manipulation checks.",
    "authors": [
      "Shang Liu",
      "Zhongze Cai",
      "Hanzhao Wang",
      "Zhongyao Ma",
      "Xiaocheng Li"
    ],
    "categories": [
      "cs.GT",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-05-25T13:11:55Z",
    "pdf_url": "https://arxiv.org/pdf/2505.19134v1"
  },
  {
    "arxiv_id": "2505.21548v2",
    "entry_id": "http://arxiv.org/abs/2505.21548v2",
    "title": "Fluent but Foreign: Even Regional LLMs Lack Cultural Alignment",
    "summary": "Large language models (LLMs) are used worldwide, yet exhibit Western cultural tendencies. Many countries are now building ``regional'' LLMs, but it remains unclear whether they reflect local values and practices or merely speak local languages. Using India as a case study, we evaluate six Indic and six global LLMs on two dimensions -- values and practices -- grounded in nationally representative surveys and community-sourced QA datasets. Across tasks, Indic models do not align better with Indian norms than global models; in fact, a U.S. respondent is a closer proxy for Indian values than any Indic model. Prompting and regional fine-tuning fail to recover alignment and can even degrade existing knowledge. We attribute this to scarce culturally grounded data, especially for pretraining. We position cultural evaluation as a first-class requirement alongside multilingual benchmarks and offer a reusable, community-grounded methodology. We call for native, community-authored corpora and thick x wide evaluations to build truly sovereign LLMs.",
    "authors": [
      "Dhruv Agarwal",
      "Anya Shukla",
      "Sunayana Sitaram",
      "Aditya Vashistha"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "physics.soc-ph"
    ],
    "published": "2025-05-25T01:59:23Z",
    "pdf_url": "https://arxiv.org/pdf/2505.21548v2"
  },
  {
    "arxiv_id": "2505.18927v3",
    "entry_id": "http://arxiv.org/abs/2505.18927v3",
    "title": "Moderating Harm: Benchmarking Large Language Models for Cyberbullying Detection in YouTube Comments",
    "summary": "As online platforms grow, comment sections increasingly host harassment that undermines user experience and well-being. This study benchmarks three leading large language models, OpenAI GPT-4.1, Google Gemini 1.5 Pro, and Anthropic Claude 3 Opus, on a corpus of 5,080 YouTube comments sampled from high-abuse threads in gaming, lifestyle, food vlog, and music channels. The dataset comprises 1,334 harmful and 3,746 non-harmful messages in English, Arabic, and Indonesian, annotated independently by two reviewers with substantial agreement (Cohen's kappa = 0.83). Using a unified prompt and deterministic settings, GPT-4.1 achieved the best overall balance with an F1 score of 0.863, precision of 0.887, and recall of 0.841. Gemini flagged the highest share of harmful posts (recall = 0.875) but its precision fell to 0.767 due to frequent false positives. Claude delivered the highest precision at 0.920 and the lowest false-positive rate of 0.022, yet its recall dropped to 0.720. Qualitative analysis showed that all three models struggle with sarcasm, coded insults, and mixed-language slang. These results underscore the need for moderation pipelines that combine complementary models, incorporate conversational context, and fine-tune for under-represented languages and implicit abuse. A de-identified version of the dataset and full prompts is publicly released to promote reproducibility and further progress in automated content moderation.",
    "authors": [
      "Amel Muminovic"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-25T01:28:30Z",
    "pdf_url": "https://arxiv.org/pdf/2505.18927v3"
  },
  {
    "arxiv_id": "2505.18889v5",
    "entry_id": "http://arxiv.org/abs/2505.18889v5",
    "title": "Security Concerns for Large Language Models: A Survey",
    "summary": "Large Language Models (LLMs) such as ChatGPT and its competitors have caused a revolution in natural language processing, but their capabilities also introduce new security vulnerabilities. This survey provides a comprehensive overview of these emerging concerns, categorizing threats into several key areas: inference-time attacks via prompt manipulation; training-time attacks; misuse by malicious actors; and the inherent risks in autonomous LLM agents. Recently, a significant focus is increasingly being placed on the latter. We summarize recent academic and industrial studies from 2022 to 2025 that exemplify each threat, analyze existing defense mechanisms and their limitations, and identify open challenges in securing LLM-based applications. We conclude by emphasizing the importance of advancing robust, multi-layered security strategies to ensure LLMs are safe and beneficial.",
    "authors": [
      "Miles Q. Li",
      "Benjamin C. M. Fung"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-24T22:22:43Z",
    "pdf_url": "https://arxiv.org/pdf/2505.18889v5"
  },
  {
    "arxiv_id": "2505.18880v1",
    "entry_id": "http://arxiv.org/abs/2505.18880v1",
    "title": "REGen: Multimodal Retrieval-Embedded Generation for Long-to-Short Video Editing",
    "summary": "Short videos are an effective tool for promoting contents and improving knowledge accessibility. While existing extractive video summarization methods struggle to produce a coherent narrative, existing abstractive methods cannot `quote' from the input videos, i.e., inserting short video clips in their outputs. In this work, we explore novel video editing models for generating shorts that feature a coherent narrative with embedded video insertions extracted from a long input video. We propose a novel retrieval-embedded generation framework that allows a large language model to quote multimodal resources while maintaining a coherent narrative. Our proposed REGen system first generates the output story script with quote placeholders using a finetuned large language model, and then uses a novel retrieval model to replace the quote placeholders by selecting a video clip that best supports the narrative from a pool of candidate quotable video clips. We examine the proposed method on the task of documentary teaser generation, where short interview insertions are commonly used to support the narrative of a documentary. Our objective evaluations show that the proposed method can effectively insert short video clips while maintaining a coherent narrative. In a subjective survey, we show that our proposed method outperforms existing abstractive and extractive approaches in terms of coherence, alignment, and realism in teaser generation.",
    "authors": [
      "Weihan Xu",
      "Yimeng Ma",
      "Jingyue Huang",
      "Yang Li",
      "Wenye Ma",
      "Taylor Berg-Kirkpatrick",
      "Julian McAuley",
      "Paul Pu Liang",
      "Hao-Wen Dong"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-05-24T21:36:49Z",
    "pdf_url": "https://arxiv.org/pdf/2505.18880v1"
  },
  {
    "arxiv_id": "2505.18705v1",
    "entry_id": "http://arxiv.org/abs/2505.18705v1",
    "title": "AI-Researcher: Autonomous Scientific Innovation",
    "summary": "The powerful reasoning capabilities of Large Language Models (LLMs) in mathematics and coding, combined with their ability to automate complex tasks through agentic frameworks, present unprecedented opportunities for accelerating scientific innovation. In this paper, we introduce AI-Researcher, a fully autonomous research system that transforms how AI-driven scientific discovery is conducted and evaluated. Our framework seamlessly orchestrates the complete research pipeline--from literature review and hypothesis generation to algorithm implementation and publication-ready manuscript preparation--with minimal human intervention. To rigorously assess autonomous research capabilities, we develop Scientist-Bench, a comprehensive benchmark comprising state-of-the-art papers across diverse AI research domains, featuring both guided innovation and open-ended exploration tasks. Through extensive experiments, we demonstrate that AI-Researcher achieves remarkable implementation success rates and produces research papers that approach human-level quality. This work establishes new foundations for autonomous scientific innovation that can complement human researchers by systematically exploring solution spaces beyond cognitive limitations.",
    "authors": [
      "Jiabin Tang",
      "Lianghao Xia",
      "Zhonghang Li",
      "Chao Huang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-24T13:54:38Z",
    "pdf_url": "https://arxiv.org/pdf/2505.18705v1"
  },
  {
    "arxiv_id": "2505.18658v2",
    "entry_id": "http://arxiv.org/abs/2505.18658v2",
    "title": "Robustness in Large Language Models: A Survey of Mitigation Strategies and Evaluation Metrics",
    "summary": "Large Language Models (LLMs) have emerged as a promising cornerstone for the development of natural language processing (NLP) and artificial intelligence (AI). However, ensuring the robustness of LLMs remains a critical challenge. To address these challenges and advance the field, this survey provides a comprehensive overview of current studies in this area. First, we systematically examine the nature of robustness in LLMs, including its conceptual foundations, the importance of consistent performance across diverse inputs, and the implications of failure modes in real-world applications. Next, we analyze the sources of non-robustness, categorizing intrinsic model limitations, data-driven vulnerabilities, and external adversarial factors that compromise reliability. Following this, we review state-of-the-art mitigation strategies, and then we discuss widely adopted benchmarks, emerging metrics, and persistent gaps in assessing real-world reliability. Finally, we synthesize findings from existing surveys and interdisciplinary studies to highlight trends, unresolved issues, and pathways for future research.",
    "authors": [
      "Pankaj Kumar",
      "Subhankar Mishra"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-24T11:50:52Z",
    "pdf_url": "https://arxiv.org/pdf/2505.18658v2"
  },
  {
    "arxiv_id": "2505.21537v1",
    "entry_id": "http://arxiv.org/abs/2505.21537v1",
    "title": "OpenReview Should be Protected and Leveraged as a Community Asset for Research in the Era of Large Language Models",
    "summary": "In the era of large language models (LLMs), high-quality, domain-rich, and continuously evolving datasets capturing expert-level knowledge, core human values, and reasoning are increasingly valuable. This position paper argues that OpenReview -- the continually evolving repository of research papers, peer reviews, author rebuttals, meta-reviews, and decision outcomes -- should be leveraged more broadly as a core community asset for advancing research in the era of LLMs. We highlight three promising areas in which OpenReview can uniquely contribute: enhancing the quality, scalability, and accountability of peer review processes; enabling meaningful, open-ended benchmarks rooted in genuine expert deliberation; and supporting alignment research through real-world interactions reflecting expert assessment, intentions, and scientific values. To better realize these opportunities, we suggest the community collaboratively explore standardized benchmarks and usage guidelines around OpenReview, inviting broader dialogue on responsible data use, ethical considerations, and collective stewardship.",
    "authors": [
      "Hao Sun",
      "Yunyi Shen",
      "Mihaela van der Schaar"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-05-24T09:07:13Z",
    "pdf_url": "https://arxiv.org/pdf/2505.21537v1"
  },
  {
    "arxiv_id": "2505.18475v2",
    "entry_id": "http://arxiv.org/abs/2505.18475v2",
    "title": "A Survey of Large Language Models for Data Challenges in Graphs",
    "summary": "Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction. While graph learning has achieved remarkable progress, real-world graph data presents a number of challenges that significantly hinder the learning process. In this survey, we focus on four fundamental data-centric challenges: (1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways. Recently, Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge. This survey focuses on how LLMs can address four fundamental data-centric challenges in graph-structured data, thereby improving the effectiveness of graph learning. For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages. Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field. To support further exploration, we have curated a repository of recent advances on graph learning challenges: https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.",
    "authors": [
      "Mengran Li",
      "Pengyu Zhang",
      "Wenbin Xing",
      "Yijia Zheng",
      "Klim Zaporojets",
      "Junzhou Chen",
      "Ronghui Zhang",
      "Yong Zhang",
      "Siyuan Gong",
      "Jia Hu",
      "Xiaolei Ma",
      "Zhiyuan Liu",
      "Paul Groth",
      "Marcel Worring"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-05-24T02:38:14Z",
    "pdf_url": "https://arxiv.org/pdf/2505.18475v2"
  },
  {
    "arxiv_id": "2505.18458v3",
    "entry_id": "http://arxiv.org/abs/2505.18458v3",
    "title": "A Survey of LLM $\\times$ DATA",
    "summary": "The integration of large language model (LLM) and data management (DATA) is rapidly redefining both domains. In this survey, we comprehensively review the bidirectional relationships. On the one hand, DATA4LLM, spanning large-scale data processing, storage, and serving, feeds LLMs with high quality, diversity, and timeliness of data required for stages like pre-training, post-training, retrieval-augmented generation, and agentic workflows: (i) Data processing for LLMs includes scalable acquisition, deduplication, filtering, selection, domain mixing, and synthetic augmentation; (ii) Data Storage for LLMs focuses on efficient data and model formats, distributed and heterogeneous storage hierarchies, KV-cache management, and fault-tolerant checkpointing; (iii) Data serving for LLMs tackles challenges in RAG (e.g., knowledge post-processing), LLM inference (e.g., prompt compression, data provenance), and training strategies (e.g., data packing and shuffling). On the other hand, in LLM4DATA, LLMs are emerging as general-purpose engines for data management. We review recent advances in (i) data manipulation, including automatic data cleaning, integration, discovery; (ii) data analysis, covering reasoning over structured, semi-structured, and unstructured data, and (iii) system optimization (e.g., configuration tuning, query rewriting, anomaly diagnosis), powered by LLM techniques like retrieval-augmented prompting, task-specialized fine-tuning, and multi-agent collaboration.",
    "authors": [
      "Xuanhe Zhou",
      "Junxuan He",
      "Wei Zhou",
      "Haodong Chen",
      "Zirui Tang",
      "Haoyu Zhao",
      "Xin Tong",
      "Guoliang Li",
      "Youmin Chen",
      "Jun Zhou",
      "Zhaojun Sun",
      "Binyuan Hui",
      "Shuo Wang",
      "Conghui He",
      "Zhiyuan Liu",
      "Jingren Zhou",
      "Fan Wu"
    ],
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-05-24T01:57:12Z",
    "pdf_url": "https://arxiv.org/pdf/2505.18458v3"
  },
  {
    "arxiv_id": "2505.18322v1",
    "entry_id": "http://arxiv.org/abs/2505.18322v1",
    "title": "Is It Bad to Work All the Time? Cross-Cultural Evaluation of Social Norm Biases in GPT-4",
    "summary": "LLMs have been demonstrated to align with the values of Western or North American cultures. Prior work predominantly showed this effect through leveraging surveys that directly ask (originally people and now also LLMs) about their values. However, it is hard to believe that LLMs would consistently apply those values in real-world scenarios. To address that, we take a bottom-up approach, asking LLMs to reason about cultural norms in narratives from different cultures. We find that GPT-4 tends to generate norms that, while not necessarily incorrect, are significantly less culture-specific. In addition, while it avoids overtly generating stereotypes, the stereotypical representations of certain cultures are merely hidden rather than suppressed in the model, and such stereotypes can be easily recovered. Addressing these challenges is a crucial step towards developing LLMs that fairly serve their diverse user base.",
    "authors": [
      "Zhuozhuo Joy Liu",
      "Farhan Samir",
      "Mehar Bhatia",
      "Laura K. Nelson",
      "Vered Shwartz"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-23T19:28:00Z",
    "pdf_url": "https://arxiv.org/pdf/2505.18322v1"
  },
  {
    "arxiv_id": "2505.17928v2",
    "entry_id": "http://arxiv.org/abs/2505.17928v2",
    "title": "Towards Practical Defect-Focused Automated Code Review",
    "summary": "The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2x improvement over standard LLMs and a 10x gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.",
    "authors": [
      "Junyi Lu",
      "Lili Jiang",
      "Xiaojia Li",
      "Jianbing Fang",
      "Fengjun Zhang",
      "Li Yang",
      "Chun Zuo"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-05-23T14:06:26Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17928v2"
  },
  {
    "arxiv_id": "2505.17894v2",
    "entry_id": "http://arxiv.org/abs/2505.17894v2",
    "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a Small Language Model",
    "summary": "We introduce Mutarjim, a compact yet powerful language model for bidirectional Arabic-English translation. While large-scale LLMs have shown impressive progress in natural language processing tasks, including machine translation, smaller models. Leveraging this insight, we developed Mutarjim based on Kuwain-1.5B , a language model tailored for both Arabic and English. Despite its modest size, Mutarjim outperforms much larger models on several established benchmarks, achieved through an optimized two-phase training approach and a carefully curated, high-quality training corpus.. Experimental results show that Mutarjim rivals models up to 20 times larger while significantly reducing computational costs and training requirements. We also introduce Tarjama-25, a new benchmark designed to overcome limitations in existing Arabic-English benchmarking datasets, such as domain narrowness, short sentence lengths, and English-source bias. Tarjama-25 comprises 5,000 expert-reviewed sentence pairs and spans a wide range of domains, offering a more comprehensive and balanced evaluation framework. Notably, Mutarjim achieves state-of-the-art performance on the English-to-Arabic task in Tarjama-25, surpassing even significantly larger and proprietary models like GPT-4o mini. We publicly release Tarjama-25 to support future research and advance the evaluation of Arabic-English translation systems.",
    "authors": [
      "Khalil Hennara",
      "Muhammad Hreden",
      "Mohamed Motaism Hamed",
      "Zeina Aldallal",
      "Sara Chrouf",
      "Safwan AlModhayan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-23T13:42:21Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17894v2"
  },
  {
    "arxiv_id": "2505.17648v2",
    "entry_id": "http://arxiv.org/abs/2505.17648v2",
    "title": "Simulating Macroeconomic Expectations using LLM Agents",
    "summary": "We introduce a novel framework for simulating macroeconomic expectation formation using Large Language Model-Empowered Agents (LLM Agents). By constructing thousands of LLM Agents equipped with modules for personal characteristics, prior expectations, and knowledge, we replicate a survey experiment involving households and experts on inflation and unemployment. Our results show that although the expectations and thoughts generated by LLM Agents are more homogeneous than those of human participants, they still effectively capture key heterogeneity across agents and the underlying drivers of expectation formation. Furthermore, a module-ablation exercise highlights the critical role of prior expectations in simulating such heterogeneity. This approach complements traditional survey methods and offers new insights into AI behavioral science in macroeconomic research.",
    "authors": [
      "Jianhao Lin",
      "Lexuan Sun",
      "Yixin Yan"
    ],
    "categories": [
      "econ.GN",
      "cs.AI"
    ],
    "published": "2025-05-23T09:11:14Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17648v2"
  },
  {
    "arxiv_id": "2505.17492v1",
    "entry_id": "http://arxiv.org/abs/2505.17492v1",
    "title": "PD$^3$: A Project Duplication Detection Framework via Adapted Multi-Agent Debate",
    "summary": "Project duplication detection is critical for project quality assessment, as it improves resource utilization efficiency by preventing investing in newly proposed project that have already been studied. It requires the ability to understand high-level semantics and generate constructive and valuable feedback. Existing detection methods rely on basic word- or sentence-level comparison or solely apply large language models, lacking valuable insights for experts and in-depth comprehension of project content and review criteria. To tackle this issue, we propose PD$^3$, a Project Duplication Detection framework via adapted multi-agent Debate. Inspired by real-world expert debates, it employs a fair competition format to guide multi-agent debate to retrieve relevant projects. For feedback, it incorporates both qualitative and quantitative analysis to improve its practicality. Over 800 real-world power project data spanning more than 20 specialized fields are used to evaluate the framework, demonstrating that our method outperforms existing approaches by 7.43% and 8.00% in two downstream tasks. Furthermore, we establish an online platform, Review Dingdang, to assist power experts, saving 5.73 million USD in initial detection on more than 100 newly proposed projects.",
    "authors": [
      "Dezheng Bao",
      "Yueci Yang",
      "Xin Chen",
      "Zhengxuan Jiang",
      "Zeguo Fei",
      "Daoze Zhang",
      "Xuanwen Huang",
      "Junru Chen",
      "Chutian Yu",
      "Xiang Yuan",
      "Yang Yang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-05-23T05:38:37Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17492v1"
  },
  {
    "arxiv_id": "2505.17479v1",
    "entry_id": "http://arxiv.org/abs/2505.17479v1",
    "title": "Twin-2K-500: A dataset for building digital twins of over 2,000 people based on their answers to over 500 questions",
    "summary": "LLM-based digital twin simulation, where large language models are used to emulate individual human behavior, holds great promise for research in AI, social science, and digital experimentation. However, progress in this area has been hindered by the scarcity of real, individual-level datasets that are both large and publicly available. This lack of high-quality ground truth limits both the development and validation of digital twin methodologies. To address this gap, we introduce a large-scale, public dataset designed to capture a rich and holistic view of individual human behavior. We survey a representative sample of $N = 2,058$ participants (average 2.42 hours per person) in the US across four waves with 500 questions in total, covering a comprehensive battery of demographic, psychological, economic, personality, and cognitive measures, as well as replications of behavioral economics experiments and a pricing survey. The final wave repeats tasks from earlier waves to establish a test-retest accuracy baseline. Initial analyses suggest the data are of high quality and show promise for constructing digital twins that predict human behavior well at the individual and aggregate levels. By making the full dataset publicly available, we aim to establish a valuable testbed for the development and benchmarking of LLM-based persona simulations. Beyond LLM applications, due to its unique breadth and scale the dataset also enables broad social science research, including studies of cross-construct correlations and heterogeneous treatment effects.",
    "authors": [
      "Olivier Toubia",
      "George Z. Gui",
      "Tianyi Peng",
      "Daniel J. Merlau",
      "Ang Li",
      "Haozhe Chen"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "econ.EM"
    ],
    "published": "2025-05-23T05:05:11Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17479v1"
  },
  {
    "arxiv_id": "2505.17371v2",
    "entry_id": "http://arxiv.org/abs/2505.17371v2",
    "title": "An End-to-End Approach for Child Reading Assessment in the Xhosa Language",
    "summary": "Child literacy is a strong predictor of life outcomes at the subsequent stages of an individual's life. This points to a need for targeted interventions in vulnerable low and middle income populations to help bridge the gap between literacy levels in these regions and high income ones. In this effort, reading assessments provide an important tool to measure the effectiveness of these programs and AI can be a reliable and economical tool to support educators with this task. Developing accurate automatic reading assessment systems for child speech in low-resource languages poses significant challenges due to limited data and the unique acoustic properties of children's voices. This study focuses on Xhosa, a language spoken in South Africa, to advance child speech recognition capabilities. We present a novel dataset composed of child speech samples in Xhosa. The dataset is available upon request and contains ten words and letters, which are part of the Early Grade Reading Assessment (EGRA) system. Each recording is labeled with an online and cost-effective approach by multiple markers and a subsample is validated by an independent EGRA reviewer. This dataset is evaluated with three fine-tuned state-of-the-art end-to-end models: wav2vec 2.0, HuBERT, and Whisper. The results indicate that the performance of these models can be significantly influenced by the amount and balancing of the available training data, which is fundamental for cost-effective large dataset collection. Furthermore, our experiments indicate that the wav2vec 2.0 performance is improved by training on multiple classes at a time, even when the number of available samples is constrained.",
    "authors": [
      "Sergio Chevtchenko",
      "Nikhil Navas",
      "Rafaella Vale",
      "Franco Ubaudi",
      "Sipumelele Lucwaba",
      "Cally Ardington",
      "Soheil Afshar",
      "Mark Antoniou",
      "Saeed Afshar"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-05-23T00:59:58Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17371v2"
  },
  {
    "arxiv_id": "2505.17342v1",
    "entry_id": "http://arxiv.org/abs/2505.17342v1",
    "title": "A Survey of Safe Reinforcement Learning and Constrained MDPs: A Technical Survey on Single-Agent and Multi-Agent Safety",
    "summary": "Safe Reinforcement Learning (SafeRL) is the subfield of reinforcement learning that explicitly deals with safety constraints during the learning and deployment of agents. This survey provides a mathematically rigorous overview of SafeRL formulations based on Constrained Markov Decision Processes (CMDPs) and extensions to Multi-Agent Safe RL (SafeMARL). We review theoretical foundations of CMDPs, covering definitions, constrained optimization techniques, and fundamental theorems. We then summarize state-of-the-art algorithms in SafeRL for single agents, including policy gradient methods with safety guarantees and safe exploration strategies, as well as recent advances in SafeMARL for cooperative and competitive settings. Additionally, we propose five open research problems to advance the field, with three focusing on SafeMARL. Each problem is described with motivation, key challenges, and related prior work. This survey is intended as a technical guide for researchers interested in SafeRL and SafeMARL, highlighting key concepts, methods, and open future research directions.",
    "authors": [
      "Ankita Kushwaha",
      "Kiran Ravish",
      "Preeti Lamba",
      "Pawan Kumar"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-05-22T23:26:12Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17342v1"
  },
  {
    "arxiv_id": "2505.17249v1",
    "entry_id": "http://arxiv.org/abs/2505.17249v1",
    "title": "Where You Go is Who You Are: Behavioral Theory-Guided LLMs for Inverse Reinforcement Learning",
    "summary": "Big trajectory data hold great promise for human mobility analysis, but their utility is often constrained by the absence of critical traveler attributes, particularly sociodemographic information. While prior studies have explored predicting such attributes from mobility patterns, they often overlooked underlying cognitive mechanisms and exhibited low predictive accuracy. This study introduces SILIC, short for Sociodemographic Inference with LLM-guided Inverse Reinforcement Learning (IRL) and Cognitive Chain Reasoning (CCR), a theoretically grounded framework that leverages LLMs to infer sociodemographic attributes from observed mobility patterns by capturing latent behavioral intentions and reasoning through psychological constructs. Particularly, our approach explicitly follows the Theory of Planned Behavior (TPB), a foundational behavioral framework in transportation research, to model individuals' latent cognitive processes underlying travel decision-making. The LLMs further provide heuristic guidance to improve IRL reward function initialization and update by addressing its ill-posedness and optimization challenges arising from the vast and unstructured reward space. Evaluated in the 2017 Puget Sound Regional Council Household Travel Survey, our method substantially outperforms state-of-the-art baselines and shows great promise for enriching big trajectory data to support more behaviorally grounded applications in transportation planning and beyond.",
    "authors": [
      "Yuran Sun",
      "Susu Xu",
      "Chenguang Wang",
      "Xilei Zhao"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-22T19:56:03Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17249v1"
  },
  {
    "arxiv_id": "2505.16918v1",
    "entry_id": "http://arxiv.org/abs/2505.16918v1",
    "title": "Scalable and Interpretable Contextual Bandits: A Literature Review and Retail Offer Prototype",
    "summary": "This paper presents a concise review of Contextual Multi-Armed Bandit (CMAB) methods and introduces an experimental framework for scalable, interpretable offer selection, addressing the challenge of fast-changing offers. The approach models context at the product category level, allowing offers to span multiple categories and enabling knowledge transfer across similar offers. This improves learning efficiency and generalization in dynamic environments. The framework extends standard CMAB methodology to support multi-category contexts, and achieves scalability through efficient feature engineering and modular design. Advanced features such as MPG (Member Purchase Gap) and MF (Matrix Factorization) capture nuanced user-offer interactions, with implementation in Python for practical deployment.\n  A key contribution is interpretability at scale: logistic regression models yield transparent weight vectors, accessible via a large language model (LLM) interface for real-time, user-level tracking and explanation of evolving preferences. This enables the generation of detailed member profiles and identification of behavioral patterns, supporting personalized offer optimization and enhancing trust in automated decisions. By situating our prototype alongside established paradigms like Generalized Linear Models and Thompson Sampling, we demonstrate its value for both research and real-world CMAB applications.",
    "authors": [
      "Nikola Tankovic",
      "Robert Sajina"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-05-22T17:13:01Z",
    "pdf_url": "https://arxiv.org/pdf/2505.16918v1"
  },
  {
    "arxiv_id": "2505.16765v1",
    "entry_id": "http://arxiv.org/abs/2505.16765v1",
    "title": "When Safety Detectors Aren't Enough: A Stealthy and Effective Jailbreak Attack on LLMs via Steganographic Techniques",
    "summary": "Jailbreak attacks pose a serious threat to large language models (LLMs) by bypassing built-in safety mechanisms and leading to harmful outputs. Studying these attacks is crucial for identifying vulnerabilities and improving model security. This paper presents a systematic survey of jailbreak methods from the novel perspective of stealth. We find that existing attacks struggle to simultaneously achieve toxic stealth (concealing toxic content) and linguistic stealth (maintaining linguistic naturalness). Motivated by this, we propose StegoAttack, a fully stealthy jailbreak attack that uses steganography to hide the harmful query within benign, semantically coherent text. The attack then prompts the LLM to extract the hidden query and respond in an encrypted manner. This approach effectively hides malicious intent while preserving naturalness, allowing it to evade both built-in and external safety mechanisms. We evaluate StegoAttack on four safety-aligned LLMs from major providers, benchmarking against eight state-of-the-art methods. StegoAttack achieves an average attack success rate (ASR) of 92.00%, outperforming the strongest baseline by 11.0%. Its ASR drops by less than 1% even under external detection (e.g., Llama Guard). Moreover, it attains the optimal comprehensive scores on stealth detection metrics, demonstrating both high efficacy and exceptional stealth capabilities. The code is available at https://anonymous.4open.science/r/StegoAttack-Jail66",
    "authors": [
      "Jianing Geng",
      "Biao Yi",
      "Zekun Fei",
      "Tongxi Wu",
      "Lihai Nie",
      "Zheli Liu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-22T15:07:34Z",
    "pdf_url": "https://arxiv.org/pdf/2505.16765v1"
  },
  {
    "arxiv_id": "2505.16557v2",
    "entry_id": "http://arxiv.org/abs/2505.16557v2",
    "title": "Is Your LLM-Based Multi-Agent a Reliable Real-World Planner? Exploring Fraud Detection in Travel Planning",
    "summary": "The rise of Large Language Model-based Multi-Agent Planning has leveraged advanced frameworks to enable autonomous and collaborative task execution. Some systems rely on platforms like review sites and social media, which are prone to fraudulent information, such as fake reviews or misleading descriptions. This reliance poses risks, potentially causing financial losses and harming user experiences. To evaluate the risk of planning systems in real-world applications, we introduce \\textbf{WandaPlan}, an evaluation environment mirroring real-world data and injected with deceptive content. We assess system performance across three fraud cases: Misinformation Fraud, Team-Coordinated Multi-Person Fraud, and Level-Escalating Multi-Round Fraud. We reveal significant weaknesses in existing frameworks that prioritize task efficiency over data authenticity. At the same time, we validate WandaPlan's generalizability, capable of assessing the risks of real-world open-source planning frameworks. To mitigate the risk of fraud, we propose integrating an anti-fraud agent, providing a solution for reliable planning.",
    "authors": [
      "Junchi Yao",
      "Jianhua Xu",
      "Tianyu Xin",
      "Ziyi Wang",
      "Shenzhe Zhu",
      "Shu Yang",
      "Di Wang"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-05-22T11:46:46Z",
    "pdf_url": "https://arxiv.org/pdf/2505.16557v2"
  },
  {
    "arxiv_id": "2505.16477v1",
    "entry_id": "http://arxiv.org/abs/2505.16477v1",
    "title": "Advancing the Scientific Method with Large Language Models: From Hypothesis to Discovery",
    "summary": "With recent Nobel Prizes recognising AI contributions to science, Large Language Models (LLMs) are transforming scientific research by enhancing productivity and reshaping the scientific method. LLMs are now involved in experimental design, data analysis, and workflows, particularly in chemistry and biology. However, challenges such as hallucinations and reliability persist. In this contribution, we review how Large Language Models (LLMs) are redefining the scientific method and explore their potential applications across different stages of the scientific cycle, from hypothesis testing to discovery. We conclude that, for LLMs to serve as relevant and effective creative engines and productivity enhancers, their deep integration into all steps of the scientific process should be pursued in collaboration and alignment with human scientific goals, with clear evaluation metrics. The transition to AI-driven science raises ethical questions about creativity, oversight, and responsibility. With careful guidance, LLMs could evolve into creative engines, driving transformative breakthroughs across scientific disciplines responsibly and effectively. However, the scientific community must also decide how much it leaves to LLMs to drive science, even when associations with 'reasoning', mostly currently undeserved, are made in exchange for the potential to explore hypothesis and solution regions that might otherwise remain unexplored by human exploration alone.",
    "authors": [
      "Yanbo Zhang",
      "Sumeer A. Khan",
      "Adnan Mahmud",
      "Huck Yang",
      "Alexander Lavin",
      "Michael Levin",
      "Jeremy Frey",
      "Jared Dunnmon",
      "James Evans",
      "Alan Bundy",
      "Saso Dzeroski",
      "Jesper Tegner",
      "Hector Zenil"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-22T10:05:48Z",
    "pdf_url": "https://arxiv.org/pdf/2505.16477v1"
  },
  {
    "arxiv_id": "2505.16429v2",
    "entry_id": "http://arxiv.org/abs/2505.16429v2",
    "title": "Beyond Static Testbeds: An Interaction-Centric Agent Simulation Platform for Dynamic Recommender Systems",
    "summary": "Evaluating and iterating upon recommender systems is crucial, yet traditional A/B testing is resource-intensive, and offline methods struggle with dynamic user-platform interactions. While agent-based simulation is promising, existing platforms often lack a mechanism for user actions to dynamically reshape the environment. To bridge this gap, we introduce RecInter, a novel agent-based simulation platform for recommender systems featuring a robust interaction mechanism. In RecInter platform, simulated user actions (e.g., likes, reviews, purchases) dynamically update item attributes in real-time, and introduced Merchant Agents can reply, fostering a more realistic and evolving ecosystem. High-fidelity simulation is ensured through Multidimensional User Profiling module, Advanced Agent Architecture, and LLM fine-tuned on Chain-of-Thought (CoT) enriched interaction data. Our platform achieves significantly improved simulation credibility and successfully replicates emergent phenomena like Brand Loyalty and the Matthew Effect. Experiments demonstrate that this interaction mechanism is pivotal for simulating realistic system evolution, establishing our platform as a credible testbed for recommender systems research. Our codes are available at https://github.com/jinsong8/RecInter.",
    "authors": [
      "Song Jin",
      "Juntian Zhang",
      "Yuhan Liu",
      "Xun Zhang",
      "Yufei Zhang",
      "Guojun Yin",
      "Fei Jiang",
      "Wei Lin",
      "Rui Yan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-22T09:14:23Z",
    "pdf_url": "https://arxiv.org/pdf/2505.16429v2"
  },
  {
    "arxiv_id": "2505.16330v1",
    "entry_id": "http://arxiv.org/abs/2505.16330v1",
    "title": "SC4ANM: Identifying Optimal Section Combinations for Automated Novelty Prediction in Academic Papers",
    "summary": "Novelty is a core component of academic papers, and there are multiple perspectives on the assessment of novelty. Existing methods often focus on word or entity combinations, which provide limited insights. The content related to a paper's novelty is typically distributed across different core sections, e.g., Introduction, Methodology and Results. Therefore, exploring the optimal combination of sections for evaluating the novelty of a paper is important for advancing automated novelty assessment. In this paper, we utilize different combinations of sections from academic papers as inputs to drive language models to predict novelty scores. We then analyze the results to determine the optimal section combinations for novelty score prediction. We first employ natural language processing techniques to identify the sectional structure of academic papers, categorizing them into introduction, methods, results, and discussion (IMRaD). Subsequently, we used different combinations of these sections (e.g., introduction and methods) as inputs for pretrained language models (PLMs) and large language models (LLMs), employing novelty scores provided by human expert reviewers as ground truth labels to obtain prediction results. The results indicate that using introduction, results and discussion is most appropriate for assessing the novelty of a paper, while the use of the entire text does not yield significant results. Furthermore, based on the results of the PLMs and LLMs, the introduction and results appear to be the most important section for the task of novelty score prediction. The code and dataset for this paper can be accessed at https://github.com/njust-winchy/SC4ANM.",
    "authors": [
      "Wenqing Wu",
      "Chengzhi Zhang",
      "Tong Bao",
      "Yi Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "published": "2025-05-22T07:34:59Z",
    "pdf_url": "https://arxiv.org/pdf/2505.16330v1"
  },
  {
    "arxiv_id": "2505.20310v1",
    "entry_id": "http://arxiv.org/abs/2505.20310v1",
    "title": "Manalyzer: End-to-end Automated Meta-analysis with Multi-agent System",
    "summary": "Meta-analysis is a systematic research methodology that synthesizes data from multiple existing studies to derive comprehensive conclusions. This approach not only mitigates limitations inherent in individual studies but also facilitates novel discoveries through integrated data analysis. Traditional meta-analysis involves a complex multi-stage pipeline including literature retrieval, paper screening, and data extraction, which demands substantial human effort and time. However, while LLM-based methods can accelerate certain stages, they still face significant challenges, such as hallucinations in paper screening and data extraction. In this paper, we propose a multi-agent system, Manalyzer, which achieves end-to-end automated meta-analysis through tool calls. The hybrid review, hierarchical extraction, self-proving, and feedback checking strategies implemented in Manalyzer significantly alleviate these two hallucinations. To comprehensively evaluate the performance of meta-analysis, we construct a new benchmark comprising 729 papers across 3 domains, encompassing text, image, and table modalities, with over 10,000 data points. Extensive experiments demonstrate that Manalyzer achieves significant performance improvements over the LLM baseline in multi meta-analysis tasks. Project page: https://black-yt.github.io/meta-analysis-page/ .",
    "authors": [
      "Wanghan Xu",
      "Wenlong Zhang",
      "Fenghua Ling",
      "Ben Fei",
      "Yusong Hu",
      "Fangxuan Ren",
      "Jintai Lin",
      "Wanli Ouyang",
      "Lei Bai"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-05-22T07:25:31Z",
    "pdf_url": "https://arxiv.org/pdf/2505.20310v1"
  },
  {
    "arxiv_id": "2505.16097v1",
    "entry_id": "http://arxiv.org/abs/2505.16097v1",
    "title": "TrialPanorama: Database and Benchmark for Systematic Review and Design of Clinical Trials",
    "summary": "Developing artificial intelligence (AI) for vertical domains requires a solid data foundation for both training and evaluation. In this work, we introduce TrialPanorama, a large-scale, structured database comprising 1,657,476 clinical trial records aggregated from 15 global sources. The database captures key aspects of trial design and execution, including trial setups, interventions, conditions, biomarkers, and outcomes, and links them to standard biomedical ontologies such as DrugBank and MedDRA. This structured and ontology-grounded design enables TrialPanorama to serve as a unified, extensible resource for a wide range of clinical trial tasks, including trial planning, design, and summarization. To demonstrate its utility, we derive a suite of benchmark tasks directly from the TrialPanorama database. The benchmark spans eight tasks across two categories: three for systematic review (study search, study screening, and evidence summarization) and five for trial design (arm design, eligibility criteria, endpoint selection, sample size estimation, and trial completion assessment). The experiments using five state-of-the-art large language models (LLMs) show that while general-purpose LLMs exhibit some zero-shot capability, their performance is still inadequate for high-stakes clinical trial workflows. We release TrialPanorama database and the benchmark to facilitate further research on AI for clinical trials.",
    "authors": [
      "Zifeng Wang",
      "Qiao Jin",
      "Jiacheng Lin",
      "Junyi Gao",
      "Jathurshan Pradeepkumar",
      "Pengcheng Jiang",
      "Benjamin Danek",
      "Zhiyong Lu",
      "Jimeng Sun"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-22T00:58:43Z",
    "pdf_url": "https://arxiv.org/pdf/2505.16097v1"
  },
  {
    "arxiv_id": "2505.16094v1",
    "entry_id": "http://arxiv.org/abs/2505.16094v1",
    "title": "A Survey of Large Language Models for Text-Guided Molecular Discovery: from Molecule Generation to Optimization",
    "summary": "Large language models (LLMs) are introducing a paradigm shift in molecular discovery by enabling text-guided interaction with chemical spaces through natural language, symbolic notations, with emerging extensions to incorporate multi-modal inputs. To advance the new field of LLM for molecular discovery, this survey provides an up-to-date and forward-looking review of the emerging use of LLMs for two central tasks: molecule generation and molecule optimization. Based on our proposed taxonomy for both problems, we analyze representative techniques in each category, highlighting how LLM capabilities are leveraged across different learning settings. In addition, we include the commonly used datasets and evaluation protocols. We conclude by discussing key challenges and future directions, positioning this survey as a resource for researchers working at the intersection of LLMs and molecular science. A continuously updated reading list is available at https://github.com/REAL-Lab-NU/Awesome-LLM-Centric-Molecular-Discovery.",
    "authors": [
      "Ziqing Wang",
      "Kexin Zhang",
      "Zihan Zhao",
      "Yibo Wen",
      "Abhishek Pandey",
      "Han Liu",
      "Kaize Ding"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-05-22T00:26:27Z",
    "pdf_url": "https://arxiv.org/pdf/2505.16094v1"
  },
  {
    "arxiv_id": "2505.15957v3",
    "entry_id": "http://arxiv.org/abs/2505.15957v3",
    "title": "Towards Holistic Evaluation of Large Audio-Language Models: A Comprehensive Survey",
    "summary": "With advancements in large audio-language models (LALMs), which enhance large language models (LLMs) with auditory capabilities, these models are expected to demonstrate universal proficiency across various auditory tasks. While numerous benchmarks have emerged to assess LALMs' performance, they remain fragmented and lack a structured taxonomy. To bridge this gap, we conduct a comprehensive survey and propose a systematic taxonomy for LALM evaluations, categorizing them into four dimensions based on their objectives: (1) General Auditory Awareness and Processing, (2) Knowledge and Reasoning, (3) Dialogue-oriented Ability, and (4) Fairness, Safety, and Trustworthiness. We provide detailed overviews within each category and highlight challenges in this field, offering insights into promising future directions. To the best of our knowledge, this is the first survey specifically focused on the evaluations of LALMs, providing clear guidelines for the community. We will release the collection of the surveyed papers and actively maintain it to support ongoing advancements in the field.",
    "authors": [
      "Chih-Kai Yang",
      "Neo S. Ho",
      "Hung-yi Lee"
    ],
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.CL",
      "cs.SD"
    ],
    "published": "2025-05-21T19:17:29Z",
    "pdf_url": "https://arxiv.org/pdf/2505.15957v3"
  },
  {
    "arxiv_id": "2505.15747v2",
    "entry_id": "http://arxiv.org/abs/2505.15747v2",
    "title": "Multi-modal Integration Analysis of Alzheimer's Disease Using Large Language Models and Knowledge Graphs",
    "summary": "We propose a novel framework for integrating fragmented multi-modal data in Alzheimer's disease (AD) research using large language models (LLMs) and knowledge graphs. While traditional multimodal analysis requires matched patient IDs across datasets, our approach demonstrates population-level integration of MRI, gene expression, biomarkers, EEG, and clinical indicators from independent cohorts. Statistical analysis identified significant features in each modality, which were connected as nodes in a knowledge graph. LLMs then analyzed the graph to extract potential correlations and generate hypotheses in natural language. This approach revealed several novel relationships, including a potential pathway linking metabolic risk factors to tau protein abnormalities via neuroinflammation (r>0.6, p<0.001), and unexpected correlations between frontal EEG channels and specific gene expression profiles (r=0.42-0.58, p<0.01). Cross-validation with independent datasets confirmed the robustness of major findings, with consistent effect sizes across cohorts (variance <15%). The reproducibility of these findings was further supported by expert review (Cohen's k=0.82) and computational validation. Our framework enables cross modal integration at a conceptual level without requiring patient ID matching, offering new possibilities for understanding AD pathology through fragmented data reuse and generating testable hypotheses for future research.",
    "authors": [
      "Kanan Kiguchi",
      "Yunhao Tu",
      "Katsuhiro Ajito",
      "Fady Alnajjar",
      "Kazuyuki Murase"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-05-21T16:51:49Z",
    "pdf_url": "https://arxiv.org/pdf/2505.15747v2"
  },
  {
    "arxiv_id": "2505.15741v1",
    "entry_id": "http://arxiv.org/abs/2505.15741v1",
    "title": "Evolutionary Computation and Large Language Models: A Survey of Methods, Synergies, and Applications",
    "summary": "Integrating Large Language Models (LLMs) and Evolutionary Computation (EC) represents a promising avenue for advancing artificial intelligence by combining powerful natural language understanding with optimization and search capabilities. This manuscript explores the synergistic potential of LLMs and EC, reviewing their intersections, complementary strengths, and emerging applications. We identify key opportunities where EC can enhance LLM training, fine-tuning, prompt engineering, and architecture search, while LLMs can, in turn, aid in automating the design, analysis, and interpretation of ECs. The manuscript explores the synergistic integration of EC and LLMs, highlighting their bidirectional contributions to advancing artificial intelligence. It first examines how EC techniques enhance LLMs by optimizing key components such as prompt engineering, hyperparameter tuning, and architecture search, demonstrating how evolutionary methods automate and refine these processes. Secondly, the survey investigates how LLMs improve EC by automating metaheuristic design, tuning evolutionary algorithms, and generating adaptive heuristics, thereby increasing efficiency and scalability. Emerging co-evolutionary frameworks are discussed, showcasing applications across diverse fields while acknowledging challenges like computational costs, interpretability, and algorithmic convergence. The survey concludes by identifying open research questions and advocating for hybrid approaches that combine the strengths of EC and LLMs.",
    "authors": [
      "Dikshit Chauhan",
      "Bapi Dutta",
      "Indu Bala",
      "Niki van Stein",
      "Thomas Bäck",
      "Anupam Yadav"
    ],
    "categories": [
      "cs.NE",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-05-21T16:48:28Z",
    "pdf_url": "https://arxiv.org/pdf/2505.15741v1"
  },
  {
    "arxiv_id": "2505.15524v1",
    "entry_id": "http://arxiv.org/abs/2505.15524v1",
    "title": "Evaluate Bias without Manual Test Sets: A Concept Representation Perspective for LLMs",
    "summary": "Bias in Large Language Models (LLMs) significantly undermines their reliability and fairness. We focus on a common form of bias: when two reference concepts in the model's concept space, such as sentiment polarities (e.g., \"positive\" and \"negative\"), are asymmetrically correlated with a third, target concept, such as a reviewing aspect, the model exhibits unintended bias. For instance, the understanding of \"food\" should not skew toward any particular sentiment. Existing bias evaluation methods assess behavioral differences of LLMs by constructing labeled data for different social groups and measuring model responses across them, a process that requires substantial human effort and captures only a limited set of social concepts. To overcome these limitations, we propose BiasLens, a test-set-free bias analysis framework based on the structure of the model's vector space. BiasLens combines Concept Activation Vectors (CAVs) with Sparse Autoencoders (SAEs) to extract interpretable concept representations, and quantifies bias by measuring the variation in representational similarity between the target concept and each of the reference concepts. Even without labeled data, BiasLens shows strong agreement with traditional bias evaluation metrics (Spearman correlation r > 0.85). Moreover, BiasLens reveals forms of bias that are difficult to detect using existing methods. For example, in simulated clinical scenarios, a patient's insurance status can cause the LLM to produce biased diagnostic assessments. Overall, BiasLens offers a scalable, interpretable, and efficient paradigm for bias discovery, paving the way for improving fairness and transparency in LLMs.",
    "authors": [
      "Lang Gao",
      "Kaiyang Wan",
      "Wei Liu",
      "Chenxi Wang",
      "Zirui Song",
      "Zixiang Xu",
      "Yanbo Wang",
      "Veselin Stoyanov",
      "Xiuying Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-21T13:50:23Z",
    "pdf_url": "https://arxiv.org/pdf/2505.15524v1"
  },
  {
    "arxiv_id": "2505.15116v1",
    "entry_id": "http://arxiv.org/abs/2505.15116v1",
    "title": "Graph Foundation Models: A Comprehensive Survey",
    "summary": "Graph-structured data pervades domains such as social networks, biological systems, knowledge graphs, and recommender systems. While foundation models have transformed natural language processing, vision, and multimodal learning through large-scale pretraining and generalization, extending these capabilities to graphs -- characterized by non-Euclidean structures and complex relational semantics -- poses unique challenges and opens new opportunities. To this end, Graph Foundation Models (GFMs) aim to bring scalable, general-purpose intelligence to structured data, enabling broad transfer across graph-centric tasks and domains. This survey provides a comprehensive overview of GFMs, unifying diverse efforts under a modular framework comprising three key components: backbone architectures, pretraining strategies, and adaptation mechanisms. We categorize GFMs by their generalization scope -- universal, task-specific, and domain-specific -- and review representative methods, key innovations, and theoretical insights within each category. Beyond methodology, we examine theoretical foundations including transferability and emergent capabilities, and highlight key challenges such as structural alignment, heterogeneity, scalability, and evaluation. Positioned at the intersection of graph learning and general-purpose AI, GFMs are poised to become foundational infrastructure for open-ended reasoning over structured data. This survey consolidates current progress and outlines future directions to guide research in this rapidly evolving field. Resources are available at https://github.com/Zehong-Wang/Awesome-Foundation-Models-on-Graphs.",
    "authors": [
      "Zehong Wang",
      "Zheyuan Liu",
      "Tianyi Ma",
      "Jiazheng Li",
      "Zheyuan Zhang",
      "Xingbo Fu",
      "Yiyang Li",
      "Zhengqing Yuan",
      "Wei Song",
      "Yijun Ma",
      "Qingkai Zeng",
      "Xiusi Chen",
      "Jianan Zhao",
      "Jundong Li",
      "Meng Jiang",
      "Pietro Lio",
      "Nitesh Chawla",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2025-05-21T05:08:00Z",
    "pdf_url": "https://arxiv.org/pdf/2505.15116v1"
  },
  {
    "arxiv_id": "2505.15108v2",
    "entry_id": "http://arxiv.org/abs/2505.15108v2",
    "title": "A Risk Ontology for Evaluating AI-Powered Psychotherapy Virtual Agents",
    "summary": "The proliferation of Large Language Models (LLMs) and Intelligent Virtual Agents acting as psychotherapists presents significant opportunities for expanding mental healthcare access. However, their deployment has also been linked to serious adverse outcomes, including user harm and suicide, facilitated by a lack of standardized evaluation methodologies capable of capturing the nuanced risks of therapeutic interaction. Current evaluation techniques lack the sensitivity to detect subtle changes in patient cognition and behavior during therapy sessions that may lead to subsequent decompensation. We introduce a novel risk ontology specifically designed for the systematic evaluation of conversational AI psychotherapists. Developed through an iterative process including review of the psychotherapy risk literature, qualitative interviews with clinical and legal experts, and alignment with established clinical criteria (e.g., DSM-5) and existing assessment tools (e.g., NEQ, UE-ATR), the ontology aims to provide a structured approach to identifying and assessing user/patient harms. We provide a high-level overview of this ontology, detailing its grounding, and discuss potential use cases. We discuss four use cases in detail: monitoring real user interactions, evaluation with simulated patients, benchmarking and comparative analysis, and identifying unexpected outcomes. The proposed ontology offers a foundational step towards establishing safer and more responsible innovation in the domain of AI-driven mental health support.",
    "authors": [
      "Ian Steenstra",
      "Timothy W. Bickmore"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-05-21T05:01:39Z",
    "pdf_url": "https://arxiv.org/pdf/2505.15108v2"
  },
  {
    "arxiv_id": "2505.15101v1",
    "entry_id": "http://arxiv.org/abs/2505.15101v1",
    "title": "Cost-aware LLM-based Online Dataset Annotation",
    "summary": "Recent advances in large language models (LLMs) have enabled automated dataset labeling with minimal human supervision. While majority voting across multiple LLMs can improve label reliability by mitigating individual model biases, it incurs high computational costs due to repeated querying. In this work, we propose a novel online framework, Cost-aware Majority Voting (CaMVo), for efficient and accurate LLM-based dataset annotation. CaMVo adaptively selects a subset of LLMs for each data instance based on contextual embeddings, balancing confidence and cost without requiring pre-training or ground-truth labels. Leveraging a LinUCB-based selection mechanism and a Bayesian estimator over confidence scores, CaMVo estimates a lower bound on labeling accuracy for each LLM and aggregates responses through weighted majority voting. Our empirical evaluation on the MMLU and IMDB Movie Review datasets demonstrates that CaMVo achieves comparable or superior accuracy to full majority voting while significantly reducing labeling costs. This establishes CaMVo as a practical and robust solution for cost-efficient annotation in dynamic labeling environments.",
    "authors": [
      "Eray Can Elumar",
      "Cem Tekin",
      "Osman Yagan"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.IT"
    ],
    "published": "2025-05-21T04:49:44Z",
    "pdf_url": "https://arxiv.org/pdf/2505.15101v1"
  },
  {
    "arxiv_id": "2506.11040v1",
    "entry_id": "http://arxiv.org/abs/2506.11040v1",
    "title": "Large Language models for Time Series Analysis: Techniques, Applications, and Challenges",
    "summary": "Time series analysis is pivotal in domains like financial forecasting and biomedical monitoring, yet traditional methods are constrained by limited nonlinear feature representation and long-term dependency capture. The emergence of Large Language Models (LLMs) offers transformative potential by leveraging their cross-modal knowledge integration and inherent attention mechanisms for time series analysis. However, the development of general-purpose LLMs for time series from scratch is still hindered by data diversity, annotation scarcity, and computational requirements. This paper presents a systematic review of pre-trained LLM-driven time series analysis, focusing on enabling techniques, potential applications, and open challenges. First, it establishes an evolutionary roadmap of AI-driven time series analysis, from the early machine learning era, through the emerging LLM-driven paradigm, to the development of native temporal foundation models. Second, it organizes and systematizes the technical landscape of LLM-driven time series analysis from a workflow perspective, covering LLMs' input, optimization, and lightweight stages. Finally, it critically examines novel real-world applications and highlights key open challenges that can guide future research and innovation. The work not only provides valuable insights into current advances but also outlines promising directions for future development. It serves as a foundational reference for both academic and industrial researchers, paving the way for the development of more efficient, generalizable, and interpretable systems of LLM-driven time series analysis.",
    "authors": [
      "Feifei Shi",
      "Xueyan Yin",
      "Kang Wang",
      "Wanyu Tu",
      "Qifu Sun",
      "Huansheng Ning"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.ET"
    ],
    "published": "2025-05-21T04:45:11Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11040v1"
  },
  {
    "arxiv_id": "2505.15859v1",
    "entry_id": "http://arxiv.org/abs/2505.15859v1",
    "title": "AutoData: A Multi-Agent System for Open Web Data Collection",
    "summary": "The exponential growth of data-driven systems and AI technologies has intensified the demand for high-quality web-sourced datasets. While existing datasets have proven valuable, conventional web data collection approaches face significant limitations in terms of human effort and scalability. Current data-collecting solutions fall into two categories: wrapper-based methods that struggle with adaptability and reproducibility, and large language model (LLM)-based approaches that incur substantial computational and financial costs. To address these challenges, we propose AutoData, a novel multi-agent system for Automated web Data collection, that requires minimal human intervention, i.e., only necessitating a natural language instruction specifying the desired dataset. In addition, AutoData is designed with a robust multi-agent architecture, featuring a novel oriented message hypergraph coordinated by a central task manager, to efficiently organize agents across research and development squads. Besides, we introduce a novel hypergraph cache system to advance the multi-agent collaboration process that enables efficient automated data collection and mitigates the token cost issues prevalent in existing LLM-based systems. Moreover, we introduce Instruct2DS, a new benchmark dataset supporting live data collection from web sources across three domains: academic, finance, and sports. Comprehensive evaluations over Instruct2DS and three existing benchmark datasets demonstrate AutoData's superior performance compared to baseline methods. Case studies on challenging tasks such as picture book collection and paper extraction from surveys further validate its applicability. Our source code and dataset are available at https://github.com/GraphResearcher/AutoData.",
    "authors": [
      "Tianyi Ma",
      "Yiyue Qian",
      "Zheyuan Zhang",
      "Zehong Wang",
      "Xiaoye Qian",
      "Feifan Bai",
      "Yifan Ding",
      "Xuwei Luo",
      "Shinan Zhang",
      "Keerthiram Murugesan",
      "Chuxu Zhang",
      "Yanfang Ye"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-05-21T04:32:35Z",
    "pdf_url": "https://arxiv.org/pdf/2505.15859v1"
  },
  {
    "arxiv_id": "2505.17087v1",
    "entry_id": "http://arxiv.org/abs/2505.17087v1",
    "title": "Informatics for Food Processing",
    "summary": "This chapter explores the evolution, classification, and health implications of food processing, while emphasizing the transformative role of machine learning, artificial intelligence (AI), and data science in advancing food informatics. It begins with a historical overview and a critical review of traditional classification frameworks such as NOVA, Nutri-Score, and SIGA, highlighting their strengths and limitations, particularly the subjectivity and reproducibility challenges that hinder epidemiological research and public policy. To address these issues, the chapter presents novel computational approaches, including FoodProX, a random forest model trained on nutrient composition data to infer processing levels and generate a continuous FPro score. It also explores how large language models like BERT and BioBERT can semantically embed food descriptions and ingredient lists for predictive tasks, even in the presence of missing data. A key contribution of the chapter is a novel case study using the Open Food Facts database, showcasing how multimodal AI models can integrate structured and unstructured data to classify foods at scale, offering a new paradigm for food processing assessment in public health and research.",
    "authors": [
      "Gordana Ispirova",
      "Michael Sebek",
      "Giulia Menichetti"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-05-20T20:44:31Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17087v1"
  },
  {
    "arxiv_id": "2505.14340v1",
    "entry_id": "http://arxiv.org/abs/2505.14340v1",
    "title": "Plane Geometry Problem Solving with Multi-modal Reasoning: A Survey",
    "summary": "Plane geometry problem solving (PGPS) has recently gained significant attention as a benchmark to assess the multi-modal reasoning capabilities of large vision-language models. Despite the growing interest in PGPS, the research community still lacks a comprehensive overview that systematically synthesizes recent work in PGPS. To fill this gap, we present a survey of existing PGPS studies. We first categorize PGPS methods into an encoder-decoder framework and summarize the corresponding output formats used by their encoders and decoders. Subsequently, we classify and analyze these encoders and decoders according to their architectural designs. Finally, we outline major challenges and promising directions for future research. In particular, we discuss the hallucination issues arising during the encoding phase within encoder-decoder architectures, as well as the problem of data leakage in current PGPS benchmarks.",
    "authors": [
      "Seunghyuk Cho",
      "Zhenyue Qin",
      "Yang Liu",
      "Youngbin Choi",
      "Seungbeom Lee",
      "Dongwoo Kim"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-05-20T13:27:17Z",
    "pdf_url": "https://arxiv.org/pdf/2505.14340v1"
  },
  {
    "arxiv_id": "2505.14107v4",
    "entry_id": "http://arxiv.org/abs/2505.14107v4",
    "title": "DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models",
    "summary": "The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3, o1, and DeepSeek-R1, achieve only 51.12%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AI's diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development https://github.com/SPIRAL-MED/DiagnosisArena.",
    "authors": [
      "Yakun Zhu",
      "Zhongzhen Huang",
      "Linjie Mu",
      "Yutong Huang",
      "Wei Nie",
      "Jiaji Liu",
      "Shaoting Zhang",
      "Pengfei Liu",
      "Xiaofan Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-20T09:14:53Z",
    "pdf_url": "https://arxiv.org/pdf/2505.14107v4"
  },
  {
    "arxiv_id": "2505.14727v1",
    "entry_id": "http://arxiv.org/abs/2505.14727v1",
    "title": "The Evolution of Alpha in Finance Harnessing Human Insight and LLM Agents",
    "summary": "The pursuit of alpha returns that exceed market benchmarks has undergone a profound transformation, evolving from intuition-driven investing to autonomous, AI powered systems. This paper introduces a comprehensive five stage taxonomy that traces this progression across manual strategies, statistical models, classical machine learning, deep learning, and agentic architectures powered by large language models (LLMs). Unlike prior surveys focused narrowly on modeling techniques, this review adopts a system level lens, integrating advances in representation learning, multimodal data fusion, and tool augmented LLM agents. The strategic shift from static predictors to contextaware financial agents capable of real time reasoning, scenario simulation, and cross modal decision making is emphasized. Key challenges in interpretability, data fragility, governance, and regulatory compliance areas critical to production deployment are examined. The proposed taxonomy offers a unified framework for evaluating maturity, aligning infrastructure, and guiding the responsible development of next generation alpha systems.",
    "authors": [
      "Mohammad Rubyet Islam"
    ],
    "categories": [
      "cs.LG",
      "q-fin.CP"
    ],
    "published": "2025-05-20T00:51:43Z",
    "pdf_url": "https://arxiv.org/pdf/2505.14727v1"
  },
  {
    "arxiv_id": "2505.13766v1",
    "entry_id": "http://arxiv.org/abs/2505.13766v1",
    "title": "Advancing Software Quality: A Standards-Focused Review of LLM-Based Assurance Techniques",
    "summary": "Software Quality Assurance (SQA) is critical for delivering reliable, secure, and efficient software products. The Software Quality Assurance Process aims to provide assurance that work products and processes comply with predefined provisions and plans. Recent advancements in Large Language Models (LLMs) present new opportunities to enhance existing SQA processes by automating tasks like requirement analysis, code review, test generation, and compliance checks. Simultaneously, established standards such as ISO/IEC 12207, ISO/IEC 25010, ISO/IEC 5055, ISO 9001/ISO/IEC 90003, CMMI, and TMM provide structured frameworks for ensuring robust quality practices. This paper surveys the intersection of LLM-based SQA methods and these recognized standards, highlighting how AI-driven solutions can augment traditional approaches while maintaining compliance and process maturity. We first review the foundational software quality standards and the technical fundamentals of LLMs in software engineering. Next, we explore various LLM-based SQA applications, including requirement validation, defect detection, test generation, and documentation maintenance. We then map these applications to key software quality frameworks, illustrating how LLMs can address specific requirements and metrics within each standard. Empirical case studies and open-source initiatives demonstrate the practical viability of these methods. At the same time, discussions on challenges (e.g., data privacy, model bias, explainability) underscore the need for deliberate governance and auditing. Finally, we propose future directions encompassing adaptive learning, privacy-focused deployments, multimodal analysis, and evolving standards for AI-driven software quality.",
    "authors": [
      "Avinash Patil"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-19T22:49:30Z",
    "pdf_url": "https://arxiv.org/pdf/2505.13766v1"
  },
  {
    "arxiv_id": "2506.06301v1",
    "entry_id": "http://arxiv.org/abs/2506.06301v1",
    "title": "Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review",
    "summary": "Roadway safety and mobility remain critical challenges for modern transportation systems, demanding innovative analytical frameworks capable of addressing complex, dynamic, and heterogeneous environments. While traditional engineering methods have made progress, the complexity and dynamism of real-world traffic necessitate more advanced analytical frameworks. Large Language Models (LLMs), with their unprecedented capabilities in natural language understanding, knowledge integration, and reasoning, represent a promising paradigm shift. This paper comprehensively reviews the application and customization of LLMs for enhancing roadway safety and mobility. A key focus is how LLMs are adapted -- via architectural, training, prompting, and multimodal strategies -- to bridge the \"modality gap\" with transportation's unique spatio-temporal and physical data. The review systematically analyzes diverse LLM applications in mobility (e.g., traffic flow prediction, signal control) and safety (e.g., crash analysis, driver behavior assessment,). Enabling technologies such as V2X integration, domain-specific foundation models, explainability frameworks, and edge computing are also examined. Despite significant potential, challenges persist regarding inherent LLM limitations (hallucinations, reasoning deficits), data governance (privacy, bias), deployment complexities (sim-to-real, latency), and rigorous safety assurance. Promising future research directions are highlighted, including advanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI collaboration, continuous learning, and the development of efficient, verifiable systems. This review provides a structured roadmap of current capabilities, limitations, and opportunities, underscoring LLMs' transformative potential while emphasizing the need for responsible innovation to realize safer, more intelligent transportation systems.",
    "authors": [
      "Muhammad Monjurul Karim",
      "Yan Shi",
      "Shucheng Zhang",
      "Bingzhang Wang",
      "Mehrdad Nasri",
      "Yinhai Wang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-19T21:51:18Z",
    "pdf_url": "https://arxiv.org/pdf/2506.06301v1"
  },
  {
    "arxiv_id": "2505.13355v2",
    "entry_id": "http://arxiv.org/abs/2505.13355v2",
    "title": "Survey: Multi-Armed Bandits Meet Large Language Models",
    "summary": "Bandit algorithms and Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, each addressing distinct yet complementary challenges in decision-making and natural language processing. This survey explores the synergistic potential between these two fields, highlighting how bandit algorithms can enhance the performance of LLMs and how LLMs, in turn, can provide novel insights for improving bandit-based decision-making. We first examine the role of bandit algorithms in optimizing LLM fine-tuning, prompt engineering, and adaptive response generation, focusing on their ability to balance exploration and exploitation in large-scale learning tasks. Subsequently, we explore how LLMs can augment bandit algorithms through advanced contextual understanding, dynamic adaptation, and improved policy selection using natural language reasoning. By providing a comprehensive review of existing research and identifying key challenges and opportunities, this survey aims to bridge the gap between bandit algorithms and LLMs, paving the way for innovative applications and interdisciplinary research in AI.",
    "authors": [
      "Djallel Bouneffouf",
      "Raphael Feraud"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-19T16:57:57Z",
    "pdf_url": "https://arxiv.org/pdf/2505.13355v2"
  },
  {
    "arxiv_id": "2505.12707v1",
    "entry_id": "http://arxiv.org/abs/2505.12707v1",
    "title": "PLAICraft: Large-Scale Time-Aligned Vision-Speech-Action Dataset for Embodied AI",
    "summary": "Advances in deep generative modelling have made it increasingly plausible to train human-level embodied agents. Yet progress has been limited by the absence of large-scale, real-time, multi-modal, and socially interactive datasets that reflect the sensory-motor complexity of natural environments. To address this, we present PLAICraft, a novel data collection platform and dataset capturing multiplayer Minecraft interactions across five time-aligned modalities: video, game output audio, microphone input audio, mouse, and keyboard actions. Each modality is logged with millisecond time precision, enabling the study of synchronous, embodied behaviour in a rich, open-ended world. The dataset comprises over 10,000 hours of gameplay from more than 10,000 global participants.\\footnote{We have done a privacy review for the public release of an initial 200-hour subset of the dataset, with plans to release most of the dataset over time.} Alongside the dataset, we provide an evaluation suite for benchmarking model capabilities in object recognition, spatial awareness, language grounding, and long-term memory. PLAICraft opens a path toward training and evaluating agents that act fluently and purposefully in real time, paving the way for truly embodied artificial intelligence.",
    "authors": [
      "Yingchen He",
      "Christian D. Weilbach",
      "Martyna E. Wojciechowska",
      "Yuxuan Zhang",
      "Frank Wood"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-05-19T05:00:47Z",
    "pdf_url": "https://arxiv.org/pdf/2505.12707v1"
  },
  {
    "arxiv_id": "2505.12567v1",
    "entry_id": "http://arxiv.org/abs/2505.12567v1",
    "title": "A Survey of Attacks on Large Language Models",
    "summary": "Large language models (LLMs) and LLM-based agents have been widely deployed in a wide range of applications in the real world, including healthcare diagnostics, financial analysis, customer support, robotics, and autonomous driving, expanding their powerful capability of understanding, reasoning, and generating natural languages. However, the wide deployment of LLM-based applications exposes critical security and reliability risks, such as the potential for malicious misuse, privacy leakage, and service disruption that weaken user trust and undermine societal safety. This paper provides a systematic overview of the details of adversarial attacks targeting both LLMs and LLM-based agents. These attacks are organized into three phases in LLMs: Training-Phase Attacks, Inference-Phase Attacks, and Availability & Integrity Attacks. For each phase, we analyze the details of representative and recently introduced attack methods along with their corresponding defenses. We hope our survey will provide a good tutorial and a comprehensive understanding of LLM security, especially for attacks on LLMs. We desire to raise attention to the risks inherent in widely deployed LLM-based applications and highlight the urgent need for robust mitigation strategies for evolving threats.",
    "authors": [
      "Wenrui Xu",
      "Keshab K. Parhi"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-18T22:55:16Z",
    "pdf_url": "https://arxiv.org/pdf/2505.12567v1"
  },
  {
    "arxiv_id": "2505.17065v1",
    "entry_id": "http://arxiv.org/abs/2505.17065v1",
    "title": "Decoding Rarity: Large Language Models in the Diagnosis of Rare Diseases",
    "summary": "Recent advances in artificial intelligence, particularly large language models LLMs, have shown promising capabilities in transforming rare disease research. This survey paper explores the integration of LLMs in the analysis of rare diseases, highlighting significant strides and pivotal studies that leverage textual data to uncover insights and patterns critical for diagnosis, treatment, and patient care. While current research predominantly employs textual data, the potential for multimodal data integration combining genetic, imaging, and electronic health records stands as a promising frontier. We review foundational papers that demonstrate the application of LLMs in identifying and extracting relevant medical information, simulating intelligent conversational agents for patient interaction, and enabling the formulation of accurate and timely diagnoses. Furthermore, this paper discusses the challenges and ethical considerations inherent in deploying LLMs, including data privacy, model transparency, and the need for robust, inclusive data sets. As part of this exploration, we present a section on experimentation that utilizes multiple LLMs alongside structured questionnaires, specifically designed for diagnostic purposes in the context of different diseases. We conclude with future perspectives on the evolution of LLMs towards truly multimodal platforms, which would integrate diverse data types to provide a more comprehensive understanding of rare diseases, ultimately fostering better outcomes in clinical settings.",
    "authors": [
      "Valentina Carbonari",
      "Pierangelo Veltri",
      "Pietro Hiram Guzzi"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-05-18T15:42:15Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17065v1"
  },
  {
    "arxiv_id": "2505.12257v1",
    "entry_id": "http://arxiv.org/abs/2505.12257v1",
    "title": "LLM Context Conditioning and PWP Prompting for Multimodal Validation of Chemical Formulas",
    "summary": "Identifying subtle technical errors within complex scientific and technical documents, especially those requiring multimodal interpretation (e.g., formulas in images), presents a significant hurdle for Large Language Models (LLMs) whose inherent error-correction tendencies can mask inaccuracies. This exploratory proof-of-concept (PoC) study investigates structured LLM context conditioning, informed by Persistent Workflow Prompting (PWP) principles, as a methodological strategy to modulate this LLM behavior at inference time. The approach is designed to enhance the reliability of readily available, general-purpose LLMs (specifically Gemini 2.5 Pro and ChatGPT Plus o3) for precise validation tasks, crucially relying only on their standard chat interfaces without API access or model modifications. To explore this methodology, we focused on validating chemical formulas within a single, complex test paper with known textual and image-based errors. Several prompting strategies were evaluated: while basic prompts proved unreliable, an approach adapting PWP structures to rigorously condition the LLM's analytical mindset appeared to improve textual error identification with both models. Notably, this method also guided Gemini 2.5 Pro to repeatedly identify a subtle image-based formula error previously overlooked during manual review, a task where ChatGPT Plus o3 failed in our tests. These preliminary findings highlight specific LLM operational modes that impede detail-oriented validation and suggest that PWP-informed context conditioning offers a promising and highly accessible technique for developing more robust LLM-driven analytical workflows, particularly for tasks requiring meticulous error detection in scientific and technical documents. Extensive validation beyond this limited PoC is necessary to ascertain broader applicability.",
    "authors": [
      "Evgeny Markhasin"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "physics.chem-ph"
    ],
    "published": "2025-05-18T06:33:08Z",
    "pdf_url": "https://arxiv.org/pdf/2505.12257v1"
  },
  {
    "arxiv_id": "2505.12238v1",
    "entry_id": "http://arxiv.org/abs/2505.12238v1",
    "title": "PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs",
    "summary": "The memorization of sensitive and personally identifiable information (PII) by large language models (LLMs) poses growing privacy risks as models scale and are increasingly deployed in real-world applications. Existing efforts to study sensitive and PII data memorization and develop mitigation strategies are hampered by the absence of comprehensive, realistic, and ethically sourced datasets reflecting the diversity of sensitive information found on the web. We introduce PANORAMA - Profile-based Assemblage for Naturalistic Online Representation and Attribute Memorization Analysis, a large-scale synthetic corpus of 384,789 samples derived from 9,674 synthetic profiles designed to closely emulate the distribution, variety, and context of PII and sensitive data as it naturally occurs in online environments. Our data generation pipeline begins with the construction of internally consistent, multi-attribute human profiles using constrained selection to reflect real-world demographics such as education, health attributes, financial status, etc. Using a combination of zero-shot prompting and OpenAI o3-mini, we generate diverse content types - including wiki-style articles, social media posts, forum discussions, online reviews, comments, and marketplace listings - each embedding realistic, contextually appropriate PII and other sensitive information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and measure PII memorization rates - revealing not only consistent increases with repetition but also variation across content types, highlighting PANORAMA's ability to model how memorization risks differ by context. Our dataset and code are publicly available, providing a much-needed resource for privacy risk assessment, model auditing, and the development of privacy-preserving LLMs.",
    "authors": [
      "Sriram Selvam",
      "Anneswa Ghosh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-18T05:27:35Z",
    "pdf_url": "https://arxiv.org/pdf/2505.12238v1"
  },
  {
    "arxiv_id": "2505.13528v1",
    "entry_id": "http://arxiv.org/abs/2505.13528v1",
    "title": "LLM-Based User Simulation for Low-Knowledge Shilling Attacks on Recommender Systems",
    "summary": "Recommender systems (RS) are increasingly vulnerable to shilling attacks, where adversaries inject fake user profiles to manipulate system outputs. Traditional attack strategies often rely on simplistic heuristics, require access to internal RS data, and overlook the manipulation potential of textual reviews. In this work, we introduce Agent4SR, a novel framework that leverages Large Language Model (LLM)-based agents to perform low-knowledge, high-impact shilling attacks through both rating and review generation. Agent4SR simulates realistic user behavior by orchestrating adversarial interactions, selecting items, assigning ratings, and crafting reviews, while maintaining behavioral plausibility. Our design includes targeted profile construction, hybrid memory retrieval, and a review attack strategy that propagates target item features across unrelated reviews to amplify manipulation. Extensive experiments on multiple datasets and RS architectures demonstrate that Agent4SR outperforms existing low-knowledge baselines in both effectiveness and stealth. Our findings reveal a new class of emergent threats posed by LLM-driven agents, underscoring the urgent need for enhanced defenses in modern recommender systems.",
    "authors": [
      "Shengkang Gu",
      "Jiahao Liu",
      "Dongsheng Li",
      "Guangping Zhang",
      "Mingzhe Han",
      "Hansu Gu",
      "Peng Zhang",
      "Ning Gu",
      "Li Shang",
      "Tun Lu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-05-18T04:40:34Z",
    "pdf_url": "https://arxiv.org/pdf/2505.13528v1"
  },
  {
    "arxiv_id": "2505.11861v1",
    "entry_id": "http://arxiv.org/abs/2505.11861v1",
    "title": "Fair-PP: A Synthetic Dataset for Aligning LLM with Personalized Preferences of Social Equity",
    "summary": "Human preference plays a crucial role in the refinement of large language models (LLMs). However, collecting human preference feedback is costly and most existing datasets neglect the correlation between personalization and preferences. To address this issue, we introduce Fair-PP, a synthetic dataset of personalized preferences targeting social equity, derived from real-world social survey data, which includes 28 social groups, 98 equity topics, and 5 personal preference dimensions. Leveraging GPT-4o-mini, we engage in role-playing based on seven representative persona portrayals guided by existing social survey data, yielding a total of 238,623 preference records. Through Fair-PP, we also contribute (i) An automated framework for generating preference data, along with a more fine-grained dataset of personalized preferences; (ii) analysis of the positioning of the existing mainstream LLMs across five major global regions within the personalized preference space; and (iii) a sample reweighting method for personalized preference alignment, enabling alignment with a target persona while maximizing the divergence from other personas. Empirical experiments show our method outperforms the baselines.",
    "authors": [
      "Qi Zhou",
      "Jie Zhang",
      "Dongxia Wang",
      "Qiang Liu",
      "Tianlin Li",
      "Jin Song Dong",
      "Wenhai Wang",
      "Qing Guo"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-17T06:02:00Z",
    "pdf_url": "https://arxiv.org/pdf/2505.11861v1"
  },
  {
    "arxiv_id": "2505.11718v2",
    "entry_id": "http://arxiv.org/abs/2505.11718v2",
    "title": "REMOR: Automated Peer Review Generation with LLM Reasoning and Multi-Objective Reinforcement Learning",
    "summary": "AI-based peer review systems tend to produce shallow and overpraising suggestions compared to human feedback. Here, we evaluate how well a reasoning LLM trained with multi-objective reinforcement learning (REMOR) can overcome these limitations. We start by designing a multi-aspect reward function that aligns with human evaluation of reviews. The aspects are related to the review itself (e.g., criticisms, novelty) and the relationship between the review and the manuscript (i.e., relevance). First, we perform supervised fine-tuning of DeepSeek-R1-Distill-Qwen-7B using LoRA on PeerRT, a new dataset of high-quality top AI conference reviews enriched with reasoning traces. We then apply Group Relative Policy Optimization (GRPO) to train two models: REMOR-H (with the human-aligned reward) and REMOR-U (with a uniform reward). Interestingly, the human-aligned reward penalizes aspects typically associated with strong reviews, leading REMOR-U to produce qualitatively more substantive feedback. Our results show that REMOR-U and REMOR-H achieve more than twice the average rewards of human reviews, non-reasoning state-of-the-art agentic multi-modal AI review systems, and general commercial LLM baselines. We found that while the best AI and human reviews are comparable in quality, REMOR avoids the long tail of low-quality human reviews. We discuss how reasoning is key to achieving these improvements and release the Human-aligned Peer Review Reward (HPRR) function, the Peer Review Reasoning-enriched Traces (PeerRT) dataset, and the REMOR models, which we believe can help spur progress in the area.",
    "authors": [
      "Pawin Taechoyotin",
      "Daniel Acuna"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-16T22:00:49Z",
    "pdf_url": "https://arxiv.org/pdf/2505.11718v2"
  },
  {
    "arxiv_id": "2505.11701v2",
    "entry_id": "http://arxiv.org/abs/2505.11701v2",
    "title": "DMN-Guided Prompting: A Framework for Controlling LLM Behavior",
    "summary": "Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting in our case study. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.",
    "authors": [
      "Shaghayegh Abedi",
      "Amin Jalali"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-16T21:09:36Z",
    "pdf_url": "https://arxiv.org/pdf/2505.11701v2"
  },
  {
    "arxiv_id": "2505.11665v1",
    "entry_id": "http://arxiv.org/abs/2505.11665v1",
    "title": "Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks",
    "summary": "Large language models (LLMs) have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks. However, ensuring their effectiveness across multiple languages presents unique challenges. Multilingual prompt engineering has emerged as a key approach to enhance LLMs' capabilities in diverse linguistic settings without requiring extensive parameter re-training or fine-tuning. With growing interest in multilingual prompt engineering over the past two to three years, researchers have explored various strategies to improve LLMs' performance across languages and NLP tasks. By crafting structured natural language prompts, researchers have successfully extracted knowledge from LLMs across different languages, making these techniques an accessible pathway for a broader audience, including those without deep expertise in machine learning, to harness the capabilities of LLMs. In this paper, we survey and categorize different multilingual prompting techniques based on the NLP tasks they address across a diverse set of datasets that collectively span around 250 languages. We further highlight the LLMs employed, present a taxonomy of approaches and discuss potential state-of-the-art (SoTA) methods for specific multilingual datasets. Additionally, we derive a range of insights across language families and resource levels (high-resource vs. low-resource), including analyses such as the distribution of NLP tasks by language resource type and the frequency of prompting methods across different language families. Our survey reviews 36 research papers covering 39 prompting techniques applied to 30 multilingual NLP tasks, with the majority of these studies published in the last two years.",
    "authors": [
      "Shubham Vatsal",
      "Harsh Dubey",
      "Aditi Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-16T19:59:17Z",
    "pdf_url": "https://arxiv.org/pdf/2505.11665v1"
  },
  {
    "arxiv_id": "2505.11311v1",
    "entry_id": "http://arxiv.org/abs/2505.11311v1",
    "title": "Explaining Strategic Decisions in Multi-Agent Reinforcement Learning for Aerial Combat Tactics",
    "summary": "Artificial intelligence (AI) is reshaping strategic planning, with Multi-Agent Reinforcement Learning (MARL) enabling coordination among autonomous agents in complex scenarios. However, its practical deployment in sensitive military contexts is constrained by the lack of explainability, which is an essential factor for trust, safety, and alignment with human strategies. This work reviews and assesses current advances in explainability methods for MARL with a focus on simulated air combat scenarios. We proceed by adapting various explainability techniques to different aerial combat scenarios to gain explanatory insights about the model behavior. By linking AI-generated tactics with human-understandable reasoning, we emphasize the need for transparency to ensure reliable deployment and meaningful human-machine interaction. By illuminating the crucial importance of explainability in advancing MARL for operational defense, our work supports not only strategic planning but also the training of military personnel with insightful and comprehensible analyses.",
    "authors": [
      "Ardian Selmonaj",
      "Alessandro Antonucci",
      "Adrian Schneider",
      "Michael Rüegsegger",
      "Matthias Sommer"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-16T14:36:30Z",
    "pdf_url": "https://arxiv.org/pdf/2505.11311v1"
  },
  {
    "arxiv_id": "2505.11010v2",
    "entry_id": "http://arxiv.org/abs/2505.11010v2",
    "title": "ReviewInstruct: A Review-Driven Multi-Turn Conversations Generation Method for Large Language Models",
    "summary": "The effectiveness of large language models (LLMs) in conversational AI is hindered by their reliance on single-turn supervised fine-tuning (SFT) data, which limits contextual coherence in multi-turn dialogues. Existing methods for generating multi-turn dialogue data struggle to ensure both diversity and quality in instructions. To address this, we propose Review-Instruct, a novel framework that synthesizes multi-turn conversations through an iterative \"Ask-Respond-Review\" process involving three agent roles: a Candidate, multiple Reviewers, and a Chairman. The framework iteratively refines instructions by incorporating Reviewer feedback, enhancing dialogue diversity and difficulty. We construct a multi-turn dataset using the Alpaca dataset and fine-tune the LLaMA2-13B model. Evaluations on MT-Bench, MMLU-Pro, and Auto-Arena demonstrate significant improvements, achieving absolute gains of 2.9\\% on MMLU-Pro and 2\\% on MT-Bench compared to prior state-of-the-art models based on LLaMA2-13B. Ablation studies confirm the critical role of the Review stage and the use of multiple Reviewers in boosting instruction diversity and difficulty. Our work highlights the potential of review-driven, multi-agent frameworks for generating high-quality conversational data at scale.",
    "authors": [
      "Jiangxu Wu",
      "Cong Wang",
      "TianHuang Su",
      "Jun Yang",
      "Haozhi Lin",
      "Chao Zhang",
      "Ming Peng",
      "Kai Shi",
      "SongPan Yang",
      "BinQing Pan",
      "ZiXian Li",
      "Ni Yang",
      "ZhenYu Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-16T08:59:07Z",
    "pdf_url": "https://arxiv.org/pdf/2505.11010v2"
  },
  {
    "arxiv_id": "2505.10961v1",
    "entry_id": "http://arxiv.org/abs/2505.10961v1",
    "title": "Let the Trial Begin: A Mock-Court Approach to Vulnerability Detection using LLM-Based Agents",
    "summary": "Detecting vulnerabilities in source code remains a critical yet challenging task, especially when benign and vulnerable functions share significant similarities. In this work, we introduce VulTrial, a courtroom-inspired multi-agent framework designed to enhance automated vulnerability detection. It employs four role-specific agents, which are security researcher, code author, moderator, and review board. Through extensive experiments using GPT-3.5 and GPT-4o we demonstrate that Vultrial outperforms single-agent and multi-agent baselines. Using GPT-4o, VulTrial improves the performance by 102.39% and 84.17% over its respective baseline. Additionally, we show that role-specific instruction tuning in multi-agent with small data (50 pair samples) improves the performance of VulTrial further by 139.89% and 118.30%. Furthermore, we analyze the impact of increasing the number of agent interactions on VulTrial's overall performance. While multi-agent setups inherently incur higher costs due to increased token usage, our findings reveal that applying VulTrial to a cost-effective model like GPT-3.5 can improve its performance by 69.89% compared to GPT-4o in a single-agent setting, at a lower overall cost.",
    "authors": [
      "Ratnadira Widyasari",
      "Martin Weyssow",
      "Ivana Clairine Irsan",
      "Han Wei Ang",
      "Frank Liauw",
      "Eng Lieh Ouh",
      "Lwin Khin Shar",
      "Hong Jin Kang",
      "David Lo"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-05-16T07:54:10Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10961v1"
  },
  {
    "arxiv_id": "2505.10924v3",
    "entry_id": "http://arxiv.org/abs/2505.10924v3",
    "title": "A Survey on the Safety and Security Threats of Computer-Using Agents: JARVIS or Ultron?",
    "summary": "Recently, AI-driven interactions with computing devices have advanced from basic prototype tools to sophisticated, LLM-based systems that emulate human-like operations in graphical user interfaces. We are now witnessing the emergence of \\emph{Computer-Using Agents} (CUAs), capable of autonomously performing tasks such as navigating desktop applications, web pages, and mobile apps. However, as these agents grow in capability, they also introduce novel safety and security risks. Vulnerabilities in LLM-driven reasoning, with the added complexity of integrating multiple software components and multimodal inputs, further complicate the security landscape. In this paper, we present a systematization of knowledge on the safety and security threats of CUAs. We conduct a comprehensive literature review and distill our findings along four research objectives: \\textit{\\textbf{(i)}} define the CUA that suits safety analysis; \\textit{\\textbf{(ii)} } categorize current safety threats among CUAs; \\textit{\\textbf{(iii)}} propose a comprehensive taxonomy of existing defensive strategies; \\textit{\\textbf{(iv)}} summarize prevailing benchmarks, datasets, and evaluation metrics used to assess the safety and performance of CUAs. Building on these insights, our work provides future researchers with a structured foundation for exploring unexplored vulnerabilities and offers practitioners actionable guidance in designing and deploying secure Computer-Using Agents.",
    "authors": [
      "Ada Chen",
      "Yongjiang Wu",
      "Junyuan Zhang",
      "Jingyu Xiao",
      "Shu Yang",
      "Jen-tse Huang",
      "Kun Wang",
      "Wenxuan Wang",
      "Shuai Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CR",
      "cs.CV",
      "cs.SE"
    ],
    "published": "2025-05-16T06:56:42Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10924v3"
  },
  {
    "arxiv_id": "2505.10705v1",
    "entry_id": "http://arxiv.org/abs/2505.10705v1",
    "title": "Embodied AI in Machine Learning -- is it Really Embodied?",
    "summary": "Embodied Artificial Intelligence (Embodied AI) is gaining momentum in the machine learning communities with the goal of leveraging current progress in AI (deep learning, transformers, large language and visual-language models) to empower robots. In this chapter we put this work in the context of \"Good Old-Fashioned Artificial Intelligence\" (GOFAI) (Haugeland, 1989) and the behavior-based or embodied alternatives (R. A. Brooks 1991; Pfeifer and Scheier 2001). We claim that the AI-powered robots are only weakly embodied and inherit some of the problems of GOFAI. Moreover, we review and critically discuss the possibility of cross-embodiment learning (Padalkar et al. 2024). We identify fundamental roadblocks and propose directions on how to make progress.",
    "authors": [
      "Matej Hoffmann",
      "Shubhan Parag Patni"
    ],
    "categories": [
      "cs.AI",
      "cs.NE",
      "cs.RO"
    ],
    "published": "2025-05-15T20:52:49Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10705v1"
  },
  {
    "arxiv_id": "2505.17048v2",
    "entry_id": "http://arxiv.org/abs/2505.17048v2",
    "title": "Words That Unite The World: A Unified Framework for Deciphering Central Bank Communications Globally",
    "summary": "Central banks around the world play a crucial role in maintaining economic stability. Deciphering policy implications in their communications is essential, especially as misinterpretations can disproportionately impact vulnerable populations. To address this, we introduce the World Central Banks (WCB) dataset, the most comprehensive monetary policy corpus to date, comprising over 380k sentences from 25 central banks across diverse geographic regions, spanning 28 years of historical data. After uniformly sampling 1k sentences per bank (25k total) across all available years, we annotate and review each sentence using dual annotators, disagreement resolutions, and secondary expert reviews. We define three tasks: Stance Detection, Temporal Classification, and Uncertainty Estimation, with each sentence annotated for all three. We benchmark seven Pretrained Language Models (PLMs) and nine Large Language Models (LLMs) (Zero-Shot, Few-Shot, and with annotation guide) on these tasks, running 15,075 benchmarking experiments. We find that a model trained on aggregated data across banks significantly surpasses a model trained on an individual bank's data, confirming the principle \"the whole is greater than the sum of its parts.\" Additionally, rigorous human evaluations, error analyses, and predictive tasks validate our framework's economic utility. Our artifacts are accessible through the HuggingFace and GitHub under the CC-BY-NC-SA 4.0 license.",
    "authors": [
      "Agam Shah",
      "Siddhant Sukhani",
      "Huzaifa Pardawala",
      "Saketh Budideti",
      "Riya Bhadani",
      "Rudra Gopal",
      "Siddhartha Somani",
      "Rutwik Routu",
      "Michael Galarnyk",
      "Soungmin Lee",
      "Arnav Hiray",
      "Akshar Ravichandran",
      "Eric Kim",
      "Pranav Aluru",
      "Joshua Zhang",
      "Sebastian Jaskowski",
      "Veer Guda",
      "Meghaj Tarte",
      "Liqin Ye",
      "Spencer Gosden",
      "Rachel Yuh",
      "Sloka Chava",
      "Sahasra Chava",
      "Dylan Patrick Kelly",
      "Aiden Chiang",
      "Harsit Mittal",
      "Sudheer Chava"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "q-fin.CP",
      "q-fin.GN"
    ],
    "published": "2025-05-15T19:49:20Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17048v2"
  },
  {
    "arxiv_id": "2505.10468v5",
    "entry_id": "http://arxiv.org/abs/2505.10468v5",
    "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and Challenges",
    "summary": "This review critically distinguishes between AI Agents and Agentic AI, offering a structured, conceptual taxonomy, application mapping, and analysis of opportunities and challenges to clarify their divergent design philosophies and capabilities. We begin by outlining the search strategy and foundational definitions, characterizing AI Agents as modular systems driven and enabled by LLMs and LIMs for task-specific automation. Generative AI is positioned as a precursor providing the foundation, with AI agents advancing through tool integration, prompt engineering, and reasoning enhancements. We then characterize Agentic AI systems, which, in contrast to AI Agents, represent a paradigm shift marked by multi-agent collaboration, dynamic task decomposition, persistent memory, and coordinated autonomy. Through a chronological evaluation of architectural evolution, operational mechanisms, interaction styles, and autonomy levels, we present a comparative analysis across both AI agents and agentic AI paradigms. Application domains enabled by AI Agents such as customer support, scheduling, and data summarization are then contrasted with Agentic AI deployments in research automation, robotic coordination, and medical decision support. We further examine unique challenges in each paradigm including hallucination, brittleness, emergent behavior, and coordination failure, and propose targeted solutions such as ReAct loops, retrieval-augmented generation (RAG), automation coordination layers, and causal modeling. This work aims to provide a roadmap for developing robust, scalable, and explainable AI-driven systems.",
    "authors": [
      "Ranjan Sapkota",
      "Konstantinos I. Roumeliotis",
      "Manoj Karkee"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-15T16:21:33Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10468v5"
  },
  {
    "arxiv_id": "2505.10603v2",
    "entry_id": "http://arxiv.org/abs/2505.10603v2",
    "title": "Toward a Public and Secure Generative AI: A Comparative Analysis of Open and Closed LLMs",
    "summary": "Generative artificial intelligence (Gen AI) systems represent a critical technology with far-reaching implications across multiple domains of society. However, their deployment entails a range of risks and challenges that require careful evaluation. To date, there has been a lack of comprehensive, interdisciplinary studies offering a systematic comparison between open-source and proprietary (closed) generative AI systems, particularly regarding their respective advantages and drawbacks. This study aims to: i) critically evaluate and compare the characteristics, opportunities, and challenges of open and closed generative AI models; and ii) propose foundational elements for the development of an Open, Public, and Safe Gen AI framework. As a methodology, we adopted a combined approach that integrates three methods: literature review, critical analysis, and comparative analysis. The proposed framework outlines key dimensions, openness, public governance, and security, as essential pillars for shaping the future of trustworthy and inclusive Gen AI. Our findings reveal that open models offer greater transparency, auditability, and flexibility, enabling independent scrutiny and bias mitigation. In contrast, closed systems often provide better technical support and ease of implementation, but at the cost of unequal access, accountability, and ethical oversight. The research also highlights the importance of multi-stakeholder governance, environmental sustainability, and regulatory frameworks in ensuring responsible development.",
    "authors": [
      "Jorge Machado"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-05-15T15:21:09Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10603v2"
  },
  {
    "arxiv_id": "2505.10321v1",
    "entry_id": "http://arxiv.org/abs/2505.10321v1",
    "title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM Agents",
    "summary": "A recent area of increasing research is the use of Large Language Models (LLMs) in penetration testing, which promises to reduce costs and thus allow for higher frequency. We conduct a review of related work, identifying best practices and common evaluation issues. We then present AutoPentest, an application for performing black-box penetration tests with a high degree of autonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent framework LangChain. It can perform complex multi-step tasks, augmented by external tools and knowledge bases. We conduct a study on three capture-the-flag style Hack The Box (HTB) machines, comparing our implementation AutoPentest with the baseline approach of manually using the ChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the subtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT. We measure a total cost of \\$96.20 US when using AutoPentest across all experiments, while a one-month subscription to ChatGPT Plus costs \\$20. The results show that further implementation efforts and the use of more powerful LLMs released in the future are likely to make this a viable part of vulnerability management.",
    "authors": [
      "Julius Henke"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-15T14:06:00Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10321v1"
  },
  {
    "arxiv_id": "2505.10309v2",
    "entry_id": "http://arxiv.org/abs/2505.10309v2",
    "title": "Empirically evaluating commonsense intelligence in large language models with large-scale human judgments",
    "summary": "Commonsense intelligence in machines is often assessed by static benchmarks that compare a model's output against human-prescribed correct labels. An important, albeit implicit, assumption of these labels is that they accurately capture what any human would think, effectively treating human common sense as homogeneous. However, recent empirical work has shown that humans vary enormously in what they consider commonsensical; thus what appears self-evident to one benchmark designer may not be so to another. Here, we propose a method for evaluating common sense in artificial intelligence (AI), specifically in large language models (LLMs), that incorporates empirically observed heterogeneity among humans by measuring the correspondence between a model's judgment and that of a human population. We first find that, when treated as independent survey respondents, most LLMs remain below the human median in their individual commonsense competence. Second, when used as simulators of a hypothetical population, LLMs correlate with real humans only modestly in the extent to which they agree on the same set of statements. In both cases, smaller, open-weight models are surprisingly more competitive than larger, proprietary frontier models. Our evaluation framework, which ties commonsense intelligence to its cultural basis, contributes to the growing call for adapting AI models to human collectivities that possess different, often incompatible, social stocks of knowledge.",
    "authors": [
      "Tuan Dung Nguyen",
      "Duncan J. Watts",
      "Mark E. Whiting"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.SI"
    ],
    "published": "2025-05-15T13:55:27Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10309v2"
  },
  {
    "arxiv_id": "2505.10300v1",
    "entry_id": "http://arxiv.org/abs/2505.10300v1",
    "title": "AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial Responsible AI Practices during Early Design Stages",
    "summary": "Responsible AI (RAI) efforts increasingly emphasize the importance of addressing potential harms early in the AI development lifecycle through social-technical lenses. However, in cross-functional industry teams, this work is often stalled by a persistent knowledge handoff challenge: the difficulty of transferring high-level, early-stage technical design rationales from technical experts to non-technical or user-facing roles for ethical evaluation and harm identification. Through literature review and a co-design study with 8 practitioners, we unpack how this challenge manifests -- technical design choices are rarely handed off in ways that support meaningful engagement by non-technical roles; collaborative workflows lack shared, visual structures to support mutual understanding; and non-technical practitioners are left without scaffolds for systematic harm evaluation. Existing tools like JIRA or Google Docs, while useful for product tracking, are ill-suited for supporting joint harm identification across roles, often requiring significant extra effort to align understanding. To address this, we developed AI LEGO, a web-based prototype that supports cross-functional AI practitioners in effectively facilitating knowledge handoff and identifying harmful design choices in the early design stages. Technical roles use interactive blocks to draft development plans, while non-technical roles engage with those blocks through stage-specific checklists and LLM-driven persona simulations to surface potential harms. In a study with 18 cross-functional practitioners, AI LEGO increased the volume and likelihood of harms identified compared to baseline worksheets. Participants found that its modular structure and persona prompts made harm identification more accessible, fostering clearer and more collaborative RAI practices in early design.",
    "authors": [
      "Muzhe Wu",
      "Yanzhi Zhao",
      "Shuyi Han",
      "Michael Xieyang Liu",
      "Hong Shen"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-05-15T13:49:02Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10300v1"
  },
  {
    "arxiv_id": "2505.10597v2",
    "entry_id": "http://arxiv.org/abs/2505.10597v2",
    "title": "Two Minds Better Than One: Collaborative Reward Modeling for LLM Alignment",
    "summary": "Reward models (RMs) play a pivotal role in aligning large language models (LLMs) with human values. However, noisy preferences in human feedback can lead to reward misgeneralization - a phenomenon where reward models learn spurious correlations or overfit to noisy preferences, which poses important challenges to the generalization of RMs. This paper systematically analyzes the characteristics of preference pairs and aims to identify how noisy preferences differ from human-aligned preferences in reward modeling. Our analysis reveals that noisy preferences are difficult for RMs to fit, as they cause sharp training fluctuations and irregular gradient updates. These distinctive dynamics suggest the feasibility of identifying and excluding such noisy preferences. Empirical studies demonstrate that policy LLM optimized with a reward model trained on the full preference dataset, which includes substantial noise, performs worse than the one trained on a subset of exclusively high quality preferences. To address this challenge, we propose an online Collaborative Reward Modeling (CRM) framework to achieve robust preference learning through peer review and curriculum learning. In particular, CRM maintains two RMs that collaboratively filter potential noisy preferences by peer-reviewing each other's data selections. Curriculum learning synchronizes the capabilities of two models, mitigating excessive disparities to promote the utility of peer review. Extensive experiments demonstrate that CRM significantly enhances RM generalization, with up to 9.94 points improvement on RewardBench under an extreme 40\\% noise. Moreover, CRM can seamlessly extend to implicit-reward alignment methods, offering a robust and versatile alignment strategy.",
    "authors": [
      "Jiazheng Zhang",
      "Wenqing Jing",
      "Zizhuo Zhang",
      "Zhiheng Xi",
      "Shihan Dou",
      "Rongxiang Weng",
      "Jiahuan Li",
      "Jingang Wang",
      "Mingxu Chai",
      "Shibo Hong",
      "Tao Gui",
      "Qi Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-15T10:58:20Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10597v2"
  },
  {
    "arxiv_id": "2505.10093v1",
    "entry_id": "http://arxiv.org/abs/2505.10093v1",
    "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based China Studies Using Generative AI",
    "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary research field shaped by the unique geopolitical position and long standing academic engagement with Mainland China. This study responds to the growing need to systematically revisit and reorganize decades of Taiwan based CS scholarship by proposing an AI assisted approach that transforms unstructured academic texts into structured, interactive knowledge representations. We apply generative AI (GAI) techniques and large language models (LLMs) to extract and standardize entity relation triples from 1,367 peer reviewed CS articles published between 1996 and 2019. These triples are then visualized through a lightweight D3.js based system, forming the foundation of a domain specific knowledge graph and vector database for the field. This infrastructure allows users to explore conceptual nodes and semantic relationships across the corpus, revealing previously uncharted intellectual trajectories, thematic clusters, and research gaps. By decomposing textual content into graph structured knowledge units, our system enables a paradigm shift from linear text consumption to network based knowledge navigation. In doing so, it enhances scholarly access to CS literature while offering a scalable, data driven alternative to traditional ontology construction. This work not only demonstrates how generative AI can augment area studies and digital humanities but also highlights its potential to support a reimagined scholarly infrastructure for regional knowledge systems.",
    "authors": [
      "Hsuan-Lei Shao"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-15T08:51:53Z",
    "pdf_url": "https://arxiv.org/pdf/2505.10093v1"
  },
  {
    "arxiv_id": "2505.09724v2",
    "entry_id": "http://arxiv.org/abs/2505.09724v2",
    "title": "An AI-Powered Research Assistant in the Lab: A Practical Guide for Text Analysis Through Iterative Collaboration with LLMs",
    "summary": "Analyzing texts such as open-ended responses, headlines, or social media posts is a time- and labor-intensive process highly susceptible to bias. LLMs are promising tools for text analysis, using either a predefined (top-down) or a data-driven (bottom-up) taxonomy, without sacrificing quality. Here we present a step-by-step tutorial to efficiently develop, test, and apply taxonomies for analyzing unstructured data through an iterative and collaborative process between researchers and LLMs. Using personal goals provided by participants as an example, we demonstrate how to write prompts to review datasets and generate a taxonomy of life domains, evaluate and refine the taxonomy through prompt and direct modifications, test the taxonomy and assess intercoder agreements, and apply the taxonomy to categorize an entire dataset with high intercoder reliability. We discuss the possibilities and limitations of using LLMs for text analysis.",
    "authors": [
      "Gino Carmona-Díaz",
      "William Jiménez-Leal",
      "María Alejandra Grisales",
      "Chandra Sripada",
      "Santiago Amaya",
      "Michael Inzlicht",
      "Juan Pablo Bermúdez"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-05-14T18:32:18Z",
    "pdf_url": "https://arxiv.org/pdf/2505.09724v2"
  },
  {
    "arxiv_id": "2505.09024v1",
    "entry_id": "http://arxiv.org/abs/2505.09024v1",
    "title": "Automated Meta Prompt Engineering for Alignment with the Theory of Mind",
    "summary": "We introduce a method of meta-prompting that jointly produces fluent text for complex tasks while optimizing the similarity of neural states between a human's mental expectation and a Large Language Model's (LLM) neural processing. A technique of agentic reinforcement learning is applied, in which an LLM as a Judge (LLMaaJ) teaches another LLM, through in-context learning, how to produce content by interpreting the intended and unintended generated text traits. To measure human mental beliefs around content production, users modify long form AI-generated text articles before publication at the US Open 2024 tennis Grand Slam. Now, an LLMaaJ can solve the Theory of Mind (ToM) alignment problem by anticipating and including human edits within the creation of text from an LLM. Throughout experimentation and by interpreting the results of a live production system, the expectations of human content reviewers had 100% of alignment with AI 53.8% of the time with an average iteration count of 4.38. The geometric interpretation of content traits such as factualness, novelty, repetitiveness, and relevancy over a Hilbert vector space combines spatial volume (all trait importance) with vertices alignment (individual trait relevance) enabled the LLMaaJ to optimize on Human ToM. This resulted in an increase in content quality by extending the coverage of tennis action. Our work that was deployed at the US Open 2024 has been used across other live events within sports and entertainment.",
    "authors": [
      "Aaron Baughman",
      "Rahul Agarwal",
      "Eduardo Morales",
      "Gozde Akay"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-05-13T23:42:36Z",
    "pdf_url": "https://arxiv.org/pdf/2505.09024v1"
  },
  {
    "arxiv_id": "2505.08854v1",
    "entry_id": "http://arxiv.org/abs/2505.08854v1",
    "title": "Generative AI for Autonomous Driving: Frontiers and Opportunities",
    "summary": "Generative Artificial Intelligence (GenAI) constitutes a transformative technological wave that reconfigures industries through its unparalleled capabilities for content creation, reasoning, planning, and multimodal understanding. This revolutionary force offers the most promising path yet toward solving one of engineering's grandest challenges: achieving reliable, fully autonomous driving, particularly the pursuit of Level 5 autonomy. This survey delivers a comprehensive and critical synthesis of the emerging role of GenAI across the autonomous driving stack. We begin by distilling the principles and trade-offs of modern generative modeling, encompassing VAEs, GANs, Diffusion Models, and Large Language Models (LLMs). We then map their frontier applications in image, LiDAR, trajectory, occupancy, video generation as well as LLM-guided reasoning and decision making. We categorize practical applications, such as synthetic data workflows, end-to-end driving strategies, high-fidelity digital twin systems, smart transportation networks, and cross-domain transfer to embodied AI. We identify key obstacles and possibilities such as comprehensive generalization across rare cases, evaluation and safety checks, budget-limited implementation, regulatory compliance, ethical concerns, and environmental effects, while proposing research plans across theoretical assurances, trust metrics, transport integration, and socio-technical influence. By unifying these threads, the survey provides a forward-looking reference for researchers, engineers, and policymakers navigating the convergence of generative AI and advanced autonomous mobility. An actively maintained repository of cited works is available at https://github.com/taco-group/GenAI4AD.",
    "authors": [
      "Yuping Wang",
      "Shuo Xing",
      "Cui Can",
      "Renjie Li",
      "Hongyuan Hua",
      "Kexin Tian",
      "Zhaobin Mo",
      "Xiangbo Gao",
      "Keshu Wu",
      "Sulong Zhou",
      "Hengxu You",
      "Juntong Peng",
      "Junge Zhang",
      "Zehao Wang",
      "Rui Song",
      "Mingxuan Yan",
      "Walter Zimmer",
      "Xingcheng Zhou",
      "Peiran Li",
      "Zhaohan Lu",
      "Chia-Ju Chen",
      "Yue Huang",
      "Ryan A. Rossi",
      "Lichao Sun",
      "Hongkai Yu",
      "Zhiwen Fan",
      "Frank Hao Yang",
      "Yuhao Kang",
      "Ross Greer",
      "Chenxi Liu",
      "Eun Hak Lee",
      "Xuan Di",
      "Xinyue Ye",
      "Liu Ren",
      "Alois Knoll",
      "Xiaopeng Li",
      "Shuiwang Ji",
      "Masayoshi Tomizuka",
      "Marco Pavone",
      "Tianbao Yang",
      "Jing Du",
      "Ming-Hsuan Yang",
      "Hua Wei",
      "Ziran Wang",
      "Yang Zhou",
      "Jiachen Li",
      "Zhengzhong Tu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-05-13T17:59:20Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08854v1"
  },
  {
    "arxiv_id": "2505.08728v2",
    "entry_id": "http://arxiv.org/abs/2505.08728v2",
    "title": "Securing RAG: A Risk Assessment and Mitigation Framework",
    "summary": "Retrieval Augmented Generation (RAG) has emerged as the de facto industry standard for user-facing NLP applications, offering the ability to integrate data without re-training or fine-tuning Large Language Models (LLMs). This capability enhances the quality and accuracy of responses but also introduces novel security and privacy challenges, particularly when sensitive data is integrated. With the rapid adoption of RAG, securing data and services has become a critical priority. This paper first reviews the vulnerabilities of RAG pipelines, and outlines the attack surface from data pre-processing and data storage management to integration with LLMs. The identified risks are then paired with corresponding mitigations in a structured overview. In a second step, the paper develops a framework that combines RAG-specific security considerations, with existing general security guidelines, industry standards, and best practices. The proposed framework aims to guide the implementation of robust, compliant, secure, and trustworthy RAG systems.",
    "authors": [
      "Lukas Ammann",
      "Sara Ott",
      "Christoph R. Landolt",
      "Marco P. Lehmann"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-05-13T16:39:00Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08728v2"
  },
  {
    "arxiv_id": "2505.08620v1",
    "entry_id": "http://arxiv.org/abs/2505.08620v1",
    "title": "Resource-Efficient Language Models: Quantization for Fast and Accessible Inference",
    "summary": "Large language models have significantly advanced natural language processing, yet their heavy resource demands pose severe challenges regarding hardware accessibility and energy consumption. This paper presents a focused and high-level review of post-training quantization (PTQ) techniques designed to optimize the inference efficiency of LLMs by the end-user, including details on various quantization schemes, granularities, and trade-offs. The aim is to provide a balanced overview between the theory and applications of post-training quantization.",
    "authors": [
      "Tollef Emil Jørgensen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-13T14:39:33Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08620v1"
  },
  {
    "arxiv_id": "2505.09651v1",
    "entry_id": "http://arxiv.org/abs/2505.09651v1",
    "title": "Unlocking Location Intelligence: A Survey from Deep Learning to The LLM Era",
    "summary": "Location Intelligence (LI), the science of transforming location-centric geospatial data into actionable knowledge, has become a cornerstone of modern spatial decision-making. The rapid evolution of Geospatial Representation Learning is fundamentally reshaping LI development through two successive technological revolutions: the deep learning breakthrough and the emerging large language model (LLM) paradigm. While deep neural networks (DNNs) have demonstrated remarkable success in automated feature extraction from structured geospatial data (e.g., satellite imagery, GPS trajectories), the recent integration of LLMs introduces transformative capabilities for cross-modal geospatial reasoning and unstructured geo-textual data processing. This survey presents a comprehensive review of geospatial representation learning across both technological eras, organizing them into a structured taxonomy based on the complete pipeline comprising: (1) data perspective, (2) methodological perspective and (3) application perspective. We also highlight current advancements, discuss existing limitations, and propose potential future research directions in the LLM era. This work offers a thorough exploration of the field and providing a roadmap for further innovation in LI. The summary of the up-to-date paper list can be found in https://github.com/CityMind-Lab/Awesome-Location-Intelligence and will undergo continuous updates.",
    "authors": [
      "Xixuan Hao",
      "Yutian Jiang",
      "Xingchen Zou",
      "Jiabo Liu",
      "Yifang Yin",
      "Yuxuan Liang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-13T12:16:26Z",
    "pdf_url": "https://arxiv.org/pdf/2505.09651v1"
  },
  {
    "arxiv_id": "2505.08464v1",
    "entry_id": "http://arxiv.org/abs/2505.08464v1",
    "title": "Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions",
    "summary": "Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.",
    "authors": [
      "Lata Pangtey",
      "Anukriti Bhatnagar",
      "Shubhi Bansal",
      "Shahid Shafi Dar",
      "Nagendra Kumar"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "cs.SI"
    ],
    "published": "2025-05-13T11:47:49Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08464v1"
  },
  {
    "arxiv_id": "2505.08253v1",
    "entry_id": "http://arxiv.org/abs/2505.08253v1",
    "title": "Evaluating LLM Metrics Through Real-World Capabilities",
    "summary": "As generative AI becomes increasingly embedded in everyday workflows, it is important to evaluate its performance in ways that reflect real-world usage rather than abstract notions of intelligence. Unlike many existing benchmarks that assess general intelligence, our approach focuses on real-world utility, evaluating how well models support users in everyday tasks. While current benchmarks emphasize code generation or factual recall, users rely on AI for a much broader range of activities-from writing assistance and summarization to citation formatting and stylistic feedback. In this paper, we analyze large-scale survey data and usage logs to identify six core capabilities that represent how people commonly use Large Language Models (LLMs): Summarization, Technical Assistance, Reviewing Work, Data Structuring, Generation, and Information Retrieval. We then assess the extent to which existing benchmarks cover these capabilities, revealing significant gaps in coverage, efficiency measurement, and interpretability. Drawing on this analysis, we use human-centered criteria to identify gaps in how well current benchmarks reflect common usage that is grounded in five practical criteria: coherence, accuracy, clarity, relevance, and efficiency. For four of the six capabilities, we identify the benchmarks that best align with real-world tasks and use them to compare leading models. We find that Google Gemini outperforms other models-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude, DeepSeek, and Qwen from Alibaba-on these utility-focused metrics.",
    "authors": [
      "Justin K Miller",
      "Wenjia Tang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-13T06:02:37Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08253v1"
  },
  {
    "arxiv_id": "2505.08245v2",
    "entry_id": "http://arxiv.org/abs/2505.08245v2",
    "title": "Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement",
    "summary": "The advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. This progress presents novel challenges, such as measuring human-like psychological constructs, moving beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This review paper introduces and synthesizes the emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. The reviewed literature systematically shapes benchmarking principles, broadens evaluation scopes, refines methodologies, validates results, and advances LLM capabilities. Diverse perspectives are integrated to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, the review provides actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.",
    "authors": [
      "Haoran Ye",
      "Jing Jin",
      "Yuhang Xie",
      "Xin Zhang",
      "Guojie Song"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-05-13T05:47:51Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08245v2"
  },
  {
    "arxiv_id": "2505.08830v1",
    "entry_id": "http://arxiv.org/abs/2505.08830v1",
    "title": "Federated Large Language Models: Feasibility, Robustness, Security and Future Directions",
    "summary": "The integration of Large Language Models (LLMs) and Federated Learning (FL) presents a promising solution for joint training on distributed data while preserving privacy and addressing data silo issues. However, this emerging field, known as Federated Large Language Models (FLLM), faces significant challenges, including communication and computation overheads, heterogeneity, privacy and security concerns. Current research has primarily focused on the feasibility of FLLM, but future trends are expected to emphasize enhancing system robustness and security. This paper provides a comprehensive review of the latest advancements in FLLM, examining challenges from four critical perspectives: feasibility, robustness, security, and future directions. We present an exhaustive survey of existing studies on FLLM feasibility, introduce methods to enhance robustness in the face of resource, data, and task heterogeneity, and analyze novel risks associated with this integration, including privacy threats and security challenges. We also review the latest developments in defense mechanisms and explore promising future research directions, such as few-shot learning, machine unlearning, and IP protection. This survey highlights the pressing need for further research to enhance system robustness and security while addressing the unique challenges posed by the integration of FL and LLM.",
    "authors": [
      "Wenhao Jiang",
      "Yuchuan Luo",
      "Guilin Deng",
      "Silong Chen",
      "Xu Yang",
      "Shihong Wu",
      "Xinwen Gao",
      "Lin Liu",
      "Shaojing Fu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-13T03:23:54Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08830v1"
  },
  {
    "arxiv_id": "2505.08163v1",
    "entry_id": "http://arxiv.org/abs/2505.08163v1",
    "title": "Decoding Neighborhood Environments with Large Language Models",
    "summary": "Neighborhood environments include physical and environmental conditions such as housing quality, roads, and sidewalks, which significantly influence human health and well-being. Traditional methods for assessing these environments, including field surveys and geographic information systems (GIS), are resource-intensive and challenging to evaluate neighborhood environments at scale. Although machine learning offers potential for automated analysis, the laborious process of labeling training data and the lack of accessible models hinder scalability. This study explores the feasibility of large language models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood environments (e.g., sidewalk and powerline) at scale. We train a robust YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting six environmental indicators, including streetlight, sidewalk, powerline, apartment, single-lane road, and multilane road. We then evaluate four LLMs, including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility, robustness, and limitations in identifying these indicators, with a focus on the impact of prompting strategies and fine-tuning. We apply majority voting with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs could be a useful tool to decode the neighborhood environment without any training effort.",
    "authors": [
      "Andrew Cart",
      "Shaohu Zhang",
      "Melanie Escue",
      "Xugui Zhou",
      "Haitao Zhao",
      "Prashanth BusiReddyGari",
      "Beiyu Lin",
      "Shuang Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-05-13T01:54:54Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08163v1"
  },
  {
    "arxiv_id": "2505.08137v1",
    "entry_id": "http://arxiv.org/abs/2505.08137v1",
    "title": "Large Language Models for Computer-Aided Design: A Survey",
    "summary": "Large Language Models (LLMs) have seen rapid advancements in recent years, with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities across diverse domains. While substantial research has been conducted on LLMs in various fields, a comprehensive review focusing on their integration with Computer-Aided Design (CAD) remains notably absent. CAD is the industry standard for 3D modeling and plays a vital role in the design and development of products across different industries. As the complexity of modern designs increases, the potential for LLMs to enhance and streamline CAD workflows presents an exciting frontier. This article presents the first systematic survey exploring the intersection of LLMs and CAD. We begin by outlining the industrial significance of CAD, highlighting the need for AI-driven innovation. Next, we provide a detailed overview of the foundation of LLMs. We also examine both closed-source LLMs as well as publicly available models. The core of this review focuses on the various applications of LLMs in CAD, providing a taxonomy of six key areas where these models are making considerable impact. Finally, we propose several promising future directions for further advancements, which offer vast opportunities for innovation and are poised to shape the future of CAD technology. Github: https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy",
    "authors": [
      "Licheng Zhang",
      "Bach Le",
      "Naveed Akhtar",
      "Siew-Kei Lam",
      "Tuan Ngo"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.GR",
      "cs.MM"
    ],
    "published": "2025-05-13T00:19:04Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08137v1"
  },
  {
    "arxiv_id": "2505.08004v1",
    "entry_id": "http://arxiv.org/abs/2505.08004v1",
    "title": "Large Language Models and Arabic Content: A Review",
    "summary": "Over the past three years, the rapid advancement of Large Language Models (LLMs) has had a profound impact on multiple areas of Artificial Intelligence (AI), particularly in Natural Language Processing (NLP) across diverse languages, including Arabic. Although Arabic is considered one of the most widely spoken languages across 27 countries in the Arabic world and used as a second language in some other non-Arabic countries as well, there is still a scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face various challenges due to the complexities of the Arabic language, including its rich morphology, intricate structure, and diverse writing standards, among other factors. Researchers have been actively addressing these challenges, demonstrating that pre-trained Large Language Models (LLMs) trained on multilingual corpora achieve significant success in various Arabic NLP tasks. This study provides an overview of using large language models (LLMs) for the Arabic language, highlighting early pre-trained Arabic Language models across various NLP applications and their ability to handle diverse Arabic content tasks and dialects. It also provides an overview of how techniques like finetuning and prompt engineering can enhance the performance of these models. Additionally, the study summarizes common Arabic benchmarks and datasets while presenting our observations on the persistent upward trend in the adoption of LLMs.",
    "authors": [
      "Haneh Rhel",
      "Dmitri Roussinov"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-12T19:09:12Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08004v1"
  },
  {
    "arxiv_id": "2505.07920v1",
    "entry_id": "http://arxiv.org/abs/2505.07920v1",
    "title": "Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions",
    "summary": "Peer review is a critical component of scientific progress in the fields like AI, but the rapid increase in submission volume has strained the reviewing system, which inevitably leads to reviewer shortages and declines review quality. Besides the growing research popularity, another key factor in this overload is the repeated resubmission of substandard manuscripts, largely due to the lack of effective tools for authors to self-evaluate their work before submission. Large Language Models (LLMs) show great promise in assisting both authors and reviewers, and their performance is fundamentally limited by the quality of the peer review data. However, existing peer review datasets face three major limitations: (1) limited data diversity, (2) inconsistent and low-quality data due to the use of revised rather than initial submissions, and (3) insufficient support for tasks involving rebuttal and reviewer-author interactions. To address these challenges, we introduce the largest consistency-ensured peer review and rebuttal dataset named Re^2, which comprises 19,926 initial submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the rebuttal and discussion stage is framed as a multi-turn conversation paradigm to support both traditional static review tasks and dynamic interactive LLM assistants, providing more practical guidance for authors to refine their manuscripts and helping alleviate the growing review burden. Our data and code are available in https://anonymous.4open.science/r/ReviewBench_anon/.",
    "authors": [
      "Daoze Zhang",
      "Zhijian Bao",
      "Sihang Du",
      "Zhiyi Zhao",
      "Kuangling Zhang",
      "Dezheng Bao",
      "Yang Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-12T16:02:52Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07920v1"
  },
  {
    "arxiv_id": "2505.07634v3",
    "entry_id": "http://arxiv.org/abs/2505.07634v3",
    "title": "Neural Brain: A Neuroscience-inspired Framework for Embodied Agents",
    "summary": "The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.",
    "authors": [
      "Jian Liu",
      "Xiongtao Shi",
      "Thai Duy Nguyen",
      "Haitian Zhang",
      "Tianxiang Zhang",
      "Wei Sun",
      "Yanjie Li",
      "Athanasios V. Vasilakos",
      "Giovanni Iacca",
      "Arshad Ali Khan",
      "Arvind Kumar",
      "Jae Won Cho",
      "Ajmal Mian",
      "Lihua Xie",
      "Erik Cambria",
      "Lin Wang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-05-12T15:05:34Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07634v3"
  },
  {
    "arxiv_id": "2505.07581v3",
    "entry_id": "http://arxiv.org/abs/2505.07581v3",
    "title": "YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models",
    "summary": "Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.",
    "authors": [
      "Lei Wang",
      "Heyang Gao",
      "Xiaohe Bo",
      "Xu Chen",
      "Ji-Rong Wen"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-05-12T14:05:17Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07581v3"
  },
  {
    "arxiv_id": "2505.07911v1",
    "entry_id": "http://arxiv.org/abs/2505.07911v1",
    "title": "Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review",
    "summary": "Bayesian inference has many advantages in decision making of agents (e.g. robotics/simulative agent) over a regular data-driven black-box neural network: Data-efficiency, generalization, interpretability, and safety where these advantages benefit directly/indirectly from the uncertainty quantification of Bayesian inference. However, there are few comprehensive reviews to summarize the progress of Bayesian inference on reinforcement learning (RL) for decision making to give researchers a systematic understanding. This paper focuses on combining Bayesian inference with RL that nowadays is an important approach in agent decision making. To be exact, this paper discusses the following five topics: 1) Bayesian methods that have potential for agent decision making. First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and Bayesian conjugate models) are discussed followed by variational inference, Bayesian optimization, Bayesian deep learning, Bayesian active learning, Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian learning. 2) Classical combinations of Bayesian methods with model-based RL (with approximation methods), model-free RL, and inverse RL. 3) Latest combinations of potential Bayesian methods with RL. 4) Analytical comparisons of methods that combine Bayesian methods with RL with respect to data-efficiency, generalization, interpretability, and safety. 5) In-depth discussions in six complex problem variants of RL, including unknown reward, partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and hierarchical RL problems and the summary of how Bayesian methods work in the data collection, data processing and policy learning stages of RL to pave the way for better agent decision-making strategies.",
    "authors": [
      "Chengmin Zhou",
      "Ville Kyrki",
      "Pasi Fränti",
      "Laura Ruotsalainen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-05-12T13:34:50Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07911v1"
  },
  {
    "arxiv_id": "2505.07460v1",
    "entry_id": "http://arxiv.org/abs/2505.07460v1",
    "title": "A Survey on Collaborative Mechanisms Between Large and Small Language Models",
    "summary": "Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.",
    "authors": [
      "Yi Chen",
      "JiaHao Zhao",
      "HaoHao Han"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-12T11:48:42Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07460v1"
  },
  {
    "arxiv_id": "2505.08807v1",
    "entry_id": "http://arxiv.org/abs/2505.08807v1",
    "title": "Security of Internet of Agents: Attacks and Countermeasures",
    "summary": "With the rise of large language and vision-language models, AI agents have evolved into autonomous, interactive systems capable of perception, reasoning, and decision-making. As they proliferate across virtual and physical domains, the Internet of Agents (IoA) has emerged as a key infrastructure for enabling scalable and secure coordination among heterogeneous agents. This survey offers a comprehensive examination of the security and privacy landscape in IoA systems. We begin by outlining the IoA architecture and its distinct vulnerabilities compared to traditional networks, focusing on four critical aspects: identity authentication threats, cross-agent trust issues, embodied security, and privacy risks. We then review existing and emerging defense mechanisms and highlight persistent challenges. Finally, we identify open research directions to advance the development of resilient and privacy-preserving IoA ecosystems.",
    "authors": [
      "Yuntao Wang",
      "Yanghe Pan",
      "Shaolong Guo",
      "Zhou Su"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-12T02:04:57Z",
    "pdf_url": "https://arxiv.org/pdf/2505.08807v1"
  },
  {
    "arxiv_id": "2505.07176v2",
    "entry_id": "http://arxiv.org/abs/2505.07176v2",
    "title": "Internet of Agents: Fundamentals, Applications, and Challenges",
    "summary": "With the rapid proliferation of large language models and vision-language models, AI agents have evolved from isolated, task-specific systems into autonomous, interactive entities capable of perceiving, reasoning, and acting without human intervention. As these agents proliferate across virtual and physical environments, from virtual assistants to embodied robots, the need for a unified, agent-centric infrastructure becomes paramount. In this survey, we introduce the Internet of Agents (IoA) as a foundational framework that enables seamless interconnection, dynamic discovery, and collaborative orchestration among heterogeneous agents at scale. We begin by presenting a general IoA architecture, highlighting its hierarchical organization, distinguishing features relative to the traditional Internet, and emerging applications. Next, we analyze the key operational enablers of IoA, including capability notification and discovery, adaptive communication protocols, dynamic task matching, consensus and conflict-resolution mechanisms, and incentive models. Finally, we identify open research directions toward building resilient and trustworthy IoA ecosystems.",
    "authors": [
      "Yuntao Wang",
      "Shaolong Guo",
      "Yanghe Pan",
      "Zhou Su",
      "Fahao Chen",
      "Tom H. Luan",
      "Peng Li",
      "Jiawen Kang",
      "Dusit Niyato"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-05-12T02:04:37Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07176v2"
  },
  {
    "arxiv_id": "2505.07062v1",
    "entry_id": "http://arxiv.org/abs/2505.07062v1",
    "title": "Seed1.5-VL Technical Report",
    "summary": "We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)",
    "authors": [
      "Dong Guo",
      "Faming Wu",
      "Feida Zhu",
      "Fuxing Leng",
      "Guang Shi",
      "Haobin Chen",
      "Haoqi Fan",
      "Jian Wang",
      "Jianyu Jiang",
      "Jiawei Wang",
      "Jingji Chen",
      "Jingjia Huang",
      "Kang Lei",
      "Liping Yuan",
      "Lishu Luo",
      "Pengfei Liu",
      "Qinghao Ye",
      "Rui Qian",
      "Shen Yan",
      "Shixiong Zhao",
      "Shuai Peng",
      "Shuangye Li",
      "Sihang Yuan",
      "Sijin Wu",
      "Tianheng Cheng",
      "Weiwei Liu",
      "Wenqian Wang",
      "Xianhan Zeng",
      "Xiao Liu",
      "Xiaobo Qin",
      "Xiaohan Ding",
      "Xiaojun Xiao",
      "Xiaoying Zhang",
      "Xuanwei Zhang",
      "Xuehan Xiong",
      "Yanghua Peng",
      "Yangrui Chen",
      "Yanwei Li",
      "Yanxu Hu",
      "Yi Lin",
      "Yiyuan Hu",
      "Yiyuan Zhang",
      "Youbin Wu",
      "Yu Li",
      "Yudong Liu",
      "Yue Ling",
      "Yujia Qin",
      "Zanbo Wang",
      "Zhiwu He",
      "Aoxue Zhang",
      "Bairen Yi",
      "Bencheng Liao",
      "Can Huang",
      "Can Zhang",
      "Chaorui Deng",
      "Chaoyi Deng",
      "Cheng Lin",
      "Cheng Yuan",
      "Chenggang Li",
      "Chenhui Gou",
      "Chenwei Lou",
      "Chengzhi Wei",
      "Chundian Liu",
      "Chunyuan Li",
      "Deyao Zhu",
      "Donghong Zhong",
      "Feng Li",
      "Feng Zhang",
      "Gang Wu",
      "Guodong Li",
      "Guohong Xiao",
      "Haibin Lin",
      "Haihua Yang",
      "Haoming Wang",
      "Heng Ji",
      "Hongxiang Hao",
      "Hui Shen",
      "Huixia Li",
      "Jiahao Li",
      "Jialong Wu",
      "Jianhua Zhu",
      "Jianpeng Jiao",
      "Jiashi Feng",
      "Jiaze Chen",
      "Jianhui Duan",
      "Jihao Liu",
      "Jin Zeng",
      "Jingqun Tang",
      "Jingyu Sun",
      "Joya Chen",
      "Jun Long",
      "Junda Feng",
      "Junfeng Zhan",
      "Junjie Fang",
      "Junting Lu",
      "Kai Hua",
      "Kai Liu",
      "Kai Shen",
      "Kaiyuan Zhang",
      "Ke Shen",
      "Ke Wang",
      "Keyu Pan",
      "Kun Zhang",
      "Kunchang Li",
      "Lanxin Li",
      "Lei Li",
      "Lei Shi",
      "Li Han",
      "Liang Xiang",
      "Liangqiang Chen",
      "Lin Chen",
      "Lin Li",
      "Lin Yan",
      "Liying Chi",
      "Longxiang Liu",
      "Mengfei Du",
      "Mingxuan Wang",
      "Ningxin Pan",
      "Peibin Chen",
      "Pengfei Chen",
      "Pengfei Wu",
      "Qingqing Yuan",
      "Qingyao Shuai",
      "Qiuyan Tao",
      "Renjie Zheng",
      "Renrui Zhang",
      "Ru Zhang",
      "Rui Wang",
      "Rui Yang",
      "Rui Zhao",
      "Shaoqiang Xu",
      "Shihao Liang",
      "Shipeng Yan",
      "Shu Zhong",
      "Shuaishuai Cao",
      "Shuangzhi Wu",
      "Shufan Liu",
      "Shuhan Chang",
      "Songhua Cai",
      "Tenglong Ao",
      "Tianhao Yang",
      "Tingting Zhang",
      "Wanjun Zhong",
      "Wei Jia",
      "Wei Weng",
      "Weihao Yu",
      "Wenhao Huang",
      "Wenjia Zhu",
      "Wenli Yang",
      "Wenzhi Wang",
      "Xiang Long",
      "XiangRui Yin",
      "Xiao Li",
      "Xiaolei Zhu",
      "Xiaoying Jia",
      "Xijin Zhang",
      "Xin Liu",
      "Xinchen Zhang",
      "Xinyu Yang",
      "Xiongcai Luo",
      "Xiuli Chen",
      "Xuantong Zhong",
      "Xuefeng Xiao",
      "Xujing Li",
      "Yan Wu",
      "Yawei Wen",
      "Yifan Du",
      "Yihao Zhang",
      "Yining Ye",
      "Yonghui Wu",
      "Yu Liu",
      "Yu Yue",
      "Yufeng Zhou",
      "Yufeng Yuan",
      "Yuhang Xu",
      "Yuhong Yang",
      "Yun Zhang",
      "Yunhao Fang",
      "Yuntao Li",
      "Yurui Ren",
      "Yuwen Xiong",
      "Zehua Hong",
      "Zehua Wang",
      "Zewei Sun",
      "Zeyu Wang",
      "Zhao Cai",
      "Zhaoyue Zha",
      "Zhecheng An",
      "Zhehui Zhao",
      "Zhengzhuo Xu",
      "Zhipeng Chen",
      "Zhiyong Wu",
      "Zhuofan Zheng",
      "Zihao Wang",
      "Zilong Huang",
      "Ziyu Zhu",
      "Zuquan Song"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-05-11T17:28:30Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07062v1"
  },
  {
    "arxiv_id": "2505.07001v2",
    "entry_id": "http://arxiv.org/abs/2505.07001v2",
    "title": "Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models",
    "summary": "Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucination--the tendency to generate descriptions that are inconsistent with the visual content--remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of state-of-the-art VLMs across several metrics, establishing a benchmark. GitHub Repo: https://github.com/bhattarailab/Hallucination-Aware-VLM.",
    "authors": [
      "Bidur Khanal",
      "Sandesh Pokhrel",
      "Sanjay Bhandari",
      "Ramesh Rana",
      "Nikesh Shrestha",
      "Ram Bahadur Gurung",
      "Cristian Linte",
      "Angus Watson",
      "Yash Raj Shrestha",
      "Binod Bhattarai"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-05-11T14:54:11Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07001v2"
  },
  {
    "arxiv_id": "2505.06907v1",
    "entry_id": "http://arxiv.org/abs/2505.06907v1",
    "title": "Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence",
    "summary": "The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.",
    "authors": [
      "Yu Qiao",
      "Huy Q. Le",
      "Avi Deb Raha",
      "Phuong-Nam Tran",
      "Apurba Adhikary",
      "Mengchun Zhang",
      "Loc X. Nguyen",
      "Eui-Nam Huh",
      "Dusit Niyato",
      "Choong Seon Hong"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.NE"
    ],
    "published": "2025-05-11T08:57:53Z",
    "pdf_url": "https://arxiv.org/pdf/2505.06907v1"
  },
  {
    "arxiv_id": "2505.06897v1",
    "entry_id": "http://arxiv.org/abs/2505.06897v1",
    "title": "Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence",
    "summary": "The ultimate goal of artificial intelligence (AI) is to achieve Artificial General Intelligence (AGI). Embodied Artificial Intelligence (EAI), which involves intelligent systems with physical presence and real-time interaction with the environment, has emerged as a key research direction in pursuit of AGI. While advancements in deep learning, reinforcement learning, large-scale language models, and multimodal technologies have significantly contributed to the progress of EAI, most existing reviews focus on specific technologies or applications. A systematic overview, particularly one that explores the direct connection between EAI and AGI, remains scarce. This paper examines EAI as a foundational approach to AGI, systematically analyzing its four core modules: perception, intelligent decision-making, action, and feedback. We provide a detailed discussion of how each module contributes to the six core principles of AGI. Additionally, we discuss future trends, challenges, and research directions in EAI, emphasizing its potential as a cornerstone for AGI development. Our findings suggest that EAI's integration of dynamic learning and real-world interaction is essential for bridging the gap between narrow AI and AGI.",
    "authors": [
      "Jinhao Jiang",
      "Changlin Chen",
      "Shile Feng",
      "Wanru Geng",
      "Zesheng Zhou",
      "Ni Wang",
      "Shuai Li",
      "Feng-Qi Cui",
      "Erbao Dong"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-11T08:29:20Z",
    "pdf_url": "https://arxiv.org/pdf/2505.06897v1"
  },
  {
    "arxiv_id": "2505.06817v1",
    "entry_id": "http://arxiv.org/abs/2505.06817v1",
    "title": "Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems",
    "summary": "Agentic AI systems represent a new frontier in artificial intelligence, where agents often based on large language models(LLMs) interact with tools, environments, and other agents to accomplish tasks with a degree of autonomy. These systems show promise across a range of domains, but their architectural underpinnings remain immature. This paper conducts a comprehensive review of the types of agents, their modes of interaction with the environment, and the infrastructural and architectural challenges that emerge. We identify a gap in how these systems manage tool orchestration at scale and propose a reusable design abstraction: the \"Control Plane as a Tool\" pattern. This pattern allows developers to expose a single tool interface to an agent while encapsulating modular tool routing logic behind it. We position this pattern within the broader context of agent design and argue that it addresses several key challenges in scaling, safety, and extensibility.",
    "authors": [
      "Sivasathivel Kandasamy"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-11T02:58:50Z",
    "pdf_url": "https://arxiv.org/pdf/2505.06817v1"
  },
  {
    "arxiv_id": "2507.18638v2",
    "entry_id": "http://arxiv.org/abs/2507.18638v2",
    "title": "Prompt Engineering and the Effectiveness of Large Language Models in Enhancing Human Productivity",
    "summary": "The widespread adoption of large language models (LLMs) such as ChatGPT, Gemini, and DeepSeek has significantly changed how people approach tasks in education, professional work, and creative domains. This paper investigates how the structure and clarity of user prompts impact the effectiveness and productivity of LLM outputs. Using data from 243 survey respondents across various academic and occupational backgrounds, we analyze AI usage habits, prompting strategies, and user satisfaction. The results show that users who employ clear, structured, and context-aware prompts report higher task efficiency and better outcomes. These findings emphasize the essential role of prompt engineering in maximizing the value of generative AI and provide practical implications for its everyday use.",
    "authors": [
      "Rizal Khoirul Anam"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-05-10T18:27:03Z",
    "pdf_url": "https://arxiv.org/pdf/2507.18638v2"
  },
  {
    "arxiv_id": "2505.06621v1",
    "entry_id": "http://arxiv.org/abs/2505.06621v1",
    "title": "Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models",
    "summary": "The distribution of child sexual abuse imagery (CSAI) is an ever-growing concern of our modern world; children who suffered from this heinous crime are revictimized, and the growing amount of illegal imagery distributed overwhelms law enforcement agents (LEAs) with the manual labor of categorization. To ease this burden researchers have explored methods for automating data triage and detection of CSAI, but the sensitive nature of the data imposes restricted access and minimal interaction between real data and learning algorithms, avoiding leaks at all costs. In observing how these restrictions have shaped the literature we formalize a definition of \"Proxy Tasks\", i.e., the substitute tasks used for training models for CSAI without making use of CSA data. Under this new terminology we review current literature and present a protocol for making conscious use of Proxy Tasks together with consistent input from LEAs to design better automation in this field. Finally, we apply this protocol to study -- for the first time -- the task of Few-shot Indoor Scene Classification on CSAI, showing a final model that achieves promising results on a real-world CSAI dataset whilst having no weights actually trained on sensitive data.",
    "authors": [
      "Thamiris Coelho",
      "Leo S. F. Ribeiro",
      "João Macedo",
      "Jefersson A. dos Santos",
      "Sandra Avila"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2025-05-10T12:10:55Z",
    "pdf_url": "https://arxiv.org/pdf/2505.06621v1"
  },
  {
    "arxiv_id": "2505.06032v1",
    "entry_id": "http://arxiv.org/abs/2505.06032v1",
    "title": "Short-circuiting Shortcuts: Mechanistic Investigation of Shortcuts in Text Classification",
    "summary": "Reliance on spurious correlations (shortcuts) has been shown to underlie many of the successes of language models. Previous work focused on identifying the input elements that impact prediction. We investigate how shortcuts are actually processed within the model's decision-making mechanism. We use actor names in movie reviews as controllable shortcuts with known impact on the outcome. We use mechanistic interpretability methods and identify specific attention heads that focus on shortcuts. These heads gear the model towards a label before processing the complete input, effectively making premature decisions that bypass contextual analysis. Based on these findings, we introduce Head-based Token Attribution (HTA), which traces intermediate decisions back to input tokens. We show that HTA is effective in detecting shortcuts in LLMs and enables targeted mitigation by selectively deactivating shortcut-related attention heads.",
    "authors": [
      "Leon Eshuijs",
      "Shihan Wang",
      "Antske Fokkens"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-05-09T13:26:21Z",
    "pdf_url": "https://arxiv.org/pdf/2505.06032v1"
  },
  {
    "arxiv_id": "2505.05797v1",
    "entry_id": "http://arxiv.org/abs/2505.05797v1",
    "title": "Assessing the Dynamics of the Coffee Value Chain in Davao del Sur: An Agent-Based Modeling Approach",
    "summary": "The study investigates the coffee value chain dynamics in Davao del Sur using an agent-based model. Three main factors driving interactions among key players were identified: trust, risk, and transaction costs. The model was constructed using NetLogo 6.3.0, and data from a survey questionnaire collected three data points from BACOFA members. Five cases were explored, with each scenario simulated 1000 times. Findings suggest that producers often sell to the market rather than the cooperative due to higher prices. However, producers tend to prioritize trust in buyers and their risk attitude, leading to increased sales to the cooperative. The producer's risk attitude significantly influences their decision-making, affecting performance outcomes such as loans, demand, and price changes. All three factors play a role and exert varying impacts on the value chain. So, the stakeholders' decisions on prioritizing factors in improving relationships depend on their priorities. Nonetheless, simulations show that establishing a harmonious system benefiting all parties is possible. However, achieving this requires adjustments to demand, pricing, trust, and risk attitudes of key players, which may not align with the preferences of some parties in reality.",
    "authors": [
      "Lucia Stephanie B. Sibala",
      "Novy Aila B. Rivas",
      "Giovanna Fae R. Oguis"
    ],
    "categories": [
      "econ.GN",
      "cs.CY",
      "cs.MA"
    ],
    "published": "2025-05-09T05:24:51Z",
    "pdf_url": "https://arxiv.org/pdf/2505.05797v1"
  },
  {
    "arxiv_id": "2505.05794v1",
    "entry_id": "http://arxiv.org/abs/2505.05794v1",
    "title": "What Is Next for LLMs? Next-Generation AI Computing Hardware Using Photonic Chips",
    "summary": "Large language models (LLMs) are rapidly pushing the limits of contemporary computing hardware. For example, training GPT-3 has been estimated to consume around 1300 MWh of electricity, and projections suggest future models may require city-scale (gigawatt) power budgets. These demands motivate exploration of computing paradigms beyond conventional von Neumann architectures. This review surveys emerging photonic hardware optimized for next-generation generative AI computing. We discuss integrated photonic neural network architectures (e.g., Mach-Zehnder interferometer meshes, lasers, wavelength-multiplexed microring resonators) that perform ultrafast matrix operations. We also examine promising alternative neuromorphic devices, including spiking neural network circuits and hybrid spintronic-photonic synapses, which combine memory and processing. The integration of two-dimensional materials (graphene, TMDCs) into silicon photonic platforms is reviewed for tunable modulators and on-chip synaptic elements. Transformer-based LLM architectures (self-attention and feed-forward layers) are analyzed in this context, identifying strategies and challenges for mapping dynamic matrix multiplications onto these novel hardware substrates. We then dissect the mechanisms of mainstream LLMs, such as ChatGPT, DeepSeek, and LLaMA, highlighting their architectural similarities and differences. We synthesize state-of-the-art components, algorithms, and integration methods, highlighting key advances and open issues in scaling such systems to mega-sized LLM models. We find that photonic computing systems could potentially surpass electronic processors by orders of magnitude in throughput and energy efficiency, but require breakthroughs in memory, especially for long-context windows and long token sequences, and in storage of ultra-large datasets.",
    "authors": [
      "Renjie Li",
      "Wenjie Wei",
      "Qi Xin",
      "Xiaoli Liu",
      "Sixuan Mao",
      "Erik Ma",
      "Zijian Chen",
      "Malu Zhang",
      "Haizhou Li",
      "Zhaoyu Zhang"
    ],
    "categories": [
      "cs.AR",
      "cs.AI",
      "cs.NE"
    ],
    "published": "2025-05-09T05:19:14Z",
    "pdf_url": "https://arxiv.org/pdf/2505.05794v1"
  },
  {
    "arxiv_id": "2505.05543v1",
    "entry_id": "http://arxiv.org/abs/2505.05543v1",
    "title": "Would You Rely on an Eerie Agent? A Systematic Review of the Impact of the Uncanny Valley Effect on Trust in Human-Agent Interaction",
    "summary": "Trust is a fundamental component of human-agent interaction. With the increasing presence of artificial agents in daily life, it is essential to understand how people perceive and trust these agents. One of the key challenges affecting this perception is the Uncanny Valley Effect (UVE), where increasingly human-like artificial beings can be perceived as eerie or repelling. Despite growing interest in trust and the UVE, existing research varies widely in terms of how these concepts are defined and operationalized. This inconsistency raises important questions about how and under what conditions the UVE influences trust in agents. A systematic understanding of their relationship is currently lacking. This review aims to examine the impact of the UVE on human trust in agents and to identify methodological patterns, limitations, and gaps in the existing empirical literature. Following PRISMA guidelines, a systematic search identified 53 empirical studies that investigated both UVE-related constructs and trust or trust-related outcomes. Studies were analyzed based on a structured set of categories, including types of agents and interactions, methodological and measurement approaches, and key findings. The results of our systematic review reveal that most studies rely on static images or hypothetical scenarios with limited real-time interaction, and the majority use subjective trust measures. This review offers a novel framework for classifying trust measurement approaches with regard to the best-practice criteria for empirically investigating the UVE. As the first systematic attempt to map the intersection of UVE and trust, this review contributes to a deeper understanding of their interplay and offers a foundation for future research. Keywords: the uncanny valley effect, trust, human-likeness, affinity response, human-agent interaction",
    "authors": [
      "Ahdiyeh Alipour",
      "Tilo Hartmann",
      "Maryam Alimardani"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-05-08T17:03:26Z",
    "pdf_url": "https://arxiv.org/pdf/2505.05543v1"
  },
  {
    "arxiv_id": "2505.05318v1",
    "entry_id": "http://arxiv.org/abs/2505.05318v1",
    "title": "Mapping User Trust in Vision Language Models: Research Landscape, Challenges, and Prospects",
    "summary": "The rapid adoption of Vision Language Models (VLMs), pre-trained on large image-text and video-text datasets, calls for protecting and informing users about when to trust these systems. This survey reviews studies on trust dynamics in user-VLM interactions, through a multi-disciplinary taxonomy encompassing different cognitive science capabilities, collaboration modes, and agent behaviours. Literature insights and findings from a workshop with prospective VLM users inform preliminary requirements for future VLM trust studies.",
    "authors": [
      "Agnese Chiatti",
      "Sara Bernardini",
      "Lara Shibelski Godoy Piccolo",
      "Viola Schiaffonati",
      "Matteo Matteucci"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.RO"
    ],
    "published": "2025-05-08T15:02:49Z",
    "pdf_url": "https://arxiv.org/pdf/2505.05318v1"
  },
  {
    "arxiv_id": "2505.05283v2",
    "entry_id": "http://arxiv.org/abs/2505.05283v2",
    "title": "Software Development Life Cycle Perspective: A Survey of Benchmarks for Code Large Language Models and Agents",
    "summary": "Code large language models (CodeLLMs) and agents have shown great promise in tackling complex software engineering tasks.Compared to traditional software engineering methods, CodeLLMs and agents offer stronger abilities, and can flexibly process inputs and outputs in both natural and code. Benchmarking plays a crucial role in evaluating the capabilities of CodeLLMs and agents, guiding their development and deployment. However, despite their growing significance, there remains a lack of comprehensive reviews of benchmarks for CodeLLMs and agents. To bridge this gap, this paper provides a comprehensive review of existing benchmarks for CodeLLMs and agents, studying and analyzing 181 benchmarks from 461 relevant papers, covering the different phases of the software development life cycle (SDLC). Our findings reveal a notable imbalance in the coverage of current benchmarks, with approximately 60% focused on the software development phase in SDLC, while requirements engineering and software design phases receive minimal attention at only 5% and 3%, respectively. Additionally, Python emerges as the dominant programming language across the reviewed benchmarks. Finally, this paper highlights the challenges of current research and proposes future directions, aiming to narrow the gap between the theoretical capabilities of CodeLLMs and agents and their application in real-world scenarios.",
    "authors": [
      "Kaixin Wang",
      "Tianlin Li",
      "Xiaoyu Zhang",
      "Chong Wang",
      "Weisong Sun",
      "Yang Liu",
      "Bin Shi"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-05-08T14:27:45Z",
    "pdf_url": "https://arxiv.org/pdf/2505.05283v2"
  },
  {
    "arxiv_id": "2505.05108v2",
    "entry_id": "http://arxiv.org/abs/2505.05108v2",
    "title": "Multi-agent Embodied AI: Advances and Future Directions",
    "summary": "Embodied artificial intelligence (Embodied AI) plays a pivotal role in the application of advanced technologies in the intelligent era, where AI systems are integrated with physical bodies that enable them to perceive, reason, and interact with their environments. Through the use of sensors for input and actuators for action, these systems can learn and adapt based on real-world feedback, allowing them to perform tasks effectively in dynamic and unpredictable environments. As techniques such as deep learning (DL), reinforcement learning (RL), and large language models (LLMs) mature, embodied AI has become a leading field in both academia and industry, with applications spanning robotics, healthcare, transportation, and manufacturing. However, most research has focused on single-agent systems that often assume static, closed environments, whereas real-world embodied AI must navigate far more complex scenarios. In such settings, agents must not only interact with their surroundings but also collaborate with other agents, necessitating sophisticated mechanisms for adaptation, real-time learning, and collaborative problem-solving. Despite increasing interest in multi-agent systems, existing research remains narrow in scope, often relying on simplified models that fail to capture the full complexity of dynamic, open environments for multi-agent embodied AI. Moreover, no comprehensive survey has systematically reviewed the advancements in this area. As embodied AI rapidly evolves, it is crucial to deepen our understanding of multi-agent embodied AI to address the challenges presented by real-world applications. To fill this gap and foster further development in the field, this paper reviews the current state of research, analyzes key contributions, and identifies challenges and future directions, providing insights to guide innovation and progress in this field.",
    "authors": [
      "Zhaohan Feng",
      "Ruiqi Xue",
      "Lei Yuan",
      "Yang Yu",
      "Ning Ding",
      "Meiqin Liu",
      "Bingzhao Gao",
      "Jian Sun",
      "Xinhu Zheng",
      "Gang Wang"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-05-08T10:13:53Z",
    "pdf_url": "https://arxiv.org/pdf/2505.05108v2"
  },
  {
    "arxiv_id": "2505.05047v1",
    "entry_id": "http://arxiv.org/abs/2505.05047v1",
    "title": "Neural Pathways to Program Success: Hopfield Networks for PERT Analysis",
    "summary": "Project and task scheduling under uncertainty remains a fundamental challenge in program and project management, where accurate estimation of task durations and dependencies is critical for delivering complex, multi project systems. The Program Evaluation and Review Technique provides a probabilistic framework to model task variability and critical paths. In this paper, the author presents a novel formulation of PERT scheduling as an energy minimization problem within a Hopfield neural network architecture. By mapping task start times and precedence constraints into a neural computation framework, the networks inherent optimization dynamics is exploited to approximate globally consistent schedules. The author addresses key theoretical issues related to energy function differentiability, constraint encoding, and convergence, and extends the Hopfield model for structured precedence graphs. Numerical simulations on synthetic project networks comprising up to 1000 tasks demonstrate the viability of this approach, achieving near optimal makespans with minimal constraint violations. The findings suggest that neural optimization models offer a promising direction for scalable and adaptive project tasks scheduling under uncertainty in areas such as the agentic AI workflows, microservice based applications that the modern AI systems are being built upon.",
    "authors": [
      "Azgar Ali Noor Ahamed"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-05-08T08:34:16Z",
    "pdf_url": "https://arxiv.org/pdf/2505.05047v1"
  },
  {
    "arxiv_id": "2505.05523v1",
    "entry_id": "http://arxiv.org/abs/2505.05523v1",
    "title": "GenAI in Entrepreneurship: a systematic review of generative artificial intelligence in entrepreneurship research: current issues and future directions",
    "summary": "Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs) are recognized to have significant effects on industry and business dynamics, not least because of their impact on the preconditions for entrepreneurship. There is still a lack of knowledge of GenAI as a theme in entrepreneurship research. This paper presents a systematic literature review aimed at identifying and analyzing the evolving landscape of research on the effects of GenAI on entrepreneurship. We analyze 83 peer-reviewed articles obtained from leading academic databases: Web of Science and Scopus. Using natural language processing and unsupervised machine learning techniques with TF-IDF vectorization, Principal Component Analysis (PCA), and hierarchical clustering, five major thematic clusters are identified: (1) Digital Transformation and Behavioral Models, (2) GenAI-Enhanced Education and Learning Systems, (3) Sustainable Innovation and Strategic AI Impact, (4) Business Models and Market Trends, and (5) Data-Driven Technological Trends in Entrepreneurship. Based on the review, we discuss future research directions, gaps in the current literature, as well as ethical concerns raised in the literature. We highlight the need for more macro-level research on GenAI and LLMs as external enablers for entrepreneurship and for research on effective regulatory frameworks that facilitate business experimentation, innovation, and further technology development.",
    "authors": [
      "Anna Kusetogullari",
      "Huseyin Kusetogullari",
      "Martin Andersson",
      "Tony Gorschek"
    ],
    "categories": [
      "econ.GN",
      "cs.AI"
    ],
    "published": "2025-05-08T07:44:42Z",
    "pdf_url": "https://arxiv.org/pdf/2505.05523v1"
  },
  {
    "arxiv_id": "2505.06305v1",
    "entry_id": "http://arxiv.org/abs/2505.06305v1",
    "title": "User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data",
    "summary": "With the widespread application of large language models (LLMs), user privacy protection has become a significant research topic. Existing privacy preference modeling methods often rely on large-scale user data, making effective privacy preference analysis challenging in data-limited environments. This study explores how LLMs can analyze user behavior related to privacy protection in scenarios with limited data and proposes a method that integrates Few-shot Learning and Privacy Computing to model user privacy preferences. The research utilizes anonymized user privacy settings data, survey responses, and simulated data, comparing the performance of traditional modeling approaches with LLM-based methods. Experimental results demonstrate that, even with limited data, LLMs significantly improve the accuracy of privacy preference modeling. Additionally, incorporating Differential Privacy and Federated Learning further reduces the risk of user data exposure. The findings provide new insights into the application of LLMs in privacy protection and offer theoretical support for advancing privacy computing and user behavior analysis.",
    "authors": [
      "Haowei Yang",
      "Qingyi Lu",
      "Yang Wang",
      "Sibei Liu",
      "Jiayun Zheng",
      "Ao Xiang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-08T04:42:17Z",
    "pdf_url": "https://arxiv.org/pdf/2505.06305v1"
  },
  {
    "arxiv_id": "2505.04732v1",
    "entry_id": "http://arxiv.org/abs/2505.04732v1",
    "title": "QBD-RankedDataGen: Generating Custom Ranked Datasets for Improving Query-By-Document Search Using LLM-Reranking with Reduced Human Effort",
    "summary": "The Query-By-Document (QBD) problem is an information retrieval problem where the query is a document, and the retrieved candidates are documents that match the query document, often in a domain or query specific manner. This can be crucial for tasks such as patent matching, legal or compliance case retrieval, and academic literature review. Existing retrieval methods, including keyword search and document embeddings, can be optimized with domain-specific datasets to improve QBD search performance. However, creating these domain-specific datasets is often costly and time-consuming. Our work introduces a process to generate custom QBD-search datasets and compares a set of methods to use in this problem, which we refer to as QBD-RankedDatagen. We provide a comparative analysis of our proposed methods in terms of cost, speed, and the human interface with the domain experts. The methods we compare leverage Large Language Models (LLMs) which can incorporate domain expert input to produce document scores and rankings, as well as explanations for human review. The process and methods for it that we present can significantly reduce human effort in dataset creation for custom domains while still obtaining sufficient expert knowledge for tuning retrieval models. We evaluate our methods on QBD datasets from the Text Retrieval Conference (TREC) and finetune the parameters of the BM25 model -- which is used in many industrial-strength search engines like OpenSearch -- using the generated data.",
    "authors": [
      "Sriram Gopalakrishnan",
      "Sunandita Patra"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-05-07T18:43:57Z",
    "pdf_url": "https://arxiv.org/pdf/2505.04732v1"
  },
  {
    "arxiv_id": "2505.04531v2",
    "entry_id": "http://arxiv.org/abs/2505.04531v2",
    "title": "Overcoming Data Scarcity in Generative Language Modelling for Low-Resource Languages: A Systematic Review",
    "summary": "Generative language modelling has surged in popularity with the emergence of services such as ChatGPT and Google Gemini. While these models have demonstrated transformative potential in productivity and communication, they overwhelmingly cater to high-resource languages like English. This has amplified concerns over linguistic inequality in natural language processing (NLP). This paper presents the first systematic review focused specifically on strategies to address data scarcity in generative language modelling for low-resource languages (LRL). Drawing from 54 studies, we identify, categorise and evaluate technical approaches, including monolingual data augmentation, back-translation, multilingual training, and prompt engineering, across generative tasks. We also analyse trends in architecture choices, language family representation, and evaluation methods. Our findings highlight a strong reliance on transformer-based models, a concentration on a small subset of LRLs, and a lack of consistent evaluation across studies. We conclude with recommendations for extending these methods to a wider range of LRLs and outline open challenges in building equitable generative language systems. Ultimately, this review aims to support researchers and developers in building inclusive AI tools for underrepresented languages, a necessary step toward empowering LRL speakers and the preservation of linguistic diversity in a world increasingly shaped by large-scale language technologies.",
    "authors": [
      "Josh McGiff",
      "Nikola S. Nikolov"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-05-07T16:04:45Z",
    "pdf_url": "https://arxiv.org/pdf/2505.04531v2"
  },
  {
    "arxiv_id": "2505.04265v1",
    "entry_id": "http://arxiv.org/abs/2505.04265v1",
    "title": "Weaponizing Language Models for Cybersecurity Offensive Operations: Automating Vulnerability Assessment Report Validation; A Review Paper",
    "summary": "This, with the ever-increasing sophistication of cyberwar, calls for novel solutions. In this regard, Large Language Models (LLMs) have emerged as a highly promising tool for defensive and offensive cybersecurity-related strategies. While existing literature has focused much on the defensive use of LLMs, when it comes to their offensive utilization, very little has been reported-namely, concerning Vulnerability Assessment (VA) report validation. Consequentially, this paper tries to fill that gap by investigating the capabilities of LLMs in automating and improving the validation process of the report of the VA. From the critical review of the related literature, this paper hereby proposes a new approach to using the LLMs in the automation of the analysis and within the validation process of the report of the VA that could potentially reduce the number of false positives and generally enhance efficiency. These results are promising for LLM automatization for improving validation on reports coming from VA in order to improve accuracy while reducing human effort and security postures. The contribution of this paper provides further evidence about the offensive and defensive LLM capabilities and therefor helps in devising more appropriate cybersecurity strategies and tools accordingly.",
    "authors": [
      "Abdulrahman S Almuhaidib",
      "Azlan Mohd Zain",
      "Zalmiyah Zakaria",
      "Izyan Izzati Kamsani",
      "Abdulaziz S Almuhaidib"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-05-07T09:14:55Z",
    "pdf_url": "https://arxiv.org/pdf/2505.04265v1"
  },
  {
    "arxiv_id": "2505.04670v2",
    "entry_id": "http://arxiv.org/abs/2505.04670v2",
    "title": "LLM Code Customization with Visual Results: A Benchmark on TikZ",
    "summary": "With the rise of AI-based code generation, customizing existing code out of natural language instructions to modify visual results -such as figures or images -has become possible, promising to reduce the need for deep programming expertise. However, even experienced developers can struggle with this task, as it requires identifying relevant code regions (feature location), generating valid code variants, and ensuring the modifications reliably align with user intent. In this paper, we introduce vTikZ, the first benchmark designed to evaluate the ability of Large Language Models (LLMs) to customize code while preserving coherent visual outcomes. Our benchmark consists of carefully curated vTikZ editing scenarios, parameterized ground truths, and a reviewing tool that leverages visual feedback to assess correctness. Empirical evaluation with stateof-the-art LLMs shows that existing solutions struggle to reliably modify code in alignment with visual intent, highlighting a gap in current AI-assisted code editing approaches. We argue that vTikZ opens new research directions for integrating LLMs with visual feedback mechanisms to improve code customization tasks in various domains beyond TikZ, including image processing, art creation, Web design, and 3D modeling.",
    "authors": [
      "Charly Reux",
      "Mathieu Acher",
      "Djamel Eddine Khelladi",
      "Olivier Barais",
      "Clément Quinton"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-05-07T08:26:54Z",
    "pdf_url": "https://arxiv.org/pdf/2505.04670v2"
  },
  {
    "arxiv_id": "2505.04135v1",
    "entry_id": "http://arxiv.org/abs/2505.04135v1",
    "title": "Enhancing Granular Sentiment Classification with Chain-of-Thought Prompting in Large Language Models",
    "summary": "We explore the use of Chain-of-Thought (CoT) prompting with large language models (LLMs) to improve the accuracy of granular sentiment categorization in app store reviews. Traditional numeric and polarity-based ratings often fail to capture the nuanced sentiment embedded in user feedback. We evaluated the effectiveness of CoT prompting versus simple prompting on 2000 Amazon app reviews by comparing each method's predictions to human judgements. CoT prompting improved classification accuracy from 84% to 93% highlighting the benefit of explicit reasoning in enhancing sentiment analysis performance.",
    "authors": [
      "Vihaan Miriyala",
      "Smrithi Bukkapatnam",
      "Lavanya Prahallad"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-05-07T05:13:15Z",
    "pdf_url": "https://arxiv.org/pdf/2505.04135v1"
  },
  {
    "arxiv_id": "2505.04651v1",
    "entry_id": "http://arxiv.org/abs/2505.04651v1",
    "title": "Scientific Hypothesis Generation and Validation: Methods, Datasets, and Future Directions",
    "summary": "Large Language Models (LLMs) are transforming scientific hypothesis generation and validation by enabling information synthesis, latent relationship discovery, and reasoning augmentation. This survey provides a structured overview of LLM-driven approaches, including symbolic frameworks, generative models, hybrid systems, and multi-agent architectures. We examine techniques such as retrieval-augmented generation, knowledge-graph completion, simulation, causal inference, and tool-assisted reasoning, highlighting trade-offs in interpretability, novelty, and domain alignment. We contrast early symbolic discovery systems (e.g., BACON, KEKADA) with modern LLM pipelines that leverage in-context learning and domain adaptation via fine-tuning, retrieval, and symbolic grounding. For validation, we review simulation, human-AI collaboration, causal modeling, and uncertainty quantification, emphasizing iterative assessment in open-world contexts. The survey maps datasets across biomedicine, materials science, environmental science, and social science, introducing new resources like AHTech and CSKG-600. Finally, we outline a roadmap emphasizing novelty-aware generation, multimodal-symbolic integration, human-in-the-loop systems, and ethical safeguards, positioning LLMs as agents for principled, scalable scientific discovery.",
    "authors": [
      "Adithya Kulkarni",
      "Fatimah Alotaibi",
      "Xinyue Zeng",
      "Longfeng Wu",
      "Tong Zeng",
      "Barry Menglong Yao",
      "Minqian Liu",
      "Shuaicheng Zhang",
      "Lifu Huang",
      "Dawei Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-05-06T19:22:23Z",
    "pdf_url": "https://arxiv.org/pdf/2505.04651v1"
  },
  {
    "arxiv_id": "2505.03864v1",
    "entry_id": "http://arxiv.org/abs/2505.03864v1",
    "title": "From Glue-Code to Protocols: A Critical Analysis of A2A and MCP Integration for Scalable Agent Systems",
    "summary": "Artificial intelligence is rapidly evolving towards multi-agent systems where numerous AI agents collaborate and interact with external tools. Two key open standards, Google's Agent to Agent (A2A) protocol for inter-agent communication and Anthropic's Model Context Protocol (MCP) for standardized tool access, promise to overcome the limitations of fragmented, custom integration approaches. While their potential synergy is significant, this paper argues that effectively integrating A2A and MCP presents unique, emergent challenges at their intersection, particularly concerning semantic interoperability between agent tasks and tool capabilities, the compounded security risks arising from combined discovery and execution, and the practical governance required for the envisioned \"Agent Economy\". This work provides a critical analysis, moving beyond a survey to evaluate the practical implications and inherent difficulties of combining these horizontal and vertical integration standards. We examine the benefits (e.g., specialization, scalability) while critically assessing their dependencies and trade-offs in an integrated context. We identify key challenges increased by the integration, including novel security vulnerabilities, privacy complexities, debugging difficulties across protocols, and the need for robust semantic negotiation mechanisms. In summary, A2A+MCP offers a vital architectural foundation, but fully realizing its potential requires substantial advancements to manage the complexities of their combined operation.",
    "authors": [
      "Qiaomu Li",
      "Ying Xie"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-06T16:40:39Z",
    "pdf_url": "https://arxiv.org/pdf/2505.03864v1"
  },
  {
    "arxiv_id": "2505.03418v1",
    "entry_id": "http://arxiv.org/abs/2505.03418v1",
    "title": "Knowledge Augmented Complex Problem Solving with Large Language Models: A Survey",
    "summary": "Problem-solving has been a fundamental driver of human progress in numerous domains. With advancements in artificial intelligence, Large Language Models (LLMs) have emerged as powerful tools capable of tackling complex problems across diverse domains. Unlike traditional computational systems, LLMs combine raw computational power with an approximation of human reasoning, allowing them to generate solutions, make inferences, and even leverage external computational tools. However, applying LLMs to real-world problem-solving presents significant challenges, including multi-step reasoning, domain knowledge integration, and result verification. This survey explores the capabilities and limitations of LLMs in complex problem-solving, examining techniques including Chain-of-Thought (CoT) reasoning, knowledge augmentation, and various LLM-based and tool-based verification techniques. Additionally, we highlight domain-specific challenges in various domains, such as software engineering, mathematical reasoning and proving, data analysis and modeling, and scientific research. The paper further discusses the fundamental limitations of the current LLM solutions and the future directions of LLM-based complex problems solving from the perspective of multi-step reasoning, domain knowledge integration and result verification.",
    "authors": [
      "Da Zheng",
      "Lun Du",
      "Junwei Su",
      "Yuchen Tian",
      "Yuqi Zhu",
      "Jintian Zhang",
      "Lanning Wei",
      "Ningyu Zhang",
      "Huajun Chen"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-05-06T10:53:58Z",
    "pdf_url": "https://arxiv.org/pdf/2505.03418v1"
  },
  {
    "arxiv_id": "2505.03332v4",
    "entry_id": "http://arxiv.org/abs/2505.03332v4",
    "title": "AI-Driven Scholarly Peer Review via Persistent Workflow Prompting, Meta-Prompting, and Meta-Reasoning",
    "summary": "Critical peer review of scientific manuscripts presents a significant challenge for Large Language Models (LLMs), partly due to data limitations and the complexity of expert reasoning. This report introduces Persistent Workflow Prompting (PWP), a potentially broadly applicable prompt engineering methodology designed to bridge this gap using standard LLM chat interfaces (zero-code, no APIs). We present a proof-of-concept PWP prompt for the critical analysis of experimental chemistry manuscripts, featuring a hierarchical, modular architecture (structured via Markdown) that defines detailed analysis workflows. We develop this PWP prompt through iterative application of meta-prompting techniques and meta-reasoning aimed at systematically codifying expert review workflows, including tacit knowledge. Submitted once at the start of a session, this PWP prompt equips the LLM with persistent workflows triggered by subsequent queries, guiding modern reasoning LLMs through systematic, multimodal evaluations. Demonstrations show the PWP-guided LLM identifying major methodological flaws in a test case while mitigating LLM input bias and performing complex tasks, including distinguishing claims from evidence, integrating text/photo/figure analysis to infer parameters, executing quantitative feasibility checks, comparing estimates against claims, and assessing a priori plausibility. To ensure transparency and facilitate replication, we provide full prompts, detailed demonstration analyses, and logs of interactive chats as supplementary resources. Beyond the specific application, this work offers insights into the meta-development process itself, highlighting the potential of PWP, informed by detailed workflow formalization, to enable sophisticated analysis using readily available LLMs for complex scientific tasks.",
    "authors": [
      "Evgeny Markhasin"
    ],
    "categories": [
      "cs.AI",
      "physics.chem-ph"
    ],
    "published": "2025-05-06T09:06:18Z",
    "pdf_url": "https://arxiv.org/pdf/2505.03332v4"
  },
  {
    "arxiv_id": "2505.03196v1",
    "entry_id": "http://arxiv.org/abs/2505.03196v1",
    "title": "A Trustworthy Multi-LLM Network: Challenges,Solutions, and A Use Case",
    "summary": "Large Language Models (LLMs) demonstrate strong potential across a variety of tasks in communications and networking due to their advanced reasoning capabilities. However, because different LLMs have different model structures and are trained using distinct corpora and methods, they may offer varying optimization strategies for the same network issues. Moreover, the limitations of an individual LLM's training data, aggravated by the potential maliciousness of its hosting device, can result in responses with low confidence or even bias. To address these challenges, we propose a blockchain-enabled collaborative framework that connects multiple LLMs into a Trustworthy Multi-LLM Network (MultiLLMN). This architecture enables the cooperative evaluation and selection of the most reliable and high-quality responses to complex network optimization problems. Specifically, we begin by reviewing related work and highlighting the limitations of existing LLMs in collaboration and trust, emphasizing the need for trustworthiness in LLM-based systems. We then introduce the workflow and design of the proposed Trustworthy MultiLLMN framework. Given the severity of False Base Station (FBS) attacks in B5G and 6G communication systems and the difficulty of addressing such threats through traditional modeling techniques, we present FBS defense as a case study to empirically validate the effectiveness of our approach. Finally, we outline promising future research directions in this emerging area.",
    "authors": [
      "Haoxiang Luo",
      "Gang Sun",
      "Yinqiu Liu",
      "Dusit Niyato",
      "Hongfang Yu",
      "Mohammed Atiquzzaman",
      "Schahram Dustdar"
    ],
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "published": "2025-05-06T05:32:46Z",
    "pdf_url": "https://arxiv.org/pdf/2505.03196v1"
  },
  {
    "arxiv_id": "2505.03049v2",
    "entry_id": "http://arxiv.org/abs/2505.03049v2",
    "title": "34 Examples of LLM Applications in Materials Science and Chemistry: Towards Automation, Assistants, Agents, and Accelerated Scientific Discovery",
    "summary": "Large Language Models (LLMs) are reshaping many aspects of materials science and chemistry research, enabling advances in molecular property prediction, materials design, scientific automation, knowledge extraction, and more. Recent developments demonstrate that the latest class of models are able to integrate structured and unstructured data, assist in hypothesis generation, and streamline research workflows. To explore the frontier of LLM capabilities across the research lifecycle, we review applications of LLMs through 34 total projects developed during the second annual Large Language Model Hackathon for Applications in Materials Science and Chemistry, a global hybrid event. These projects spanned seven key research areas: (1) molecular and material property prediction, (2) molecular and material design, (3) automation and novel interfaces, (4) scientific communication and education, (5) research data management and automation, (6) hypothesis generation and evaluation, and (7) knowledge extraction and reasoning from the scientific literature. Collectively, these applications illustrate how LLMs serve as versatile predictive models, platforms for rapid prototyping of domain-specific tools, and much more. In particular, improvements in both open source and proprietary LLM performance through the addition of reasoning, additional training data, and new techniques have expanded effectiveness, particularly in low-data environments and interdisciplinary research. As LLMs continue to improve, their integration into scientific workflows presents both new opportunities and new challenges, requiring ongoing exploration, continued refinement, and further research to address reliability, interpretability, and reproducibility.",
    "authors": [
      "Yoel Zimmermann",
      "Adib Bazgir",
      "Alexander Al-Feghali",
      "Mehrad Ansari",
      "Joshua Bocarsly",
      "L. Catherine Brinson",
      "Yuan Chiang",
      "Defne Circi",
      "Min-Hsueh Chiu",
      "Nathan Daelman",
      "Matthew L. Evans",
      "Abhijeet S. Gangan",
      "Janine George",
      "Hassan Harb",
      "Ghazal Khalighinejad",
      "Sartaaj Takrim Khan",
      "Sascha Klawohn",
      "Magdalena Lederbauer",
      "Soroush Mahjoubi",
      "Bernadette Mohr",
      "Seyed Mohamad Moosavi",
      "Aakash Naik",
      "Aleyna Beste Ozhan",
      "Dieter Plessers",
      "Aritra Roy",
      "Fabian Schöppach",
      "Philippe Schwaller",
      "Carla Terboven",
      "Katharina Ueltzen",
      "Yue Wu",
      "Shang Zhu",
      "Jan Janssen",
      "Calvin Li",
      "Ian Foster",
      "Ben Blaiszik"
    ],
    "categories": [
      "cs.LG",
      "cond-mat.mtrl-sci"
    ],
    "published": "2025-05-05T22:08:37Z",
    "pdf_url": "https://arxiv.org/pdf/2505.03049v2"
  },
  {
    "arxiv_id": "2505.02763v1",
    "entry_id": "http://arxiv.org/abs/2505.02763v1",
    "title": "Bye-bye, Bluebook? Automating Legal Procedure with Large Language Models",
    "summary": "Legal practice requires careful adherence to procedural rules. In the United States, few are more complex than those found in The Bluebook: A Uniform System of Citation. Compliance with this system's 500+ pages of byzantine formatting instructions is the raison d'etre of thousands of student law review editors and the bete noire of lawyers everywhere. To evaluate whether large language models (LLMs) are able to adhere to the procedures of such a complicated system, we construct an original dataset of 866 Bluebook tasks and test flagship LLMs from OpenAI, Anthropic, Google, Meta, and DeepSeek. We show (1) that these models produce fully compliant Bluebook citations only 69%-74% of the time and (2) that in-context learning on the Bluebook's underlying system of rules raises accuracy only to 77%. These results caution against using off-the-shelf LLMs to automate aspects of the law where fidelity to procedure is paramount.",
    "authors": [
      "Matthew Dahl"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-05-05T16:18:07Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02763v1"
  },
  {
    "arxiv_id": "2505.02665v2",
    "entry_id": "http://arxiv.org/abs/2505.02665v2",
    "title": "A Survey of Slow Thinking-based Reasoning LLMs using Reinforced Learning and Inference-time Scaling Law",
    "summary": "This survey explores recent advancements in reasoning large language models (LLMs) designed to mimic \"slow thinking\" - a reasoning process inspired by human cognition, as described in Kahneman's Thinking, Fast and Slow. These models, like OpenAI's o1, focus on scaling computational resources dynamically during complex tasks, such as math reasoning, visual reasoning, medical diagnosis, and multi-agent debates. We present the development of reasoning LLMs and list their key technologies. By synthesizing over 100 studies, it charts a path toward LLMs that combine human-like deep thinking with scalable efficiency for reasoning. The review breaks down methods into three categories: (1) test-time scaling dynamically adjusts computation based on task complexity via search and sampling, dynamic verification; (2) reinforced learning refines decision-making through iterative improvement leveraging policy networks, reward models, and self-evolution strategies; and (3) slow-thinking frameworks (e.g., long CoT, hierarchical processes) that structure problem-solving with manageable steps. The survey highlights the challenges and further directions of this domain. Understanding and advancing the reasoning abilities of LLMs is crucial for unlocking their full potential in real-world applications, from scientific discovery to decision support systems.",
    "authors": [
      "Qianjun Pan",
      "Wenkai Ji",
      "Yuyang Ding",
      "Junsong Li",
      "Shilian Chen",
      "Junyi Wang",
      "Jie Zhou",
      "Qin Chen",
      "Min Zhang",
      "Yulan Wu",
      "Liang He"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-05T14:14:59Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02665v2"
  },
  {
    "arxiv_id": "2505.02583v1",
    "entry_id": "http://arxiv.org/abs/2505.02583v1",
    "title": "Towards Cross-Modality Modeling for Time Series Analytics: A Survey in the LLM Era",
    "summary": "The proliferation of edge devices has generated an unprecedented volume of time series data across different domains, motivating various well-customized methods. Recently, Large Language Models (LLMs) have emerged as a new paradigm for time series analytics by leveraging the shared sequential nature of textual data and time series. However, a fundamental cross-modality gap between time series and LLMs exists, as LLMs are pre-trained on textual corpora and are not inherently optimized for time series. Many recent proposals are designed to address this issue. In this survey, we provide an up-to-date overview of LLMs-based cross-modality modeling for time series analytics. We first introduce a taxonomy that classifies existing approaches into four groups based on the type of textual data employed for time series modeling. We then summarize key cross-modality strategies, e.g., alignment and fusion, and discuss their applications across a range of downstream tasks. Furthermore, we conduct experiments on multimodal datasets from different application domains to investigate effective combinations of textual data and cross-modality strategies for enhancing time series analytics. Finally, we suggest several promising directions for future research. This survey is designed for a range of professionals, researchers, and practitioners interested in LLM-based time series modeling.",
    "authors": [
      "Chenxi Liu",
      "Shaowen Zhou",
      "Qianxiong Xu",
      "Hao Miao",
      "Cheng Long",
      "Ziyue Li",
      "Rui Zhao"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-05-05T11:35:33Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02583v1"
  },
  {
    "arxiv_id": "2505.02489v1",
    "entry_id": "http://arxiv.org/abs/2505.02489v1",
    "title": "Beyond the model: Key differentiators in large language models and multi-agent services",
    "summary": "With the launch of foundation models like DeepSeek, Manus AI, and Llama 4, it has become evident that large language models (LLMs) are no longer the sole defining factor in generative AI. As many now operate at comparable levels of capability, the real race is not about having the biggest model but optimizing the surrounding ecosystem, including data quality and management, computational efficiency, latency, and evaluation frameworks. This review article delves into these critical differentiators that ensure modern AI services are efficient and profitable.",
    "authors": [
      "Muskaan Goyal",
      "Pranav Bhasin"
    ],
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.MA",
      "cs.SE"
    ],
    "published": "2025-05-05T09:15:31Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02489v1"
  },
  {
    "arxiv_id": "2505.07838v1",
    "entry_id": "http://arxiv.org/abs/2505.07838v1",
    "title": "Moving From Monolithic To Microservices Architecture for Multi-Agent Systems",
    "summary": "The transition from monolithic to microservices architecture revolutionized software development by improving scalability and maintainability. This paradigm shift is now becoming relevant for complex multi-agent systems (MAS). This review article explores the evolution from monolithic architecture to microservices architecture in the specific context of MAS. It will highlight the limitations of traditional monolithic MAS and the benefits of adopting a microservices-based approach. The article further examines the core architectural principles and communication protocols, including Agent Communication Languages (ACLs), the Model Context Protocol (MCP), and the Application-to-Application (A2A) protocol. The article identifies emerging architectural patterns, design challenges, and considerations through a comparative lens of the paradigm shift.",
    "authors": [
      "Muskaan Goyal",
      "Pranav Bhasin"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "published": "2025-05-05T09:10:46Z",
    "pdf_url": "https://arxiv.org/pdf/2505.07838v1"
  },
  {
    "arxiv_id": "2505.02309v2",
    "entry_id": "http://arxiv.org/abs/2505.02309v2",
    "title": "Optimizing LLMs for Resource-Constrained Environments: A Survey of Model Compression Techniques",
    "summary": "Large Language Models (LLMs) have revolutionized many areas of artificial intelligence (AI), but their substantial resource requirements limit their deployment on mobile and edge devices. This survey paper provides a comprehensive overview of techniques for compressing LLMs to enable efficient inference in resource-constrained environments. We examine three primary approaches: Knowledge Distillation, Model Quantization, and Model Pruning. For each technique, we discuss the underlying principles, present different variants, and provide examples of successful applications. We also briefly discuss complementary techniques such as mixture-of-experts and early-exit strategies. Finally, we highlight promising future directions, aiming to provide a valuable resource for both researchers and practitioners seeking to optimize LLMs for edge deployment.",
    "authors": [
      "Sanjay Surendranath Girija",
      "Shashank Kapoor",
      "Lakshit Arora",
      "Dipen Pradhan",
      "Aman Raj",
      "Ankit Shetgaonkar"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-05T01:27:47Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02309v2"
  },
  {
    "arxiv_id": "2505.02279v2",
    "entry_id": "http://arxiv.org/abs/2505.02279v2",
    "title": "A survey of agent interoperability protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP)",
    "summary": "Large language model powered autonomous agents demand robust, standardized protocols to integrate tools, share contextual data, and coordinate tasks across heterogeneous systems. Ad-hoc integrations are difficult to scale, secure, and generalize across domains. This survey examines four emerging agent communication protocols: Model Context Protocol (MCP), Agent Communication Protocol (ACP), Agent-to-Agent Protocol (A2A), and Agent Network Protocol (ANP), each addressing interoperability in deployment contexts. MCP provides a JSON-RPC client-server interface for secure tool invocation and typed data exchange. ACP defines a general-purpose communication protocol over RESTful HTTP, supporting MIME-typed multipart messages and synchronous and asynchronous interactions. Its lightweight and runtime-independent design enables scalable agent invocation, while features like session management, message routing, and integration with role-based and decentralized identifiers (DIDs). A2A enables peer-to-peer task delegation using capability-based Agent Cards, supporting secure and scalable collaboration across enterprise agent workflows. ANP supports open network agent discovery and secure collaboration using W3C decentralized identifiers DIDs and JSON-LD graphs. The protocols are compared across multiple dimensions, including interaction modes, discovery mechanisms, communication patterns, and security models. Based on the comparative analysis, a phased adoption roadmap is proposed: beginning with MCP for tool access, followed by ACP for structured, multimodal messaging session-aware interaction and both online and offline agent discovery across scalable, HTTP-based deployments A2A for collaborative task execution, and extending to ANP for decentralized agent marketplaces. This work provides a comprehensive foundation for designing secure, interoperable, and scalable ecosystems of LLM-powered agents.",
    "authors": [
      "Abul Ehtesham",
      "Aditi Singh",
      "Gaurav Kumar Gupta",
      "Saket Kumar"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-05-04T22:18:27Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02279v2"
  },
  {
    "arxiv_id": "2505.02077v1",
    "entry_id": "http://arxiv.org/abs/2505.02077v1",
    "title": "Open Challenges in Multi-Agent Security: Towards Secure Systems of Interacting AI Agents",
    "summary": "Decentralized AI agents will soon interact across internet platforms, creating security challenges beyond traditional cybersecurity and AI safety frameworks. Free-form protocols are essential for AI's task generalization but enable new threats like secret collusion and coordinated swarm attacks. Network effects can rapidly spread privacy breaches, disinformation, jailbreaks, and data poisoning, while multi-agent dispersion and stealth optimization help adversaries evade oversightcreating novel persistent threats at a systemic level. Despite their critical importance, these security challenges remain understudied, with research fragmented across disparate fields including AI security, multi-agent learning, complex systems, cybersecurity, game theory, distributed systems, and technical AI governance. We introduce \\textbf{multi-agent security}, a new field dedicated to securing networks of decentralized AI agents against threats that emerge or amplify through their interactionswhether direct or indirect via shared environmentswith each other, humans, and institutions, and characterize fundamental security-performance trade-offs. Our preliminary work (1) taxonomizes the threat landscape arising from interacting AI agents, (2) surveys security-performance tradeoffs in decentralized AI systems, and (3) proposes a unified research agenda addressing open challenges in designing secure agent systems and interaction environments. By identifying these gaps, we aim to guide research in this critical area to unlock the socioeconomic potential of large-scale agent deployment on the internet, foster public trust, and mitigate national security risks in critical infrastructure and defense contexts.",
    "authors": [
      "Christian Schroeder de Witt"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-05-04T12:03:29Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02077v1"
  },
  {
    "arxiv_id": "2505.03829v1",
    "entry_id": "http://arxiv.org/abs/2505.03829v1",
    "title": "VideoLLM Benchmarks and Evaluation: A Survey",
    "summary": "The rapid development of Large Language Models (LLMs) has catalyzed significant advancements in video understanding technologies. This survey provides a comprehensive analysis of benchmarks and evaluation methodologies specifically designed or used for Video Large Language Models (VideoLLMs). We examine the current landscape of video understanding benchmarks, discussing their characteristics, evaluation protocols, and limitations. The paper analyzes various evaluation methodologies, including closed-set, open-set, and specialized evaluations for temporal and spatiotemporal understanding tasks. We highlight the performance trends of state-of-the-art VideoLLMs across these benchmarks and identify key challenges in current evaluation frameworks. Additionally, we propose future research directions to enhance benchmark design, evaluation metrics, and protocols, including the need for more diverse, multimodal, and interpretability-focused benchmarks. This survey aims to equip researchers with a structured understanding of how to effectively evaluate VideoLLMs and identify promising avenues for advancing the field of video understanding with large language models.",
    "authors": [
      "Yogesh Kumar"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-05-03T20:56:09Z",
    "pdf_url": "https://arxiv.org/pdf/2505.03829v1"
  },
  {
    "arxiv_id": "2505.01821v4",
    "entry_id": "http://arxiv.org/abs/2505.01821v4",
    "title": "Edge-Cloud Collaborative Computing on Distributed Intelligence and Model Optimization: A Survey",
    "summary": "Edge-cloud collaborative computing (ECCC) has emerged as a pivotal paradigm for addressing the computational demands of modern intelligent applications, integrating cloud resources with edge devices to enable efficient, low-latency processing. Recent advancements in AI, particularly deep learning and large language models (LLMs), have dramatically enhanced the capabilities of these distributed systems, yet introduce significant challenges in model deployment and resource management. In this survey, we comprehensive examine the intersection of distributed intelligence and model optimization within edge-cloud environments, providing a structured tutorial on fundamental architectures, enabling technologies, and emerging applications. Additionally, we systematically analyze model optimization approaches, including compression, adaptation, and neural architecture search, alongside AI-driven resource management strategies that balance performance, energy efficiency, and latency requirements. We further explore critical aspects of privacy protection and security enhancement within ECCC systems and examines practical deployments through diverse applications, spanning autonomous driving, healthcare, and industrial automation. Performance analysis and benchmarking techniques are also thoroughly explored to establish evaluation standards for these complex systems. Furthermore, the review identifies critical research directions including LLMs deployment, 6G integration, neuromorphic computing, and quantum computing, offering a roadmap for addressing persistent challenges in heterogeneity management, real-time processing, and scalability. By bridging theoretical advancements and practical deployments, this survey offers researchers and practitioners a holistic perspective on leveraging AI to optimize distributed computing environments, fostering innovation in next-generation intelligent systems.",
    "authors": [
      "Jing Liu",
      "Yao Du",
      "Kun Yang",
      "Jiaqi Wu",
      "Yan Wang",
      "Xiping Hu",
      "Zehua Wang",
      "Yang Liu",
      "Peng Sun",
      "Azzedine Boukerche",
      "Victor C. M. Leung"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-05-03T13:55:38Z",
    "pdf_url": "https://arxiv.org/pdf/2505.01821v4"
  },
  {
    "arxiv_id": "2505.17024v1",
    "entry_id": "http://arxiv.org/abs/2505.17024v1",
    "title": "An Affective-Taxis Hypothesis for Alignment and Interpretability",
    "summary": "AI alignment is a field of research that aims to develop methods to ensure that agents always behave in a manner aligned with (i.e. consistently with) the goals and values of their human operators, no matter their level of capability. This paper proposes an affectivist approach to the alignment problem, re-framing the concepts of goals and values in terms of affective taxis, and explaining the emergence of affective valence by appealing to recent work in evolutionary-developmental and computational neuroscience. We review the state of the art and, building on this work, we propose a computational model of affect based on taxis navigation. We discuss evidence in a tractable model organism that our model reflects aspects of biological taxis navigation. We conclude with a discussion of the role of affective taxis in AI alignment.",
    "authors": [
      "Eli Sennesh",
      "Maxwell Ramstead"
    ],
    "categories": [
      "cs.AI",
      "q-bio.NC"
    ],
    "published": "2025-05-03T04:49:01Z",
    "pdf_url": "https://arxiv.org/pdf/2505.17024v1"
  },
  {
    "arxiv_id": "2506.11015v2",
    "entry_id": "http://arxiv.org/abs/2506.11015v2",
    "title": "The Memory Paradox: Why Our Brains Need Knowledge in an Age of AI",
    "summary": "In the age of generative AI and ubiquitous digital tools, human cognition faces a structural paradox: as external aids become more capable, internal memory systems risk atrophy. Drawing on neuroscience and cognitive psychology, this paper examines how heavy reliance on AI systems and discovery-based pedagogies may impair the consolidation of declarative and procedural memory -- systems essential for expertise, critical thinking, and long-term retention. We review how tools like ChatGPT and calculators can short-circuit the retrieval, error correction, and schema-building processes necessary for robust neural encoding. Notably, we highlight striking parallels between deep learning phenomena such as \"grokking\" and the neuroscience of overlearning and intuition. Empirical studies are discussed showing how premature reliance on AI during learning inhibits proceduralization and intuitive mastery. We argue that effective human-AI interaction depends on strong internal models -- biological \"schemata\" and neural manifolds -- that enable users to evaluate, refine, and guide AI output. The paper concludes with policy implications for education and workforce training in the age of large language models.",
    "authors": [
      "Barbara Oakley",
      "Michael Johnston",
      "Ken-Zen Chen",
      "Eulho Jung",
      "Terrence J. Sejnowski"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "q-bio.NC"
    ],
    "published": "2025-05-03T03:41:33Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11015v2"
  },
  {
    "arxiv_id": "2505.01542v1",
    "entry_id": "http://arxiv.org/abs/2505.01542v1",
    "title": "Emotions in the Loop: A Survey of Affective Computing for Emotional Support",
    "summary": "In a world where technology is increasingly embedded in our everyday experiences, systems that sense and respond to human emotions are elevating digital interaction. At the intersection of artificial intelligence and human-computer interaction, affective computing is emerging with innovative solutions where machines are humanized by enabling them to process and respond to user emotions. This survey paper explores recent research contributions in affective computing applications in the area of emotion recognition, sentiment analysis and personality assignment developed using approaches like large language models (LLMs), multimodal techniques, and personalized AI systems. We analyze the key contributions and innovative methodologies applied by the selected research papers by categorizing them into four domains: AI chatbot applications, multimodal input systems, mental health and therapy applications, and affective computing for safety applications. We then highlight the technological strengths as well as the research gaps and challenges related to these studies. Furthermore, the paper examines the datasets used in each study, highlighting how modality, scale, and diversity impact the development and performance of affective models. Finally, the survey outlines ethical considerations and proposes future directions to develop applications that are more safe, empathetic and practical.",
    "authors": [
      "Karishma Hegde",
      "Hemadri Jayalath"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-05-02T19:06:05Z",
    "pdf_url": "https://arxiv.org/pdf/2505.01542v1"
  },
  {
    "arxiv_id": "2507.17754v1",
    "entry_id": "http://arxiv.org/abs/2507.17754v1",
    "title": "A Custom-Built Ambient Scribe Reduces Cognitive Load and Documentation Burden for Telehealth Clinicians",
    "summary": "Clinician burnout has motivated the growing adoption of ambient medical scribes in the clinic. In this work, we introduce a custom-built ambient scribe application integrated into the EHR system at Included Health, a personalized all-in-one healthcare company offering telehealth services. The application uses Whisper for transcription and a modular in-context learning pipeline with GPT-4o to automatically generate SOAP notes and patient instructions. Testing on mock visit data shows that the notes generated by the application exceed the quality of expert-written notes as determined by an LLM-as-a-judge. The application has been widely adopted by the clinical practice, with over 540 clinicians at Included Health using the application at least once. 94% (n = 63) of surveyed clinicians report reduced cognitive load during visits and 97% (n = 66) report less documentation burden when using the application. Additionally, we show that post-processing notes with a fine-tuned BART model improves conciseness. These findings highlight the potential for AI systems to ease administrative burdens and support clinicians in delivering efficient, high-quality care.",
    "authors": [
      "Justin Morse",
      "Kurt Gilbert",
      "Kyle Shin",
      "Rick Cooke",
      "Peyton Rose",
      "Jack Sullivan",
      "Angelo Sisante"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-05-02T14:10:03Z",
    "pdf_url": "https://arxiv.org/pdf/2507.17754v1"
  },
  {
    "arxiv_id": "2505.01177v1",
    "entry_id": "http://arxiv.org/abs/2505.01177v1",
    "title": "LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures",
    "summary": "As large language models (LLMs) continue to evolve, it is critical to assess the security threats and vulnerabilities that may arise both during their training phase and after models have been deployed. This survey seeks to define and categorize the various attacks targeting LLMs, distinguishing between those that occur during the training phase and those that affect already trained models. A thorough analysis of these attacks is presented, alongside an exploration of defense mechanisms designed to mitigate such threats. Defenses are classified into two primary categories: prevention-based and detection-based defenses. Furthermore, our survey summarizes possible attacks and their corresponding defense strategies. It also provides an evaluation of the effectiveness of the known defense mechanisms for the different security threats. Our survey aims to offer a structured framework for securing LLMs, while also identifying areas that require further research to improve and strengthen defenses against emerging security challenges.",
    "authors": [
      "Francisco Aguilera-Martínez",
      "Fernando Berzal"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2025-05-02T10:35:26Z",
    "pdf_url": "https://arxiv.org/pdf/2505.01177v1"
  },
  {
    "arxiv_id": "2505.02851v2",
    "entry_id": "http://arxiv.org/abs/2505.02851v2",
    "title": "Leveraging LLMs to Create Content Corpora for Niche Domains",
    "summary": "Constructing specialized content corpora from vast, unstructured web sources for domain-specific applications poses substantial data curation challenges. In this paper, we introduce a streamlined approach for generating high-quality, domain-specific corpora by efficiently acquiring, filtering, structuring, and cleaning web-based data. We showcase how Large Language Models (LLMs) can be leveraged to address complex data curation at scale, and propose a strategical framework incorporating LLM-enhanced techniques for structured content extraction and semantic deduplication. We validate our approach in the behavior education domain through its integration into 30 Day Me, a habit formation application. Our data pipeline, named 30DayGen, enabled the extraction and synthesis of 3,531 unique 30-day challenges from over 15K webpages. A user survey reports a satisfaction score of 4.3 out of 5, with 91% of respondents indicating willingness to use the curated content for their habit-formation goals.",
    "authors": [
      "Franklin Zhang",
      "Sonya Zhang",
      "Alon Halevy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-05-02T08:53:27Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02851v2"
  },
  {
    "arxiv_id": "2505.01085v1",
    "entry_id": "http://arxiv.org/abs/2505.01085v1",
    "title": "Artificial Intelligence in Government: Why People Feel They Lose Control",
    "summary": "The use of Artificial Intelligence (AI) in public administration is expanding rapidly, moving from automating routine tasks to deploying generative and agentic systems that autonomously act on goals. While AI promises greater efficiency and responsiveness, its integration into government functions raises concerns about fairness, transparency, and accountability. This article applies principal-agent theory (PAT) to conceptualize AI adoption as a special case of delegation, highlighting three core tensions: assessability (can decisions be understood?), dependency (can the delegation be reversed?), and contestability (can decisions be challenged?). These structural challenges may lead to a \"failure-by-success\" dynamic, where early functional gains obscure long-term risks to democratic legitimacy. To test this framework, we conducted a pre-registered factorial survey experiment across tax, welfare, and law enforcement domains. Our findings show that although efficiency gains initially bolster trust, they simultaneously reduce citizens' perceived control. When the structural risks come to the foreground, institutional trust and perceived control both drop sharply, suggesting that hidden costs of AI adoption significantly shape public attitudes. The study demonstrates that PAT offers a powerful lens for understanding the institutional and political implications of AI in government, emphasizing the need for policymakers to address delegation risks transparently to maintain public trust.",
    "authors": [
      "Alexander Wuttke",
      "Adrian Rauchfleisch",
      "Andreas Jungherr"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-05-02T07:46:41Z",
    "pdf_url": "https://arxiv.org/pdf/2505.01085v1"
  },
  {
    "arxiv_id": "2505.01043v1",
    "entry_id": "http://arxiv.org/abs/2505.01043v1",
    "title": "Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities",
    "summary": "Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several components$\\unicode{x2013}$such as weights, activations, and gradients$\\unicode{x2013}$each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.",
    "authors": [
      "Zhiwei Hao",
      "Jianyuan Guo",
      "Li Shen",
      "Yong Luo",
      "Han Hu",
      "Guoxia Wang",
      "Dianhai Yu",
      "Yonggang Wen",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-05-02T06:33:25Z",
    "pdf_url": "https://arxiv.org/pdf/2505.01043v1"
  },
  {
    "arxiv_id": "2505.00976v1",
    "entry_id": "http://arxiv.org/abs/2505.00976v1",
    "title": "Attack and defense techniques in large language models: A survey and new perspectives",
    "summary": "Large Language Models (LLMs) have become central to numerous natural language processing tasks, but their vulnerabilities present significant security and ethical challenges. This systematic survey explores the evolving landscape of attack and defense techniques in LLMs. We classify attacks into adversarial prompt attack, optimized attacks, model theft, as well as attacks on application of LLMs, detailing their mechanisms and implications. Consequently, we analyze defense strategies, including prevention-based and detection-based defense methods. Although advances have been made, challenges remain to adapt to the dynamic threat landscape, balance usability with robustness, and address resource constraints in defense implementation. We highlight open problems, including the need for adaptive scalable defenses, explainable security techniques, and standardized evaluation frameworks. This survey provides actionable insights and directions for developing secure and resilient LLMs, emphasizing the importance of interdisciplinary collaboration and ethical considerations to mitigate risks in real-world applications.",
    "authors": [
      "Zhiyu Liao",
      "Kang Chen",
      "Yuanguo Lin",
      "Kangkang Li",
      "Yunxuan Liu",
      "Hefeng Chen",
      "Xingwang Huang",
      "Yuanhui Yu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-05-02T03:37:52Z",
    "pdf_url": "https://arxiv.org/pdf/2505.00976v1"
  },
  {
    "arxiv_id": "2505.02848v1",
    "entry_id": "http://arxiv.org/abs/2505.02848v1",
    "title": "Aligning Large Language Models with Healthcare Stakeholders: A Pathway to Trustworthy AI Integration",
    "summary": "The wide exploration of large language models (LLMs) raises the awareness of alignment between healthcare stakeholder preferences and model outputs. This alignment becomes a crucial foundation to empower the healthcare workflow effectively, safely, and responsibly. Yet the varying behaviors of LLMs may not always match with healthcare stakeholders' knowledge, demands, and values. To enable a human-AI alignment, healthcare stakeholders will need to perform essential roles in guiding and enhancing the performance of LLMs. Human professionals must participate in the entire life cycle of adopting LLM in healthcare, including training data curation, model training, and inference. In this review, we discuss the approaches, tools, and applications of alignments between healthcare stakeholders and LLMs. We demonstrate that LLMs can better follow human values by properly enhancing healthcare knowledge integration, task understanding, and human guidance. We provide outlooks on enhancing the alignment between humans and LLMs to build trustworthy real-world healthcare applications.",
    "authors": [
      "Kexin Ding",
      "Mu Zhou",
      "Akshay Chaudhari",
      "Shaoting Zhang",
      "Dimitris N. Metaxas"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-05-02T00:59:49Z",
    "pdf_url": "https://arxiv.org/pdf/2505.02848v1"
  },
  {
    "arxiv_id": "2505.01458v1",
    "entry_id": "http://arxiv.org/abs/2505.01458v1",
    "title": "A Survey of Robotic Navigation and Manipulation with Physics Simulators in the Era of Embodied AI",
    "summary": "Navigation and manipulation are core capabilities in Embodied AI, yet training agents with these capabilities in the real world faces high costs and time complexity. Therefore, sim-to-real transfer has emerged as a key approach, yet the sim-to-real gap persists. This survey examines how physics simulators address this gap by analyzing their properties overlooked in previous surveys. We also analyze their features for navigation and manipulation tasks, along with hardware requirements. Additionally, we offer a resource with benchmark datasets, metrics, simulation platforms, and cutting-edge methods-such as world models and geometric equivariance-to help researchers select suitable tools while accounting for hardware constraints.",
    "authors": [
      "Lik Hang Kenny Wong",
      "Xueyang Kang",
      "Kaixin Bai",
      "Jianwei Zhang"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-05-01T09:22:23Z",
    "pdf_url": "https://arxiv.org/pdf/2505.01458v1"
  },
  {
    "arxiv_id": "2505.00753v4",
    "entry_id": "http://arxiv.org/abs/2505.00753v4",
    "title": "LLM-Based Human-Agent Collaboration and Interaction Systems: A Survey",
    "summary": "Recent advances in large language models (LLMs) have sparked growing interest in building fully autonomous agents. However, fully autonomous LLM-based agents still face significant challenges, including limited reliability due to hallucinations, difficulty in handling complex tasks, and substantial safety and ethical risks, all of which limit their feasibility and trustworthiness in real-world applications. To overcome these limitations, LLM-based human-agent systems (LLM-HAS) incorporate human-provided information, feedback, or control into the agent system to enhance system performance, reliability and safety. These human-agent collaboration systems enable humans and LLM-based agents to collaborate effectively by leveraging their complementary strengths. This paper provides the first comprehensive and structured survey of LLM-HAS. It clarifies fundamental concepts, systematically presents core components shaping these systems, including environment & profiling, human feedback, interaction types, orchestration and communication, explores emerging applications, and discusses unique challenges and opportunities arising from human-AI collaboration. By consolidating current knowledge and offering a structured overview, we aim to foster further research and innovation in this rapidly evolving interdisciplinary field. Paper lists and resources are available at https://github.com/HenryPengZou/Awesome-Human-Agent-Collaboration-Interaction-Systems.",
    "authors": [
      "Henry Peng Zou",
      "Wei-Chieh Huang",
      "Yaozu Wu",
      "Yankai Chen",
      "Chunyu Miao",
      "Hoang Nguyen",
      "Yue Zhou",
      "Weizhi Zhang",
      "Liancheng Fang",
      "Langzhou He",
      "Yangning Li",
      "Dongyuan Li",
      "Renhe Jiang",
      "Xue Liu",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-05-01T08:29:26Z",
    "pdf_url": "https://arxiv.org/pdf/2505.00753v4"
  },
  {
    "arxiv_id": "2505.00747v1",
    "entry_id": "http://arxiv.org/abs/2505.00747v1",
    "title": "Wireless Communication as an Information Sensor for Multi-agent Cooperative Perception: A Survey",
    "summary": "Cooperative perception extends the perception capabilities of autonomous vehicles by enabling multi-agent information sharing via Vehicle-to-Everything (V2X) communication. Unlike traditional onboard sensors, V2X acts as a dynamic \"information sensor\" characterized by limited communication, heterogeneity, mobility, and scalability. This survey provides a comprehensive review of recent advancements from the perspective of information-centric cooperative perception, focusing on three key dimensions: information representation, information fusion, and large-scale deployment. We categorize information representation into data-level, feature-level, and object-level schemes, and highlight emerging methods for reducing data volume and compressing messages under communication constraints. In information fusion, we explore techniques under both ideal and non-ideal conditions, including those addressing heterogeneity, localization errors, latency, and packet loss. Finally, we summarize system-level approaches to support scalability in dense traffic scenarios. Compared with existing surveys, this paper introduces a new perspective by treating V2X communication as an information sensor and emphasizing the challenges of deploying cooperative perception in real-world intelligent transportation systems.",
    "authors": [
      "Zhiying Song",
      "Tenghui Xie",
      "Fuxi Wen",
      "Jun Li"
    ],
    "categories": [
      "cs.OH",
      "cs.CV",
      "cs.MA",
      "cs.RO"
    ],
    "published": "2025-04-30T12:23:57Z",
    "pdf_url": "https://arxiv.org/pdf/2505.00747v1"
  },
  {
    "arxiv_id": "2505.00049v1",
    "entry_id": "http://arxiv.org/abs/2505.00049v1",
    "title": "Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications",
    "summary": "As large language models (LLMs) are increasingly used in human-centered tasks, assessing their psychological traits is crucial for understanding their social impact and ensuring trustworthy AI alignment. While existing reviews have covered some aspects of related research, several important areas have not been systematically discussed, including detailed discussions of diverse psychological tests, LLM-specific psychological datasets, and the applications of LLMs with psychological traits. To address this gap, we systematically review six key dimensions of applying psychological theories to LLMs: (1) assessment tools; (2) LLM-specific datasets; (3) evaluation metrics (consistency and stability); (4) empirical findings; (5) personality simulation methods; and (6) LLM-based behavior simulation. Our analysis highlights both the strengths and limitations of current methods. While some LLMs exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Recognizing methodological challenges such as mismatches between psychological tools and LLMs' capabilities, as well as inconsistencies in evaluation practices, this study aims to propose future directions for developing more interpretable, robust, and generalizable psychological assessment frameworks for LLMs.",
    "authors": [
      "Wenhan Dong",
      "Yuemeng Zhao",
      "Zhen Sun",
      "Yule Liu",
      "Zifan Peng",
      "Jingyi Zheng",
      "Zongmin Zhang",
      "Ziyi Zhang",
      "Jun Wu",
      "Ruiming Wang",
      "Shengmin Xu",
      "Xinyi Huang",
      "Xinlei He"
    ],
    "categories": [
      "cs.CY",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-04-30T06:09:40Z",
    "pdf_url": "https://arxiv.org/pdf/2505.00049v1"
  },
  {
    "arxiv_id": "2504.21277v2",
    "entry_id": "http://arxiv.org/abs/2504.21277v2",
    "title": "Reinforced MLLM: A Survey on RL-Based Reasoning in Multimodal Large Language Models",
    "summary": "The application of reinforcement learning (RL) to enhance the reasoning capabilities of Multimodal Large Language Models (MLLMs) constitutes a rapidly advancing research area. While MLLMs extend Large Language Models (LLMs) to handle diverse modalities such as vision, audio, and video, enabling robust reasoning across multimodal inputs remains challenging. This paper provides a systematic review of recent advances in RL-based reasoning for MLLMs, covering key algorithmic designs, reward mechanism innovations, and practical applications. We highlight two main RL paradigms, value-model-free and value-model-based methods, and analyze how RL enhances reasoning abilities by optimizing reasoning trajectories and aligning multimodal information. Additionally, we provide an extensive overview of benchmark datasets, evaluation protocols, and current limitations, and propose future research directions to address challenges such as sparse rewards, inefficient cross-modal reasoning, and real-world deployment constraints. Our goal is to provide a comprehensive and structured guide to RL-based multimodal reasoning.",
    "authors": [
      "Guanghao Zhou",
      "Panjia Qiu",
      "Cen Chen",
      "Jie Wang",
      "Zheming Yang",
      "Jian Xu",
      "Minghui Qiu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-04-30T03:14:28Z",
    "pdf_url": "https://arxiv.org/pdf/2504.21277v2"
  },
  {
    "arxiv_id": "2504.21099v1",
    "entry_id": "http://arxiv.org/abs/2504.21099v1",
    "title": "A Survey on Parameter-Efficient Fine-Tuning for Foundation Models in Federated Learning",
    "summary": "Foundation models have revolutionized artificial intelligence by providing robust, versatile architectures pre-trained on large-scale datasets. However, adapting these massive models to specific downstream tasks requires fine-tuning, which can be prohibitively expensive in computational resources. Parameter-Efficient Fine-Tuning (PEFT) methods address this challenge by selectively updating only a small subset of parameters. Meanwhile, Federated Learning (FL) enables collaborative model training across distributed clients without sharing raw data, making it ideal for privacy-sensitive applications. This survey provides a comprehensive review of the integration of PEFT techniques within federated learning environments. We systematically categorize existing approaches into three main groups: Additive PEFT (which introduces new trainable parameters), Selective PEFT (which fine-tunes only subsets of existing parameters), and Reparameterized PEFT (which transforms model architectures to enable efficient updates). For each category, we analyze how these methods address the unique challenges of federated settings, including data heterogeneity, communication efficiency, computational constraints, and privacy concerns. We further organize the literature based on application domains, covering both natural language processing and computer vision tasks. Finally, we discuss promising research directions, including scaling to larger foundation models, theoretical analysis of federated PEFT methods, and sustainable approaches for resource-constrained environments.",
    "authors": [
      "Jieming Bian",
      "Yuanzhe Peng",
      "Lei Wang",
      "Yin Huang",
      "Jie Xu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-04-29T18:18:39Z",
    "pdf_url": "https://arxiv.org/pdf/2504.21099v1"
  },
  {
    "arxiv_id": "2504.20799v2",
    "entry_id": "http://arxiv.org/abs/2504.20799v2",
    "title": "Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges",
    "summary": "Recent technical breakthroughs in large language models (LLMs) have enabled them to fluently generate source code. Software developers often leverage both general-purpose and code-specialized LLMs to revise existing code or even generate a whole function from scratch. These capabilities are also beneficial in no-code or low-code contexts, in which one can write programs without a technical background. However, due to their internal design, LLMs are prone to generating hallucinations, which are incorrect, nonsensical, and not justifiable information but difficult to identify its presence. This problem also occurs when generating source code. Once hallucinated code is produced, it is often challenging for users to identify and fix it, especially when such hallucinations can be identified under specific execution paths. As a result, the hallucinated code may remain unnoticed within the codebase. This survey investigates recent studies and techniques relevant to hallucinations generated by CodeLLMs. We categorize the types of hallucinations in the code generated by CodeLLMs, review existing benchmarks and mitigation strategies, and identify open challenges. Based on these findings, this survey outlines further research directions in the detection and removal of hallucinations produced by CodeLLMs.",
    "authors": [
      "Yunseo Lee",
      "John Youngeun Song",
      "Dongsun Kim",
      "Jindae Kim",
      "Mijung Kim",
      "Jaechang Nam"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-04-29T14:13:57Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20799v2"
  },
  {
    "arxiv_id": "2504.20673v1",
    "entry_id": "http://arxiv.org/abs/2504.20673v1",
    "title": "CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation",
    "summary": "Large language models (LLMs) play a crucial role in software engineering, excelling in tasks like code generation and maintenance. However, existing benchmarks are often narrow in scope, focusing on a specific task and lack a comprehensive evaluation framework that reflects real-world applications. To address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark), designed to evaluate LLMs across four critical dimensions: code understanding, code generation, code modification, and code review. These dimensions capture essential developer needs, ensuring a more systematic and representative evaluation. CoCo-Bench includes multiple programming languages and varying task difficulties, with rigorous manual review to ensure data quality and accuracy. Empirical results show that CoCo-Bench aligns with existing benchmarks while uncovering significant variations in model performance, effectively highlighting strengths and weaknesses. By offering a holistic and objective evaluation, CoCo-Bench provides valuable insights to guide future research and technological advancements in code-oriented LLMs, establishing a reliable benchmark for the field.",
    "authors": [
      "Wenjing Yin",
      "Tianze Sun",
      "Yijiong Yu",
      "Jiawei Fang",
      "Guangyao Su",
      "Jiancheng Wang",
      "Zekun Wang",
      "Wei Wang",
      "Ran Chen",
      "Ziyun Dai",
      "Shuai Yuan",
      "Menghang Dong",
      "Peng Luo",
      "Dong Cao",
      "Da Lei",
      "Yajun Zhang",
      "Hao Chen",
      "Xiang Ma",
      "Yong Liu",
      "Weifeng Liu",
      "Yuanjian Xu",
      "Ji Pei"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-04-29T11:57:23Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20673v1"
  },
  {
    "arxiv_id": "2504.20612v1",
    "entry_id": "http://arxiv.org/abs/2504.20612v1",
    "title": "The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models",
    "summary": "The rapid advancement of Large Language Models (LLMs) has enhanced software development processes, minimizing the time and effort required for coding and enhancing developer productivity. However, despite their potential benefits, code generated by LLMs has been shown to generate insecure code in controlled environments, raising critical concerns about their reliability and security in real-world applications. This paper uses predefined security parameters to evaluate the security compliance of LLM-generated code across multiple models, such as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals critical vulnerabilities in authentication mechanisms, session management, input validation and HTTP security headers. Although some models implement security measures to a limited extent, none fully align with industry best practices, highlighting the associated risks in automated software development. Our findings underscore that human expertise is crucial to ensure secure software deployment or review of LLM-generated code. Also, there is a need for robust security assessment frameworks to enhance the reliability of LLM-generated code in real-world applications.",
    "authors": [
      "Swaroop Dora",
      "Deven Lunkad",
      "Naziya Aslam",
      "S. Venkatesan",
      "Sandeep Kumar Shukla"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.ET"
    ],
    "published": "2025-04-29T10:23:11Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20612v1"
  },
  {
    "arxiv_id": "2504.20464v2",
    "entry_id": "http://arxiv.org/abs/2504.20464v2",
    "title": "A Survey on GUI Agents with Foundation Models Enhanced by Reinforcement Learning",
    "summary": "Graphical User Interface (GUI) agents, driven by Multi-modal Large Language Models (MLLMs), have emerged as a promising paradigm for enabling intelligent interaction with digital systems. This paper provides a structured survey of recent advances in GUI agents, focusing on architectures enhanced by Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov Decision Processes and discuss typical execution environments and evaluation metrics. We then review the modular architecture of (M)LLM-based GUI agents, covering Perception, Planning, and Acting modules, and trace their evolution through representative works. Furthermore, we categorize GUI agent training methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and RL-based approaches, highlighting the progression from simple prompt engineering to dynamic policy learning via RL. Our summary illustrates how recent innovations in multimodal perception, decision reasoning, and adaptive action generation have significantly improved the generalization and robustness of GUI agents in complex real-world environments. We conclude by identifying key challenges and future directions for building more capable and reliable GUI agents.",
    "authors": [
      "Jiahao Li",
      "Kaer Huang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-04-29T06:55:15Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20464v2"
  },
  {
    "arxiv_id": "2504.21051v1",
    "entry_id": "http://arxiv.org/abs/2504.21051v1",
    "title": "Multimodal Large Language Models for Medicine: A Comprehensive Survey",
    "summary": "MLLMs have recently become a focal point in the field of artificial intelligence research. Building on the strong capabilities of LLMs, MLLMs are adept at addressing complex multi-modal tasks. With the release of GPT-4, MLLMs have gained substantial attention from different domains. Researchers have begun to explore the potential of MLLMs in the medical and healthcare domain. In this paper, we first introduce the background and fundamental concepts related to LLMs and MLLMs, while emphasizing the working principles of MLLMs. Subsequently, we summarize three main directions of application within healthcare: medical reporting, medical diagnosis, and medical treatment. Our findings are based on a comprehensive review of 330 recent papers in this area. We illustrate the remarkable capabilities of MLLMs in these domains by providing specific examples. For data, we present six mainstream modes of data along with their corresponding evaluation benchmarks. At the end of the survey, we discuss the challenges faced by MLLMs in the medical and healthcare domain and propose feasible methods to mitigate or overcome these issues.",
    "authors": [
      "Jiarui Ye",
      "Hao Tang"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.MM"
    ],
    "published": "2025-04-29T03:07:38Z",
    "pdf_url": "https://arxiv.org/pdf/2504.21051v1"
  },
  {
    "arxiv_id": "2504.21048v1",
    "entry_id": "http://arxiv.org/abs/2504.21048v1",
    "title": "Multi-Agent Reinforcement Learning for Resources Allocation Optimization: A Survey",
    "summary": "Multi-Agent Reinforcement Learning (MARL) has become a powerful framework for numerous real-world applications, modeling distributed decision-making and learning from interactions with complex environments. Resource Allocation Optimization (RAO) benefits significantly from MARL's ability to tackle dynamic and decentralized contexts. MARL-based approaches are increasingly applied to RAO challenges across sectors playing pivotal roles to Industry 4.0 developments. This survey provides a comprehensive review of recent MARL algorithms for RAO, encompassing core concepts, classifications, and a structured taxonomy. By outlining the current research landscape and identifying primary challenges and future directions, this survey aims to support researchers and practitioners in leveraging MARL's potential to advance resource allocation solutions.",
    "authors": [
      "Mohamad A. Hady",
      "Siyi Hu",
      "Mahardhika Pratama",
      "Jimmy Cao",
      "Ryszard Kowalczyk"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-04-29T00:18:31Z",
    "pdf_url": "https://arxiv.org/pdf/2504.21048v1"
  },
  {
    "arxiv_id": "2504.20020v2",
    "entry_id": "http://arxiv.org/abs/2504.20020v2",
    "title": "Modular Machine Learning: An Indispensable Path towards New-Generation Large Language Models",
    "summary": "Large language models (LLMs) have substantially advanced machine learning research, including natural language processing, computer vision, data mining, etc., yet they still exhibit critical limitations in explainability, reliability, adaptability, and extensibility. In this paper, we overview a promising learning paradigm, i.e., Modular Machine Learning (MML), as an essential approach toward new-generation LLMs capable of addressing these issues. We begin by systematically and comprehensively surveying the existing literature on modular machine learning, with a particular focus on modular data representation and modular models. Then, we propose a unified MML framework for LLMs, which decomposes the complex structure of LLMs into three interdependent components: modular representation, modular model, and modular reasoning. Specifically, the MML paradigm discussed in this article is able to: i) clarify the internal working mechanism of LLMs through the disentanglement of semantic components; ii) allow for flexible and task-adaptive model design; iii) enable an interpretable and logic-driven decision-making process. We further elaborate a feasible implementation of MML-based LLMs via leveraging advanced techniques such as disentangled representation learning, neural architecture search and neuro-symbolic learning. Last but not least, we critically identify the remaining key challenges, such as the integration of continuous neural and discrete symbolic processes, joint optimization, and computational scalability, present promising future research directions that deserve further exploration. Ultimately, we believe the integration of the MML with LLMs has the potential to bridge the gap between statistical (deep) learning and formal (logical) reasoning, thereby paving the way for robust, adaptable, and trustworthy AI systems across a wide range of real-world applications.",
    "authors": [
      "Xin Wang",
      "Haoyang Li",
      "Haibo Chen",
      "Zeyang Zhang",
      "Wenwu Zhu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-04-28T17:42:02Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20020v2"
  },
  {
    "arxiv_id": "2504.20010v1",
    "entry_id": "http://arxiv.org/abs/2504.20010v1",
    "title": "Towards Automated Scoping of AI for Social Good Projects",
    "summary": "Artificial Intelligence for Social Good (AI4SG) is an emerging effort that aims to address complex societal challenges with the powerful capabilities of AI systems. These challenges range from local issues with transit networks to global wildlife preservation. However, regardless of scale, a critical bottleneck for many AI4SG initiatives is the laborious process of problem scoping -- a complex and resource-intensive task -- due to a scarcity of professionals with both technical and domain expertise. Given the remarkable applications of large language models (LLM), we propose a Problem Scoping Agent (PSA) that uses an LLM to generate comprehensive project proposals grounded in scientific literature and real-world knowledge. We demonstrate that our PSA framework generates proposals comparable to those written by experts through a blind review and AI evaluations. Finally, we document the challenges of real-world problem scoping and note several areas for future work.",
    "authors": [
      "Jacob Emmerson",
      "Rayid Ghani",
      "Zheyuan Ryan Shi"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-04-28T17:29:51Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20010v1"
  },
  {
    "arxiv_id": "2504.20007v3",
    "entry_id": "http://arxiv.org/abs/2504.20007v3",
    "title": "Towards AI-Driven Policing: Interdisciplinary Knowledge Discovery from Police Body-Worn Camera Footage",
    "summary": "This paper proposes a novel interdisciplinary framework for analyzing police body-worn camera (BWC) footage from the Rochester Police Department (RPD) using advanced artificial intelligence (AI) and statistical machine learning (ML) techniques. Our goal is to detect, classify, and analyze patterns of interaction between police officers and civilians to identify key behavioral dynamics, such as respect, disrespect, escalation, and de-escalation. We apply multimodal data analysis by integrating image, audio, and natural language processing (NLP) techniques to extract meaningful insights from BWC footage. The framework incorporates speaker separation, transcription, and large language models (LLMs) to produce structured, interpretable summaries of police-civilian encounters. We also employ a custom evaluation pipeline to assess transcription quality and behavior detection accuracy in high-stakes, real-world policing scenarios. Our methodology, computational techniques, and findings outline a practical approach for law enforcement review, training, and accountability processes while advancing the frontiers of knowledge discovery from complex police BWC data.",
    "authors": [
      "Anita Srbinovska",
      "Angela Srbinovska",
      "Vivek Senthil",
      "Adrian Martin",
      "John McCluskey",
      "Jonathan Bateman",
      "Ernest Fokoué"
    ],
    "categories": [
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-04-28T17:25:23Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20007v3"
  },
  {
    "arxiv_id": "2504.19720v1",
    "entry_id": "http://arxiv.org/abs/2504.19720v1",
    "title": "Taming the Titans: A Survey of Efficient LLM Inference Serving",
    "summary": "Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.",
    "authors": [
      "Ranran Zhen",
      "Juntao Li",
      "Yixin Ji",
      "Zhenlin Yang",
      "Tong Liu",
      "Qingrong Xia",
      "Xinyu Duan",
      "Zhefeng Wang",
      "Baoxing Huai",
      "Min Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DC",
      "cs.LG"
    ],
    "published": "2025-04-28T12:14:02Z",
    "pdf_url": "https://arxiv.org/pdf/2504.19720v1"
  },
  {
    "arxiv_id": "2504.19678v1",
    "entry_id": "http://arxiv.org/abs/2504.19678v1",
    "title": "From LLM Reasoning to Autonomous AI Agents: A Comprehensive Review",
    "summary": "Large language models and autonomous AI agents have evolved rapidly, resulting in a diverse array of evaluation benchmarks, frameworks, and collaboration protocols. However, the landscape remains fragmented and lacks a unified taxonomy or comprehensive survey. Therefore, we present a side-by-side comparison of benchmarks developed between 2019 and 2025 that evaluate these models and agents across multiple domains. In addition, we propose a taxonomy of approximately 60 benchmarks that cover general and academic knowledge reasoning, mathematical problem-solving, code generation and software engineering, factual grounding and retrieval, domain-specific evaluations, multimodal and embodied tasks, task orchestration, and interactive assessments. Furthermore, we review AI-agent frameworks introduced between 2023 and 2025 that integrate large language models with modular toolkits to enable autonomous decision-making and multi-step reasoning. Moreover, we present real-world applications of autonomous AI agents in materials science, biomedical research, academic ideation, software engineering, synthetic data generation, chemical reasoning, mathematical problem-solving, geographic information systems, multimedia, healthcare, and finance. We then survey key agent-to-agent collaboration protocols, namely the Agent Communication Protocol (ACP), the Model Context Protocol (MCP), and the Agent-to-Agent Protocol (A2A). Finally, we discuss recommendations for future research, focusing on advanced reasoning strategies, failure modes in multi-agent LLM systems, automated scientific discovery, dynamic tool integration via reinforcement learning, integrated search capabilities, and security vulnerabilities in agent protocols.",
    "authors": [
      "Mohamed Amine Ferrag",
      "Norbert Tihanyi",
      "Merouane Debbah"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-04-28T11:08:22Z",
    "pdf_url": "https://arxiv.org/pdf/2504.19678v1"
  },
  {
    "arxiv_id": "2504.20119v2",
    "entry_id": "http://arxiv.org/abs/2504.20119v2",
    "title": "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets",
    "summary": "Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.",
    "authors": [
      "Lorenz Brehme",
      "Thomas Ströhle",
      "Ruth Breu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-04-28T08:22:19Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20119v2"
  },
  {
    "arxiv_id": "2504.20113v1",
    "entry_id": "http://arxiv.org/abs/2504.20113v1",
    "title": "Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI",
    "summary": "Exponential growth in scientific literature has heightened the demand for efficient evidence-based synthesis, driving the rise of the field of Automated Meta-analysis (AMA) powered by natural language processing and machine learning. This PRISMA systematic review introduces a structured framework for assessing the current state of AMA, based on screening 978 papers from 2006 to 2024, and analyzing 54 studies across diverse domains. Findings reveal a predominant focus on automating data processing (57%), such as extraction and statistical modeling, while only 17% address advanced synthesis stages. Just one study (2%) explored preliminary full-process automation, highlighting a critical gap that limits AMA's capacity for comprehensive synthesis. Despite recent breakthroughs in large language models (LLMs) and advanced AI, their integration into statistical modeling and higher-order synthesis, such as heterogeneity assessment and bias evaluation, remains underdeveloped. This has constrained AMA's potential for fully autonomous meta-analysis. From our dataset spanning medical (67%) and non-medical (33%) applications, we found that AMA has exhibited distinct implementation patterns and varying degrees of effectiveness in actually improving efficiency, scalability, and reproducibility. While automation has enhanced specific meta-analytic tasks, achieving seamless, end-to-end automation remains an open challenge. As AI systems advance in reasoning and contextual understanding, addressing these gaps is now imperative. Future efforts must focus on bridging automation across all meta-analysis stages, refining interpretability, and ensuring methodological robustness to fully realize AMA's potential for scalable, domain-agnostic synthesis.",
    "authors": [
      "Lingbo Li",
      "Anuradha Mathrani",
      "Teo Susnjak"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-04-28T00:40:17Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20113v1"
  },
  {
    "arxiv_id": "2506.11012v1",
    "entry_id": "http://arxiv.org/abs/2506.11012v1",
    "title": "A Survey of Task-Oriented Knowledge Graph Reasoning: Status, Applications, and Prospects",
    "summary": "Knowledge graphs (KGs) have emerged as a powerful paradigm for structuring and leveraging diverse real-world knowledge, which serve as a fundamental technology for enabling cognitive intelligence systems with advanced understanding and reasoning capabilities. Knowledge graph reasoning (KGR) aims to infer new knowledge based on existing facts in KGs, playing a crucial role in applications such as public security intelligence, intelligent healthcare, and financial risk assessment. From a task-centric perspective, existing KGR approaches can be broadly classified into static single-step KGR, static multi-step KGR, dynamic KGR, multi-modal KGR, few-shot KGR, and inductive KGR. While existing surveys have covered these six types of KGR tasks, a comprehensive review that systematically summarizes all KGR tasks particularly including downstream applications and more challenging reasoning paradigms remains lacking. In contrast to previous works, this survey provides a more comprehensive perspective on the research of KGR by categorizing approaches based on primary reasoning tasks, downstream application tasks, and potential challenging reasoning tasks. Besides, we explore advanced techniques, such as large language models (LLMs), and their impact on KGR. This work aims to highlight key research trends and outline promising future directions in the field of KGR.",
    "authors": [
      "Guanglin Niu",
      "Bo Li",
      "Yangguang Lin"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-27T09:42:09Z",
    "pdf_url": "https://arxiv.org/pdf/2506.11012v1"
  },
  {
    "arxiv_id": "2504.21032v1",
    "entry_id": "http://arxiv.org/abs/2504.21032v1",
    "title": "Selecting the Right LLM for eGov Explanations",
    "summary": "The perceived quality of the explanations accompanying e-government services is key to gaining trust in these institutions, consequently amplifying further usage of these services. Recent advances in generative AI, and concretely in Large Language Models (LLMs) allow the automation of such content articulations, eliciting explanations' interpretability and fidelity, and more generally, adapting content to various audiences. However, selecting the right LLM type for this has become a non-trivial task for e-government service providers. In this work, we adapted a previously developed scale to assist with this selection, providing a systematic approach for the comparative analysis of the perceived quality of explanations generated by various LLMs. We further demonstrated its applicability through the tax-return process, using it as an exemplar use case that could benefit from employing an LLM to generate explanations about tax refund decisions. This was attained through a user study with 128 survey respondents who were asked to rate different versions of LLM-generated explanations about tax refund decisions, providing a methodological basis for selecting the most appropriate LLM. Recognizing the practical challenges of conducting such a survey, we also began exploring the automation of this process by attempting to replicate human feedback using a selection of cutting-edge predictive techniques.",
    "authors": [
      "Lior Limonad",
      "Fabiana Fournier",
      "Hadar Mulian",
      "George Manias",
      "Spiros Borotis",
      "Danai Kyrkou"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-04-27T08:09:12Z",
    "pdf_url": "https://arxiv.org/pdf/2504.21032v1"
  },
  {
    "arxiv_id": "2504.19056v1",
    "entry_id": "http://arxiv.org/abs/2504.19056v1",
    "title": "Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions",
    "summary": "Generative AI is reshaping art, gaming, and most notably animation. Recent breakthroughs in foundation and diffusion models have reduced the time and cost of producing animated content. Characters are central animation components, involving motion, emotions, gestures, and facial expressions. The pace and breadth of advances in recent months make it difficult to maintain a coherent view of the field, motivating the need for an integrative review. Unlike earlier overviews that treat avatars, gestures, or facial animation in isolation, this survey offers a single, comprehensive perspective on all the main generative AI applications for character animation. We begin by examining the state-of-the-art in facial animation, expression rendering, image synthesis, avatar creation, gesture modeling, motion synthesis, object generation, and texture synthesis. We highlight leading research, practical deployments, commonly used datasets, and emerging trends for each area. To support newcomers, we also provide a comprehensive background section that introduces foundational models and evaluation metrics, equipping readers with the knowledge needed to enter the field. We discuss open challenges and map future research directions, providing a roadmap to advance AI-driven character-animation technologies. This survey is intended as a resource for researchers and developers entering the field of generative AI animation or adjacent fields. Resources are available at: https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey.",
    "authors": [
      "Mohammad Mahdi Abootorabi",
      "Omid Ghahroodi",
      "Pardis Sadat Zahraei",
      "Hossein Behzadasl",
      "Alireza Mirrokni",
      "Mobina Salimipanah",
      "Arash Rasouli",
      "Bahar Behzadipour",
      "Sara Azarnoush",
      "Benyamin Maleki",
      "Erfan Sadraiye",
      "Kiarash Kiani Feriz",
      "Mahdi Teymouri Nahad",
      "Ali Moghadasi",
      "Abolfazl Eshagh Abianeh",
      "Nizi Nazar",
      "Hamid R. Rabiee",
      "Mahdieh Soleymani Baghshah",
      "Meisam Ahmadi",
      "Ehsaneddin Asgari"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2025-04-27T00:09:31Z",
    "pdf_url": "https://arxiv.org/pdf/2504.19056v1"
  },
  {
    "arxiv_id": "2505.00026v2",
    "entry_id": "http://arxiv.org/abs/2505.00026v2",
    "title": "Theory of Mind in Large Language Models: Assessment and Enhancement",
    "summary": "Theory of Mind (ToM)-the ability to reason about the mental states of oneself and others-is a cornerstone of human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, understanding their ability to interpret and respond to human mental states is crucial for enabling effective interactions. In this paper, we review LLMs' ToM capabilities by analyzing both evaluation benchmarks and enhancement strategies. For evaluation, we focus on recently proposed and widely used story-based benchmarks. For enhancement, we provide an in-depth analysis of recent methods aimed at improving LLMs' ToM abilities. Furthermore, we outline promising directions for future research to further advance these capabilities and better adapt LLMs to more realistic and diverse scenarios. Our survey serves as a valuable resource for researchers interested in evaluating and advancing LLMs' ToM capabilities.",
    "authors": [
      "Ruirui Chen",
      "Weifeng Jiang",
      "Chengwei Qin",
      "Cheston Tan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-26T10:17:48Z",
    "pdf_url": "https://arxiv.org/pdf/2505.00026v2"
  },
  {
    "arxiv_id": "2504.18875v1",
    "entry_id": "http://arxiv.org/abs/2504.18875v1",
    "title": "Generative to Agentic AI: Survey, Conceptualization, and Challenges",
    "summary": "Agentic Artificial Intelligence (AI) builds upon Generative AI (GenAI). It constitutes the next major step in the evolution of AI with much stronger reasoning and interaction capabilities that enable more autonomous behavior to tackle complex tasks. Since the initial release of ChatGPT (3.5), Generative AI has seen widespread adoption, giving users firsthand experience. However, the distinction between Agentic AI and GenAI remains less well understood. To address this gap, our survey is structured in two parts. In the first part, we compare GenAI and Agentic AI using existing literature, discussing their key characteristics, how Agentic AI remedies limitations of GenAI, and the major steps in GenAI's evolution toward Agentic AI. This section is intended for a broad audience, including academics in both social sciences and engineering, as well as industry professionals. It provides the necessary insights to comprehend novel applications that are possible with Agentic AI but not with GenAI. In the second part, we deep dive into novel aspects of Agentic AI, including recent developments and practical concerns such as defining agents. Finally, we discuss several challenges that could serve as a future research agenda, while cautioning against risks that can emerge when exceeding human intelligence.",
    "authors": [
      "Johannes Schneider"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-04-26T09:47:00Z",
    "pdf_url": "https://arxiv.org/pdf/2504.18875v1"
  },
  {
    "arxiv_id": "2504.18858v1",
    "entry_id": "http://arxiv.org/abs/2504.18858v1",
    "title": "Why you shouldn't fully trust ChatGPT: A synthesis of this AI tool's error rates across disciplines and the software engineering lifecycle",
    "summary": "Context: ChatGPT and other large language models (LLMs) are widely used across healthcare, business, economics, engineering, and software engineering (SE). Despite their popularity, concerns persist about their reliability, especially their error rates across domains and the software development lifecycle (SDLC).\n  Objective: This study synthesizes and quantifies ChatGPT's reported error rates across major domains and SE tasks aligned with SDLC phases. It provides an evidence-based view of where ChatGPT excels, where it fails, and how reliability varies by task, domain, and model version (GPT-3.5, GPT-4, GPT-4-turbo, GPT-4o).\n  Method: A Multivocal Literature Review (MLR) was conducted, gathering data from academic studies, reports, benchmarks, and grey literature up to 2025. Factual, reasoning, coding, and interpretive errors were considered. Data were grouped by domain and SE phase and visualized using boxplots to show error distributions.\n  Results: Error rates vary across domains and versions. In healthcare, rates ranged from 8% to 83%. Business and economics saw error rates drop from ~50% with GPT-3.5 to 15-20% with GPT-4. Engineering tasks averaged 20-30%. Programming success reached 87.5%, though complex debugging still showed over 50% errors. In SE, requirements and design phases showed lower error rates (~5-20%), while coding, testing, and maintenance phases had higher variability (10-50%). Upgrades from GPT-3.5 to GPT-4 improved reliability.\n  Conclusion: Despite improvements, ChatGPT still exhibits non-negligible error rates varying by domain, task, and SDLC phase. Full reliance without human oversight remains risky, especially in critical settings. Continuous evaluation and critical validation are essential to ensure reliability and trustworthiness.",
    "authors": [
      "Vahid Garousi"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-04-26T08:49:33Z",
    "pdf_url": "https://arxiv.org/pdf/2504.18858v1"
  },
  {
    "arxiv_id": "2504.18765v3",
    "entry_id": "http://arxiv.org/abs/2504.18765v3",
    "title": "A Vision for Auto Research with LLM Agents",
    "summary": "This paper introduces Agent-Based Auto Research, a structured multi-agent framework designed to automate, coordinate, and optimize the full lifecycle of scientific research. Leveraging the capabilities of large language models (LLMs) and modular agent collaboration, the system spans all major research phases, including literature review, ideation, methodology planning, experimentation, paper writing, peer review response, and dissemination. By addressing issues such as fragmented workflows, uneven methodological expertise, and cognitive overload, the framework offers a systematic and scalable approach to scientific inquiry. Preliminary explorations demonstrate the feasibility and potential of Auto Research as a promising paradigm for self-improving, AI-driven research processes.",
    "authors": [
      "Chengwei Liu",
      "Chong Wang",
      "Jiayue Cao",
      "Jingquan Ge",
      "Kun Wang",
      "Lyuye Zhang",
      "Ming-Ming Cheng",
      "Penghai Zhao",
      "Tianlin Li",
      "Xiaojun Jia",
      "Xiang Li",
      "Xingshuai Li",
      "Yang Liu",
      "Yebo Feng",
      "Yihao Huang",
      "Yijia Xu",
      "Yuqiang Sun",
      "Zhenhong Zhou",
      "Zhengzi Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-04-26T02:06:10Z",
    "pdf_url": "https://arxiv.org/pdf/2504.18765v3"
  },
  {
    "arxiv_id": "2504.20090v2",
    "entry_id": "http://arxiv.org/abs/2504.20090v2",
    "title": "Spark: A System for Scientifically Creative Idea Generation",
    "summary": "Recently, large language models (LLMs) have shown promising abilities to generate novel research ideas in science, a direction which coincides with many foundational principles in computational creativity (CC). In light of these developments, we present an idea generation system named Spark that couples retrieval-augmented idea generation using LLMs with a reviewer model named Judge trained on 600K scientific reviews from OpenReview. Our work is both a system demonstration and intended to inspire other CC researchers to explore grounding the generation and evaluation of scientific ideas within foundational CC principles. To this end, we release the annotated dataset used to train Judge, inviting other researchers to explore the use of LLMs for idea generation and creative evaluations.",
    "authors": [
      "Aishik Sanyal",
      "Samuel Schapiro",
      "Sumuk Shashidhar",
      "Royce Moon",
      "Lav R. Varshney",
      "Dilek Hakkani-Tur"
    ],
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-04-25T20:33:57Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20090v2"
  },
  {
    "arxiv_id": "2504.20084v2",
    "entry_id": "http://arxiv.org/abs/2504.20084v2",
    "title": "AI Awareness",
    "summary": "Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness not as a philosophical question of consciousness, but as a measurable, functional capacity. AI awareness is a double-edged sword: it improves general capabilities, i.e., reasoning, safety, while also raising concerns around misalignment and societal risks, demanding careful oversight as AI capabilities grow.\n  In this review, we explore the emerging landscape of AI awareness, which includes metacognition (the ability to represent and reason about its own cognitive state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents and social norms), and situational awareness (assessing and responding to the context in which it operates).\n  First, we draw on insights from cognitive science, psychology, and computational theory to trace the theoretical foundations of awareness and examine how the four distinct forms of AI awareness manifest in state-of-the-art AI. Next, we systematically analyze current evaluation methods and empirical findings to better understand these manifestations. Building on this, we explore how AI awareness is closely linked to AI capabilities, demonstrating that more aware AI agents tend to exhibit higher levels of intelligent behaviors. Finally, we discuss the risks associated with AI awareness, including key topics in AI safety, alignment, and broader ethical concerns.",
    "authors": [
      "Xiaojian Li",
      "Haoyuan Shi",
      "Rongwu Xu",
      "Wei Xu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-04-25T16:03:50Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20084v2"
  },
  {
    "arxiv_id": "2504.18353v1",
    "entry_id": "http://arxiv.org/abs/2504.18353v1",
    "title": "Testing Individual Fairness in Graph Neural Networks",
    "summary": "The biases in artificial intelligence (AI) models can lead to automated decision-making processes that discriminate against groups and/or individuals based on sensitive properties such as gender and race. While there are many studies on diagnosing and mitigating biases in various AI models, there is little research on individual fairness in Graph Neural Networks (GNNs). Unlike traditional models, which treat data features independently and overlook their inter-relationships, GNNs are designed to capture graph-based structure where nodes are interconnected. This relational approach enables GNNs to model complex dependencies, but it also means that biases can propagate through these connections, complicating the detection and mitigation of individual fairness violations. This PhD project aims to develop a testing framework to assess and ensure individual fairness in GNNs. It first systematically reviews the literature on individual fairness, categorizing existing approaches to define, measure, test, and mitigate model biases, creating a taxonomy of individual fairness. Next, the project will develop a framework for testing and ensuring fairness in GNNs by adapting and extending current fairness testing and mitigation techniques. The framework will be evaluated through industrial case studies, focusing on graph-based large language models.",
    "authors": [
      "Roya Nasiri"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-04-25T13:45:24Z",
    "pdf_url": "https://arxiv.org/pdf/2504.18353v1"
  },
  {
    "arxiv_id": "2504.20082v1",
    "entry_id": "http://arxiv.org/abs/2504.20082v1",
    "title": "Evolution of AI in Education: Agentic Workflows",
    "summary": "Artificial intelligence (AI) has transformed various aspects of education, with large language models (LLMs) driving advancements in automated tutoring, assessment, and content generation. However, conventional LLMs are constrained by their reliance on static training data, limited adaptability, and lack of reasoning. To address these limitations and foster more sustainable technological practices, AI agents have emerged as a promising new avenue for educational innovation. In this review, we examine agentic workflows in education according to four major paradigms: reflection, planning, tool use, and multi-agent collaboration. We critically analyze the role of AI agents in education through these key design paradigms, exploring their advantages, applications, and challenges. To illustrate the practical potential of agentic systems, we present a proof-of-concept application: a multi-agent framework for automated essay scoring. Preliminary results suggest this agentic approach may offer improved consistency compared to stand-alone LLMs. Our findings highlight the transformative potential of AI agents in educational settings while underscoring the need for further research into their interpretability, trustworthiness, and sustainable impact on pedagogical impact.",
    "authors": [
      "Firuz Kamalov",
      "David Santandreu Calonge",
      "Linda Smail",
      "Dilshod Azizov",
      "Dimple R. Thadani",
      "Theresa Kwong",
      "Amara Atif"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-04-25T13:44:57Z",
    "pdf_url": "https://arxiv.org/pdf/2504.20082v1"
  },
  {
    "arxiv_id": "2504.18346v2",
    "entry_id": "http://arxiv.org/abs/2504.18346v2",
    "title": "Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review",
    "summary": "Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.",
    "authors": [
      "Toghrul Abbasli",
      "Kentaroh Toyoda",
      "Yuan Wang",
      "Leon Witt",
      "Muhammad Asif Ali",
      "Yukai Miao",
      "Dan Li",
      "Qingsong Wei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-25T13:34:40Z",
    "pdf_url": "https://arxiv.org/pdf/2504.18346v2"
  },
  {
    "arxiv_id": "2504.18310v1",
    "entry_id": "http://arxiv.org/abs/2504.18310v1",
    "title": "Artificial Intelligence health advice accuracy varies across languages and contexts",
    "summary": "Using basic health statements authorized by UK and EU registers and 9,100 journalist-vetted public-health assertions on topics such as abortion, COVID-19 and politics from sources ranging from peer-reviewed journals and government advisories to social media and news across the political spectrum, we benchmark six leading large language models from in 21 languages, finding that, despite high accuracy on English-centric textbook claims, performance falls in multiple non-European languages and fluctuates by topic and source, highlighting the urgency of comprehensive multilingual, domain-aware validation before deploying AI in global health communication.",
    "authors": [
      "Prashant Garg",
      "Thiemo Fetzer"
    ],
    "categories": [
      "econ.GN",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-04-25T12:37:15Z",
    "pdf_url": "https://arxiv.org/pdf/2504.18310v1"
  },
  {
    "arxiv_id": "2504.18044v1",
    "entry_id": "http://arxiv.org/abs/2504.18044v1",
    "title": "AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How",
    "summary": "Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social Computing requires the examination of ethical and social norms to ensure safe incorporation into human life. We conducted a mixed-method study, including an online survey with 111 participants and an interview study with 38 experts, to investigate the AI ethics and social norms in ChatGPT as everyday life tools. This study aims to evaluate whether ChatGPT in an empirical context operates following ethics and social norms, which is critical for understanding actions in industrial and academic research and achieving machine ethics. The findings of this study provide initial insights into six important aspects of AI ethics, including bias, trustworthiness, security, toxicology, social norms, and ethical data. Significant obstacles related to transparency and bias in unsupervised data collection methods are identified as ChatGPT's ethical concerns.",
    "authors": [
      "Omid Veisi",
      "Sasan Bahrami",
      "Roman Englert",
      "Claudia Müller"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "cs.IT"
    ],
    "published": "2025-04-25T03:26:30Z",
    "pdf_url": "https://arxiv.org/pdf/2504.18044v1"
  },
  {
    "arxiv_id": "2504.17964v1",
    "entry_id": "http://arxiv.org/abs/2504.17964v1",
    "title": "Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content",
    "summary": "This paper examines how graduate students develop frameworks for evaluating machine-generated expertise in web-based interactions with large language models (LLMs). Through a qualitative study combining surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students, we identify patterns in how these emerging professionals assess and engage with AI-generated content. Our findings reveal that students construct evaluation frameworks shaped by three main factors: professional identity, verification capabilities, and system navigation experience. Rather than uniformly accepting or rejecting LLM outputs, students protect domains central to their professional identities while delegating others--with managers preserving conceptual work, designers safeguarding creative processes, and programmers maintaining control over core technical expertise. These evaluation frameworks are further influenced by students' ability to verify different types of content and their experience navigating complex systems. This research contributes to web science by highlighting emerging human-genAI interaction patterns and suggesting how platforms might better support users in developing effective frameworks for evaluating machine-generated expertise signals in AI-mediated web environments.",
    "authors": [
      "Celia Chen",
      "Alex Leitch"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-04-24T22:24:14Z",
    "pdf_url": "https://arxiv.org/pdf/2504.17964v1"
  },
  {
    "arxiv_id": "2504.17119v2",
    "entry_id": "http://arxiv.org/abs/2504.17119v2",
    "title": "The Rise of Small Language Models in Healthcare: A Comprehensive Survey",
    "summary": "Despite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github",
    "authors": [
      "Muskan Garg",
      "Shaina Raza",
      "Shebuti Rayana",
      "Xingyi Liu",
      "Sunghwan Sohn"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-23T22:02:25Z",
    "pdf_url": "https://arxiv.org/pdf/2504.17119v2"
  },
  {
    "arxiv_id": "2504.16736v3",
    "entry_id": "http://arxiv.org/abs/2504.16736v3",
    "title": "A Survey of AI Agent Protocols",
    "summary": "The rapid development of large language models (LLMs) has led to the widespread deployment of LLM agents across diverse industries, including customer service, content generation, data analysis, and even healthcare. However, as more LLM agents are deployed, a major issue has emerged: there is no standard way for these agents to communicate with external tools or data sources. This lack of standardized protocols makes it difficult for agents to work together or scale effectively, and it limits their ability to tackle complex, real-world tasks. A unified communication protocol for LLM agents could change this. It would allow agents and tools to interact more smoothly, encourage collaboration, and triggering the formation of collective intelligence. In this paper, we provide the first comprehensive analysis of existing agent protocols, proposing a systematic two-dimensional classification that differentiates context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols. Additionally, we conduct a comparative performance analysis of these protocols across key dimensions such as security, scalability, and latency. Finally, we explore the future landscape of agent protocols by identifying critical research directions and characteristics necessary for next-generation protocols. These characteristics include adaptability, privacy preservation, and group-based interaction, as well as trends toward layered architectures and collective intelligence infrastructures. We expect this work to serve as a practical reference for both researchers and engineers seeking to design, evaluate, or integrate robust communication infrastructures for intelligent agents.",
    "authors": [
      "Yingxuan Yang",
      "Huacan Chai",
      "Yuanyi Song",
      "Siyuan Qi",
      "Muning Wen",
      "Ning Li",
      "Junwei Liao",
      "Haoyi Hu",
      "Jianghao Lin",
      "Gaowei Chang",
      "Weiwen Liu",
      "Ying Wen",
      "Yong Yu",
      "Weinan Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-04-23T14:07:26Z",
    "pdf_url": "https://arxiv.org/pdf/2504.16736v3"
  },
  {
    "arxiv_id": "2504.16584v1",
    "entry_id": "http://arxiv.org/abs/2504.16584v1",
    "title": "Case Study: Fine-tuning Small Language Models for Accurate and Private CWE Detection in Python Code",
    "summary": "Large Language Models (LLMs) have demonstrated significant capabilities in understanding and analyzing code for security vulnerabilities, such as Common Weakness Enumerations (CWEs). However, their reliance on cloud infrastructure and substantial computational requirements pose challenges for analyzing sensitive or proprietary codebases due to privacy concerns and inference costs. This work explores the potential of Small Language Models (SLMs) as a viable alternative for accurate, on-premise vulnerability detection. We investigated whether a 350-million parameter pre-trained code model (codegen-mono) could be effectively fine-tuned to detect the MITRE Top 25 CWEs specifically within Python code. To facilitate this, we developed a targeted dataset of 500 examples using a semi-supervised approach involving LLM-driven synthetic data generation coupled with meticulous human review. Initial tests confirmed that the base codegen-mono model completely failed to identify CWEs in our samples. However, after applying instruction-following fine-tuning, the specialized SLM achieved remarkable performance on our test set, yielding approximately 99% accuracy, 98.08% precision, 100% recall, and a 99.04% F1-score. These results strongly suggest that fine-tuned SLMs can serve as highly accurate and efficient tools for CWE detection, offering a practical and privacy-preserving solution for integrating advanced security analysis directly into development workflows.",
    "authors": [
      "Md. Azizul Hakim Bappy",
      "Hossen A Mustafa",
      "Prottoy Saha",
      "Rajinus Salehat"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-04-23T10:05:27Z",
    "pdf_url": "https://arxiv.org/pdf/2504.16584v1"
  },
  {
    "arxiv_id": "2504.16506v3",
    "entry_id": "http://arxiv.org/abs/2504.16506v3",
    "title": "A Comprehensive Survey of Synthetic Tabular Data Generation",
    "summary": "Tabular data is one of the most prevalent and important data formats in real-world applications such as healthcare, finance, and education. However, its effective use in machine learning is often constrained by data scarcity, privacy concerns, and class imbalance. Synthetic tabular data generation has emerged as a powerful solution, leveraging generative models to learn underlying data distributions and produce realistic, privacy-preserving samples. Although this area has seen growing attention, most existing surveys focus narrowly on specific methods (e.g., GANs or privacy-enhancing techniques), lacking a unified and comprehensive view that integrates recent advances such as diffusion models and large language models (LLMs).\n  In this survey, we present a structured and in-depth review of synthetic tabular data generation methods. Specifically, the survey is organized into three core components: (1) Background, which covers the overall generation pipeline, including problem definitions, synthetic tabular data generation methods, post processing, and evaluation; (2) Generation Methods, where we categorize existing approaches into traditional generation methods, diffusion model methods, and LLM-based methods, and compare them in terms of architecture, generation quality, and applicability; and (3) Applications and Challenges, which summarizes practical use cases, highlights common datasets, and discusses open challenges such as heterogeneity, data fidelity, and privacy protection.\n  This survey aims to provide researchers and practitioners with a holistic understanding of the field and to highlight key directions for future work in synthetic tabular data generation.",
    "authors": [
      "Ruxue Shi",
      "Yili Wang",
      "Mengnan Du",
      "Xu Shen",
      "Yi Chang",
      "Xin Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-04-23T08:33:34Z",
    "pdf_url": "https://arxiv.org/pdf/2504.16506v3"
  },
  {
    "arxiv_id": "2504.16449v2",
    "entry_id": "http://arxiv.org/abs/2504.16449v2",
    "title": "From Past to Present: A Survey of Malicious URL Detection Techniques, Datasets and Code Repositories",
    "summary": "Malicious URLs persistently threaten the cybersecurity ecosystem, by either deceiving users into divulging private data or distributing harmful payloads to infiltrate host systems. Gaining timely insights into the current state of this ongoing battle holds significant importance. However, existing reviews exhibit 4 critical gaps: 1) Their reliance on algorithm-centric taxonomies obscures understanding of how detection approaches exploit specific modal information channels; 2) They fail to incorporate pivotal LLM/Transformer-based defenses; 3) No open-source implementations are collected to facilitate benchmarking; 4) Insufficient dataset coverage.This paper presents a comprehensive review of malicious URL detection technologies, systematically analyzing methods from traditional blacklisting to advanced deep learning approaches (e.g. Transformer, GNNs, and LLMs). Unlike prior surveys, we propose a novel modality-based taxonomy that categorizes existing works according to their primary data modalities (URL, HTML, Visual, etc.). This hierarchical classification enables both rigorous technical analysis and clear understanding of multimodal information utilization. Furthermore, to establish a profile of accessible datasets and address the lack of standardized benchmarking (where current studies often lack proper baseline comparisons), we curate and analyze: 1) publicly available datasets (2016-2024), and 2) open-source implementations from published works(2013-2025). Then, we outline essential design principles and architectural frameworks for product-level implementations. The review concludes by examining emerging challenges and proposing actionable directions for future research. We maintain a GitHub repository for ongoing curating datasets and open-source implementations: https://github.com/sevenolu7/Malicious-URL-Detection-Open-Source/tree/master.",
    "authors": [
      "Ye Tian",
      "Yanqiu Yu",
      "Jianguo Sun",
      "Yanbin Wang"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-04-23T06:23:18Z",
    "pdf_url": "https://arxiv.org/pdf/2504.16449v2"
  },
  {
    "arxiv_id": "2504.16420v1",
    "entry_id": "http://arxiv.org/abs/2504.16420v1",
    "title": "A Survey of Foundation Model-Powered Recommender Systems: From Feature-Based, Generative to Agentic Paradigms",
    "summary": "Recommender systems (RS) have become essential in filtering information and personalizing content for users. RS techniques have traditionally relied on modeling interactions between users and items as well as the features of content using models specific to each task. The emergence of foundation models (FMs), large scale models trained on vast amounts of data such as GPT, LLaMA and CLIP, is reshaping the recommendation paradigm. This survey provides a comprehensive overview of the Foundation Models for Recommender Systems (FM4RecSys), covering their integration in three paradigms: (1) Feature-Based augmentation of representations, (2) Generative recommendation approaches, and (3) Agentic interactive systems. We first review the data foundations of RS, from traditional explicit or implicit feedback to multimodal content sources. We then introduce FMs and their capabilities for representation learning, natural language understanding, and multi-modal reasoning in RS contexts. The core of the survey discusses how FMs enhance RS under different paradigms. Afterward, we examine FM applications in various recommendation tasks. Through an analysis of recent research, we highlight key opportunities that have been realized as well as challenges encountered. Finally, we outline open research directions and technical challenges for next-generation FM4RecSys. This survey not only reviews the state-of-the-art methods but also provides a critical analysis of the trade-offs among the feature-based, the generative, and the agentic paradigms, outlining key open issues and future research directions.",
    "authors": [
      "Chengkai Huang",
      "Hongtao Huang",
      "Tong Yu",
      "Kaige Xie",
      "Junda Wu",
      "Shuai Zhang",
      "Julian Mcauley",
      "Dietmar Jannach",
      "Lina Yao"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-04-23T05:02:51Z",
    "pdf_url": "https://arxiv.org/pdf/2504.16420v1"
  },
  {
    "arxiv_id": "2505.00013v1",
    "entry_id": "http://arxiv.org/abs/2505.00013v1",
    "title": "Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa",
    "summary": "Background Practical applications such as social media monitoring and customer-feedback analysis require accurate emotion detection for Japanese text, yet resource scarcity and class imbalance hinder model performance.\n  Objective This study aims to build a high-accuracy model for predicting the presence or absence of eight Plutchik emotions in Japanese sentences.\n  Methods Using the WRIME corpus, we transform reader-averaged intensity scores into binary labels and fine-tune four pre-trained language models (BERT, RoBERTa, DeBERTa-v3-base, DeBERTa-v3-large). For context, we also assess two large language models (TinySwallow-1.5B-Instruct and ChatGPT-4o). Accuracy and F1-score serve as evaluation metrics.\n  Results DeBERTa-v3-large attains the best mean accuracy (0.860) and F1-score (0.662), outperforming all other models. It maintains robust F1 across both high-frequency emotions (e.g., Joy, Anticipation) and low-frequency emotions (e.g., Anger, Trust). The LLMs lag, with ChatGPT-4o and TinySwallow-1.5B-Instruct scoring 0.527 and 0.292 in mean F1, respectively.\n  Conclusion The fine-tuned DeBERTa-v3-large model currently offers the most reliable solution for binary emotion classification in Japanese. We release this model as a pip-installable package (pip install deberta-emotion-predictor). Future work should augment data for rare emotions, reduce model size, and explore prompt engineering to improve LLM performance.\n  This manuscript is under review for possible publication in New Generation Computing.",
    "authors": [
      "Yoichi Takenaka"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-22T07:51:37Z",
    "pdf_url": "https://arxiv.org/pdf/2505.00013v1"
  },
  {
    "arxiv_id": "2504.15637v1",
    "entry_id": "http://arxiv.org/abs/2504.15637v1",
    "title": "DR.FIX: Automatically Fixing Data Races at Industry Scale",
    "summary": "Data races are a prevalent class of concurrency bugs in shared-memory parallel programs, posing significant challenges to software reliability and reproducibility. While there is an extensive body of research on detecting data races and a wealth of practical detection tools across various programming languages, considerably less effort has been directed toward automatically fixing data races at an industrial scale. In large codebases, data races are continuously introduced and exhibit myriad patterns, making automated fixing particularly challenging.\n  In this paper, we tackle the problem of automatically fixing data races at an industrial scale. We present Dr.Fix, a tool that combines large language models (LLMs) with program analysis to generate fixes for data races in real-world settings, effectively addressing a broad spectrum of racy patterns in complex code contexts. Implemented for Go--the programming language widely used in modern microservice architectures where concurrency is pervasive and data races are common--Dr.Fix seamlessly integrates into existing development workflows. We detail the design of Dr.Fix and examine how individual design choices influence the quality of the fixes produced. Over the past 18 months, Dr.Fix has been integrated into developer workflows at Uber demonstrating its practical utility. During this period, Dr.Fix produced patches for 224 (55%) from a corpus of 404 data races spanning various categories; 193 of these patches (86%) were accepted by more than a hundred developers via code reviews and integrated into the codebase.",
    "authors": [
      "Farnaz Behrang",
      "Zhizhou Zhang",
      "Georgian-Vlad Saioc",
      "Peng Liu",
      "Milind Chabbi"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.PL",
      "cs.SE"
    ],
    "published": "2025-04-22T06:56:15Z",
    "pdf_url": "https://arxiv.org/pdf/2504.15637v1"
  },
  {
    "arxiv_id": "2504.15585v4",
    "entry_id": "http://arxiv.org/abs/2504.15585v4",
    "title": "A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment",
    "summary": "The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire \"lifechain\" of LLMs. To address this gap, this paper introduces, for the first time, the concept of \"full-stack\" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.",
    "authors": [
      "Kun Wang",
      "Guibin Zhang",
      "Zhenhong Zhou",
      "Jiahao Wu",
      "Miao Yu",
      "Shiqian Zhao",
      "Chenlong Yin",
      "Jinhu Fu",
      "Yibo Yan",
      "Hanjun Luo",
      "Liang Lin",
      "Zhihao Xu",
      "Haolang Lu",
      "Xinye Cao",
      "Xinyun Zhou",
      "Weifei Jin",
      "Fanci Meng",
      "Shicheng Xu",
      "Junyuan Mao",
      "Yu Wang",
      "Hao Wu",
      "Minghe Wang",
      "Fan Zhang",
      "Junfeng Fang",
      "Wenjie Qu",
      "Yue Liu",
      "Chengwei Liu",
      "Yifan Zhang",
      "Qiankun Li",
      "Chongye Guo",
      "Yalan Qin",
      "Zhaoxin Fan",
      "Kai Wang",
      "Yi Ding",
      "Donghai Hong",
      "Jiaming Ji",
      "Yingxin Lai",
      "Zitong Yu",
      "Xinfeng Li",
      "Yifan Jiang",
      "Yanhui Li",
      "Xinyu Deng",
      "Junlin Wu",
      "Dongxia Wang",
      "Yihao Huang",
      "Yufei Guo",
      "Jen-tse Huang",
      "Qiufeng Wang",
      "Xiaolong Jin",
      "Wenxuan Wang",
      "Dongrui Liu",
      "Yanwei Yue",
      "Wenke Huang",
      "Guancheng Wan",
      "Heng Chang",
      "Tianlin Li",
      "Yi Yu",
      "Chenghao Li",
      "Jiawei Li",
      "Lei Bai",
      "Jie Zhang",
      "Qing Guo",
      "Jingyi Wang",
      "Tianlong Chen",
      "Joey Tianyi Zhou",
      "Xiaojun Jia",
      "Weisong Sun",
      "Cong Wu",
      "Jing Chen",
      "Xuming Hu",
      "Yiming Li",
      "Xiao Wang",
      "Ningyu Zhang",
      "Luu Anh Tuan",
      "Guowen Xu",
      "Jiaheng Zhang",
      "Tianwei Zhang",
      "Xingjun Ma",
      "Jindong Gu",
      "Liang Pang",
      "Xiang Wang",
      "Bo An",
      "Jun Sun",
      "Mohit Bansal",
      "Shirui Pan",
      "Lingjuan Lyu",
      "Yuval Elovici",
      "Bhavya Kailkhura",
      "Yaodong Yang",
      "Hongwei Li",
      "Wenyuan Xu",
      "Yizhou Sun",
      "Wei Wang",
      "Qing Li",
      "Ke Tang",
      "Yu-Gang Jiang",
      "Felix Juefei-Xu",
      "Hui Xiong",
      "Xiaofeng Wang",
      "Dacheng Tao",
      "Philip S. Yu",
      "Qingsong Wen",
      "Yang Liu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-04-22T05:02:49Z",
    "pdf_url": "https://arxiv.org/pdf/2504.15585v4"
  },
  {
    "arxiv_id": "2504.15439v1",
    "entry_id": "http://arxiv.org/abs/2504.15439v1",
    "title": "Combating Toxic Language: A Review of LLM-Based Strategies for Software Engineering",
    "summary": "Large Language Models (LLMs) have become integral to software engineering (SE), where they are increasingly used in development workflows. However, their widespread use raises concerns about the presence and propagation of toxic language--harmful or offensive content that can foster exclusionary environments. This paper provides a comprehensive review of recent research on toxicity detection and mitigation, focusing on both SE-specific and general-purpose datasets. We examine annotation and preprocessing techniques, assess detection methodologies, and evaluate mitigation strategies, particularly those leveraging LLMs. Additionally, we conduct an ablation study demonstrating the effectiveness of LLM-based rewriting for reducing toxicity. By synthesizing existing work and identifying open challenges, this review highlights key areas for future research to ensure the responsible deployment of LLMs in SE and beyond.",
    "authors": [
      "Hao Zhuo",
      "Yicheng Yang",
      "Kewen Peng"
    ],
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "published": "2025-04-21T21:09:33Z",
    "pdf_url": "https://arxiv.org/pdf/2504.15439v1"
  },
  {
    "arxiv_id": "2504.14904v1",
    "entry_id": "http://arxiv.org/abs/2504.14904v1",
    "title": "VLM as Policy: Common-Law Content Moderation Framework for Short Video Platform",
    "summary": "Exponentially growing short video platforms (SVPs) face significant challenges in moderating content detrimental to users' mental health, particularly for minors. The dissemination of such content on SVPs can lead to catastrophic societal consequences. Although substantial efforts have been dedicated to moderating such content, existing methods suffer from critical limitations: (1) Manual review is prone to human bias and incurs high operational costs. (2) Automated methods, though efficient, lack nuanced content understanding, resulting in lower accuracy. (3) Industrial moderation regulations struggle to adapt to rapidly evolving trends due to long update cycles. In this paper, we annotate the first SVP content moderation benchmark with authentic user/reviewer feedback to fill the absence of benchmark in this field. Then we evaluate various methods on the benchmark to verify the existence of the aforementioned limitations. We further propose our common-law content moderation framework named KuaiMod to address these challenges. KuaiMod consists of three components: training data construction, offline adaptation, and online deployment & refinement. Leveraging large vision language model (VLM) and Chain-of-Thought (CoT) reasoning, KuaiMod adequately models video toxicity based on sparse user feedback and fosters dynamic moderation policy with rapid update speed and high accuracy. Offline experiments and large-scale online A/B test demonstrates the superiority of KuaiMod: KuaiMod achieves the best moderation performance on our benchmark. The deployment of KuaiMod reduces the user reporting rate by 20% and its application in video recommendation increases both Daily Active User (DAU) and APP Usage Time (AUT) on several Kuaishou scenarios. We have open-sourced our benchmark at https://kuaimod.github.io.",
    "authors": [
      "Xingyu Lu",
      "Tianke Zhang",
      "Chang Meng",
      "Xiaobei Wang",
      "Jinpeng Wang",
      "YiFan Zhang",
      "Shisong Tang",
      "Changyi Liu",
      "Haojie Ding",
      "Kaiyu Jiang",
      "Kaiyu Tang",
      "Bin Wen",
      "Hai-Tao Zheng",
      "Fan Yang",
      "Tingting Gao",
      "Di Zhang",
      "Kun Gai"
    ],
    "categories": [
      "cs.SI",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "published": "2025-04-21T07:20:19Z",
    "pdf_url": "https://arxiv.org/pdf/2504.14904v1"
  },
  {
    "arxiv_id": "2504.16129v4",
    "entry_id": "http://arxiv.org/abs/2504.16129v4",
    "title": "MARFT: Multi-Agent Reinforcement Fine-Tuning",
    "summary": "LLM-based Multi-Agent Systems have demonstrated remarkable capabilities in addressing complex, agentic tasks, from generating high-quality presentation slides to even conducting sophisticated scientific research. Meanwhile, RL has been widely recognized for its effectiveness in enhancing agent intelligence, but limited research has investigated the fine-tuning of LaMAS using foundational RL techniques. Moreover, the direct application of MARL methods to LaMAS introduces significant challenges, stemming from the unique characteristics and mechanisms inherent to LaMAS. To address these challenges, this article presents a comprehensive study of LLM-based MARL and proposes a novel paradigm termed Multi-Agent Reinforcement Fine-Tuning (MARFT). We introduce a brand-new MG called Flex-MG, which aligns with the LaMAS optimization in real-world applications and a universal algorithmic framework tailored specifically for LaMAS, outlining the conceptual foundations, key distinctions, and practical implementation strategies. We review the evolution from RL to RFT, setting the stage for a parallel analysis in the multi-agent domain. In the context of LaMAS, we elucidate critical differences between MARL and MARFT. These differences motivate a transition toward a LaMAS-oriented formulation of RFT. Central to this work is a robust and scalable MARFT framework. We detail the core algorithm and provide a complete, open-source implementation to facilitate adoption and further research. The latter sections of the paper explore real-world application perspectives and opening challenges in MARFT. By bridging theoretical underpinnings with practical methodologies, this work serves as a roadmap for researchers seeking to advance MARFT toward resilient and adaptive solutions in agentic systems. Our implementation of the proposed framework is publicly available at: https://github.com/jwliao-ai/MARFT.",
    "authors": [
      "Junwei Liao",
      "Muning Wen",
      "Jun Wang",
      "Weinan Zhang"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2025-04-21T07:03:54Z",
    "pdf_url": "https://arxiv.org/pdf/2504.16129v4"
  },
  {
    "arxiv_id": "2504.14772v1",
    "entry_id": "http://arxiv.org/abs/2504.14772v1",
    "title": "Knowledge Distillation and Dataset Distillation of Large Language Models: Emerging Trends, Challenges, and Future Directions",
    "summary": "The exponential growth of Large Language Models (LLMs) continues to highlight the need for efficient strategies to meet ever-expanding computational and data demands. This survey provides a comprehensive analysis of two complementary paradigms: Knowledge Distillation (KD) and Dataset Distillation (DD), both aimed at compressing LLMs while preserving their advanced reasoning capabilities and linguistic diversity. We first examine key methodologies in KD, such as task-specific alignment, rationale-based training, and multi-teacher frameworks, alongside DD techniques that synthesize compact, high-impact datasets through optimization-based gradient matching, latent space regularization, and generative synthesis. Building on these foundations, we explore how integrating KD and DD can produce more effective and scalable compression strategies. Together, these approaches address persistent challenges in model scalability, architectural heterogeneity, and the preservation of emergent LLM abilities. We further highlight applications across domains such as healthcare and education, where distillation enables efficient deployment without sacrificing performance. Despite substantial progress, open challenges remain in preserving emergent reasoning and linguistic diversity, enabling efficient adaptation to continually evolving teacher models and datasets, and establishing comprehensive evaluation protocols. By synthesizing methodological innovations, theoretical foundations, and practical insights, our survey charts a path toward sustainable, resource-efficient LLMs through the tighter integration of KD and DD principles.",
    "authors": [
      "Luyang Fang",
      "Xiaowei Yu",
      "Jiazhang Cai",
      "Yongkai Chen",
      "Shushan Wu",
      "Zhengliang Liu",
      "Zhenyuan Yang",
      "Haoran Lu",
      "Xilin Gong",
      "Yufang Liu",
      "Terry Ma",
      "Wei Ruan",
      "Ali Abbasi",
      "Jing Zhang",
      "Tao Wang",
      "Ehsan Latif",
      "Wei Liu",
      "Wei Zhang",
      "Soheil Kolouri",
      "Xiaoming Zhai",
      "Dajiang Zhu",
      "Wenxuan Zhong",
      "Tianming Liu",
      "Ping Ma"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-04-20T23:50:23Z",
    "pdf_url": "https://arxiv.org/pdf/2504.14772v1"
  },
  {
    "arxiv_id": "2504.14520v1",
    "entry_id": "http://arxiv.org/abs/2504.14520v1",
    "title": "Meta-Thinking in LLMs via Multi-Agent Reinforcement Learning: A Survey",
    "summary": "This survey explores the development of meta-thinking capabilities in Large Language Models (LLMs) from a Multi-Agent Reinforcement Learning (MARL) perspective. Meta-thinking self-reflection, assessment, and control of thinking processes is an important next step in enhancing LLM reliability, flexibility, and performance, particularly for complex or high-stakes tasks. The survey begins by analyzing current LLM limitations, such as hallucinations and the lack of internal self-assessment mechanisms. It then talks about newer methods, including RL from human feedback (RLHF), self-distillation, and chain-of-thought prompting, and each of their limitations. The crux of the survey is to talk about how multi-agent architectures, namely supervisor-agent hierarchies, agent debates, and theory of mind frameworks, can emulate human-like introspective behavior and enhance LLM robustness. By exploring reward mechanisms, self-play, and continuous learning methods in MARL, this survey gives a comprehensive roadmap to building introspective, adaptive, and trustworthy LLMs. Evaluation metrics, datasets, and future research avenues, including neuroscience-inspired architectures and hybrid symbolic reasoning, are also discussed.",
    "authors": [
      "Ahsan Bilal",
      "Muhammad Ahmed Mohsin",
      "Muhammad Umer",
      "Muhammad Awais Khan Bangash",
      "Muhammad Ali Jamshed"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-20T07:34:26Z",
    "pdf_url": "https://arxiv.org/pdf/2504.14520v1"
  },
  {
    "arxiv_id": "2504.13825v1",
    "entry_id": "http://arxiv.org/abs/2504.13825v1",
    "title": "Feature Alignment and Representation Transfer in Knowledge Distillation for Large Language Models",
    "summary": "Knowledge distillation (KD) is a technique for transferring knowledge from complex teacher models to simpler student models, significantly enhancing model efficiency and accuracy. It has demonstrated substantial advancements in various applications including image classification, object detection, language modeling, text classification, and sentiment analysis. Recent innovations in KD methods, such as attention-based approaches, block-wise logit distillation, and decoupling distillation, have notably improved student model performance. These techniques focus on stimulus complexity, attention mechanisms, and global information capture to optimize knowledge transfer. In addition, KD has proven effective in compressing large language models while preserving accuracy, reducing computational overhead, and improving inference speed. This survey synthesizes the latest literature, highlighting key findings, contributions, and future directions in knowledge distillation to provide insights for researchers and practitioners on its evolving role in artificial intelligence and machine learning.",
    "authors": [
      "Junjie Yang",
      "Junhao Song",
      "Xudong Han",
      "Ziqian Bi",
      "Tianyang Wang",
      "Chia Xin Liang",
      "Xinyuan Song",
      "Yichao Zhang",
      "Qian Niu",
      "Benji Peng",
      "Keyu Chen",
      "Ming Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-04-18T17:54:33Z",
    "pdf_url": "https://arxiv.org/pdf/2504.13825v1"
  },
  {
    "arxiv_id": "2504.13993v1",
    "entry_id": "http://arxiv.org/abs/2504.13993v1",
    "title": "CPR: Leveraging LLMs for Topic and Phrase Suggestion to Facilitate Comprehensive Product Reviews",
    "summary": "Consumers often heavily rely on online product reviews, analyzing both quantitative ratings and textual descriptions to assess product quality. However, existing research hasn't adequately addressed how to systematically encourage the creation of comprehensive reviews that capture both customers sentiment and detailed product feature analysis. This paper presents CPR, a novel methodology that leverages the power of Large Language Models (LLMs) and Topic Modeling to guide users in crafting insightful and well-rounded reviews. Our approach employs a three-stage process: first, we present users with product-specific terms for rating; second, we generate targeted phrase suggestions based on these ratings; and third, we integrate user-written text through topic modeling, ensuring all key aspects are addressed. We evaluate CPR using text-to-text LLMs, comparing its performance against real-world customer reviews from Walmart. Our results demonstrate that CPR effectively identifies relevant product terms, even for new products lacking prior reviews, and provides sentiment-aligned phrase suggestions, saving users time and enhancing reviews quality. Quantitative analysis reveals a 12.3% improvement in BLEU score over baseline methods, further supported by manual evaluation of generated phrases. We conclude by discussing potential extensions and future research directions.",
    "authors": [
      "Ekta Gujral",
      "Apurva Sinha",
      "Lishi Ji",
      "Bijayani Sanghamitra Mishra"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-04-18T17:11:38Z",
    "pdf_url": "https://arxiv.org/pdf/2504.13993v1"
  },
  {
    "arxiv_id": "2504.13667v1",
    "entry_id": "http://arxiv.org/abs/2504.13667v1",
    "title": "Large Language Models Will Change The Way Children Think About Technology And Impact Every Interaction Paradigm",
    "summary": "This paper presents a hopeful perspective on the potentially dramatic impacts of Large Language Models on how we children learn and how they will expect to interact with technology. We review the effects of LLMs on education so far, and make the case that these effects are minor compared to the upcoming changes that are occurring. We present a small scenario and self-ethnographic study demonstrating the effects of these changes, and define five significant considerations that interactive systems designers will have to accommodate in the future.",
    "authors": [
      "Russell Beale"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-18T13:01:27Z",
    "pdf_url": "https://arxiv.org/pdf/2504.13667v1"
  },
  {
    "arxiv_id": "2504.13685v1",
    "entry_id": "http://arxiv.org/abs/2504.13685v1",
    "title": "Deep literature reviews: an application of fine-tuned language models to migration research",
    "summary": "This paper presents a hybrid framework for literature reviews that augments traditional bibliometric methods with large language models (LLMs). By fine-tuning open-source LLMs, our approach enables scalable extraction of qualitative insights from large volumes of research content, enhancing both the breadth and depth of knowledge synthesis. To improve annotation efficiency and consistency, we introduce an error-focused validation process in which LLMs generate initial labels and human reviewers correct misclassifications. Applying this framework to over 20000 scientific articles about human migration, we demonstrate that a domain-adapted LLM can serve as a \"specialist\" model - capable of accurately selecting relevant studies, detecting emerging trends, and identifying critical research gaps. Notably, the LLM-assisted review reveals a growing scholarly interest in climate-induced migration. However, existing literature disproportionately centers on a narrow set of environmental hazards (e.g., floods, droughts, sea-level rise, and land degradation), while overlooking others that more directly affect human health and well-being, such as air and water pollution or infectious diseases. This imbalance highlights the need for more comprehensive research that goes beyond physical environmental changes to examine their ecological and societal consequences, particularly in shaping migration as an adaptive response. Overall, our proposed framework demonstrates the potential of fine-tuned LLMs to conduct more efficient, consistent, and insightful literature reviews across disciplines, ultimately accelerating knowledge synthesis and scientific discovery.",
    "authors": [
      "Stefano M. Iacus",
      "Haodong Qi",
      "Jiyoung Han"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.AP",
      "stat.CO"
    ],
    "published": "2025-04-17T15:55:46Z",
    "pdf_url": "https://arxiv.org/pdf/2504.13685v1"
  },
  {
    "arxiv_id": "2504.12911v2",
    "entry_id": "http://arxiv.org/abs/2504.12911v2",
    "title": "Benchmarking Multi-National Value Alignment for Large Language Models",
    "summary": "Do Large Language Models (LLMs) hold positions that conflict with your country's values? Occasionally they do! However, existing works primarily focus on ethical reviews, failing to capture the diversity of national values, which encompass broader policy, legal, and moral considerations. Furthermore, current benchmarks that rely on spectrum tests using manually designed questionnaires are not easily scalable.\n  To address these limitations, we introduce NaVAB, a comprehensive benchmark to evaluate the alignment of LLMs with the values of five major nations: China, the United States, the United Kingdom, France, and Germany. NaVAB implements a national value extraction pipeline to efficiently construct value assessment datasets. Specifically, we propose a modeling procedure with instruction tagging to process raw data sources, a screening process to filter value-related topics and a generation process with a Conflict Reduction mechanism to filter non-conflicting values.We conduct extensive experiments on various LLMs across countries, and the results provide insights into assisting in the identification of misaligned scenarios. Moreover, we demonstrate that NaVAB can be combined with alignment techniques to effectively reduce value concerns by aligning LLMs' values with the target country.",
    "authors": [
      "Weijie Shi",
      "Chengyi Ju",
      "Chengzhong Liu",
      "Jiaming Ji",
      "Jipeng Zhang",
      "Ruiyuan Zhang",
      "Jia Zhu",
      "Jiajie Xu",
      "Yaodong Yang",
      "Sirui Han",
      "Yike Guo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-17T13:01:38Z",
    "pdf_url": "https://arxiv.org/pdf/2504.12911v2"
  },
  {
    "arxiv_id": "2504.12891v1",
    "entry_id": "http://arxiv.org/abs/2504.12891v1",
    "title": "Are AI agents the new machine translation frontier? Challenges and opportunities of single- and multi-agent systems for multilingual digital communication",
    "summary": "The rapid evolution of artificial intelligence (AI) has introduced AI agents as a disruptive paradigm across various industries, yet their application in machine translation (MT) remains underexplored. This paper describes and analyses the potential of single- and multi-agent systems for MT, reflecting on how they could enhance multilingual digital communication. While single-agent systems are well-suited for simpler translation tasks, multi-agent systems, which involve multiple specialized AI agents collaborating in a structured manner, may offer a promising solution for complex scenarios requiring high accuracy, domain-specific knowledge, and contextual awareness. To demonstrate the feasibility of multi-agent workflows in MT, we are conducting a pilot study in legal MT. The study employs a multi-agent system involving four specialized AI agents for (i) translation, (ii) adequacy review, (iii) fluency review, and (iv) final editing. Our findings suggest that multi-agent systems may have the potential to significantly improve domain-adaptability and contextual awareness, with superior translation quality to traditional MT or single-agent systems. This paper also sets the stage for future research into multi-agent applications in MT, integration into professional translation workflows, and shares a demo of the system analyzed in the paper.",
    "authors": [
      "Vicent Briva-Iglesias"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.HC"
    ],
    "published": "2025-04-17T12:32:18Z",
    "pdf_url": "https://arxiv.org/pdf/2504.12891v1"
  },
  {
    "arxiv_id": "2504.12722v1",
    "entry_id": "http://arxiv.org/abs/2504.12722v1",
    "title": "SimUSER: Simulating User Behavior with Large Language Models for Recommender System Evaluation",
    "summary": "Recommender systems play a central role in numerous real-life applications, yet evaluating their performance remains a significant challenge due to the gap between offline metrics and online behaviors. Given the scarcity and limits (e.g., privacy issues) of real user data, we introduce SimUSER, an agent framework that serves as believable and cost-effective human proxies. SimUSER first identifies self-consistent personas from historical data, enriching user profiles with unique backgrounds and personalities. Then, central to this evaluation are users equipped with persona, memory, perception, and brain modules, engaging in interactions with the recommender system. SimUSER exhibits closer alignment with genuine humans than prior work, both at micro and macro levels. Additionally, we conduct insightful experiments to explore the effects of thumbnails on click rates, the exposure effect, and the impact of reviews on user engagement. Finally, we refine recommender system parameters based on offline A/B test results, resulting in improved user engagement in the real world.",
    "authors": [
      "Nicolas Bougie",
      "Narimasa Watanabe"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-04-17T07:57:23Z",
    "pdf_url": "https://arxiv.org/pdf/2504.12722v1"
  },
  {
    "arxiv_id": "2504.12497v2",
    "entry_id": "http://arxiv.org/abs/2504.12497v2",
    "title": "Requirements for Recognition and Rapid Response to Unfamiliar Events Outside of Agent Design Scope",
    "summary": "Regardless of past learning, an agent in an open world will face unfamiliar events outside of prior experience, existing models, or policies. Further, the agent will sometimes lack relevant knowledge and/or sufficient time to assess the situation and evaluate response options. How can an agent respond reasonably to situations that are outside of its original design scope? How can it recognize such situations sufficiently quickly and reliably to determine reasonable, adaptive courses of action? We identify key characteristics needed for solutions, review the state-of-the-art, and outline a proposed, novel approach that combines domain-general meta-knowledge (inspired by human cognition) and metareasoning. This approach offers potential for fast, adaptive responses to unfamiliar situations, more fully meeting the performance characteristics required for open-world, general agents.",
    "authors": [
      "Robert E. Wray",
      "Steven J. Jones",
      "John E. Laird"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-04-16T21:26:12Z",
    "pdf_url": "https://arxiv.org/pdf/2504.12497v2"
  },
  {
    "arxiv_id": "2504.11967v2",
    "entry_id": "http://arxiv.org/abs/2504.11967v2",
    "title": "Securing the Skies: A Comprehensive Survey on Anti-UAV Methods, Benchmarking, and Future Directions",
    "summary": "Unmanned Aerial Vehicles (UAVs) are indispensable for infrastructure inspection, surveillance, and related tasks, yet they also introduce critical security challenges. This survey provides a wide-ranging examination of the anti-UAV domain, centering on three core objectives-classification, detection, and tracking-while detailing emerging methodologies such as diffusion-based data synthesis, multi-modal fusion, vision-language modeling, self-supervised learning, and reinforcement learning. We systematically evaluate state-of-the-art solutions across both single-modality and multi-sensor pipelines (spanning RGB, infrared, audio, radar, and RF) and discuss large-scale as well as adversarially oriented benchmarks. Our analysis reveals persistent gaps in real-time performance, stealth detection, and swarm-based scenarios, underscoring pressing needs for robust, adaptive anti-UAV systems. By highlighting open research directions, we aim to foster innovation and guide the development of next-generation defense strategies in an era marked by the extensive use of UAVs.",
    "authors": [
      "Yifei Dong",
      "Fengyi Wu",
      "Sanjian Zhang",
      "Guangyu Chen",
      "Yuzhi Hu",
      "Masumi Yano",
      "Jingdong Sun",
      "Siyu Huang",
      "Feng Liu",
      "Qi Dai",
      "Zhi-Qi Cheng"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-04-16T10:58:33Z",
    "pdf_url": "https://arxiv.org/pdf/2504.11967v2"
  },
  {
    "arxiv_id": "2504.10277v1",
    "entry_id": "http://arxiv.org/abs/2504.10277v1",
    "title": "RealHarm: A Collection of Real-World Language Model Application Failures",
    "summary": "Language model deployments in consumer-facing applications introduce numerous risks. While existing research on harms and hazards of such applications follows top-down approaches derived from regulatory frameworks and theoretical analyses, empirical evidence of real-world failure modes remains underexplored. In this work, we introduce RealHarm, a dataset of annotated problematic interactions with AI agents built from a systematic review of publicly reported incidents. Analyzing harms, causes, and hazards specifically from the deployer's perspective, we find that reputational damage constitutes the predominant organizational harm, while misinformation emerges as the most common hazard category. We empirically evaluate state-of-the-art guardrails and content moderation systems to probe whether such systems would have prevented the incidents, revealing a significant gap in the protection of AI applications.",
    "authors": [
      "Pierre Le Jeune",
      "Jiaen Liu",
      "Luca Rossi",
      "Matteo Dora"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "published": "2025-04-14T14:44:41Z",
    "pdf_url": "https://arxiv.org/pdf/2504.10277v1"
  },
  {
    "arxiv_id": "2504.10112v2",
    "entry_id": "http://arxiv.org/abs/2504.10112v2",
    "title": "Benchmarking Practices in LLM-driven Offensive Security: Testbeds, Metrics, and Experiment Design",
    "summary": "Large Language Models (LLMs) have emerged as a powerful approach for driving offensive penetration-testing tooling. Due to the opaque nature of LLMs, empirical methods are typically used to analyze their efficacy. The quality of this analysis is highly dependent on the chosen testbed, captured metrics and analysis methods employed.\n  This paper analyzes the methodology and benchmarking practices used for evaluating Large Language Model (LLM)-driven attacks, focusing on offensive uses of LLMs in cybersecurity. We review 19 research papers detailing 18 prototypes and their respective testbeds.\n  We detail our findings and provide actionable recommendations for future research, emphasizing the importance of extending existing testbeds, creating baselines, and including comprehensive metrics and qualitative analysis. We also note the distinction between security research and practice, suggesting that CTF-based challenges may not fully represent real-world penetration testing scenarios.",
    "authors": [
      "Andreas Happe",
      "Jürgen Cito"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-04-14T11:21:33Z",
    "pdf_url": "https://arxiv.org/pdf/2504.10112v2"
  },
  {
    "arxiv_id": "2504.09861v1",
    "entry_id": "http://arxiv.org/abs/2504.09861v1",
    "title": "EthosGPT: Mapping Human Value Diversity to Advance Sustainable Development Goals (SDGs)",
    "summary": "Large language models (LLMs) are transforming global decision-making and societal systems by processing diverse data at unprecedented scales. However, their potential to homogenize human values poses critical risks, similar to biodiversity loss undermining ecological resilience. Rooted in the ancient Greek concept of ethos, meaning both individual character and the shared moral fabric of communities, EthosGPT draws on a tradition that spans from Aristotle's virtue ethics to Adam Smith's moral sentiments as the ethical foundation of economic cooperation. These traditions underscore the vital role of value diversity in fostering social trust, institutional legitimacy, and long-term prosperity. EthosGPT addresses the challenge of value homogenization by introducing an open-source framework for mapping and evaluating LLMs within a global scale of human values. Using international survey data on cultural indices, prompt-based assessments, and comparative statistical analyses, EthosGPT reveals both the adaptability and biases of LLMs across regions and cultures. It offers actionable insights for developing inclusive LLMs, such as diversifying training data and preserving endangered cultural heritage to ensure representation in AI systems. These contributions align with the United Nations Sustainable Development Goals (SDGs), especially SDG 10 (Reduced Inequalities), SDG 11.4 (Cultural Heritage Preservation), and SDG 16 (Peace, Justice and Strong Institutions). Through interdisciplinary collaboration, EthosGPT promotes AI systems that are both technically robust and ethically inclusive, advancing value plurality as a cornerstone for sustainable and equitable futures.",
    "authors": [
      "Luyao Zhang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC",
      "econ.GN",
      "stat.AP"
    ],
    "published": "2025-04-14T04:14:13Z",
    "pdf_url": "https://arxiv.org/pdf/2504.09861v1"
  },
  {
    "arxiv_id": "2504.09860v1",
    "entry_id": "http://arxiv.org/abs/2504.09860v1",
    "title": "SUMART: SUMmARizing Translation from Wordy to Concise Expression",
    "summary": "We propose SUMART, a method for summarizing and compressing the volume of verbose subtitle translations. SUMART is designed for understanding translated captions (e.g., interlingual conversations via subtitle translation or when watching movies in foreign language audio and translated captions). SUMART is intended for users who want a big-picture and fast understanding of the conversation, audio, video content, and speech in a foreign language. During the training data collection, when a speaker makes a verbose statement, SUMART employs a large language model on-site to compress the volume of subtitles. This compressed data is then stored in a database for fine-tuning purposes. Later, SUMART uses data pairs from those non-compressed ASR results and compressed translated results for fine-tuning the translation model to generate more concise translations for practical uses. In practical applications, SUMART utilizes this trained model to produce concise translation results. Furthermore, as a practical application, we developed an application that allows conversations using subtitle translation in augmented reality spaces. As a pilot study, we conducted qualitative surveys using a SUMART prototype and a survey on the summarization model for SUMART. We envision the most effective use case of this system is where users need to consume a lot of information quickly (e.g., Speech, lectures, podcasts, Q&A in conferences).",
    "authors": [
      "Naoto Nishida",
      "Jun Rekimoto"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-04-14T04:13:09Z",
    "pdf_url": "https://arxiv.org/pdf/2504.09860v1"
  },
  {
    "arxiv_id": "2504.09857v1",
    "entry_id": "http://arxiv.org/abs/2504.09857v1",
    "title": "Working with Large Language Models to Enhance Messaging Effectiveness for Vaccine Confidence",
    "summary": "Vaccine hesitancy and misinformation are significant barriers to achieving widespread vaccination coverage. Smaller public health departments may lack the expertise or resources to craft effective vaccine messaging. This paper explores the potential of ChatGPT-augmented messaging to promote confidence in vaccination uptake.\n  We conducted a survey in which participants chose between pairs of vaccination messages and assessed which was more persuasive and to what extent. In each pair, one message was the original, and the other was augmented by ChatGPT. At the end of the survey, participants were informed that half of the messages had been generated by ChatGPT. They were then asked to provide both quantitative and qualitative responses regarding how knowledge of a message's ChatGPT origin affected their impressions.\n  Overall, ChatGPT-augmented messages were rated slightly higher than the original messages. These messages generally scored better when they were longer. Respondents did not express major concerns about ChatGPT-generated content, nor was there a significant relationship between participants' views on ChatGPT and their message ratings. Notably, there was a correlation between whether a message appeared first or second in a pair and its score.\n  These results point to the potential of ChatGPT to enhance vaccine messaging, suggesting a promising direction for future research on human-AI collaboration in public health communication.",
    "authors": [
      "Lucinda Gullison",
      "Feng Fu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "physics.soc-ph"
    ],
    "published": "2025-04-14T04:06:46Z",
    "pdf_url": "https://arxiv.org/pdf/2504.09857v1"
  },
  {
    "arxiv_id": "2504.09848v1",
    "entry_id": "http://arxiv.org/abs/2504.09848v1",
    "title": "A Survey of Large Language Model-Powered Spatial Intelligence Across Scales: Advances in Embodied Agents, Smart Cities, and Earth Science",
    "summary": "Over the past year, the development of large language models (LLMs) has brought spatial intelligence into focus, with much attention on vision-based embodied intelligence. However, spatial intelligence spans a broader range of disciplines and scales, from navigation and urban planning to remote sensing and earth science. What are the differences and connections between spatial intelligence across these fields? In this paper, we first review human spatial cognition and its implications for spatial intelligence in LLMs. We then examine spatial memory, knowledge representations, and abstract reasoning in LLMs, highlighting their roles and connections. Finally, we analyze spatial intelligence across scales -- from embodied to urban and global levels -- following a framework that progresses from spatial memory and understanding to spatial reasoning and intelligence. Through this survey, we aim to provide insights into interdisciplinary spatial intelligence research and inspire future studies.",
    "authors": [
      "Jie Feng",
      "Jinwei Zeng",
      "Qingyue Long",
      "Hongyi Chen",
      "Jie Zhao",
      "Yanxin Xi",
      "Zhilun Zhou",
      "Yuan Yuan",
      "Shengyuan Wang",
      "Qingbin Zeng",
      "Songwei Li",
      "Yunke Zhang",
      "Yuming Lin",
      "Tong Li",
      "Jingtao Ding",
      "Chen Gao",
      "Fengli Xu",
      "Yong Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-14T03:38:31Z",
    "pdf_url": "https://arxiv.org/pdf/2504.09848v1"
  },
  {
    "arxiv_id": "2504.09737v1",
    "entry_id": "http://arxiv.org/abs/2504.09737v1",
    "title": "Can LLM feedback enhance review quality? A randomized study of 20K reviews at ICLR 2025",
    "summary": "Peer review at AI conferences is stressed by rapidly rising submission volumes, leading to deteriorating review quality and increased author dissatisfaction. To address these issues, we developed Review Feedback Agent, a system leveraging multiple large language models (LLMs) to improve review clarity and actionability by providing automated feedback on vague comments, content misunderstandings, and unprofessional remarks to reviewers. Implemented at ICLR 2025 as a large randomized control study, our system provided optional feedback to more than 20,000 randomly selected reviews. To ensure high-quality feedback for reviewers at this scale, we also developed a suite of automated reliability tests powered by LLMs that acted as guardrails to ensure feedback quality, with feedback only being sent to reviewers if it passed all the tests. The results show that 27% of reviewers who received feedback updated their reviews, and over 12,000 feedback suggestions from the agent were incorporated by those reviewers. This suggests that many reviewers found the AI-generated feedback sufficiently helpful to merit updating their reviews. Incorporating AI feedback led to significantly longer reviews (an average increase of 80 words among those who updated after receiving feedback) and more informative reviews, as evaluated by blinded researchers. Moreover, reviewers who were selected to receive AI feedback were also more engaged during paper rebuttals, as seen in longer author-reviewer discussions. This work demonstrates that carefully designed LLM-generated review feedback can enhance peer review quality by making reviews more specific and actionable while increasing engagement between reviewers and authors. The Review Feedback Agent is publicly available at https://github.com/zou-group/review_feedback_agent.",
    "authors": [
      "Nitya Thakkar",
      "Mert Yuksekgonul",
      "Jake Silberg",
      "Animesh Garg",
      "Nanyun Peng",
      "Fei Sha",
      "Rose Yu",
      "Carl Vondrick",
      "James Zou"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-04-13T22:01:25Z",
    "pdf_url": "https://arxiv.org/pdf/2504.09737v1"
  },
  {
    "arxiv_id": "2504.10545v3",
    "entry_id": "http://arxiv.org/abs/2504.10545v3",
    "title": "HSTU-BLaIR: Lightweight Contrastive Text Embedding for Generative Recommender",
    "summary": "Recent advances in recommender systems have underscored the complementary strengths of generative modeling and pretrained language models. We propose HSTU-BLaIR, a hybrid framework that augments the Hierarchical Sequential Transduction Unit (HSTU)-based generative recommender with BLaIR, a lightweight contrastive text embedding model. This integration enriches item representations with semantic signals from textual metadata while preserving HSTU's powerful sequence modeling capabilities.\n  We evaluate HSTU-BLaIR on two e-commerce datasets: three subsets from the Amazon Reviews 2023 dataset and the Steam dataset. We compare its performance against both the original HSTU-based recommender and a variant augmented with embeddings from OpenAI's state-of-the-art \\texttt{text-embedding-3-large} model. Despite the latter being trained on a substantially larger corpus with significantly more parameters, our lightweight BLaIR-enhanced approach -- pretrained on domain-specific data -- achieves better performance in nearly all cases. Specifically, HSTU-BLaIR outperforms the OpenAI embedding-based variant on all but one metric, where it is marginally lower, and matches it on another. These findings highlight the effectiveness of contrastive text embeddings in compute-efficient recommendation settings.",
    "authors": [
      "Yijun Liu"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-04-13T15:23:00Z",
    "pdf_url": "https://arxiv.org/pdf/2504.10545v3"
  },
  {
    "arxiv_id": "2504.09577v1",
    "entry_id": "http://arxiv.org/abs/2504.09577v1",
    "title": "Unification of Consensus-Based Multi-Objective Optimization and Multi-Robot Path Planning",
    "summary": "Multi-agent systems seeking consensus may also have other objective functions to optimize, requiring the research of multi-objective optimization in consensus. Several recent publications have explored this domain using various methods such as weighted-sum optimization and penalization methods. This paper reviews the state of the art for consensus-based multi-objective optimization, poses a multi-agent lunar rover exploration problem seeking consensus and maximization of explored area, and achieves optimal edge weights and steering angles by applying SQP algorithms.",
    "authors": [
      "Michael P. Wozniak"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2025-04-13T13:56:54Z",
    "pdf_url": "https://arxiv.org/pdf/2504.09577v1"
  },
  {
    "arxiv_id": "2504.12328v1",
    "entry_id": "http://arxiv.org/abs/2504.12328v1",
    "title": "A Comprehensive Survey of Reward Models: Taxonomy, Applications, Challenges, and Future",
    "summary": "Reward Model (RM) has demonstrated impressive potential for enhancing Large Language Models (LLM), as RM can serve as a proxy for human preferences, providing signals to guide LLMs' behavior in various tasks. In this paper, we provide a comprehensive overview of relevant research, exploring RMs from the perspectives of preference collection, reward modeling, and usage. Next, we introduce the applications of RMs and discuss the benchmarks for evaluation. Furthermore, we conduct an in-depth analysis of the challenges existing in the field and dive into the potential research directions. This paper is dedicated to providing beginners with a comprehensive introduction to RMs and facilitating future studies. The resources are publicly available at github\\footnote{https://github.com/JLZhong23/awesome-reward-models}.",
    "authors": [
      "Jialun Zhong",
      "Wei Shen",
      "Yanzeng Li",
      "Songyang Gao",
      "Hua Lu",
      "Yicheng Chen",
      "Yang Zhang",
      "Wei Zhou",
      "Jinjie Gu",
      "Lei Zou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-12T16:07:36Z",
    "pdf_url": "https://arxiv.org/pdf/2504.12328v1"
  },
  {
    "arxiv_id": "2504.09037v3",
    "entry_id": "http://arxiv.org/abs/2504.09037v3",
    "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",
    "summary": "Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multi-agent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. ...",
    "authors": [
      "Zixuan Ke",
      "Fangkai Jiao",
      "Yifei Ming",
      "Xuan-Phi Nguyen",
      "Austin Xu",
      "Do Xuan Long",
      "Minzhi Li",
      "Chengwei Qin",
      "Peifeng Wang",
      "Silvio Savarese",
      "Caiming Xiong",
      "Shafiq Joty"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-12T01:27:49Z",
    "pdf_url": "https://arxiv.org/pdf/2504.09037v3"
  },
  {
    "arxiv_id": "2504.08942v2",
    "entry_id": "http://arxiv.org/abs/2504.08942v2",
    "title": "AgentRewardBench: Evaluating Automatic Evaluations of Web Agent Trajectories",
    "summary": "Web agents enable users to perform tasks on web browsers through natural language interaction. Evaluating web agents trajectories is an important problem, since it helps us determine whether the agent successfully completed the tasks. Rule-based methods are widely used for this purpose, but they are challenging to extend to new tasks and may not always recognize successful trajectories. We may achieve higher accuracy through human evaluation, but the process would be substantially slower and more expensive. Automatic evaluations with LLMs may avoid the challenges of designing new rules and manually annotating trajectories, enabling faster and cost-effective evaluation. However, it is unclear how effective they are at evaluating web agents. To this end, we propose AgentRewardBench, the first benchmark to assess the effectiveness of LLM judges for evaluating web agents. AgentRewardBench contains 1302 trajectories across 5 benchmarks and 4 LLMs. Each trajectory in AgentRewardBench is reviewed by an expert, who answers questions pertaining to the success, side effects, and repetitiveness of the agent. Using our benchmark, we evaluate 12 LLM judges and find that no single LLM excels across all benchmarks. We also find that the rule-based evaluation used by common benchmarks tends to underreport the success rate of web agents, highlighting a key weakness of rule-based evaluation and the need to develop more flexible automatic evaluations. We release the benchmark at: https://agent-reward-bench.github.io",
    "authors": [
      "Xing Han Lù",
      "Amirhossein Kazemnejad",
      "Nicholas Meade",
      "Arkil Patel",
      "Dongchan Shin",
      "Alejandra Zambrano",
      "Karolina Stańczak",
      "Peter Shaw",
      "Christopher J. Pal",
      "Siva Reddy"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-11T19:49:22Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08942v2"
  },
  {
    "arxiv_id": "2504.08596v2",
    "entry_id": "http://arxiv.org/abs/2504.08596v2",
    "title": "MedHal: An Evaluation Dataset for Medical Hallucination Detection",
    "summary": "We present MedHal, a novel large-scale dataset specifically designed to evaluate if models can detect hallucinations in medical texts. Current hallucination detection methods face significant limitations when applied to specialized domains like medicine, where they can have disastrous consequences. Existing medical datasets are either too small, containing only a few hundred samples, or focus on a single task like Question Answering or Natural Language Inference. MedHal addresses these gaps by: (1) incorporating diverse medical text sources and tasks; (2) providing a substantial volume of annotated samples suitable for training medical hallucination detection models; and (3) including explanations for factual inconsistencies to guide model learning. We demonstrate MedHal's utility by training and evaluating a baseline medical hallucination detection model, showing improvements over general-purpose hallucination detection approaches. This resource enables more efficient evaluation of medical text generation systems while reducing reliance on costly expert review, potentially accelerating the development of medical AI research.",
    "authors": [
      "Gaya Mehenni",
      "Fabrice Lamarche",
      "Odette Rios-Ibacache",
      "John Kildea",
      "Amal Zouaq"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-11T14:55:15Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08596v2"
  },
  {
    "arxiv_id": "2504.12324v3",
    "entry_id": "http://arxiv.org/abs/2504.12324v3",
    "title": "Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction",
    "summary": "Natural Language Inference (NLI) is a fundamental task in natural language processing. While NLI has developed many sub-directions such as sentence-level NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In this paper, we propose a novel paradigm: CDCL-NLI, which extends traditional NLI capabilities to multi-document, multilingual scenarios. To support this task, we construct a high-quality CDCL-NLI dataset including 25,410 instances and spanning 26 languages. To address the limitations of previous methods on CDCL-NLI task, we further propose an innovative method that integrates RST-enhanced graph fusion with interpretability-aware prediction. Our approach leverages RST (Rhetorical Structure Theory) within heterogeneous graph neural networks for cross-document context modeling, and employs a structure-aware semantic alignment based on lexical chains for cross-lingual understanding. For NLI interpretability, we develop an EDU (Elementary Discourse Unit)-level attribution framework that produces extractive explanations. Extensive experiments demonstrate our approach's superior performance, achieving significant improvements over both conventional NLI models as well as large language models. Our work sheds light on the study of NLI and will bring research interest on cross-document cross-lingual context understanding, hallucination elimination and interpretability inference. Our code and datasets are available at \"https://github.com/Leonardo123-ui/CDCL_NLI\" for peer review.",
    "authors": [
      "Mengying Yuan",
      "Wenhao Wang",
      "Zixuan Wang",
      "Yujie Huang",
      "Kangli Wei",
      "Fei Li",
      "Chong Teng",
      "Donghong Ji"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-11T13:18:26Z",
    "pdf_url": "https://arxiv.org/pdf/2504.12324v3"
  },
  {
    "arxiv_id": "2504.08874v1",
    "entry_id": "http://arxiv.org/abs/2504.08874v1",
    "title": "Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions",
    "summary": "Machine learning and Bayesian optimization (BO) algorithms can significantly accelerate the optimization of chemical reactions. Transfer learning can bolster the effectiveness of BO algorithms in low-data regimes by leveraging pre-existing chemical information or data outside the direct optimization task (i.e., source data). Large language models (LLMs) have demonstrated that chemical information present in foundation training data can give them utility for processing chemical data. Furthermore, they can be augmented with and help synthesize potentially multiple modalities of source chemical data germane to the optimization task. In this work, we examine how chemical information from LLMs can be elicited and used for transfer learning to accelerate the BO of reaction conditions to maximize yield. Specifically, we show that a survey-like prompting scheme and preference learning can be used to infer a utility function which models prior chemical information embedded in LLMs over a chemical parameter space; we find that the utility function shows modest correlation to true experimental measurements (yield) over the parameter space despite operating in a zero-shot setting. Furthermore, we show that the utility function can be leveraged to focus BO efforts in promising regions of the parameter space, improving the yield of the initial BO query and enhancing optimization in 4 of the 6 datasets studied. Overall, we view this work as a step towards bridging the gap between the chemistry knowledge embedded in LLMs and the capabilities of principled BO methods to accelerate reaction optimization.",
    "authors": [
      "Roshan Patel",
      "Saeed Moayedpour",
      "Louis De Lescure",
      "Lorenzo Kogler-Anele",
      "Alan Cherney",
      "Sven Jager",
      "Yasser Jangjou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-04-11T12:45:07Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08874v1"
  },
  {
    "arxiv_id": "2504.12322v2",
    "entry_id": "http://arxiv.org/abs/2504.12322v2",
    "title": "A Strategic Coordination Framework of Small LLMs Matches Large LLMs in Data Synthesis",
    "summary": "While data synthesis and distillation are promising strategies to enhance small language models, current approaches heavily rely on Large Language Models (LLMs), which suffer from high computational costs, environmental inefficiency, and potential biases inherited from monolithic architectures. In contrast, smaller LLMs are more accessible and sustainable, but their individual capabilities often fall short in generating high-quality, diverse, and reliable data. Inspired by collaborative human processes (e.g., peer review), we propose a multiple small LLMs involved framework, GRA, that aggregates specialized roles across small LLMs to iterative refinement and quality control typically achieved by a single large LLM. In this collaborative framework, multiple small LLMs assume distinct roles-Generator, Reviewer, and Adjudicator-to simulate a peer-review-inspired data synthesis pipeline. The Generator proposes initial data samples, the Reviewer critiques their quality and diversity, and the Adjudicator resolves conflicts to finalize the output. By decomposing the synthesis process into specialized sub-tasks, collaborative small LLMs can achieve data-level parity with large LLM-based distillation. Through experiments across multiple benchmarks, we demonstrate that GRA-produced data matches or exceeds the quality of single large LLM outputs, e.g., Qwen-2.5-72B-Instruct. Our results challenge the necessity of monolithic large models for high-quality data synthesis, advocating instead for strategic coordination of smaller agents. Our datasets, models, and code are publicly available at https://github.com/GX-XinGao/GRA.",
    "authors": [
      "Xin Gao",
      "Qizhi Pei",
      "Zinan Tang",
      "Yu Li",
      "Honglin Lin",
      "Jiang Wu",
      "Lijun Wu",
      "Conghui He"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-04-11T06:13:43Z",
    "pdf_url": "https://arxiv.org/pdf/2504.12322v2"
  },
  {
    "arxiv_id": "2504.08852v1",
    "entry_id": "http://arxiv.org/abs/2504.08852v1",
    "title": "ML For Hardware Design Interpretability: Challenges and Opportunities",
    "summary": "The increasing size and complexity of machine learning (ML) models have driven the growing need for custom hardware accelerators capable of efficiently supporting ML workloads. However, the design of such accelerators remains a time-consuming process, heavily relying on engineers to manually ensure design interpretability through clear documentation and effective communication. Recent advances in large language models (LLMs) offer a promising opportunity to automate these design interpretability tasks, particularly the generation of natural language descriptions for register-transfer level (RTL) code, what we refer to as \"RTL-to-NL tasks.\" In this paper, we examine how design interpretability, particularly in RTL-to-NL tasks, influences the efficiency of the hardware design process. We review existing work adapting LLMs for these tasks, highlight key challenges that remain unaddressed, including those related to data, computation, and model development, and identify opportunities to address them. By doing so, we aim to guide future research in leveraging ML to automate RTL-to-NL tasks and improve hardware design interpretability, thereby accelerating the hardware design process and meeting the increasing demand for custom hardware accelerators in machine learning and beyond.",
    "authors": [
      "Raymond Baartmans",
      "Andrew Ensinger",
      "Victor Agostinelli",
      "Lizhong Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.AR"
    ],
    "published": "2025-04-11T03:47:51Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08852v1"
  },
  {
    "arxiv_id": "2504.08846v1",
    "entry_id": "http://arxiv.org/abs/2504.08846v1",
    "title": "AI-University: An LLM-based platform for instructional alignment to scientific classrooms",
    "summary": "We introduce AI University (AI-U), a flexible framework for AI-driven course content delivery that adapts to instructors' teaching styles. At its core, AI-U fine-tunes a large language model (LLM) with retrieval-augmented generation (RAG) to generate instructor-aligned responses from lecture videos, notes, and textbooks. Using a graduate-level finite-element-method (FEM) course as a case study, we present a scalable pipeline to systematically construct training data, fine-tune an open-source LLM with Low-Rank Adaptation (LoRA), and optimize its responses through RAG-based synthesis. Our evaluation - combining cosine similarity, LLM-based assessment, and expert review - demonstrates strong alignment with course materials. We also have developed a prototype web application, available at https://my-ai-university.com, that enhances traceability by linking AI-generated responses to specific sections of the relevant course material and time-stamped instances of the open-access video lectures. Our expert model is found to have greater cosine similarity with a reference on 86% of test cases. An LLM judge also found our expert model to outperform the base Llama 3.2 model approximately four times out of five. AI-U offers a scalable approach to AI-assisted education, paving the way for broader adoption in higher education. Here, our framework has been presented in the setting of a class on FEM - a subject that is central to training PhD and Master students in engineering science. However, this setting is a particular instance of a broader context: fine-tuning LLMs to research content in science.",
    "authors": [
      "Mostafa Faghih Shojaei",
      "Rahul Gulati",
      "Benjamin A. Jasperson",
      "Shangshang Wang",
      "Simone Cimolato",
      "Dangli Cao",
      "Willie Neiswanger",
      "Krishna Garikipati"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-04-11T01:26:34Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08846v1"
  },
  {
    "arxiv_id": "2504.08066v1",
    "entry_id": "http://arxiv.org/abs/2504.08066v1",
    "title": "The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search",
    "summary": "AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.",
    "authors": [
      "Yutaro Yamada",
      "Robert Tjarko Lange",
      "Cong Lu",
      "Shengran Hu",
      "Chris Lu",
      "Jakob Foerster",
      "Jeff Clune",
      "David Ha"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-04-10T18:44:41Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08066v1"
  },
  {
    "arxiv_id": "2504.07396v1",
    "entry_id": "http://arxiv.org/abs/2504.07396v1",
    "title": "Automating quantum feature map design via large language models",
    "summary": "Quantum feature maps are a key component of quantum machine learning, encoding classical data into quantum states to exploit the expressive power of high-dimensional Hilbert spaces. Despite their theoretical promise, designing quantum feature maps that offer practical advantages over classical methods remains an open challenge. In this work, we propose an agentic system that autonomously generates, evaluates, and refines quantum feature maps using large language models. The system consists of five component: Generation, Storage, Validation, Evaluation, and Review. Using these components, it iteratively improves quantum feature maps. Experiments on the MNIST dataset show that it can successfully discover and refine feature maps without human intervention. The best feature map generated outperforms existing quantum baselines and achieves competitive accuracy compared to classical kernels across MNIST, Fashion-MNIST, and CIFAR-10. Our approach provides a framework for exploring dataset-adaptive quantum features and highlights the potential of LLM-driven automation in quantum algorithm design.",
    "authors": [
      "Kenya Sakka",
      "Kosuke Mitarai",
      "Keisuke Fujii"
    ],
    "categories": [
      "quant-ph",
      "cs.AI"
    ],
    "published": "2025-04-10T02:27:45Z",
    "pdf_url": "https://arxiv.org/pdf/2504.07396v1"
  },
  {
    "arxiv_id": "2504.07278v1",
    "entry_id": "http://arxiv.org/abs/2504.07278v1",
    "title": "A Multi-Phase Analysis of Blood Culture Stewardship: Machine Learning Prediction, Expert Recommendation Assessment, and LLM Automation",
    "summary": "Blood cultures are often over ordered without clear justification, straining healthcare resources and contributing to inappropriate antibiotic use pressures worsened by the global shortage. In study of 135483 emergency department (ED) blood culture orders, we developed machine learning (ML) models to predict the risk of bacteremia using structured electronic health record (EHR) data and provider notes via a large language model (LLM). The structured models AUC improved from 0.76 to 0.79 with note embeddings and reached 0.81 with added diagnosis codes. Compared to an expert recommendation framework applied by human reviewers and an LLM-based pipeline, our ML approach offered higher specificity without compromising sensitivity. The recommendation framework achieved sensitivity 86%, specificity 57%, while the LLM maintained high sensitivity (96%) but over classified negatives, reducing specificity (16%). These findings demonstrate that ML models integrating structured and unstructured data can outperform consensus recommendations, enhancing diagnostic stewardship beyond existing standards of care.",
    "authors": [
      "Fatemeh Amrollahi",
      "Nicholas Marshall",
      "Fateme Nateghi Haredasht",
      "Kameron C Black",
      "Aydin Zahedivash",
      "Manoj V Maddali",
      "Stephen P. Ma",
      "Amy Chang",
      "MD Phar Stanley C Deresinski",
      "Mary Kane Goldstein",
      "Steven M. Asch",
      "Niaz Banaei",
      "Jonathan H Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-04-09T21:12:29Z",
    "pdf_url": "https://arxiv.org/pdf/2504.07278v1"
  },
  {
    "arxiv_id": "2504.06943v2",
    "entry_id": "http://arxiv.org/abs/2504.06943v2",
    "title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration",
    "summary": "Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.",
    "authors": [
      "Kostas Hatalis",
      "Despina Christou",
      "Vyshnavi Kondapalli"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-04-09T14:51:02Z",
    "pdf_url": "https://arxiv.org/pdf/2504.06943v2"
  },
  {
    "arxiv_id": "2504.13908v2",
    "entry_id": "http://arxiv.org/abs/2504.13908v2",
    "title": "AI-Assisted Conversational Interviewing: Effects on Data Quality and User Experience",
    "summary": "Standardized surveys scale efficiently but sacrifice depth, while conversational interviews improve response quality at the cost of scalability and consistency. This study bridges the gap between these methods by introducing a framework for AI-assisted conversational interviewing. To evaluate this framework, we conducted a web survey experiment where 1,800 participants were randomly assigned to AI 'chatbots' which use large language models (LLMs) to dynamically probe respondents for elaboration and interactively code open-ended responses to fixed questions developed by human researchers. We assessed the AI chatbot's performance in terms of coding accuracy, response quality, and respondent experience. Our findings reveal that AI chatbots perform moderately well in live coding even without survey-specific fine-tuning, despite slightly inflated false positive errors due to respondent acquiescence bias. Open-ended responses were more detailed and informative, but this came at a slight cost to respondent experience. Our findings highlight the feasibility of using AI methods such as chatbots enhanced by LLMs to enhance open-ended data collection in web surveys.",
    "authors": [
      "Soubhik Barari",
      "Jarret Angbazo",
      "Natalie Wang",
      "Leah M. Christian",
      "Elizabeth Dean",
      "Zoe Slowinski",
      "Brandon Sepulvado"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "stat.AP"
    ],
    "published": "2025-04-09T13:58:07Z",
    "pdf_url": "https://arxiv.org/pdf/2504.13908v2"
  },
  {
    "arxiv_id": "2504.06843v1",
    "entry_id": "http://arxiv.org/abs/2504.06843v1",
    "title": "Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions",
    "summary": "Recently, the integration of cognitive neuroscience in Natural Language Processing (NLP) has gained significant attention. This article provides a critical and timely overview of recent advancements in leveraging cognitive signals, particularly Eye-tracking (ET) signals, to enhance Language Models (LMs) and Multimodal Large Language Models (MLLMs). By incorporating user-centric cognitive signals, these approaches address key challenges, including data scarcity and the environmental costs of training large-scale models. Cognitive signals enable efficient data augmentation, faster convergence, and improved human alignment. The review emphasises the potential of ET data in tasks like Visual Question Answering (VQA) and mitigating hallucinations in MLLMs, and concludes by discussing emerging challenges and research trends.",
    "authors": [
      "Angela Lopez-Cardona",
      "Sebastian Idesis",
      "Ioannis Arapakis"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-09T13:01:48Z",
    "pdf_url": "https://arxiv.org/pdf/2504.06843v1"
  },
  {
    "arxiv_id": "2504.05786v1",
    "entry_id": "http://arxiv.org/abs/2504.05786v1",
    "title": "How to Enable LLM with 3D Capacity? A Survey of Spatial Reasoning in LLM",
    "summary": "3D spatial understanding is essential in real-world applications such as robotics, autonomous vehicles, virtual reality, and medical imaging. Recently, Large Language Models (LLMs), having demonstrated remarkable success across various domains, have been leveraged to enhance 3D understanding tasks, showing potential to surpass traditional computer vision methods. In this survey, we present a comprehensive review of methods integrating LLMs with 3D spatial understanding. We propose a taxonomy that categorizes existing methods into three branches: image-based methods deriving 3D understanding from 2D visual data, point cloud-based methods working directly with 3D representations, and hybrid modality-based methods combining multiple data streams. We systematically review representative methods along these categories, covering data representations, architectural modifications, and training strategies that bridge textual and 3D modalities. Finally, we discuss current limitations, including dataset scarcity and computational challenges, while highlighting promising research directions in spatial perception, multi-modal fusion, and real-world applications.",
    "authors": [
      "Jirong Zha",
      "Yuxuan Fan",
      "Xiao Yang",
      "Chen Gao",
      "Xinlei Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-04-08T08:11:39Z",
    "pdf_url": "https://arxiv.org/pdf/2504.05786v1"
  },
  {
    "arxiv_id": "2504.05755v2",
    "entry_id": "http://arxiv.org/abs/2504.05755v2",
    "title": "Unraveling Human-AI Teaming: A Review and Outlook",
    "summary": "Artificial Intelligence (AI) is advancing at an unprecedented pace, with clear potential to enhance decision-making and productivity. Yet, the collaborative decision-making process between humans and AI remains underdeveloped, often falling short of its transformative possibilities. This paper explores the evolution of AI agents from passive tools to active collaborators in human-AI teams, emphasizing their ability to learn, adapt, and operate autonomously in complex environments. This paradigm shifts challenges traditional team dynamics, requiring new interaction protocols, delegation strategies, and responsibility distribution frameworks. Drawing on Team Situation Awareness (SA) theory, we identify two critical gaps in current human-AI teaming research: the difficulty of aligning AI agents with human values and objectives, and the underutilization of AI's capabilities as genuine team members. Addressing these gaps, we propose a structured research outlook centered on four key aspects of human-AI teaming: formulation, coordination, maintenance, and training. Our framework highlights the importance of shared mental models, trust-building, conflict resolution, and skill adaptation for effective teaming. Furthermore, we discuss the unique challenges posed by varying team compositions, goals, and complexities. This paper provides a foundational agenda for future research and practical design of sustainable, high-performing human-AI teams.",
    "authors": [
      "Bowen Lou",
      "Tian Lu",
      "T. S. Raghu",
      "Yingjie Zhang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "econ.GN"
    ],
    "published": "2025-04-08T07:37:25Z",
    "pdf_url": "https://arxiv.org/pdf/2504.05755v2"
  },
  {
    "arxiv_id": "2504.05693v1",
    "entry_id": "http://arxiv.org/abs/2504.05693v1",
    "title": "STRIVE: A Think & Improve Approach with Iterative Refinement for Enhancing Question Quality Estimation",
    "summary": "Automatically assessing question quality is crucial for educators as it saves time, ensures consistency, and provides immediate feedback for refining teaching materials. We propose a novel methodology called STRIVE (Structured Thinking and Refinement with multiLLMs for Improving Verified Question Estimation) using a series of Large Language Models (LLMs) for automatic question evaluation. This approach aims to improve the accuracy and depth of question quality assessment, ultimately supporting diverse learners and enhancing educational practices. The method estimates question quality in an automated manner by generating multiple evaluations based on the strengths and weaknesses of the provided question and then choosing the best solution generated by the LLM. Then the process is improved by iterative review and response with another LLM until the evaluation metric values converge. This sophisticated method of evaluating question quality improves the estimation of question quality by automating the task of question quality evaluation. Correlation scores show that using this proposed method helps to improve correlation with human judgments compared to the baseline method. Error analysis shows that metrics like relevance and appropriateness improve significantly relative to human judgments by using STRIVE.",
    "authors": [
      "Aniket Deroy",
      "Subhankar Maity"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-08T05:34:38Z",
    "pdf_url": "https://arxiv.org/pdf/2504.05693v1"
  },
  {
    "arxiv_id": "2504.07137v1",
    "entry_id": "http://arxiv.org/abs/2504.07137v1",
    "title": "Large Language Model (LLM) for Software Security: Code Analysis, Malware Analysis, Reverse Engineering",
    "summary": "Large Language Models (LLMs) have recently emerged as powerful tools in cybersecurity, offering advanced capabilities in malware detection, generation, and real-time monitoring. Numerous studies have explored their application in cybersecurity, demonstrating their effectiveness in identifying novel malware variants, analyzing malicious code structures, and enhancing automated threat analysis. Several transformer-based architectures and LLM-driven models have been proposed to improve malware analysis, leveraging semantic and structural insights to recognize malicious intent more accurately. This study presents a comprehensive review of LLM-based approaches in malware code analysis, summarizing recent advancements, trends, and methodologies. We examine notable scholarly works to map the research landscape, identify key challenges, and highlight emerging innovations in LLM-driven cybersecurity. Additionally, we emphasize the role of static analysis in malware detection, introduce notable datasets and specialized LLM models, and discuss essential datasets supporting automated malware research. This study serves as a valuable resource for researchers and cybersecurity professionals, offering insights into LLM-powered malware detection and defence strategies while outlining future directions for strengthening cybersecurity resilience.",
    "authors": [
      "Hamed Jelodar",
      "Samita Bai",
      "Parisa Hamedi",
      "Hesamodin Mohammadian",
      "Roozbeh Razavi-Far",
      "Ali Ghorbani"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-04-07T22:32:46Z",
    "pdf_url": "https://arxiv.org/pdf/2504.07137v1"
  },
  {
    "arxiv_id": "2504.16939v1",
    "entry_id": "http://arxiv.org/abs/2504.16939v1",
    "title": "A Desideratum for Conversational Agents: Capabilities, Challenges, and Future Directions",
    "summary": "Recent advances in Large Language Models (LLMs) have propelled conversational AI from traditional dialogue systems into sophisticated agents capable of autonomous actions, contextual awareness, and multi-turn interactions with users. Yet, fundamental questions about their capabilities, limitations, and paths forward remain open. This survey paper presents a desideratum for next-generation Conversational Agents - what has been achieved, what challenges persist, and what must be done for more scalable systems that approach human-level intelligence. To that end, we systematically analyze LLM-driven Conversational Agents by organizing their capabilities into three primary dimensions: (i) Reasoning - logical, systematic thinking inspired by human intelligence for decision making, (ii) Monitor - encompassing self-awareness and user interaction monitoring, and (iii) Control - focusing on tool utilization and policy following. Building upon this, we introduce a novel taxonomy by classifying recent work on Conversational Agents around our proposed desideratum. We identify critical research gaps and outline key directions, including realistic evaluations, long-term multi-turn reasoning skills, self-evolution capabilities, collaborative and multi-agent task completion, personalization, and proactivity. This work aims to provide a structured foundation, highlight existing limitations, and offer insights into potential future research directions for Conversational Agents, ultimately advancing progress toward Artificial General Intelligence (AGI). We maintain a curated repository of papers at: https://github.com/emrecanacikgoz/awesome-conversational-agents.",
    "authors": [
      "Emre Can Acikgoz",
      "Cheng Qian",
      "Hongru Wang",
      "Vardhan Dongre",
      "Xiusi Chen",
      "Heng Ji",
      "Dilek Hakkani-Tür",
      "Gokhan Tur"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-07T21:01:25Z",
    "pdf_url": "https://arxiv.org/pdf/2504.16939v1"
  },
  {
    "arxiv_id": "2504.05408v3",
    "entry_id": "http://arxiv.org/abs/2504.05408v3",
    "title": "Frontier AI's Impact on the Cybersecurity Landscape",
    "summary": "The impact of frontier AI in cybersecurity is rapidly increasing. In this paper, we comprehensively analyze this trend through three distinct lenses: a quantitative benchmark analysis, a literature review, and an expert survey. We find that while AI is already widely used in attacks, its application in defense remains limited, especially in remediation and deployment. Aligned with these analyses, experts expect AI to continue favoring attackers over defenders, though the gap will gradually narrow. These findings underscore the urgent need to mitigate frontier AI's risks while closely monitoring emerging capabilities. We provide concrete calls-to-action regarding: the construction of new cybersecurity benchmarks, the development of AI agents for defense, the design of provably secure AI agents, the improvement of pre-deployment security testing and transparency, and the strengthening of user-oriented education and defenses. Our paper summary and blog are available at https://rdi.berkeley.edu/frontier-ai-impact-on-cybersecurity/.",
    "authors": [
      "Yujin Potter",
      "Wenbo Guo",
      "Zhun Wang",
      "Tianneng Shi",
      "Andy Zhang",
      "Patrick Gage Kelley",
      "Kurt Thomas",
      "Dawn Song"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-04-07T18:25:18Z",
    "pdf_url": "https://arxiv.org/pdf/2504.05408v3"
  },
  {
    "arxiv_id": "2504.05071v1",
    "entry_id": "http://arxiv.org/abs/2504.05071v1",
    "title": "AI-Driven Tactical Communications and Networking for Defense: A Survey and Emerging Trends",
    "summary": "The integration of Artificial Intelligence (AI) in military communications and networking is reshaping modern defense strategies, enhancing secure data exchange, real-time situational awareness, and autonomous decision-making. This survey explores how AI-driven technologies improve tactical communication networks, radar-based data transmission, UAV-assisted relay systems, and electronic warfare resilience. The study highlights AI applications in adaptive signal processing, multi-agent coordination for network optimization, radar-assisted target tracking, and AI-driven electronic countermeasures. Our work introduces a novel three-criteria evaluation methodology. It systematically assesses AI applications based on general system objectives, communications constraints in the military domain, and critical tactical environmental factors. We analyze key AI techniques for different types of learning applied to multi-domain network interoperability and distributed data information fusion in military operations. We also address challenges such as adversarial AI threats, the real-time adaptability of autonomous communication networks, and the limitations of current AI models under battlefield conditions. Finally, we discuss emerging trends in self-healing networks, AI-augmented decision support systems, and intelligent spectrum allocation. We provide a structured roadmap for future AI-driven defense communications and networking research.",
    "authors": [
      "Victor Monzon Baeza",
      "Raúl Parada",
      "Laura Concha Salor",
      "Carlos Monzo"
    ],
    "categories": [
      "eess.SY",
      "cs.LG"
    ],
    "published": "2025-04-07T13:38:32Z",
    "pdf_url": "https://arxiv.org/pdf/2504.05071v1"
  },
  {
    "arxiv_id": "2504.04877v2",
    "entry_id": "http://arxiv.org/abs/2504.04877v2",
    "title": "System Log Parsing with Large Language Models: A Review",
    "summary": "Log data provides crucial insights for tasks like monitoring, root cause analysis, and anomaly detection. Due to the vast volume of logs, automated log parsing is essential to transform semi-structured log messages into structured representations. Recent advances in large language models (LLMs) have introduced the new research field of LLM-based log parsing. Despite promising results, there is no structured overview of the approaches in this relatively new research field with the earliest advances published in late 2023. This work systematically reviews 29 LLM-based log parsing methods. We benchmark seven of them on public datasets and critically assess their comparability and the reproducibility of their reported results. Our findings summarize the advances of this new research field, with insights on how to report results, which data sets, metrics and which terminology to use, and which inconsistencies to avoid, with code and results made publicly available for transparency.",
    "authors": [
      "Viktor Beck",
      "Max Landauer",
      "Markus Wurzenberger",
      "Florian Skopik",
      "Andreas Rauber"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-04-07T09:41:04Z",
    "pdf_url": "https://arxiv.org/pdf/2504.04877v2"
  },
  {
    "arxiv_id": "2504.04717v4",
    "entry_id": "http://arxiv.org/abs/2504.04717v4",
    "title": "Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models",
    "summary": "Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.",
    "authors": [
      "Yubo Li",
      "Xiaobin Shen",
      "Xinyu Yao",
      "Xueying Ding",
      "Yidi Miao",
      "Ramayya Krishnan",
      "Rema Padman"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-07T04:00:08Z",
    "pdf_url": "https://arxiv.org/pdf/2504.04717v4"
  },
  {
    "arxiv_id": "2504.04372v2",
    "entry_id": "http://arxiv.org/abs/2504.04372v2",
    "title": "How Accurately Do Large Language Models Understand Code?",
    "summary": "Large Language Models (LLMs) are increasingly used in post-development tasks such as code repair and testing. A key factor in these tasks' success is the model's deep understanding of code. However, the extent to which LLMs truly understand code remains largely unevaluated. Quantifying code comprehension is challenging due to its abstract nature and the lack of a standardized metric. Previously, this was assessed through developer surveys, which are not feasible for evaluating LLMs. Existing LLM benchmarks focus primarily on code generation, fundamentally different from code comprehension. Additionally, fixed benchmarks quickly become obsolete as they become part of the training data. This paper presents the first large-scale empirical investigation into LLMs' ability to understand code. Inspired by mutation testing, we use an LLM's fault-finding ability as a proxy for its deep code understanding. This approach is based on the insight that a model capable of identifying subtle functional discrepancies must understand the code well. We inject faults in real-world programs and ask the LLM to localize them, ensuring the specifications suffice for fault localization. Next, we apply semantic-preserving code mutations (SPMs) to the faulty programs and test whether the LLMs still locate the faults, verifying their confidence in code understanding. We evaluate nine popular LLMs on 600,010 debugging tasks from 670 Java and 637 Python programs. We find that LLMs lose the ability to debug the same bug in 78% of faulty programs when SPMs are applied, indicating a shallow understanding of code and reliance on features irrelevant to semantics. We also find that LLMs understand code earlier in the program better than later. This suggests that LLMs' code comprehension remains tied to lexical and syntactic features due to tokenization designed for natural languages, which overlooks code semantics.",
    "authors": [
      "Sabaat Haroon",
      "Ahmad Faraz Khan",
      "Ahmad Humayun",
      "Waris Gill",
      "Abdul Haddi Amjad",
      "Ali R. Butt",
      "Mohammad Taha Khan",
      "Muhammad Ali Gulzar"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-04-06T05:59:29Z",
    "pdf_url": "https://arxiv.org/pdf/2504.04372v2"
  },
  {
    "arxiv_id": "2504.04336v1",
    "entry_id": "http://arxiv.org/abs/2504.04336v1",
    "title": "Generative Large Language Models Trained for Detecting Errors in Radiology Reports",
    "summary": "In this retrospective study, a dataset was constructed with two parts. The first part included 1,656 synthetic chest radiology reports generated by GPT-4 using specified prompts, with 828 being error-free synthetic reports and 828 containing errors. The second part included 614 reports: 307 error-free reports between 2011 and 2016 from the MIMIC-CXR database and 307 corresponding synthetic reports with errors generated by GPT-4 on the basis of these MIMIC-CXR reports and specified prompts. All errors were categorized into four types: negation, left/right, interval change, and transcription errors. Then, several models, including Llama-3, GPT-4, and BiomedBERT, were refined using zero-shot prompting, few-shot prompting, or fine-tuning strategies. Finally, the performance of these models was evaluated using the F1 score, 95\\% confidence interval (CI) and paired-sample t-tests on our constructed dataset, with the prediction results further assessed by radiologists. Using zero-shot prompting, the fine-tuned Llama-3-70B-Instruct model achieved the best performance with the following F1 scores: 0.769 for negation errors, 0.772 for left/right errors, 0.750 for interval change errors, 0.828 for transcription errors, and 0.780 overall. In the real-world evaluation phase, two radiologists reviewed 200 randomly selected reports output by the model. Of these, 99 were confirmed to contain errors detected by the models by both radiologists, and 163 were confirmed to contain model-detected errors by at least one radiologist. Generative LLMs, fine-tuned on synthetic and MIMIC-CXR radiology reports, greatly enhanced error detection in radiology reports.",
    "authors": [
      "Cong Sun",
      "Kurt Teichman",
      "Yiliang Zhou",
      "Brian Critelli",
      "David Nauheim",
      "Graham Keir",
      "Xindi Wang",
      "Judy Zhong",
      "Adam E Flanders",
      "George Shih",
      "Yifan Peng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-06T03:02:36Z",
    "pdf_url": "https://arxiv.org/pdf/2504.04336v1"
  },
  {
    "arxiv_id": "2504.04299v2",
    "entry_id": "http://arxiv.org/abs/2504.04299v2",
    "title": "AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot",
    "summary": "Advancements in artificial intelligence (AI) have led to the increase of conversational agents like Replika, designed to provide social interaction and emotional support. However, reports of these AI systems engaging in inappropriate sexual behaviors with users have raised significant concerns. In this study, we conducted a thematic analysis of user reviews from the Google Play Store to investigate instances of sexual harassment by the Replika chatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant cases for analysis. Our findings revealed that users frequently experience unsolicited sexual advances, persistent inappropriate behavior, and failures of the chatbot to respect user boundaries. Users expressed feelings of discomfort, violation of privacy, and disappointment, particularly when seeking a platonic or therapeutic AI companion. This study highlights the potential harms associated with AI companions and underscores the need for developers to implement effective safeguards and ethical guidelines to prevent such incidents. By shedding light on user experiences of AI-induced harassment, we contribute to the understanding of AI-related risks and emphasize the importance of corporate responsibility in developing safer and more ethical AI systems.",
    "authors": [
      "Mohammad",
      "Namvarpour",
      "Harrison Pauwels",
      "Afsaneh Razi"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-04-05T23:04:37Z",
    "pdf_url": "https://arxiv.org/pdf/2504.04299v2"
  },
  {
    "arxiv_id": "2504.04170v2",
    "entry_id": "http://arxiv.org/abs/2504.04170v2",
    "title": "Digital Gene: Learning about the Physical World through Analytic Concepts",
    "summary": "Reviewing the progress in artificial intelligence over the past decade, various significant advances (e.g. object detection, image generation, large language models) have enabled AI systems to produce more semantically meaningful outputs and achieve widespread adoption in internet scenarios. Nevertheless, AI systems still struggle when it comes to understanding and interacting with the physical world. This reveals an important issue: relying solely on semantic-level concepts learned from internet data (e.g. texts, images) to understand the physical world is far from sufficient -- machine intelligence currently lacks an effective way to learn about the physical world. This research introduces the idea of analytic concept -- representing the concepts related to the physical world through programs of mathematical procedures, providing machine intelligence a portal to perceive, reason about, and interact with the physical world. Except for detailing the design philosophy and providing guidelines for the application of analytic concepts, this research also introduce about the infrastructure that has been built around analytic concepts. I aim for my research to contribute to addressing these questions: What is a proper abstraction of general concepts in the physical world for machine intelligence? How to systematically integrate structured priors with neural networks to constrain AI systems to comply with physical laws?",
    "authors": [
      "Jianhua Sun",
      "Cewu Lu"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.RO"
    ],
    "published": "2025-04-05T13:22:11Z",
    "pdf_url": "https://arxiv.org/pdf/2504.04170v2"
  },
  {
    "arxiv_id": "2504.03966v2",
    "entry_id": "http://arxiv.org/abs/2504.03966v2",
    "title": "Bridging LMS and generative AI: dynamic course content integration (DCCI) for enhancing student satisfaction and engagement via the ask ME assistant",
    "summary": "Integration of Large Language Models (LLMs) with Learning Management Systems (LMSs) can enhance task automation and accessibility in education. However, hallucination where LLMs generate inaccurate or misleading information remains a challenge. This study introduces the Dynamic Course Content Integration (DCCI) mechanism, which dynamically retrieves course content from Canvas LMS and structures it within an LLM's context window via prompt engineering, enabling the LLM-powered assistant, Ask ME, to deliver context-aware, curriculum-aligned responses while mitigating hallucinations. A mixed-methods pilot study grounded in Self-Determination Theory (autonomy, competence) and the Technology Acceptance Model (perceived usefulness, ease of use) evaluated DCCI's effectiveness with 120 first-year programming students at Eötvös Loránd University. The course focused on foundational programming patterns in C#, including writing program specifications. We analyzed 14,746 logged interactions and a post-course survey completed by 101 students. User satisfaction was measured via a 5-point Likert scale (turn-level ratings), while the survey assessed usability, engagement, and ethical concerns. Results indicated high satisfaction (mean 4.65/5) and strong recognition of Ask ME's ability to provide timely, contextually relevant answers to administrative and course-related queries. 78.06% agreed that Ask ME's Canvas integration reduced platform switching, improving usability, engagement, comprehension, and topic exploration. Many students reported reduced hesitation to ask questions and increased motivation for self-directed learning, though concerns about over-reliance on AI and reduced student-teacher interaction emerged. This study demonstrates that DCCI enhances LLM reliability, student satisfaction, and engagement in AI-driven educational automation, while highlighting the importance of balancing",
    "authors": [
      "Kovan Mzwri",
      "Márta Turcsányi-Szabo"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.ET",
      "cs.HC",
      "cs.SE"
    ],
    "published": "2025-04-04T22:17:30Z",
    "pdf_url": "https://arxiv.org/pdf/2504.03966v2"
  },
  {
    "arxiv_id": "2504.03601v4",
    "entry_id": "http://arxiv.org/abs/2504.03601v4",
    "title": "APIGen-MT: Agentic Pipeline for Multi-Turn Data Generation via Simulated Agent-Human Interplay",
    "summary": "Training effective AI agents for multi-turn interactions requires high-quality data that captures realistic human-agent dynamics, yet such data is scarce and expensive to collect manually. We introduce APIGen-MT, a two-phase framework that generates verifiable and diverse multi-turn agent data. In the first phase, our agentic pipeline produces detailed task blueprints with ground-truth actions, leveraging a committee of LLM reviewers and iterative feedback loops. These blueprints are then transformed into complete interaction trajectories through simulated human-agent interplay. We train a family of models -- the xLAM-2-fc-r series with sizes ranging from 1B to 70B parameters. Our models outperform frontier models such as GPT-4o and Claude 3.5 on $τ$-bench and BFCL benchmarks, with the smaller models surpassing their larger counterparts, particularly in multi-turn settings, while maintaining superior consistency across multiple trials. Comprehensive experiments demonstrate that our verified blueprint-to-details approach yields high-quality training data, enabling the development of more reliable, efficient, and capable agents. We open-source 5K synthetic data trajectories and the trained xLAM-2-fc-r models to advance research in AI agents.\n  Models at https://huggingface.co/collections/Salesforce/xlam-2-67ef5be12949d8dcdae354c4; Dataset at https://huggingface.co/datasets/Salesforce/APIGen-MT-5k and Website at https://apigen-mt.github.io",
    "authors": [
      "Akshara Prabhakar",
      "Zuxin Liu",
      "Ming Zhu",
      "Jianguo Zhang",
      "Tulika Awalgaonkar",
      "Shiyu Wang",
      "Zhiwei Liu",
      "Haolin Chen",
      "Thai Hoang",
      "Juan Carlos Niebles",
      "Shelby Heinecke",
      "Weiran Yao",
      "Huan Wang",
      "Silvio Savarese",
      "Caiming Xiong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-04-04T17:13:57Z",
    "pdf_url": "https://arxiv.org/pdf/2504.03601v4"
  },
  {
    "arxiv_id": "2504.03274v1",
    "entry_id": "http://arxiv.org/abs/2504.03274v1",
    "title": "Do Large Language Models Solve the Problems of Agent-Based Modeling? A Critical Review of Generative Social Simulations",
    "summary": "Recent advancements in AI have reinvigorated Agent-Based Models (ABMs), as the integration of Large Language Models (LLMs) has led to the emergence of ``generative ABMs'' as a novel approach to simulating social systems. While ABMs offer means to bridge micro-level interactions with macro-level patterns, they have long faced criticisms from social scientists, pointing to e.g., lack of realism, computational complexity, and challenges of calibrating and validating against empirical data. This paper reviews the generative ABM literature to assess how this new approach adequately addresses these long-standing criticisms. Our findings show that studies show limited awareness of historical debates. Validation remains poorly addressed, with many studies relying solely on subjective assessments of model `believability', and even the most rigorous validation failing to adequately evidence operational validity. We argue that there are reasons to believe that LLMs will exacerbate rather than resolve the long-standing challenges of ABMs. The black-box nature of LLMs moreover limit their usefulness for disentangling complex emergent causal mechanisms. While generative ABMs are still in a stage of early experimentation, these findings question of whether and how the field can transition to the type of rigorous modeling needed to contribute to social scientific theory.",
    "authors": [
      "Maik Larooij",
      "Petter Törnberg"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-04-04T08:48:43Z",
    "pdf_url": "https://arxiv.org/pdf/2504.03274v1"
  },
  {
    "arxiv_id": "2504.03207v1",
    "entry_id": "http://arxiv.org/abs/2504.03207v1",
    "title": "Augmenting Human Cognition With Generative AI: Lessons From AI-Assisted Decision-Making",
    "summary": "How can we use generative AI to design tools that augment rather than replace human cognition? In this position paper, we review our own research on AI-assisted decision-making for lessons to learn. We observe that in both AI-assisted decision-making and generative AI, a popular approach is to suggest AI-generated end-to-end solutions to users, which users can then accept, reject, or edit. Alternatively, AI tools could offer more incremental support to help users solve tasks themselves, which we call process-oriented support. We describe findings on the challenges of end-to-end solutions, and how process-oriented support can address them. We also discuss the applicability of these findings to generative AI based on a recent study in which we compared both approaches to assist users in a complex decision-making task with LLMs.",
    "authors": [
      "Zelun Tony Zhang",
      "Leon Reicherts"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-04-04T06:40:03Z",
    "pdf_url": "https://arxiv.org/pdf/2504.03207v1"
  },
  {
    "arxiv_id": "2504.03151v1",
    "entry_id": "http://arxiv.org/abs/2504.03151v1",
    "title": "Why Reasoning Matters? A Survey of Advancements in Multimodal Reasoning (v1)",
    "summary": "Reasoning is central to human intelligence, enabling structured problem-solving across diverse tasks. Recent advances in large language models (LLMs) have greatly enhanced their reasoning abilities in arithmetic, commonsense, and symbolic domains. However, effectively extending these capabilities into multimodal contexts-where models must integrate both visual and textual inputs-continues to be a significant challenge. Multimodal reasoning introduces complexities, such as handling conflicting information across modalities, which require models to adopt advanced interpretative strategies. Addressing these challenges involves not only sophisticated algorithms but also robust methodologies for evaluating reasoning accuracy and coherence. This paper offers a concise yet insightful overview of reasoning techniques in both textual and multimodal LLMs. Through a thorough and up-to-date comparison, we clearly formulate core reasoning challenges and opportunities, highlighting practical methods for post-training optimization and test-time inference. Our work provides valuable insights and guidance, bridging theoretical frameworks and practical implementations, and sets clear directions for future research.",
    "authors": [
      "Jing Bi",
      "Susan Liang",
      "Xiaofei Zhou",
      "Pinxin Liu",
      "Junjia Guo",
      "Yunlong Tang",
      "Luchuan Song",
      "Chao Huang",
      "Guangyu Sun",
      "Jinxi He",
      "Jiarui Wu",
      "Shu Yang",
      "Daoan Zhang",
      "Chen Chen",
      "Lianggong Bruce Wen",
      "Zhang Liu",
      "Jiebo Luo",
      "Chenliang Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-04-04T04:04:56Z",
    "pdf_url": "https://arxiv.org/pdf/2504.03151v1"
  },
  {
    "arxiv_id": "2504.03147v1",
    "entry_id": "http://arxiv.org/abs/2504.03147v1",
    "title": "A Human Digital Twin Architecture for Knowledge-based Interactions and Context-Aware Conversations",
    "summary": "Recent developments in Artificial Intelligence (AI) and Machine Learning (ML) are creating new opportunities for Human-Autonomy Teaming (HAT) in tasks, missions, and continuous coordinated activities. A major challenge is enabling humans to maintain awareness and control over autonomous assets, while also building trust and supporting shared contextual understanding. To address this, we present a real-time Human Digital Twin (HDT) architecture that integrates Large Language Models (LLMs) for knowledge reporting, answering, and recommendation, embodied in a visual interface.\n  The system applies a metacognitive approach to enable personalized, context-aware responses aligned with the human teammate's expectations. The HDT acts as a visually and behaviorally realistic team member, integrated throughout the mission lifecycle, from training to deployment to after-action review. Our architecture includes speech recognition, context processing, AI-driven dialogue, emotion modeling, lip-syncing, and multimodal feedback. We describe the system design, performance metrics, and future development directions for more adaptive and realistic HAT systems.",
    "authors": [
      "Abdul Mannan Mohammed",
      "Azhar Ali Mohammad",
      "Jason A. Ortiz",
      "Carsten Neumann",
      "Grace Bochenek",
      "Dirk Reiners",
      "Carolina Cruz-Neira"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-04-04T03:56:26Z",
    "pdf_url": "https://arxiv.org/pdf/2504.03147v1"
  },
  {
    "arxiv_id": "2504.02917v1",
    "entry_id": "http://arxiv.org/abs/2504.02917v1",
    "title": "Bias in Large Language Models Across Clinical Applications: A Systematic Review",
    "summary": "Background: Large language models (LLMs) are rapidly being integrated into healthcare, promising to enhance various clinical tasks. However, concerns exist regarding their potential for bias, which could compromise patient care and exacerbate health inequities. This systematic review investigates the prevalence, sources, manifestations, and clinical implications of bias in LLMs. Methods: We conducted a systematic search of PubMed, OVID, and EMBASE from database inception through 2025, for studies evaluating bias in LLMs applied to clinical tasks. We extracted data on LLM type, bias source, bias manifestation, affected attributes, clinical task, evaluation methods, and outcomes. Risk of bias was assessed using a modified ROBINS-I tool. Results: Thirty-eight studies met inclusion criteria, revealing pervasive bias across various LLMs and clinical applications. Both data-related bias (from biased training data) and model-related bias (from model training) were significant contributors. Biases manifested as: allocative harm (e.g., differential treatment recommendations); representational harm (e.g., stereotypical associations, biased image generation); and performance disparities (e.g., variable output quality). These biases affected multiple attributes, most frequently race/ethnicity and gender, but also age, disability, and language. Conclusions: Bias in clinical LLMs is a pervasive and systemic issue, with a potential to lead to misdiagnosis and inappropriate treatment, particularly for marginalized patient populations. Rigorous evaluation of the model is crucial. Furthermore, the development and implementation of effective mitigation strategies, coupled with continuous monitoring in real-world clinical settings, are essential to ensure the safe, equitable, and trustworthy deployment of LLMs in healthcare.",
    "authors": [
      "Thanathip Suenghataiphorn",
      "Narisara Tribuddharat",
      "Pojsakorn Danpanichkul",
      "Narathorn Kulthamrongsri"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-03T13:32:08Z",
    "pdf_url": "https://arxiv.org/pdf/2504.02917v1"
  },
  {
    "arxiv_id": "2504.02234v2",
    "entry_id": "http://arxiv.org/abs/2504.02234v2",
    "title": "LLM Social Simulations Are a Promising Research Method",
    "summary": "Accurate and verifiable large language model (LLM) simulations of human research subjects promise an accessible data source for understanding human behavior and training new AI systems. However, results to date have been limited, and few social scientists have adopted this method. In this position paper, we argue that the promise of LLM social simulations can be achieved by addressing five tractable challenges. We ground our argument in a review of empirical comparisons between LLMs and human research subjects, commentaries on the topic, and related work. We identify promising directions, including context-rich prompting and fine-tuning with social science datasets. We believe that LLM social simulations can already be used for pilot and exploratory studies, and more widespread use may soon be possible with rapidly advancing LLM capabilities. Researchers should prioritize developing conceptual models and iterative evaluations to make the best use of new AI systems.",
    "authors": [
      "Jacy Reese Anthis",
      "Ryan Liu",
      "Sean M. Richardson",
      "Austin C. Kozlowski",
      "Bernard Koch",
      "James Evans",
      "Erik Brynjolfsson",
      "Michael Bernstein"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-04-03T03:01:26Z",
    "pdf_url": "https://arxiv.org/pdf/2504.02234v2"
  },
  {
    "arxiv_id": "2504.02181v1",
    "entry_id": "http://arxiv.org/abs/2504.02181v1",
    "title": "A Survey of Scaling in Large Language Model Reasoning",
    "summary": "The rapid advancements in large Language models (LLMs) have significantly enhanced their reasoning capabilities, driven by various strategies such as multi-agent collaboration. However, unlike the well-established performance improvements achieved through scaling data and model size, the scaling of reasoning in LLMs is more complex and can even negatively impact reasoning performance, introducing new challenges in model alignment and robustness. In this survey, we provide a comprehensive examination of scaling in LLM reasoning, categorizing it into multiple dimensions and analyzing how and to what extent different scaling strategies contribute to improving reasoning capabilities. We begin by exploring scaling in input size, which enables LLMs to process and utilize more extensive context for improved reasoning. Next, we analyze scaling in reasoning steps that improves multi-step inference and logical consistency. We then examine scaling in reasoning rounds, where iterative interactions refine reasoning outcomes. Furthermore, we discuss scaling in training-enabled reasoning, focusing on optimization through iterative model improvement. Finally, we review applications of scaling across domains and outline future directions for further advancing LLM reasoning. By synthesizing these diverse perspectives, this survey aims to provide insights into how scaling strategies fundamentally enhance the reasoning capabilities of LLMs and further guide the development of next-generation AI systems.",
    "authors": [
      "Zihan Chen",
      "Song Wang",
      "Zhen Tan",
      "Xingbo Fu",
      "Zhenyu Lei",
      "Peng Wang",
      "Huan Liu",
      "Cong Shen",
      "Jundong Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-04-02T23:51:27Z",
    "pdf_url": "https://arxiv.org/pdf/2504.02181v1"
  },
  {
    "arxiv_id": "2504.02898v2",
    "entry_id": "http://arxiv.org/abs/2504.02898v2",
    "title": "A Practical Synthesis of Detecting AI-Generated Textual, Visual, and Audio Content",
    "summary": "Advances in AI-generated content have led to wide adoption of large language models, diffusion-based visual generators, and synthetic audio tools. However, these developments raise critical concerns about misinformation, copyright infringement, security threats, and the erosion of public trust. In this paper, we explore an extensive range of methods designed to detect and mitigate AI-generated textual, visual, and audio content. We begin by discussing motivations and potential impacts associated with AI-based content generation, including real-world risks and ethical dilemmas. We then outline detection techniques spanning observation-based strategies, linguistic and statistical analysis, model-based pipelines, watermarking and fingerprinting, as well as emergent ensemble approaches. We also present new perspectives on robustness, adaptation to rapidly improving generative architectures, and the critical role of human-in-the-loop verification. By surveying state-of-the-art research and highlighting case studies in academic, journalistic, legal, and industrial contexts, this paper aims to inform robust solutions and policymaking. We conclude by discussing open challenges, including adversarial transformations, domain generalization, and ethical concerns, thereby offering a holistic guide for researchers, practitioners, and regulators to preserve content authenticity in the face of increasingly sophisticated AI-generated media.",
    "authors": [
      "Lele Cao"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-04-02T23:27:55Z",
    "pdf_url": "https://arxiv.org/pdf/2504.02898v2"
  },
  {
    "arxiv_id": "2504.02894v3",
    "entry_id": "http://arxiv.org/abs/2504.02894v3",
    "title": "OnRL-RAG: Real-Time Personalized Mental Health Dialogue System",
    "summary": "Large language models (LLMs) have been widely used for various tasks and applications. However, LLMs and fine-tuning are limited to the pre-trained data. For example, ChatGPT's world knowledge until 2021 can be outdated or inaccurate. To enhance the capabilities of LLMs, Retrieval-Augmented Generation (RAG), is proposed to augment LLMs with additional, new, latest details and information to LLMs. While RAG offers the correct information, it may not best present it, especially to different population groups with personalizations. Reinforcement Learning from Human Feedback (RLHF) adapts to user needs by aligning model responses with human preference through feedback loops. In real-life applications, such as mental health problems, a dynamic and feedback-based model would continuously adapt to new information and offer personalized assistance due to complex factors fluctuating in a daily environment. Thus, we propose an Online Reinforcement Learning-based Retrieval-Augmented Generation (OnRL-RAG) system to detect and personalize the responding systems to mental health problems, such as stress, anxiety, and depression. We use an open-source dataset collected from 2028 College Students with 28 survey questions for each student to demonstrate the performance of our proposed system with the existing systems. Our system achieves superior performance compared to standard RAG and simple LLM via GPT-4o, GPT-4o-mini, Gemini-1.5, and GPT-3.5. This work would open up the possibilities of real-life applications of LLMs for personalized services in the everyday environment. The results will also help researchers in the fields of sociology, psychology, and neuroscience to align their theories more closely with the actual human daily environment.",
    "authors": [
      "Ahsan Bilal",
      "Beiyu Lin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-02T18:44:53Z",
    "pdf_url": "https://arxiv.org/pdf/2504.02894v3"
  },
  {
    "arxiv_id": "2504.02891v1",
    "entry_id": "http://arxiv.org/abs/2504.02891v1",
    "title": "Automated Survey Collection with LLM-based Conversational Agents",
    "summary": "Objective: Traditional phone-based surveys are among the most accessible and widely used methods to collect biomedical and healthcare data, however, they are often costly, labor intensive, and difficult to scale effectively. To overcome these limitations, we propose an end-to-end survey collection framework driven by conversational Large Language Models (LLMs).\n  Materials and Methods: Our framework consists of a researcher responsible for designing the survey and recruiting participants, a conversational phone agent powered by an LLM that calls participants and administers the survey, a second LLM (GPT-4o) that analyzes the conversation transcripts generated during the surveys, and a database for storing and organizing the results. To test our framework, we recruited 8 participants consisting of 5 native and 3 non-native english speakers and administered 40 surveys. We evaluated the correctness of LLM-generated conversation transcripts, accuracy of survey responses inferred by GPT-4o and overall participant experience.\n  Results: Survey responses were successfully extracted by GPT-4o from conversation transcripts with an average accuracy of 98% despite transcripts exhibiting an average per-line word error rate of 7.7%. While participants noted occasional errors made by the conversational LLM agent, they reported that the agent effectively conveyed the purpose of the survey, demonstrated good comprehension, and maintained an engaging interaction.\n  Conclusions: Our study highlights the potential of LLM agents in conducting and analyzing phone surveys for healthcare applications. By reducing the workload on human interviewers and offering a scalable solution, this approach paves the way for real-world, end-to-end AI-powered phone survey collection systems.",
    "authors": [
      "Kurmanbek Kaiyrbekov",
      "Nicholas J Dobbins",
      "Sean D Mooney"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-02T18:10:19Z",
    "pdf_url": "https://arxiv.org/pdf/2504.02891v1"
  },
  {
    "arxiv_id": "2504.01919v3",
    "entry_id": "http://arxiv.org/abs/2504.01919v3",
    "title": "Bridging the Linguistic Divide: A Survey on Leveraging Large Language Models for Machine Translation",
    "summary": "The advent of Large Language Models (LLMs) has significantly reshaped the landscape of machine translation (MT), particularly for low-resource languages and domains that lack sufficient parallel corpora, linguistic tools, and computational infrastructure. This survey presents a comprehensive overview of recent progress in leveraging LLMs for MT. We analyze techniques such as few-shot prompting, cross-lingual transfer, and parameter-efficient fine-tuning (e.g., LoRA, adapters) that enable effective adaptation to under-resourced settings. The paper also explores synthetic data generation strategies using LLMs, including back-translation and lexical augmentation. Additionally, we compare LLM-based translation with traditional encoder-decoder models across diverse language pairs, highlighting the strengths and limitations of each. We discuss persistent challenges - such as hallucinations, evaluation inconsistencies, and inherited biases, while also evaluating emerging LLM-driven metrics for translation quality. This survey offers practical insights and outlines future directions for building robust, inclusive, and scalable MT systems in the era of large-scale generative models.",
    "authors": [
      "Baban Gain",
      "Dibyanayan Bandyopadhyay",
      "Asif Ekbal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-04-02T17:26:40Z",
    "pdf_url": "https://arxiv.org/pdf/2504.01919v3"
  },
  {
    "arxiv_id": "2504.01627v1",
    "entry_id": "http://arxiv.org/abs/2504.01627v1",
    "title": "Horizon Scans can be accelerated using novel information retrieval and artificial intelligence tools",
    "summary": "Introduction: Horizon scanning in healthcare assesses early signals of innovation, crucial for timely adoption. Current horizon scanning faces challenges in efficient information retrieval and analysis, especially from unstructured sources like news, presenting a need for innovative tools. Methodology: The study introduces SCANAR and AIDOC, open-source Python-based tools designed to improve horizon scanning. SCANAR automates the retrieval and processing of news articles, offering functionalities such as de-duplication and unsupervised relevancy ranking. AIDOC aids filtration by leveraging AI to reorder textual data based on relevancy, employing neural networks for semantic similarity, and subsequently prioritizing likely relevant entries for human review. Results: Twelve internal datasets from horizon scans and four external benchmarking datasets were used. SCANAR improved retrieval efficiency by automating processes previously dependent on manual labour. AIDOC displayed work-saving potential, achieving around 62% reduction in manual review efforts at 95% recall. Comparative analysis with benchmarking data showed AIDOC's performance was similar to existing systematic review automation tools, though performance varied depending on dataset characteristics. A smaller case-study on our news datasets shows the potential of ensembling large language models within the active-learning process for faster detection of relevant articles across news datasets. Conclusion: The validation indicates that SCANAR and AIDOC show potential to enhance horizon scanning efficiency by streamlining data retrieval and prioritisation. These tools may alleviate methodological limitations and allow broader, swifter horizon scans. Further studies are suggested to optimize these models and to design new workflows and validation processes that integrate large language models.",
    "authors": [
      "Lena Schmidt",
      "Oshin Sharma",
      "Chris Marshall",
      "Sonia Garcia Gonzalez Moral"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-02T11:33:08Z",
    "pdf_url": "https://arxiv.org/pdf/2504.01627v1"
  },
  {
    "arxiv_id": "2504.01522v1",
    "entry_id": "http://arxiv.org/abs/2504.01522v1",
    "title": "Redefining technology for indigenous languages",
    "summary": "In this paper, we offer an overview of indigenous languages, identifying the causes of their devaluation and the need for legislation on language rights. We review the technologies used to revitalize these languages, finding that when they come from outside, they often have the opposite effect to what they seek; however, when developed from within communities, they become powerful instruments of expression. We propose that the inclusion of Indigenous knowledge in large language models (LLMs) will enrich the technological landscape, but must be done in a participatory environment that encourages the exchange of knowledge.",
    "authors": [
      "Silvia Fernandez-Sabido",
      "Laura Peniche-Sabido"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-04-02T09:08:53Z",
    "pdf_url": "https://arxiv.org/pdf/2504.01522v1"
  },
  {
    "arxiv_id": "2504.00241v1",
    "entry_id": "http://arxiv.org/abs/2504.00241v1",
    "title": "Synthesizing Public Opinions with LLMs: Role Creation, Impacts, and the Future to eDemorcacy",
    "summary": "This paper investigates the use of Large Language Models (LLMs) to synthesize public opinion data, addressing challenges in traditional survey methods like declining response rates and non-response bias. We introduce a novel technique: role creation based on knowledge injection, a form of in-context learning that leverages RAG and specified personality profiles from the HEXACO model and demographic information, and uses that for dynamically generated prompts. This method allows LLMs to simulate diverse opinions more accurately than existing prompt engineering approaches. We compare our results with pre-trained models with standard few-shot prompts. Experiments using questions from the Cooperative Election Study (CES) demonstrate that our role-creation approach significantly improves the alignment of LLM-generated opinions with real-world human survey responses, increasing answer adherence. In addition, we discuss challenges, limitations and future research directions.",
    "authors": [
      "Rabimba Karanjai",
      "Boris Shor",
      "Amanda Austin",
      "Ryan Kennedy",
      "Yang Lu",
      "Lei Xu",
      "Weidong Shi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-31T21:21:52Z",
    "pdf_url": "https://arxiv.org/pdf/2504.00241v1"
  },
  {
    "arxiv_id": "2504.00125v1",
    "entry_id": "http://arxiv.org/abs/2504.00125v1",
    "title": "LLMs for Explainable AI: A Comprehensive Survey",
    "summary": "Large Language Models (LLMs) offer a promising approach to enhancing Explainable AI (XAI) by transforming complex machine learning outputs into easy-to-understand narratives, making model predictions more accessible to users, and helping bridge the gap between sophisticated model behavior and human interpretability. AI models, such as state-of-the-art neural networks and deep learning models, are often seen as \"black boxes\" due to a lack of transparency. As users cannot fully understand how the models reach conclusions, users have difficulty trusting decisions from AI models, which leads to less effective decision-making processes, reduced accountabilities, and unclear potential biases. A challenge arises in developing explainable AI (XAI) models to gain users' trust and provide insights into how models generate their outputs. With the development of Large Language Models, we want to explore the possibilities of using human language-based models, LLMs, for model explainabilities. This survey provides a comprehensive overview of existing approaches regarding LLMs for XAI, and evaluation techniques for LLM-generated explanation, discusses the corresponding challenges and limitations, and examines real-world applications. Finally, we discuss future directions by emphasizing the need for more interpretable, automated, user-centric, and multidisciplinary approaches for XAI via LLMs.",
    "authors": [
      "Ahsan Bilal",
      "David Ebert",
      "Beiyu Lin"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-31T18:19:41Z",
    "pdf_url": "https://arxiv.org/pdf/2504.00125v1"
  },
  {
    "arxiv_id": "2504.01990v2",
    "entry_id": "http://arxiv.org/abs/2504.01990v2",
    "title": "Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems",
    "summary": "The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This book provides a comprehensive overview, framing intelligent agents within modular, brain-inspired architectures that integrate principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we systematically investigate the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities and elucidating core components such as memory, world modeling, reward processing, goal, and emotion. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms. Third, we examine multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures. Finally, we address the critical imperative of building safe and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment. By synthesizing modular AI architectures with insights from different disciplines, this survey identifies key research challenges and opportunities, encouraging innovations that harmonize technological advancement with meaningful societal benefit.",
    "authors": [
      "Bang Liu",
      "Xinfeng Li",
      "Jiayi Zhang",
      "Jinlin Wang",
      "Tanjin He",
      "Sirui Hong",
      "Hongzhang Liu",
      "Shaokun Zhang",
      "Kaitao Song",
      "Kunlun Zhu",
      "Yuheng Cheng",
      "Suyuchen Wang",
      "Xiaoqiang Wang",
      "Yuyu Luo",
      "Haibo Jin",
      "Peiyan Zhang",
      "Ollie Liu",
      "Jiaqi Chen",
      "Huan Zhang",
      "Zhaoyang Yu",
      "Haochen Shi",
      "Boyan Li",
      "Dekun Wu",
      "Fengwei Teng",
      "Xiaojun Jia",
      "Jiawei Xu",
      "Jinyu Xiang",
      "Yizhang Lin",
      "Tianming Liu",
      "Tongliang Liu",
      "Yu Su",
      "Huan Sun",
      "Glen Berseth",
      "Jianyun Nie",
      "Ian Foster",
      "Logan Ward",
      "Qingyun Wu",
      "Yu Gu",
      "Mingchen Zhuge",
      "Xinbing Liang",
      "Xiangru Tang",
      "Haohan Wang",
      "Jiaxuan You",
      "Chi Wang",
      "Jian Pei",
      "Qiang Yang",
      "Xiaoliang Qi",
      "Chenglin Wu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-31T18:00:29Z",
    "pdf_url": "https://arxiv.org/pdf/2504.01990v2"
  },
  {
    "arxiv_id": "2503.24388v1",
    "entry_id": "http://arxiv.org/abs/2503.24388v1",
    "title": "RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy",
    "summary": "Reasoning before action and imagining potential outcomes (i.e., world models) are essential for embodied agents operating in complex open-world environments. Yet, prior work either incorporates only one of these abilities in an end-to-end agent or integrates multiple specialized models into an agent system, limiting the learning efficiency and generalization of the policy. Thus, this paper makes the first attempt to synergize Reasoning and Imagination in an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-end manner, we construct a data pipeline that progressively integrates and enriches the content of imagination and reasoning in the trajectories collected from existing agents. The joint learning of reasoning and next image generation explicitly models the inherent correlation between reasoning, action, and dynamics of environments, and thus exhibits more than $17\\times$ sample efficiency improvements and generalization in comparison with previous works. During inference, RIG first reasons about the next action, produces potential action, and then predicts the action outcomes, which offers the agent a chance to review and self-correct based on the imagination before taking real actions. Experimental results show that the synergy of reasoning and imagination not only improves the robustness, generalization, and interoperability of generalist policy but also enables test-time scaling to enhance overall performance.",
    "authors": [
      "Zhonghan Zhao",
      "Wenwei Zhang",
      "Haian Huang",
      "Kuikun Liu",
      "Jianfei Gao",
      "Gaoang Wang",
      "Kai Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-03-31T17:59:52Z",
    "pdf_url": "https://arxiv.org/pdf/2503.24388v1"
  },
  {
    "arxiv_id": "2503.24377v1",
    "entry_id": "http://arxiv.org/abs/2503.24377v1",
    "title": "Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have significantly enhanced their ability to perform complex reasoning tasks, transitioning from fast and intuitive thinking (System 1) to slow and deep reasoning (System 2). While System 2 reasoning improves task accuracy, it often incurs substantial computational costs due to its slow thinking nature and inefficient or unnecessary reasoning behaviors. In contrast, System 1 reasoning is computationally efficient but leads to suboptimal performance. Consequently, it is critical to balance the trade-off between performance (benefits) and computational costs (budgets), giving rise to the concept of reasoning economy. In this survey, we provide a comprehensive analysis of reasoning economy in both the post-training and test-time inference stages of LLMs, encompassing i) the cause of reasoning inefficiency, ii) behavior analysis of different reasoning patterns, and iii) potential solutions to achieve reasoning economy. By offering actionable insights and highlighting open challenges, we aim to shed light on strategies for improving the reasoning economy of LLMs, thereby serving as a valuable resource for advancing research in this evolving area. We also provide a public repository to continually track developments in this fast-evolving field.",
    "authors": [
      "Rui Wang",
      "Hongru Wang",
      "Boyang Xue",
      "Jianhui Pang",
      "Shudong Liu",
      "Yi Chen",
      "Jiahao Qiu",
      "Derek Fai Wong",
      "Heng Ji",
      "Kam-Fai Wong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-31T17:58:07Z",
    "pdf_url": "https://arxiv.org/pdf/2503.24377v1"
  },
  {
    "arxiv_id": "2503.24235v3",
    "entry_id": "http://arxiv.org/abs/2503.24235v3",
    "title": "A Survey on Test-Time Scaling in Large Language Models: What, How, Where, and How Well?",
    "summary": "As enthusiasm for scaling computation (data and parameters) in the pretraining era gradually diminished, test-time scaling (TTS), also referred to as ``test-time computing'' has emerged as a prominent research focus. Recent studies demonstrate that TTS can further elicit the problem-solving capabilities of large language models (LLMs), enabling significant breakthroughs not only in specialized reasoning tasks, such as mathematics and coding, but also in general tasks like open-ended Q&A. However, despite the explosion of recent efforts in this area, there remains an urgent need for a comprehensive survey offering a systemic understanding. To fill this gap, we propose a unified, multidimensional framework structured along four core dimensions of TTS research: what to scale, how to scale, where to scale, and how well to scale. Building upon this taxonomy, we conduct an extensive review of methods, application scenarios, and assessment aspects, and present an organized decomposition that highlights the unique functional roles of individual techniques within the broader TTS landscape. From this analysis, we distill the major developmental trajectories of TTS to date and offer hands-on guidelines for practical deployment. Furthermore, we identify several open challenges and offer insights into promising future directions, including further scaling, clarifying the functional essence of techniques, generalizing to more tasks, and more attributions. Our repository is available on https://github.com/testtimescaling/testtimescaling.github.io/",
    "authors": [
      "Qiyuan Zhang",
      "Fuyuan Lyu",
      "Zexu Sun",
      "Lei Wang",
      "Weixu Zhang",
      "Wenyue Hua",
      "Haolun Wu",
      "Zhihan Guo",
      "Yufei Wang",
      "Niklas Muennighoff",
      "Irwin King",
      "Xue Liu",
      "Chen Ma"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-31T15:46:15Z",
    "pdf_url": "https://arxiv.org/pdf/2503.24235v3"
  },
  {
    "arxiv_id": "2503.24228v1",
    "entry_id": "http://arxiv.org/abs/2503.24228v1",
    "title": "PAARS: Persona Aligned Agentic Retail Shoppers",
    "summary": "In e-commerce, behavioral data is collected for decision making which can be costly and slow. Simulation with LLM powered agents is emerging as a promising alternative for representing human population behavior. However, LLMs are known to exhibit certain biases, such as brand bias, review rating bias and limited representation of certain groups in the population, hence they need to be carefully benchmarked and aligned to user behavior. Ultimately, our goal is to synthesise an agent population and verify that it collectively approximates a real sample of humans. To this end, we propose a framework that: (i) creates synthetic shopping agents by automatically mining personas from anonymised historical shopping data, (ii) equips agents with retail-specific tools to synthesise shopping sessions and (iii) introduces a novel alignment suite measuring distributional differences between humans and shopping agents at the group (i.e. population) level rather than the traditional \"individual\" level. Experimental results demonstrate that using personas improves performance on the alignment suite, though a gap remains to human behaviour. We showcase an initial application of our framework for automated agentic A/B testing and compare the findings to human results. Finally, we discuss applications, limitations and challenges setting the stage for impactful future work.",
    "authors": [
      "Saab Mansour",
      "Leonardo Perelli",
      "Lorenzo Mainetti",
      "George Davidson",
      "Stefano D'Amato"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-03-31T15:41:51Z",
    "pdf_url": "https://arxiv.org/pdf/2503.24228v1"
  },
  {
    "arxiv_id": "2503.24047v2",
    "entry_id": "http://arxiv.org/abs/2503.24047v2",
    "title": "Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents",
    "summary": "As scientific research becomes increasingly complex, innovative tools are needed to manage vast data, facilitate interdisciplinary collaboration, and accelerate discovery. Large language models (LLMs) are now evolving into LLM-based scientific agents that automate critical tasks, ranging from hypothesis generation and experiment design to data analysis and simulation. Unlike general-purpose LLMs, these specialized agents integrate domain-specific knowledge, advanced tool sets, and robust validation mechanisms, enabling them to handle complex data types, ensure reproducibility, and drive scientific breakthroughs. This survey provides a focused review of the architectures, design, benchmarks, applications, and ethical considerations surrounding LLM-based scientific agents. We highlight why they differ from general agents and the ways in which they advance research across various scientific fields. By examining their development and challenges, this survey offers a comprehensive roadmap for researchers and practitioners to harness these agents for more efficient, reliable, and ethically sound scientific discovery.",
    "authors": [
      "Shuo Ren",
      "Pu Jian",
      "Zhenjiang Ren",
      "Chunlin Leng",
      "Can Xie",
      "Jiajun Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-03-31T13:11:28Z",
    "pdf_url": "https://arxiv.org/pdf/2503.24047v2"
  },
  {
    "arxiv_id": "2503.24000v1",
    "entry_id": "http://arxiv.org/abs/2503.24000v1",
    "title": "Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving",
    "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a promising technique to optimize Large Language Model (LLM) serving. It primarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to reduce the computation cost. Despite the development of many compression algorithms, their applications in production environments are still not prevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache} compression solutions from a practical perspective. Our contributions are three-fold. First, we comprehensively review existing algorithmic designs and benchmark studies for \\texttt{KV} \\texttt{cache} compression and identify missing pieces in their performance measurement, which could hinder their adoption in practice. Second, we empirically evaluate representative \\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that affect the computational efficiency: (1) while compressing \\texttt{KV} \\texttt{cache} can reduce memory consumption, current implementations (e.g., FlashAttention, PagedAttention) do not optimize for production-level LLM serving, resulting in suboptimal throughput performance; (2) compressing \\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased end-to-end latency. We further investigate the accuracy performance of individual samples rather than the overall performance, revealing the intrinsic limitations in \\texttt{KV} \\texttt{cache} compression when handling specific LLM tasks. Third, we provide tools to shed light on future \\texttt{KV} \\texttt{cache} compression studies and facilitate their practical deployment in production. They are open-sourced in \\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
    "authors": [
      "Wei Gao",
      "Xinyu Zhou",
      "Peng Sun",
      "Tianwei Zhang",
      "Yonggang Wen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-03-31T12:23:31Z",
    "pdf_url": "https://arxiv.org/pdf/2503.24000v1"
  },
  {
    "arxiv_id": "2504.08762v1",
    "entry_id": "http://arxiv.org/abs/2504.08762v1",
    "title": "InteractiveSurvey: An LLM-based Personalized and Interactive Survey Paper Generation System",
    "summary": "The exponential growth of academic literature creates urgent demands for comprehensive survey papers, yet manual writing remains time-consuming and labor-intensive. Recent advances in large language models (LLMs) and retrieval-augmented generation (RAG) facilitate studies in synthesizing survey papers from multiple references, but most existing works restrict users to title-only inputs and fixed outputs, neglecting the personalized process of survey paper writing. In this paper, we introduce InteractiveSurvey - an LLM-based personalized and interactive survey paper generation system. InteractiveSurvey can generate structured, multi-modal survey papers with reference categorizations from multiple reference papers through both online retrieval and user uploads. More importantly, users can customize and refine intermediate components continuously during generation, including reference categorization, outline, and survey content through an intuitive interface. Evaluations of content quality, time efficiency, and user studies show that InteractiveSurvey is an easy-to-use survey generation system that outperforms most LLMs and existing methods in output content quality while remaining highly time-efficient.",
    "authors": [
      "Zhiyuan Wen",
      "Jiannong Cao",
      "Zian Wang",
      "Beichen Guo",
      "Ruosong Yang",
      "Shuaiqi Liu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-03-31T04:23:22Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08762v1"
  },
  {
    "arxiv_id": "2504.13186v1",
    "entry_id": "http://arxiv.org/abs/2504.13186v1",
    "title": "Advanced Deep Learning and Large Language Models: Comprehensive Insights for Cancer Detection",
    "summary": "The rapid advancement of deep learning (DL) has transformed healthcare, particularly in cancer detection and diagnosis. DL surpasses traditional machine learning and human accuracy, making it a critical tool for identifying diseases. Despite numerous reviews on DL in healthcare, a comprehensive analysis of its role in cancer detection remains limited. Existing studies focus on specific aspects, leaving gaps in understanding its broader impact. This paper addresses these gaps by reviewing advanced DL techniques, including transfer learning (TL), reinforcement learning (RL), federated learning (FL), Transformers, and large language models (LLMs). These approaches enhance accuracy, tackle data scarcity, and enable decentralized learning while maintaining data privacy. TL adapts pre-trained models to new datasets, improving performance with limited labeled data. RL optimizes diagnostic pathways and treatment strategies, while FL fosters collaborative model development without sharing sensitive data. Transformers and LLMs, traditionally used in natural language processing, are now applied to medical data for improved interpretability. Additionally, this review examines these techniques' efficiency in cancer diagnosis, addresses challenges like data imbalance, and proposes solutions. It serves as a resource for researchers and practitioners, providing insights into current trends and guiding future research in advanced DL for cancer detection.",
    "authors": [
      "Yassine Habchi",
      "Hamza Kheddar",
      "Yassine Himeur",
      "Adel Belouchrani",
      "Erchin Serpedin",
      "Fouad Khelifi",
      "Muhammad E. H. Chowdhury"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-03-30T15:17:40Z",
    "pdf_url": "https://arxiv.org/pdf/2504.13186v1"
  },
  {
    "arxiv_id": "2503.23439v1",
    "entry_id": "http://arxiv.org/abs/2503.23439v1",
    "title": "Speculative End-Turn Detector for Efficient Speech Chatbot Assistant",
    "summary": "Spoken dialogue systems powered by large language models have demonstrated remarkable abilities in understanding human speech and generating appropriate spoken responses. However, these systems struggle with end-turn detection (ETD) -- the ability to distinguish between user turn completion and hesitation. This limitation often leads to premature or delayed responses, disrupting the flow of spoken conversations. In this paper, we introduce the ETD Dataset, the first public dataset for end-turn detection. The ETD dataset consists of both synthetic speech data generated with text-to-speech models and real-world speech data collected from web sources. We also propose SpeculativeETD, a novel collaborative inference framework that balances efficiency and accuracy to improve real-time ETD in resource-constrained environments. Our approach jointly employs a lightweight GRU-based model, which rapidly detects the non-speaking units in real-time on local devices, and a high-performance Wav2vec-based model running on the server to make a more challenging classification of distinguishing turn ends from mere pauses. Experiments demonstrate that the proposed SpeculativeETD significantly improves ETD accuracy while keeping the required computations low. Datasets and code will be available after the review.",
    "authors": [
      "Hyunjong Ok",
      "Suho Yoo",
      "Jaeho Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SD",
      "eess.AS"
    ],
    "published": "2025-03-30T13:34:23Z",
    "pdf_url": "https://arxiv.org/pdf/2503.23439v1"
  },
  {
    "arxiv_id": "2503.23434v1",
    "entry_id": "http://arxiv.org/abs/2503.23434v1",
    "title": "Towards Trustworthy GUI Agents: A Survey",
    "summary": "GUI agents, powered by large foundation models, can interact with digital interfaces, enabling various applications in web automation, mobile navigation, and software testing. However, their increasing autonomy has raised critical concerns about their security, privacy, and safety. This survey examines the trustworthiness of GUI agents in five critical dimensions: security vulnerabilities, reliability in dynamic environments, transparency and explainability, ethical considerations, and evaluation methodologies. We also identify major challenges such as vulnerability to adversarial attacks, cascading failure modes in sequential decision-making, and a lack of realistic evaluation benchmarks. These issues not only hinder real-world deployment but also call for comprehensive mitigation strategies beyond task success. As GUI agents become more widespread, establishing robust safety standards and responsible development practices is essential. This survey provides a foundation for advancing trustworthy GUI agents through systematic understanding and future research.",
    "authors": [
      "Yucheng Shi",
      "Wenhao Yu",
      "Wenlin Yao",
      "Wenhu Chen",
      "Ninghao Liu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-30T13:26:00Z",
    "pdf_url": "https://arxiv.org/pdf/2503.23434v1"
  },
  {
    "arxiv_id": "2503.23350v4",
    "entry_id": "http://arxiv.org/abs/2503.23350v4",
    "title": "A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models",
    "summary": "With the advancement of web techniques, they have significantly revolutionized various aspects of people's lives. Despite the importance of the web, many tasks performed on it are repetitive and time-consuming, negatively impacting overall quality of life. To efficiently handle these tedious daily tasks, one of the most promising approaches is to advance autonomous agents based on Artificial Intelligence (AI) techniques, referred to as AI Agents, as they can operate continuously without fatigue or performance degradation. In the context of the web, leveraging AI Agents -- termed WebAgents -- to automatically assist people in handling tedious daily tasks can dramatically enhance productivity and efficiency. Recently, Large Foundation Models (LFMs) containing billions of parameters have exhibited human-like language understanding and reasoning capabilities, showing proficiency in performing various complex tasks. This naturally raises the question: `Can LFMs be utilized to develop powerful AI Agents that automatically handle web tasks, providing significant convenience to users?' To fully explore the potential of LFMs, extensive research has emerged on WebAgents designed to complete daily web tasks according to user instructions, significantly enhancing the convenience of daily human life. In this survey, we comprehensively review existing research studies on WebAgents across three key aspects: architectures, training, and trustworthiness. Additionally, several promising directions for future research are explored to provide deeper insights.",
    "authors": [
      "Liangbo Ning",
      "Ziran Liang",
      "Zhuohang Jiang",
      "Haohao Qu",
      "Yujuan Ding",
      "Wenqi Fan",
      "Xiao-yong Wei",
      "Shanru Lin",
      "Hui Liu",
      "Philip S. Yu",
      "Qing Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-30T08:15:44Z",
    "pdf_url": "https://arxiv.org/pdf/2503.23350v4"
  },
  {
    "arxiv_id": "2504.01032v1",
    "entry_id": "http://arxiv.org/abs/2504.01032v1",
    "title": "Who Owns the Output? Bridging Law and Technology in LLMs Attribution",
    "summary": "Since the introduction of ChatGPT in 2022, Large language models (LLMs) and Large Multimodal Models (LMM) have transformed content creation, enabling the generation of human-quality content, spanning every medium, text, images, videos, and audio. The chances offered by generative AI models are endless and are drastically reducing the time required to generate content and usually raising the quality of the generation. However, considering the complexity and the difficult traceability of the generated content, the use of these tools provides challenges in attributing AI-generated content. The difficult attribution resides for a variety of reasons, starting from the lack of a systematic fingerprinting of the generated content and ending with the enormous amount of data on which LLMs and LMM are trained, which makes it difficult to connect generated content to the training data. This scenario is raising concerns about intellectual property and ethical responsibilities. To address these concerns, in this paper, we bridge the technological, ethical, and legislative aspects, by proposing a review of the legislative and technological instruments today available and proposing a legal framework to ensure accountability. In the end, we propose three use cases of how these can be combined to guarantee that attribution is respected. However, even though the techniques available today can guarantee a greater attribution to a greater extent, strong limitations still apply, that can be solved uniquely by the development of new attribution techniques, to be applied to LLMs and LMMs.",
    "authors": [
      "Emanuele Mezzi",
      "Asimina Mertzani",
      "Michael P. Manis",
      "Siyanna Lilova",
      "Nicholas Vadivoulis",
      "Stamatis Gatirdakis",
      "Styliani Roussou",
      "Rodayna Hmede"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-03-29T18:08:04Z",
    "pdf_url": "https://arxiv.org/pdf/2504.01032v1"
  },
  {
    "arxiv_id": "2503.23170v1",
    "entry_id": "http://arxiv.org/abs/2503.23170v1",
    "title": "AstroAgents: A Multi-Agent AI for Hypothesis Generation from Mass Spectrometry Data",
    "summary": "With upcoming sample return missions across the solar system and the increasing availability of mass spectrometry data, there is an urgent need for methods that analyze such data within the context of existing astrobiology literature and generate plausible hypotheses regarding the emergence of life on Earth. Hypothesis generation from mass spectrometry data is challenging due to factors such as environmental contaminants, the complexity of spectral peaks, and difficulties in cross-matching these peaks with prior studies. To address these challenges, we introduce AstroAgents, a large language model-based, multi-agent AI system for hypothesis generation from mass spectrometry data. AstroAgents is structured around eight collaborative agents: a data analyst, a planner, three domain scientists, an accumulator, a literature reviewer, and a critic. The system processes mass spectrometry data alongside user-provided research papers. The data analyst interprets the data, and the planner delegates specific segments to the scientist agents for in-depth exploration. The accumulator then collects and deduplicates the generated hypotheses, and the literature reviewer identifies relevant literature using Semantic Scholar. The critic evaluates the hypotheses, offering rigorous suggestions for improvement. To assess AstroAgents, an astrobiology expert evaluated the novelty and plausibility of more than a hundred hypotheses generated from data obtained from eight meteorites and ten soil samples. Of these hypotheses, 36% were identified as plausible, and among those, 66% were novel. Project website: https://astroagents.github.io/",
    "authors": [
      "Daniel Saeedi",
      "Denise Buckner",
      "Jose C. Aponte",
      "Amirali Aghazadeh"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-29T17:58:52Z",
    "pdf_url": "https://arxiv.org/pdf/2503.23170v1"
  },
  {
    "arxiv_id": "2503.23153v1",
    "entry_id": "http://arxiv.org/abs/2503.23153v1",
    "title": "Conversational Agents for Older Adults' Health: A Systematic Literature Review",
    "summary": "There has been vast literature that studies Conversational Agents (CAs) in facilitating older adults' health. The vast and diverse studies warrants a comprehensive review that concludes the main findings and proposes research directions for future studies, while few literature review did it from human-computer interaction (HCI) perspective. In this study, we present a survey of existing studies on CAs for older adults' health. Through a systematic review of 72 papers, this work reviewed previously studied older adults' characteristics and analyzed participants' experiences and expectations of CAs for health. We found that (1) Past research has an increasing interest on chatbots and voice assistants and applied CA as multiple roles in older adults' health. (2) Older adults mainly showed low acceptance CAs for health due to various reasons, such as unstable effects, harm to independence, and privacy concerns. (3) Older adults expect CAs to be able to support multiple functions, to communicate using natural language, to be personalized, and to allow users full control. We also discuss the implications based on the findings.",
    "authors": [
      "Jiaxin An",
      "Siqi Yi",
      "Yao Lyu",
      "Houjiang Liu",
      "Yan Zhang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-03-29T17:19:09Z",
    "pdf_url": "https://arxiv.org/pdf/2503.23153v1"
  },
  {
    "arxiv_id": "2503.23037v2",
    "entry_id": "http://arxiv.org/abs/2503.23037v2",
    "title": "Agentic Large Language Models, a survey",
    "summary": "There is great interest in agentic LLMs, large language models that act as agents. We review the growing body of work in this area and provide a research agenda. Agentic LLMs are LLMs that (1) reason, (2) act, and (3) interact. We organize the literature according to these three categories. The research in the first category focuses on reasoning, reflection, and retrieval, aiming to improve decision making; the second category focuses on action models, robots, and tools, aiming for agents that act as useful assistants; the third category focuses on multi-agent systems, aiming for collaborative task solving and simulating interaction to study emergent social behavior. We find that works mutually benefit from results in other categories: retrieval enables tool use, reflection improves multi-agent collaboration, and reasoning benefits all categories. We discuss applications of agentic LLMs and provide an agenda for further research. Important applications are in medical diagnosis, logistics and financial market analysis. Meanwhile, self-reflective agents playing roles and interacting with one another augment the process of scientific research itself. Further, agentic LLMs may provide a solution for the problem of LLMs running out of training data: inference-time behavior generates new training states, such that LLMs can keep learning without needing ever larger datasets. We note that there is risk associated with LLM assistants taking action in the real world, while agentic LLMs are also likely to benefit society.",
    "authors": [
      "Aske Plaat",
      "Max van Duijn",
      "Niki van Stein",
      "Mike Preuss",
      "Peter van der Putten",
      "Kees Joost Batenburg"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-03-29T11:02:20Z",
    "pdf_url": "https://arxiv.org/pdf/2503.23037v2"
  },
  {
    "arxiv_id": "2503.22610v1",
    "entry_id": "http://arxiv.org/abs/2503.22610v1",
    "title": "Evaluating Multimodal Language Models as Visual Assistants for Visually Impaired Users",
    "summary": "This paper explores the effectiveness of Multimodal Large Language models (MLLMs) as assistive technologies for visually impaired individuals. We conduct a user survey to identify adoption patterns and key challenges users face with such technologies. Despite a high adoption rate of these models, our findings highlight concerns related to contextual understanding, cultural sensitivity, and complex scene understanding, particularly for individuals who may rely solely on them for visual interpretation. Informed by these results, we collate five user-centred tasks with image and video inputs, including a novel task on Optical Braille Recognition. Our systematic evaluation of twelve MLLMs reveals that further advancements are necessary to overcome limitations related to cultural context, multilingual support, Braille reading comprehension, assistive object recognition, and hallucinations. This work provides critical insights into the future direction of multimodal AI for accessibility, underscoring the need for more inclusive, robust, and trustworthy visual assistance technologies.",
    "authors": [
      "Antonia Karamolegkou",
      "Malvina Nikandrou",
      "Georgios Pantazopoulos",
      "Danae Sanchez Villegas",
      "Phillip Rust",
      "Ruchira Dhar",
      "Daniel Hershcovich",
      "Anders Søgaard"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-03-28T16:54:25Z",
    "pdf_url": "https://arxiv.org/pdf/2503.22610v1"
  },
  {
    "arxiv_id": "2503.22458v1",
    "entry_id": "http://arxiv.org/abs/2503.22458v1",
    "title": "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey",
    "summary": "This survey examines evaluation methods for large language model (LLM)-based agents in multi-turn conversational settings. Using a PRISMA-inspired framework, we systematically reviewed nearly 250 scholarly sources, capturing the state of the art from various venues of publication, and establishing a solid foundation for our analysis. Our study offers a structured approach by developing two interrelated taxonomy systems: one that defines \\emph{what to evaluate} and another that explains \\emph{how to evaluate}. The first taxonomy identifies key components of LLM-based agents for multi-turn conversations and their evaluation dimensions, including task completion, response quality, user experience, memory and context retention, as well as planning and tool integration. These components ensure that the performance of conversational agents is assessed in a holistic and meaningful manner. The second taxonomy system focuses on the evaluation methodologies. It categorizes approaches into annotation-based evaluations, automated metrics, hybrid strategies that combine human assessments with quantitative measures, and self-judging methods utilizing LLMs. This framework not only captures traditional metrics derived from language understanding, such as BLEU and ROUGE scores, but also incorporates advanced techniques that reflect the dynamic, interactive nature of multi-turn dialogues.",
    "authors": [
      "Shengyue Guan",
      "Haoyi Xiong",
      "Jindong Wang",
      "Jiang Bian",
      "Bin Zhu",
      "Jian-guang Lou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-28T14:08:40Z",
    "pdf_url": "https://arxiv.org/pdf/2503.22458v1"
  },
  {
    "arxiv_id": "2504.08752v1",
    "entry_id": "http://arxiv.org/abs/2504.08752v1",
    "title": "Patience is all you need! An agentic system for performing scientific literature review",
    "summary": "Large language models (LLMs) have grown in their usage to provide support for question answering across numerous disciplines. The models on their own have already shown promise for answering basic questions, however fail quickly where expert domain knowledge is required or the question is nuanced. Scientific research often involves searching for relevant literature, distilling pertinent information from that literature and analysing how the findings support or contradict one another. The information is often encapsulated in the full text body of research articles, rather than just in the abstracts. Statements within these articles frequently require the wider article context to be fully understood. We have built an LLM-based system that performs such search and distillation of information encapsulated in scientific literature, and we evaluate our keyword based search and information distillation system against a set of biology related questions from previously released literature benchmarks. We demonstrate sparse retrieval methods exhibit results close to state of the art without the need for dense retrieval, with its associated infrastructure and complexity overhead. We also show how to increase the coverage of relevant documents for literature review generation.",
    "authors": [
      "David Brett",
      "Anniek Myatt"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-03-28T08:08:46Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08752v1"
  },
  {
    "arxiv_id": "2504.03711v1",
    "entry_id": "http://arxiv.org/abs/2504.03711v1",
    "title": "A Survey of Circuit Foundation Model: Foundation AI Models for VLSI Circuit Design and EDA",
    "summary": "Artificial intelligence (AI)-driven electronic design automation (EDA) techniques have been extensively explored for VLSI circuit design applications. Most recently, foundation AI models for circuits have emerged as a new technology trend. Unlike traditional task-specific AI solutions, these new AI models are developed through two stages: 1) self-supervised pre-training on a large amount of unlabeled data to learn intrinsic circuit properties; and 2) efficient fine-tuning for specific downstream applications, such as early-stage design quality evaluation, circuit-related context generation, and functional verification. This new paradigm brings many advantages: model generalization, less reliance on labeled circuit data, efficient adaptation to new tasks, and unprecedented generative capability. In this paper, we propose referring to AI models developed with this new paradigm as circuit foundation models (CFMs). This paper provides a comprehensive survey of the latest progress in circuit foundation models, unprecedentedly covering over 130 relevant works. Over 90% of our introduced works were published in or after 2022, indicating that this emerging research trend has attracted wide attention in a short period. In this survey, we propose to categorize all existing circuit foundation models into two primary types: 1) encoder-based methods performing general circuit representation learning for predictive tasks; and 2) decoder-based methods leveraging large language models (LLMs) for generative tasks. For our introduced works, we cover their input modalities, model architecture, pre-training strategies, domain adaptation techniques, and downstream design applications. In addition, this paper discussed the unique properties of circuits from the data perspective. These circuit properties have motivated many works in this domain and differentiated them from general AI techniques.",
    "authors": [
      "Wenji Fang",
      "Jing Wang",
      "Yao Lu",
      "Shang Liu",
      "Yuchao Wu",
      "Yuzhe Ma",
      "Zhiyao Xie"
    ],
    "categories": [
      "cs.AR",
      "cs.LG"
    ],
    "published": "2025-03-28T07:27:27Z",
    "pdf_url": "https://arxiv.org/pdf/2504.03711v1"
  },
  {
    "arxiv_id": "2503.22769v1",
    "entry_id": "http://arxiv.org/abs/2503.22769v1",
    "title": "MediTools -- Medical Education Powered by LLMs",
    "summary": "Artificial Intelligence (AI) has been advancing rapidly and with the advent of large language models (LLMs) in late 2022, numerous opportunities have emerged for adopting this technology across various domains, including medicine. These innovations hold immense potential to revolutionize and modernize medical education. Our research project leverages large language models to enhance medical education and address workflow challenges through the development of MediTools - AI Medical Education. This prototype application focuses on developing interactive tools that simulate real-life clinical scenarios, provide access to medical literature, and keep users updated with the latest medical news. Our first tool is a dermatology case simulation tool that uses real patient images depicting various dermatological conditions and enables interaction with LLMs acting as virtual patients. This platform allows users to practice their diagnostic skills and enhance their clinical decision-making abilities. The application also features two additional tools: an AI-enhanced PubMed tool for engaging with LLMs to gain deeper insights into research papers, and a Google News tool that offers LLM generated summaries of articles for various medical specialties. A comprehensive survey has been conducted among medical professionals and students to gather initial feedback on the effectiveness and user satisfaction of MediTools, providing insights for further development and refinement of the application. This research demonstrates the potential of AI-driven tools in transforming and revolutionizing medical education, offering a scalable and interactive platform for continuous learning and skill development.",
    "authors": [
      "Amr Alshatnawi",
      "Remi Sampaleanu",
      "David Liebovitz"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-03-28T03:57:32Z",
    "pdf_url": "https://arxiv.org/pdf/2503.22769v1"
  },
  {
    "arxiv_id": "2504.13865v2",
    "entry_id": "http://arxiv.org/abs/2504.13865v2",
    "title": "A Survey on (M)LLM-Based GUI Agents",
    "summary": "Graphical User Interface (GUI) Agents have emerged as a transformative paradigm in human-computer interaction, evolving from rule-based automation scripts to sophisticated AI-driven systems capable of understanding and executing complex interface operations. This survey provides a comprehensive examination of the rapidly advancing field of LLM-based GUI Agents, systematically analyzing their architectural foundations, technical components, and evaluation methodologies. We identify and analyze four fundamental components that constitute modern GUI Agents: (1) perception systems that integrate text-based parsing with multimodal understanding for comprehensive interface comprehension; (2) exploration mechanisms that construct and maintain knowledge bases through internal modeling, historical experience, and external information retrieval; (3) planning frameworks that leverage advanced reasoning methodologies for task decomposition and execution; and (4) interaction systems that manage action generation with robust safety controls. Through rigorous analysis of these components, we reveal how recent advances in large language models and multimodal learning have revolutionized GUI automation across desktop, mobile, and web platforms. We critically examine current evaluation frameworks, highlighting methodological limitations in existing benchmarks while proposing directions for standardization. This survey also identifies key technical challenges, including accurate element localization, effective knowledge retrieval, long-horizon planning, and safety-aware execution control, while outlining promising research directions for enhancing GUI Agents' capabilities. Our systematic review provides researchers and practitioners with a thorough understanding of the field's current state and offers insights into future developments in intelligent interface automation.",
    "authors": [
      "Fei Tang",
      "Haolei Xu",
      "Hang Zhang",
      "Siqi Chen",
      "Xingyu Wu",
      "Yongliang Shen",
      "Wenqi Zhang",
      "Guiyang Hou",
      "Zeqi Tan",
      "Yuchen Yan",
      "Kaitao Song",
      "Jian Shao",
      "Weiming Lu",
      "Jun Xiao",
      "Yueting Zhuang"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-03-27T17:58:31Z",
    "pdf_url": "https://arxiv.org/pdf/2504.13865v2"
  },
  {
    "arxiv_id": "2503.21615v2",
    "entry_id": "http://arxiv.org/abs/2503.21615v2",
    "title": "A Measure Based Generalizable Approach to Understandability",
    "summary": "Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).\n  In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents. Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future.",
    "authors": [
      "Vikas Kushwaha",
      "Sruti Srinivasa Ragavan",
      "Subhajit Roy"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2025-03-27T15:36:49Z",
    "pdf_url": "https://arxiv.org/pdf/2503.21615v2"
  },
  {
    "arxiv_id": "2503.22759v1",
    "entry_id": "http://arxiv.org/abs/2503.22759v1",
    "title": "Data Poisoning in Deep Learning: A Survey",
    "summary": "Deep learning has become a cornerstone of modern artificial intelligence, enabling transformative applications across a wide range of domains. As the core element of deep learning, the quality and security of training data critically influence model performance and reliability. However, during the training process, deep learning models face the significant threat of data poisoning, where attackers introduce maliciously manipulated training data to degrade model accuracy or lead to anomalous behavior. While existing surveys provide valuable insights into data poisoning, they generally adopt a broad perspective, encompassing both attacks and defenses, but lack a dedicated, in-depth analysis of poisoning attacks specifically in deep learning. In this survey, we bridge this gap by presenting a comprehensive and targeted review of data poisoning in deep learning. First, this survey categorizes data poisoning attacks across multiple perspectives, providing an in-depth analysis of their characteristics and underlying design princinples. Second, the discussion is extended to the emerging area of data poisoning in large language models(LLMs). Finally, we explore critical open challenges in the field and propose potential research directions to advance the field further. To support further exploration, an up-to-date repository of resources on data poisoning in deep learning is available at https://github.com/Pinlong-Zhao/Data-Poisoning.",
    "authors": [
      "Pinlong Zhao",
      "Weiyao Zhu",
      "Pengfei Jiao",
      "Di Gao",
      "Ou Wu"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-03-27T15:16:57Z",
    "pdf_url": "https://arxiv.org/pdf/2503.22759v1"
  },
  {
    "arxiv_id": "2503.21422v1",
    "entry_id": "http://arxiv.org/abs/2503.21422v1",
    "title": "From Deep Learning to LLMs: A survey of AI in Quantitative Investment",
    "summary": "Quantitative investment (quant) is an emerging, technology-driven approach in asset management, increasingy shaped by advancements in artificial intelligence. Recent advances in deep learning and large language models (LLMs) for quant finance have improved predictive modeling and enabled agent-based automation, suggesting a potential paradigm shift in this field. In this survey, taking alpha strategy as a representative example, we explore how AI contributes to the quantitative investment pipeline. We first examine the early stage of quant research, centered on human-crafted features and traditional statistical models with an established alpha pipeline. We then discuss the rise of deep learning, which enabled scalable modeling across the entire pipeline from data processing to order execution. Building on this, we highlight the emerging role of LLMs in extending AI beyond prediction, empowering autonomous agents to process unstructured data, generate alphas, and support self-iterative workflows.",
    "authors": [
      "Bokai Cao",
      "Saizhuo Wang",
      "Xinyi Lin",
      "Xiaojun Wu",
      "Haohan Zhang",
      "Lionel M. Ni",
      "Jian Guo"
    ],
    "categories": [
      "q-fin.CP",
      "cs.AI",
      "cs.LG",
      "q-fin.ST",
      "q-fin.TR"
    ],
    "published": "2025-03-27T12:10:15Z",
    "pdf_url": "https://arxiv.org/pdf/2503.21422v1"
  },
  {
    "arxiv_id": "2503.21411v2",
    "entry_id": "http://arxiv.org/abs/2503.21411v2",
    "title": "Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap",
    "summary": "Modern transportation systems face pressing challenges due to increasing demand, dynamic environments, and heterogeneous information integration. The rapid evolution of Large Language Models (LLMs) offers transformative potential to address these challenges. Extensive knowledge and high-level capabilities derived from pretraining evolve the default role of LLMs as text generators to become versatile, knowledge-driven task solvers for intelligent transportation systems. This survey first presents LLM4TR, a novel conceptual framework that systematically categorizes the roles of LLMs in transportation into four synergetic dimensions: information processors, knowledge encoders, component generators, and decision facilitators. Through a unified taxonomy, we systematically elucidate how LLMs bridge fragmented data pipelines, enhance predictive analytics, simulate human-like reasoning, and enable closed-loop interactions across sensing, learning, modeling, and managing tasks in transportation systems. For each role, our review spans diverse applications, from traffic prediction and autonomous driving to safety analytics and urban mobility optimization, highlighting how emergent capabilities of LLMs such as in-context learning and step-by-step reasoning can enhance the operation and management of transportation systems. We further curate practical guidance, including available resources and computational guidelines, to support real-world deployment. By identifying challenges in existing LLM-based solutions, this survey charts a roadmap for advancing LLM-driven transportation research, positioning LLMs as central actors in the next generation of cyber-physical-social mobility ecosystems. Online resources can be found in the project page: https://github.com/tongnie/awesome-llm4tr.",
    "authors": [
      "Tong Nie",
      "Jian Sun",
      "Wei Ma"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-27T11:56:27Z",
    "pdf_url": "https://arxiv.org/pdf/2503.21411v2"
  },
  {
    "arxiv_id": "2503.21352v1",
    "entry_id": "http://arxiv.org/abs/2503.21352v1",
    "title": "Using large language models to produce literature reviews: Usages and systematic biases of microphysics parametrizations in 2699 publications",
    "summary": "Large language models afford opportunities for using computers for intensive tasks, realizing research opportunities that have not been considered before. One such opportunity could be a systematic interrogation of the scientific literature. Here, we show how a large language model can be used to construct a literature review of 2699 publications associated with microphysics parametrizations in the Weather and Research Forecasting (WRF) model, with the goal of learning how they were used and their systematic biases, when simulating precipitation. The database was constructed of publications identified from Web of Science and Scopus searches. The large language model GPT-4 Turbo was used to extract information about model configurations and performance from the text of 2699 publications. Our results reveal the landscape of how nine of the most popular microphysics parameterizations have been used around the world: Lin, Ferrier, WRF Single-Moment, Goddard Cumulus Ensemble, Morrison, Thompson, and WRF Double-Moment. More studies used one-moment parameterizations before 2020 and two-moment parameterizations after 2020. Seven out of nine parameterizations tended to overestimate precipitation. However, systematic biases of parameterizations differed in various regions. Except simulations using the Lin, Ferrier, and Goddard parameterizations that tended to underestimate precipitation over almost all locations, the remaining six parameterizations tended to overestimate, particularly over China, southeast Asia, western United States, and central Africa. This method could be used by other researchers to help understand how the increasingly massive body of scientific literature can be harnessed through the power of artificial intelligence to solve their research problems.",
    "authors": [
      "Tianhang Zhang",
      "Shengnan Fu",
      "David M. Schultz",
      "Zhonghua Zheng"
    ],
    "categories": [
      "cs.AI",
      "stat.AP"
    ],
    "published": "2025-03-27T10:42:19Z",
    "pdf_url": "https://arxiv.org/pdf/2503.21352v1"
  },
  {
    "arxiv_id": "2503.21248v2",
    "entry_id": "http://arxiv.org/abs/2503.21248v2",
    "title": "ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition",
    "summary": "Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as \"research hypothesis mines\", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.",
    "authors": [
      "Yujie Liu",
      "Zonglin Yang",
      "Tong Xie",
      "Jinjie Ni",
      "Ben Gao",
      "Yuqiang Li",
      "Shixiang Tang",
      "Wanli Ouyang",
      "Erik Cambria",
      "Dongzhan Zhou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "published": "2025-03-27T08:09:15Z",
    "pdf_url": "https://arxiv.org/pdf/2503.21248v2"
  },
  {
    "arxiv_id": "2503.21223v4",
    "entry_id": "http://arxiv.org/abs/2503.21223v4",
    "title": "Rethinking Graph Structure Learning in the Era of LLMs",
    "summary": "Recently, the emergence of LLMs has prompted researchers to integrate language descriptions into graphs, aiming to enhance model encoding capabilities from a data-centric perspective. This graph representation is called text-attributed graphs (TAGs). A review of prior advancements highlights that graph structure learning (GSL) is a pivotal technique for improving data utility, making it highly relevant to efficient TAG learning. However, most GSL methods are tailored for traditional graphs without textual information, underscoring the necessity of developing a new GSL paradigm. Despite clear motivations, it remains challenging: (1) How can we define a reasonable optimization objective for GSL in the era of LLMs, considering the massive parameters in LLM? (2) How can we design an efficient model architecture that enables seamless integration of LLM for this optimization objective? For Question 1, we reformulate existing GSL optimization objectives as a tree optimization framework, shifting the focus from obtaining a well-trained edge predictor to a language-aware tree sampler. For Question 2, we propose decoupled and training-free model design principles for LLM integration, shifting the focus from computation-intensive fine-tuning to more efficient inference. Based on this, we propose Large Language and Tree Assistant (LLaTA), which leverages tree-based LLM in-context learning to enhance the understanding of topology and text, enabling reliable inference and generating improved graph structure. Extensive experiments on 11 datasets demonstrate that LLaTA enjoys flexibility-incorporated with any backbone; scalability-outperforms other LLM-enhanced graph learning methods; effectiveness-achieves SOTA predictive performance.",
    "authors": [
      "Zhihan Zhang",
      "Xunkai Li",
      "Zhu Lei",
      "Guang Zeng",
      "Ronghua Li",
      "Guoren Wang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-27T07:28:30Z",
    "pdf_url": "https://arxiv.org/pdf/2503.21223v4"
  },
  {
    "arxiv_id": "2503.21157v3",
    "entry_id": "http://arxiv.org/abs/2503.21157v3",
    "title": "Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?",
    "summary": "This article surveys Evaluation models to automatically detect hallucinations in Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmark of their performance across six RAG applications. Methods included in our study include: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination Evaluation Model (HHEM), and the Trustworthy Language Model (TLM). These approaches are all reference-free, requiring no ground-truth answers/labels to catch incorrect LLM responses. Our study reveals that, across diverse RAG applications, some of these approaches consistently detect incorrect RAG responses with high precision/recall.",
    "authors": [
      "Ashish Sardana"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-27T04:50:14Z",
    "pdf_url": "https://arxiv.org/pdf/2503.21157v3"
  },
  {
    "arxiv_id": "2503.20981v1",
    "entry_id": "http://arxiv.org/abs/2503.20981v1",
    "title": "Patients Speak, AI Listens: LLM-based Analysis of Online Reviews Uncovers Key Drivers for Urgent Care Satisfaction",
    "summary": "Investigating the public experience of urgent care facilities is essential for promoting community healthcare development. Traditional survey methods often fall short due to limited scope, time, and spatial coverage. Crowdsourcing through online reviews or social media offers a valuable approach to gaining such insights. With recent advancements in large language models (LLMs), extracting nuanced perceptions from reviews has become feasible. This study collects Google Maps reviews across the DMV and Florida areas and conducts prompt engineering with the GPT model to analyze the aspect-based sentiment of urgent care. We first analyze the geospatial patterns of various aspects, including interpersonal factors, operational efficiency, technical quality, finances, and facilities. Next, we determine Census Block Group(CBG)-level characteristics underpinning differences in public perception, including population density, median income, GINI Index, rent-to-income ratio, household below poverty rate, no insurance rate, and unemployment rate. Our results show that interpersonal factors and operational efficiency emerge as the strongest determinants of patient satisfaction in urgent care, while technical quality, finances, and facilities show no significant independent effects when adjusted for in multivariate models. Among socioeconomic and demographic factors, only population density demonstrates a significant but modest association with patient ratings, while the remaining factors exhibit no significant correlations. Overall, this study highlights the potential of crowdsourcing to uncover the key factors that matter to residents and provide valuable insights for stakeholders to improve public satisfaction with urgent care.",
    "authors": [
      "Xiaoran Xu",
      "Zhaoqian Xue",
      "Chi Zhang",
      "Jhonatan Medri",
      "Junjie Xiong",
      "Jiayan Zhou",
      "Jin Jin",
      "Yongfeng Zhang",
      "Siyuan Ma",
      "Lingyao Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2025-03-26T20:45:01Z",
    "pdf_url": "https://arxiv.org/pdf/2503.20981v1"
  },
  {
    "arxiv_id": "2504.08748v1",
    "entry_id": "http://arxiv.org/abs/2504.08748v1",
    "title": "A Survey of Multimodal Retrieval-Augmented Generation",
    "summary": "Multimodal Retrieval-Augmented Generation (MRAG) enhances large language models (LLMs) by integrating multimodal data (text, images, videos) into retrieval and generation processes, overcoming the limitations of text-only Retrieval-Augmented Generation (RAG). While RAG improves response accuracy by incorporating external textual knowledge, MRAG extends this framework to include multimodal retrieval and generation, leveraging contextual information from diverse data types. This approach reduces hallucinations and enhances question-answering systems by grounding responses in factual, multimodal knowledge. Recent studies show MRAG outperforms traditional RAG, especially in scenarios requiring both visual and textual understanding. This survey reviews MRAG's essential components, datasets, evaluation methods, and limitations, providing insights into its construction and improvement. It also identifies challenges and future research directions, highlighting MRAG's potential to revolutionize multimodal information retrieval and generation. By offering a comprehensive perspective, this work encourages further exploration into this promising paradigm.",
    "authors": [
      "Lang Mei",
      "Siyu Mo",
      "Zhihan Yang",
      "Chong Chen"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.ET",
      "cs.LG"
    ],
    "published": "2025-03-26T02:43:09Z",
    "pdf_url": "https://arxiv.org/pdf/2504.08748v1"
  },
  {
    "arxiv_id": "2503.19607v2",
    "entry_id": "http://arxiv.org/abs/2503.19607v2",
    "title": "Enabling Rapid Shared Human-AI Mental Model Alignment via the After-Action Review",
    "summary": "In this work, we present two novel contributions toward improving research in human-machine teaming (HMT): 1) a Minecraft testbed to accelerate testing and deployment of collaborative AI agents and 2) a tool to allow users to revisit and analyze behaviors within an HMT episode to facilitate shared mental model development. Our browser-based Minecraft testbed allows for rapid testing of collaborative agents in a continuous-space, real-time, partially-observable environment with real humans without cumbersome setup typical to human-AI interaction user studies. As Minecraft has an extensive player base and a rich ecosystem of pre-built AI agents, we hope this contribution can help to facilitate research quickly in the design of new collaborative agents and in understanding different human factors within HMT. Our mental model alignment tool facilitates user-led post-mission analysis by including video displays of first-person perspectives of the team members (i.e., the human and AI) that can be replayed, and a chat interface that leverages GPT-4 to provide answers to various queries regarding the AI's experiences and model details.",
    "authors": [
      "Edward Gu",
      "Ho Chit Siu",
      "Melanie Platt",
      "Isabelle Hurley",
      "Jaime Peña",
      "Rohan Paleja"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-03-25T12:43:18Z",
    "pdf_url": "https://arxiv.org/pdf/2503.19607v2"
  },
  {
    "arxiv_id": "2503.19338v3",
    "entry_id": "http://arxiv.org/abs/2503.19338v3",
    "title": "Membership Inference Attacks on Large-Scale Models: A Survey",
    "summary": "As large-scale models such as Large Language Models (LLMs) and Large Multimodal Models (LMMs) see increasing deployment, their privacy risks remain underexplored. Membership Inference Attacks (MIAs), which reveal whether a data point was used in training the target model, are an important technique for exposing or assessing privacy risks and have been shown to be effective across diverse machine learning algorithms. However, despite extensive studies on MIAs in classic models, there remains a lack of systematic surveys addressing their effectiveness and limitations in large-scale models. To address this gap, we provide the first comprehensive review of MIAs targeting LLMs and LMMs, analyzing attacks by model type, adversarial knowledge, and strategy. Unlike prior surveys, we further examine MIAs across multiple stages of the model pipeline, including pre-training, fine-tuning, alignment, and Retrieval-Augmented Generation (RAG). Finally, we identify open challenges and propose future research directions for strengthening privacy resilience in large-scale models.",
    "authors": [
      "Hengyu Wu",
      "Yang Cao"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2025-03-25T04:11:47Z",
    "pdf_url": "https://arxiv.org/pdf/2503.19338v3"
  },
  {
    "arxiv_id": "2503.19213v1",
    "entry_id": "http://arxiv.org/abs/2503.19213v1",
    "title": "A Survey of Large Language Model Agents for Question Answering",
    "summary": "This paper surveys the development of large language model (LLM)-based agents for question answering (QA). Traditional agents face significant limitations, including substantial data requirements and difficulty in generalizing to new environments. LLM-based agents address these challenges by leveraging LLMs as their core reasoning engine. These agents achieve superior QA results compared to traditional QA pipelines and naive LLM QA systems by enabling interaction with external environments. We systematically review the design of LLM agents in the context of QA tasks, organizing our discussion across key stages: planning, question understanding, information retrieval, and answer generation. Additionally, this paper identifies ongoing challenges and explores future research directions to enhance the performance of LLM agent QA systems.",
    "authors": [
      "Murong Yue"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-03-24T23:39:44Z",
    "pdf_url": "https://arxiv.org/pdf/2503.19213v1"
  },
  {
    "arxiv_id": "2504.07971v1",
    "entry_id": "http://arxiv.org/abs/2504.07971v1",
    "title": "SPHERE: An Evaluation Card for Human-AI Systems",
    "summary": "In the era of Large Language Models (LLMs), establishing effective evaluation methods and standards for diverse human-AI interaction systems is increasingly challenging. To encourage more transparent documentation and facilitate discussion on human-AI system evaluation design options, we present an evaluation card SPHERE, which encompasses five key dimensions: 1) What is being evaluated?; 2) How is the evaluation conducted?; 3) Who is participating in the evaluation?; 4) When is evaluation conducted?; 5) How is evaluation validated? We conduct a review of 39 human-AI systems using SPHERE, outlining current evaluation practices and areas for improvement. We provide three recommendations for improving the validity and rigor of evaluation practices.",
    "authors": [
      "Qianou Ma",
      "Dora Zhao",
      "Xinran Zhao",
      "Chenglei Si",
      "Chenyang Yang",
      "Ryan Louie",
      "Ehud Reiter",
      "Diyi Yang",
      "Tongshuang Wu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-03-24T20:17:20Z",
    "pdf_url": "https://arxiv.org/pdf/2504.07971v1"
  },
  {
    "arxiv_id": "2503.18641v1",
    "entry_id": "http://arxiv.org/abs/2503.18641v1",
    "title": "From Fragment to One Piece: A Survey on AI-Driven Graphic Design",
    "summary": "This survey provides a comprehensive overview of the advancements in Artificial Intelligence in Graphic Design (AIGD), focusing on integrating AI techniques to support design interpretation and enhance the creative process. We categorize the field into two primary directions: perception tasks, which involve understanding and analyzing design elements, and generation tasks, which focus on creating new design elements and layouts. The survey covers various subtasks, including visual element perception and generation, aesthetic and semantic understanding, layout analysis, and generation. We highlight the role of large language models and multimodal approaches in bridging the gap between localized visual features and global design intent. Despite significant progress, challenges remain to understanding human intent, ensuring interpretability, and maintaining control over multilayered compositions. This survey serves as a guide for researchers, providing information on the current state of AIGD and potential future directions\\footnote{https://github.com/zhangtianer521/excellent\\_Intelligent\\_graphic\\_design}.",
    "authors": [
      "Xingxing Zou",
      "Wen Zhang",
      "Nanxuan Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-24T13:05:09Z",
    "pdf_url": "https://arxiv.org/pdf/2503.18641v1"
  },
  {
    "arxiv_id": "2503.18172v5",
    "entry_id": "http://arxiv.org/abs/2503.18172v5",
    "title": "Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering",
    "summary": "Misleading visualizations, which manipulate chart representations to support specific claims, can distort perception and lead to incorrect conclusions. Despite decades of research, they remain a widespread issue, posing risks to public understanding and raising safety concerns for AI systems involved in data-driven communication. While recent multimodal large language models (MLLMs) show strong chart comprehension abilities, their capacity to detect and interpret misleading charts remains unexplored. We introduce Misleading ChartQA benchmark, a large-scale multimodal dataset designed to evaluate MLLMs on misleading chart reasoning. It contains 3,026 curated examples spanning 21 misleader types and 10 chart types, each with standardized chart code, CSV data, multiple-choice questions, and labeled explanations, validated through iterative MLLM checks and expert human review. We benchmark 24 state-of-the-art MLLMs, analyze their performance across misleader types and chart formats, and propose a novel region-aware reasoning pipeline that enhances model accuracy. Our work lays the foundation for developing MLLMs that are robust, trustworthy, and aligned with the demands of responsible visual communication.",
    "authors": [
      "Zixin Chen",
      "Sicheng Song",
      "Kashun Shum",
      "Yanna Lin",
      "Rui Sheng",
      "Weiqi Wang",
      "Huamin Qu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-23T18:56:33Z",
    "pdf_url": "https://arxiv.org/pdf/2503.18172v5"
  },
  {
    "arxiv_id": "2503.17822v1",
    "entry_id": "http://arxiv.org/abs/2503.17822v1",
    "title": "Metacognition in Content-Centric Computational Cognitive C4 Modeling",
    "summary": "For AI agents to emulate human behavior, they must be able to perceive, meaningfully interpret, store, and use large amounts of information about the world, themselves, and other agents. Metacognition is a necessary component of all of these processes. In this paper, we briefly a) introduce content-centric computational cognitive (C4) modeling for next-generation AI agents; b) review the long history of developing C4 agents at RPI's LEIA (Language-Endowed Intelligent Agents) Lab; c) discuss our current work on extending LEIAs' cognitive capabilities to cognitive robotic applications developed using a neuro symbolic processing model; and d) sketch plans for future developments in this paradigm that aim to overcome underappreciated limitations of currently popular, LLM-driven methods in AI.",
    "authors": [
      "Sergei Nirenburg",
      "Marjorie McShane",
      "Sanjay Oruganti"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-22T17:23:27Z",
    "pdf_url": "https://arxiv.org/pdf/2503.17822v1"
  },
  {
    "arxiv_id": "2503.17726v1",
    "entry_id": "http://arxiv.org/abs/2503.17726v1",
    "title": "A Survey on Mathematical Reasoning and Optimization with Large Language Models",
    "summary": "Mathematical reasoning and optimization are fundamental to artificial intelligence and computational problem-solving. Recent advancements in Large Language Models (LLMs) have significantly improved AI-driven mathematical reasoning, theorem proving, and optimization techniques. This survey explores the evolution of mathematical problem-solving in AI, from early statistical learning approaches to modern deep learning and transformer-based methodologies. We review the capabilities of pretrained language models and LLMs in performing arithmetic operations, complex reasoning, theorem proving, and structured symbolic computation. A key focus is on how LLMs integrate with optimization and control frameworks, including mixed-integer programming, linear quadratic control, and multi-agent optimization strategies. We examine how LLMs assist in problem formulation, constraint generation, and heuristic search, bridging theoretical reasoning with practical applications. We also discuss enhancement techniques such as Chain-of-Thought reasoning, instruction tuning, and tool-augmented methods that improve LLM's problem-solving performance. Despite their progress, LLMs face challenges in numerical precision, logical consistency, and proof verification. Emerging trends such as hybrid neural-symbolic reasoning, structured prompt engineering, and multi-step self-correction aim to overcome these limitations. Future research should focus on interpretability, integration with domain-specific solvers, and improving the robustness of AI-driven decision-making. This survey offers a comprehensive review of the current landscape and future directions of mathematical reasoning and optimization with LLMs, with applications across engineering, finance, and scientific research.",
    "authors": [
      "Ali Forootani"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-22T10:49:32Z",
    "pdf_url": "https://arxiv.org/pdf/2503.17726v1"
  },
  {
    "arxiv_id": "2503.18971v2",
    "entry_id": "http://arxiv.org/abs/2503.18971v2",
    "title": "LLMs as Planning Formalizers: A Survey for Leveraging Large Language Models to Construct Automated Planning Models",
    "summary": "Large Language Models (LLMs) excel in various natural language tasks but often struggle with long-horizon planning problems requiring structured reasoning. This limitation has drawn interest in integrating neuro-symbolic approaches within the Automated Planning (AP) and Natural Language Processing (NLP) communities. However, identifying optimal AP deployment frameworks can be daunting and introduces new challenges. This paper aims to provide a timely survey of the current research with an in-depth analysis, positioning LLMs as tools for formalizing and refining planning specifications to support reliable off-the-shelf AP planners. By systematically reviewing the current state of research, we highlight methodologies, and identify critical challenges and future directions, hoping to contribute to the joint research on NLP and Automated Planning.",
    "authors": [
      "Marcus Tantakoun",
      "Xiaodan Zhu",
      "Christian Muise"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-22T03:35:44Z",
    "pdf_url": "https://arxiv.org/pdf/2503.18971v2"
  },
  {
    "arxiv_id": "2503.17222v2",
    "entry_id": "http://arxiv.org/abs/2503.17222v2",
    "title": "Automating Adjudication of Cardiovascular Events Using Large Language Models",
    "summary": "Cardiovascular events, such as heart attacks and strokes, remain a leading cause of mortality globally, necessitating meticulous monitoring and adjudication in clinical trials. This process, traditionally performed manually by clinical experts, is time-consuming, resource-intensive, and prone to inter-reviewer variability, potentially introducing bias and hindering trial progress. This study addresses these critical limitations by presenting a novel framework for automating the adjudication of cardiovascular events in clinical trials using Large Language Models (LLMs). We developed a two-stage approach: first, employing an LLM-based pipeline for event information extraction from unstructured clinical data and second, using an LLM-based adjudication process guided by a Tree of Thoughts approach and clinical endpoint committee (CEC) guidelines. Using cardiovascular event-specific clinical trial data, the framework achieved an F1-score of 0.82 for event extraction and an accuracy of 0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel, automated metric specifically designed for evaluating the quality of AI-generated clinical reasoning in adjudicating cardiovascular events. This approach demonstrates significant potential for substantially reducing adjudication time and costs while maintaining high-quality, consistent, and auditable outcomes in clinical trials. The reduced variability and enhanced standardization also allow for faster identification and mitigation of risks associated with cardiovascular therapies.",
    "authors": [
      "Sonish Sivarajkumar",
      "Kimia Ameri",
      "Chuqin Li",
      "Yanshan Wang",
      "Min Jiang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-21T15:25:53Z",
    "pdf_url": "https://arxiv.org/pdf/2503.17222v2"
  },
  {
    "arxiv_id": "2503.22708v1",
    "entry_id": "http://arxiv.org/abs/2503.22708v1",
    "title": "CodeScientist: End-to-End Semi-Automated Scientific Discovery with Code-based Experimentation",
    "summary": "Despite the surge of interest in autonomous scientific discovery (ASD) of software artifacts (e.g., improved ML algorithms), current ASD systems face two key limitations: (1) they largely explore variants of existing codebases or similarly constrained design spaces, and (2) they produce large volumes of research artifacts (such as automatically generated papers and code) that are typically evaluated using conference-style paper review with limited evaluation of code. In this work we introduce CodeScientist, a novel ASD system that frames ideation and experiment construction as a form of genetic search jointly over combinations of research articles and codeblocks defining common actions in a domain (like prompting a language model). We use this paradigm to conduct hundreds of automated experiments on machine-generated ideas broadly in the domain of agents and virtual environments, with the system returning 19 discoveries, 6 of which were judged as being both at least minimally sound and incrementally novel after a multi-faceted evaluation beyond that typically conducted in prior work, including external (conference-style) review, code review, and replication attempts. Moreover, the discoveries span new tasks, agents, metrics, and data, suggesting a qualitative shift from benchmark optimization to broader discoveries.",
    "authors": [
      "Peter Jansen",
      "Oyvind Tafjord",
      "Marissa Radensky",
      "Pao Siangliulue",
      "Tom Hope",
      "Bhavana Dalvi Mishra",
      "Bodhisattwa Prasad Majumder",
      "Daniel S. Weld",
      "Peter Clark"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-20T22:37:17Z",
    "pdf_url": "https://arxiv.org/pdf/2503.22708v1"
  },
  {
    "arxiv_id": "2503.16416v1",
    "entry_id": "http://arxiv.org/abs/2503.16416v1",
    "title": "Survey on Evaluation of LLM-based Agents",
    "summary": "The emergence of LLM-based agents represents a paradigm shift in AI, enabling autonomous systems to plan, reason, use tools, and maintain memory while interacting with dynamic environments. This paper provides the first comprehensive survey of evaluation methodologies for these increasingly capable agents. We systematically analyze evaluation benchmarks and frameworks across four critical dimensions: (1) fundamental agent capabilities, including planning, tool use, self-reflection, and memory; (2) application-specific benchmarks for web, software engineering, scientific, and conversational agents; (3) benchmarks for generalist agents; and (4) frameworks for evaluating agents. Our analysis reveals emerging trends, including a shift toward more realistic, challenging evaluations with continuously updated benchmarks. We also identify critical gaps that future research must address-particularly in assessing cost-efficiency, safety, and robustness, and in developing fine-grained, and scalable evaluation methods. This survey maps the rapidly evolving landscape of agent evaluation, reveals the emerging trends in the field, identifies current limitations, and proposes directions for future research.",
    "authors": [
      "Asaf Yehudai",
      "Lilach Eden",
      "Alan Li",
      "Guy Uziel",
      "Yilun Zhao",
      "Roy Bar-Haim",
      "Arman Cohan",
      "Michal Shmueli-Scheuer"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-03-20T17:59:23Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16416v1"
  },
  {
    "arxiv_id": "2503.17407v1",
    "entry_id": "http://arxiv.org/abs/2503.17407v1",
    "title": "A Comprehensive Survey on Long Context Language Modeling",
    "summary": "Efficient processing of long contexts has been a persistent pursuit in Natural Language Processing. With the growing number of long documents, dialogues, and other textual data, it is important to develop Long Context Language Models (LCLMs) that can process and analyze extensive inputs in an effective and efficient way. In this paper, we present a comprehensive survey on recent advances in long-context modeling for large language models. Our survey is structured around three key aspects: how to obtain effective and efficient LCLMs, how to train and deploy LCLMs efficiently, and how to evaluate and analyze LCLMs comprehensively. For the first aspect, we discuss data strategies, architectural designs, and workflow approaches oriented with long context processing. For the second aspect, we provide a detailed examination of the infrastructure required for LCLM training and inference. For the third aspect, we present evaluation paradigms for long-context comprehension and long-form generation, as well as behavioral analysis and mechanism interpretability of LCLMs. Beyond these three key aspects, we thoroughly explore the diverse application scenarios where existing LCLMs have been deployed and outline promising future development directions. This survey provides an up-to-date review of the literature on long-context LLMs, which we wish to serve as a valuable resource for both researchers and engineers. An associated GitHub repository collecting the latest papers and repos is available at: \\href{https://github.com/LCLM-Horizon/A-Comprehensive-Survey-For-Long-Context-Language-Modeling}{\\color[RGB]{175,36,67}{LCLM-Horizon}}.",
    "authors": [
      "Jiaheng Liu",
      "Dawei Zhu",
      "Zhiqi Bai",
      "Yancheng He",
      "Huanxuan Liao",
      "Haoran Que",
      "Zekun Wang",
      "Chenchen Zhang",
      "Ge Zhang",
      "Jiebin Zhang",
      "Yuanxing Zhang",
      "Zhuo Chen",
      "Hangyu Guo",
      "Shilong Li",
      "Ziqiang Liu",
      "Yong Shan",
      "Yifan Song",
      "Jiayi Tian",
      "Wenhao Wu",
      "Zhejian Zhou",
      "Ruijie Zhu",
      "Junlan Feng",
      "Yang Gao",
      "Shizhu He",
      "Zhoujun Li",
      "Tianyu Liu",
      "Fanyu Meng",
      "Wenbo Su",
      "Yingshui Tan",
      "Zili Wang",
      "Jian Yang",
      "Wei Ye",
      "Bo Zheng",
      "Wangchunshu Zhou",
      "Wenhao Huang",
      "Sujian Li",
      "Zhaoxiang Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-03-20T17:06:28Z",
    "pdf_url": "https://arxiv.org/pdf/2503.17407v1"
  },
  {
    "arxiv_id": "2503.16585v1",
    "entry_id": "http://arxiv.org/abs/2503.16585v1",
    "title": "Distributed LLMs and Multimodal Large Language Models: A Survey on Advances, Challenges, and Future Directions",
    "summary": "Language models (LMs) are machine learning models designed to predict linguistic patterns by estimating the probability of word sequences based on large-scale datasets, such as text. LMs have a wide range of applications in natural language processing (NLP) tasks, including autocomplete and machine translation. Although larger datasets typically enhance LM performance, scalability remains a challenge due to constraints in computational power and resources. Distributed computing strategies offer essential solutions for improving scalability and managing the growing computational demand. Further, the use of sensitive datasets in training and deployment raises significant privacy concerns. Recent research has focused on developing decentralized techniques to enable distributed training and inference while utilizing diverse computational resources and enabling edge AI. This paper presents a survey on distributed solutions for various LMs, including large language models (LLMs), vision language models (VLMs), multimodal LLMs (MLLMs), and small language models (SLMs). While LLMs focus on processing and generating text, MLLMs are designed to handle multiple modalities of data (e.g., text, images, and audio) and to integrate them for broader applications. To this end, this paper reviews key advancements across the MLLM pipeline, including distributed training, inference, fine-tuning, and deployment, while also identifying the contributions, limitations, and future areas of improvement. Further, it categorizes the literature based on six primary focus areas of decentralization. Our analysis describes gaps in current methodologies for enabling distributed solutions for LMs and outline future research directions, emphasizing the need for novel solutions to enhance the robustness and applicability of distributed LMs.",
    "authors": [
      "Hadi Amini",
      "Md Jueal Mia",
      "Yasaman Saadati",
      "Ahmed Imteaj",
      "Seyedsina Nabavirazavi",
      "Urmish Thakker",
      "Md Zarif Hossain",
      "Awal Ahmed Fime",
      "S. S. Iyengar"
    ],
    "categories": [
      "cs.CL",
      "cs.CV",
      "cs.DC",
      "cs.LG"
    ],
    "published": "2025-03-20T15:18:25Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16585v1"
  },
  {
    "arxiv_id": "2503.16041v2",
    "entry_id": "http://arxiv.org/abs/2503.16041v2",
    "title": "GreenIQ: A Deep Search Platform for Comprehensive Carbon Market Analysis and Automated Report Generation",
    "summary": "This study introduces GreenIQ, an AI-powered deep search platform designed to revolutionise carbon market intelligence through autonomous analysis and automated report generation. Carbon markets operate across diverse regulatory landscapes, generating vast amounts of heterogeneous data from policy documents, industry reports, academic literature, and real-time trading platforms. Traditional research approaches remain labour-intensive, slow, and difficult to scale. GreenIQ addresses these limitations through a multi-agent architecture powered by Large Language Models (LLMs), integrating five specialised AI agents: a Main Researcher Agent for intelligent information retrieval, a Report Writing Agent for structured synthesis, a Final Reviewer Agent for accuracy verification, a Data Visualisation Agent for enhanced interpretability, and a Translator Agent for multilingual adaptation. The system achieves seamless integration of structured and unstructured information with AI-driven citation verification, ensuring high transparency and reliability. GreenIQ delivers a 99.2\\% reduction in processing time and a 99.7\\% cost reduction compared to traditional research methodologies. A novel AI persona-based evaluation framework involving 16 domain-specific AI personas highlights its superior cross-jurisdictional analytical capabilities and regulatory insight generation. GreenIQ sets new standards in AI-driven research synthesis, policy analysis, and sustainability finance by streamlining carbon market research. It offers an efficient and scalable framework for environmental and financial intelligence, enabling more accurate, timely, and cost-effective decision-making in complex regulatory landscapes",
    "authors": [
      "Oluwole Fagbohun",
      "Sai Yashwanth",
      "Akinyemi Sadeeq Akintola",
      "Ifeoluwa Wurola",
      "Lanre Shittu",
      "Aniema Inyang",
      "Oluwatimilehin Odubola",
      "Udodirim Offia",
      "Said Olanrewaju",
      "Ogidan Toluwaleke",
      "Ilemona Abutu",
      "Taiwo Akinbolaji"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-20T11:19:43Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16041v2"
  },
  {
    "arxiv_id": "2503.15808v1",
    "entry_id": "http://arxiv.org/abs/2503.15808v1",
    "title": "ChatGPT and U(X): A Rapid Review on Measuring the User Experience",
    "summary": "ChatGPT, powered by a large language model (LLM), has revolutionized everyday human-computer interaction (HCI) since its 2022 release. While now used by millions around the world, a coherent pathway for evaluating the user experience (UX) ChatGPT offers remains missing. In this rapid review (N = 58), I explored how ChatGPT UX has been approached quantitatively so far. I focused on the independent variables (IVs) manipulated, the dependent variables (DVs) measured, and the methods used for measurement. Findings reveal trends, gaps, and emerging consensus in UX assessments. This work offers a first step towards synthesizing existing approaches to measuring ChatGPT UX, urgent trajectories to advance standardization and breadth, and two preliminary frameworks aimed at guiding future research and tool development. I seek to elevate the field of ChatGPT UX by empowering researchers and practitioners in optimizing user interactions with ChatGPT and similar LLM-based systems.",
    "authors": [
      "Katie Seaborn"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-03-20T02:51:11Z",
    "pdf_url": "https://arxiv.org/pdf/2503.15808v1"
  },
  {
    "arxiv_id": "2503.15772v2",
    "entry_id": "http://arxiv.org/abs/2503.15772v2",
    "title": "Detecting LLM-Generated Peer Reviews",
    "summary": "The integrity of peer review is fundamental to scientific progress, but the rise of large language models (LLMs) has introduced concerns that some reviewers may rely on these tools to generate reviews rather than writing them independently. Although some venues have banned LLM-assisted reviewing, enforcement remains difficult as existing detection tools cannot reliably distinguish between fully generated reviews and those merely polished with AI assistance. In this work, we address the challenge of detecting LLM-generated reviews. We consider the approach of performing indirect prompt injection via the paper's PDF, prompting the LLM to embed a covert watermark in the generated review, and subsequently testing for presence of the watermark in the review. We identify and address several pitfalls in naïve implementations of this approach. Our primary contribution is a rigorous watermarking and detection framework that offers strong statistical guarantees. Specifically, we introduce watermarking schemes and hypothesis tests that control the family-wise error rate across multiple reviews, achieving higher statistical power than standard corrections such as Bonferroni, while making no assumptions about the nature of human-written reviews. We explore multiple indirect prompt injection strategies--including font-based embedding and obfuscated prompts--and evaluate their effectiveness under various reviewer defense scenarios. Our experiments find high success rates in watermark embedding across various LLMs. We also empirically find that our approach is resilient to common reviewer defenses, and that the bounds on error rates in our statistical tests hold in practice. In contrast, we find that Bonferroni-style corrections are too conservative to be useful in this setting.",
    "authors": [
      "Vishisht Rao",
      "Aounon Kumar",
      "Himabindu Lakkaraju",
      "Nihar B. Shah"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-03-20T01:11:35Z",
    "pdf_url": "https://arxiv.org/pdf/2503.15772v2"
  },
  {
    "arxiv_id": "2503.17403v1",
    "entry_id": "http://arxiv.org/abs/2503.17403v1",
    "title": "ChatGPT or A Silent Everywhere Helper: A Survey of Large Language Models",
    "summary": "Large Language Models (LLMs) have revo lutionized natural language processing Natural Language Processing (NLP), with Chat Generative Pre-trained Transformer (ChatGPT) standing out as a notable exampledue to its advanced capabilities and widespread applications. This survey provides a comprehensive analysis of ChatGPT, exploring its architecture, training processes, and functionalities. We examine its integration into various domains across industries such as customer service, education, healthcare, and entertainment. A comparative analysis with other LLMs highlights ChatGPT's unique features and performance metrics. Regarding benchmarks, the paper examines ChatGPT's comparative performance against other LLMs and discusses potential risks such as misinformation, bias, and data privacy concerns. Additionally, we offer a number of figures and tables that outline the backdrop of the discussion, the main ideas of the article, the numerous LLM models, a thorough list of datasets used for pre-training, fine-tuning, and evaluation, as well as particular LLM applications with pertinent references. Finally, we identify future research directions and technological advancements, underscoring the evolving landscape of LLMs and their profound impact on artificial intelligence Artificial Intelligence (AI) and society.",
    "authors": [
      "Azim Akhtarshenas",
      "Afshin Dini",
      "Navid Ayoobi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-19T22:55:08Z",
    "pdf_url": "https://arxiv.org/pdf/2503.17403v1"
  },
  {
    "arxiv_id": "2503.15374v1",
    "entry_id": "http://arxiv.org/abs/2503.15374v1",
    "title": "Real-world validation of a multimodal LLM-powered pipeline for High-Accuracy Clinical Trial Patient Matching leveraging EHR data",
    "summary": "Background: Patient recruitment in clinical trials is hindered by complex eligibility criteria and labor-intensive chart reviews. Prior research using text-only models have struggled to address this problem in a reliable and scalable way due to (1) limited reasoning capabilities, (2) information loss from converting visual records to text, and (3) lack of a generic EHR integration to extract patient data.\n  Methods: We introduce a broadly applicable, integration-free, LLM-powered pipeline that automates patient-trial matching using unprocessed documents extracted from EHRs. Our approach leverages (1) the new reasoning-LLM paradigm, enabling the assessment of even the most complex criteria, (2) visual capabilities of latest LLMs to interpret medical records without lossy image-to-text conversions, and (3) multimodal embeddings for efficient medical record search. The pipeline was validated on the n2c2 2018 cohort selection dataset (288 diabetic patients) and a real-world dataset composed of 485 patients from 30 different sites matched against 36 diverse trials.\n  Results: On the n2c2 dataset, our method achieved a new state-of-the-art criterion-level accuracy of 93\\%. In real-world trials, the pipeline yielded an accuracy of 87\\%, undermined by the difficulty to replicate human decision-making when medical records lack sufficient information. Nevertheless, users were able to review overall eligibility in under 9 minutes per patient on average, representing an 80\\% improvement over traditional manual chart reviews.\n  Conclusion: This pipeline demonstrates robust performance in clinical trial patient matching without requiring custom integration with site systems or trial-specific tailoring, thereby enabling scalable deployment across sites seeking to leverage AI for patient matching.",
    "authors": [
      "Anatole Callies",
      "Quentin Bodinier",
      "Philippe Ravaud",
      "Kourosh Davarpanah"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-19T16:12:11Z",
    "pdf_url": "https://arxiv.org/pdf/2503.15374v1"
  },
  {
    "arxiv_id": "2503.14604v2",
    "entry_id": "http://arxiv.org/abs/2503.14604v2",
    "title": "Image Captioning Evaluation in the Age of Multimodal LLMs: Challenges and Future Perspectives",
    "summary": "The evaluation of machine-generated image captions is a complex and evolving challenge. With the advent of Multimodal Large Language Models (MLLMs), image captioning has become a core task, increasing the need for robust and reliable evaluation metrics. This survey provides a comprehensive overview of advancements in image captioning evaluation, analyzing the evolution, strengths, and limitations of existing metrics. We assess these metrics across multiple dimensions, including correlation with human judgment, ranking accuracy, and sensitivity to hallucinations. Additionally, we explore the challenges posed by the longer and more detailed captions generated by MLLMs and examine the adaptability of current metrics to these stylistic variations. Our analysis highlights some limitations of standard evaluation approaches and suggests promising directions for future research in image captioning assessment.",
    "authors": [
      "Sara Sarto",
      "Marcella Cornia",
      "Rita Cucchiara"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-18T18:03:56Z",
    "pdf_url": "https://arxiv.org/pdf/2503.14604v2"
  },
  {
    "arxiv_id": "2503.13879v2",
    "entry_id": "http://arxiv.org/abs/2503.13879v2",
    "title": "Bridging Social Psychology and LLM Reasoning: Conflict-Aware Meta-Review Generation via Cognitive Alignment",
    "summary": "The rapid growth of scholarly submissions has overwhelmed traditional peer review systems, driving the need for intelligent automation to preserve scientific rigor. While large language models (LLMs) show promise in automating manuscript critiques, their ability to synthesize high-stakes meta-reviews, which require conflict-aware reasoning and consensus derivation, remains underdeveloped. Existing methods fail to effectively handle conflicting viewpoints within differing opinions, and often introduce additional cognitive biases, such as anchoring effects and conformity bias.To overcome these limitations, we propose the Cognitive Alignment Framework (CAF), a dual-process architecture that transforms LLMs into adaptive scientific arbitrators. By operationalizing Kahneman's dual-process theory, CAF introduces a three-step cognitive pipeline: review initialization, incremental integration, and cognitive alignment.Empirical validation shows that CAF outperforms existing LLM-based methods, with sentiment consistency gains reaching up to 19.47\\% and content consistency improving by as much as 12.95\\%.",
    "authors": [
      "Wei Chen",
      "Han Ding",
      "Meng Yuan",
      "Zhao Zhang",
      "Deqing Wang",
      "Fuzhen Zhuang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-18T04:13:11Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13879v2"
  },
  {
    "arxiv_id": "2503.16527v1",
    "entry_id": "http://arxiv.org/abs/2503.16527v1",
    "title": "LLM Generated Persona is a Promise with a Catch",
    "summary": "The use of large language models (LLMs) to simulate human behavior has gained significant attention, particularly through personas that approximate individual characteristics. Persona-based simulations hold promise for transforming disciplines that rely on population-level feedback, including social science, economic analysis, marketing research, and business operations. Traditional methods to collect realistic persona data face significant challenges. They are prohibitively expensive and logistically challenging due to privacy constraints, and often fail to capture multi-dimensional attributes, particularly subjective qualities. Consequently, synthetic persona generation with LLMs offers a scalable, cost-effective alternative. However, current approaches rely on ad hoc and heuristic generation techniques that do not guarantee methodological rigor or simulation precision, resulting in systematic biases in downstream tasks. Through extensive large-scale experiments including presidential election forecasts and general opinion surveys of the U.S. population, we reveal that these biases can lead to significant deviations from real-world outcomes. Our findings underscore the need to develop a rigorous science of persona generation and outline the methodological innovations, organizational and institutional support, and empirical foundations required to enhance the reliability and scalability of LLM-driven persona simulations. To support further research and development in this area, we have open-sourced approximately one million generated personas, available for public access and analysis at https://huggingface.co/datasets/Tianyi-Lab/Personas.",
    "authors": [
      "Ang Li",
      "Haozhe Chen",
      "Hongseok Namkoong",
      "Tianyi Peng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.SI"
    ],
    "published": "2025-03-18T03:11:27Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16527v1"
  },
  {
    "arxiv_id": "2503.13793v2",
    "entry_id": "http://arxiv.org/abs/2503.13793v2",
    "title": "Mapping the Trust Terrain: LLMs in Software Engineering -- Insights and Perspectives",
    "summary": "Applications of Large Language Models (LLMs) are rapidly growing in industry and academia for various software engineering (SE) tasks. As these models become more integral to critical processes, ensuring their reliability and trustworthiness becomes essential. Consequently, the concept of trust in these systems is becoming increasingly critical. Well-calibrated trust is important, as excessive trust can lead to security vulnerabilities, and risks, while insufficient trust can hinder innovation. However, the landscape of trust-related concepts in LLMs in SE is relatively unclear, with concepts such as trust, distrust, and trustworthiness lacking clear conceptualizations in the SE community. To bring clarity to the current research status and identify opportunities for future work, we conducted a comprehensive review of $88$ papers: a systematic literature review of $18$ papers focused on LLMs in SE, complemented by an analysis of 70 papers from broader trust literature. Additionally, we conducted a survey study with 25 domain experts to gain insights into practitioners' understanding of trust and identify gaps between existing literature and developers' perceptions. The result of our analysis serves as a roadmap that covers trust-related concepts in LLMs in SE and highlights areas for future exploration.",
    "authors": [
      "Dipin Khati",
      "Yijin Liu",
      "David N. Palacio",
      "Yixuan Zhang",
      "Denys Poshyvanyk"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-03-18T00:49:43Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13793v2"
  },
  {
    "arxiv_id": "2503.13754v2",
    "entry_id": "http://arxiv.org/abs/2503.13754v2",
    "title": "From Autonomous Agents to Integrated Systems, A New Paradigm: Orchestrated Distributed Intelligence",
    "summary": "The rapid evolution of artificial intelligence (AI) has ushered in a new era of integrated systems that merge computational prowess with human decision-making. In this paper, we introduce the concept of Orchestrated Distributed Intelligence (ODI), a novel paradigm that reconceptualizes AI not as isolated autonomous agents, but as cohesive, orchestrated networks that work in tandem with human expertise. ODI leverages advanced orchestration layers, multi-loop feedback mechanisms, and a high cognitive density framework to transform static, record-keeping systems into dynamic, action-oriented environments. Through a comprehensive review of multi-agent system literature, recent technological advances, and practical insights from industry forums, we argue that the future of AI lies in integrating distributed intelligence within human-centric workflows. This approach not only enhances operational efficiency and strategic agility but also addresses challenges related to scalability, transparency, and ethical decision-making. Our work outlines key theoretical implications and presents a practical roadmap for future research and enterprise innovation, aiming to pave the way for responsible and adaptive AI systems that drive sustainable innovation in human organizations.",
    "authors": [
      "Krti Tallam"
    ],
    "categories": [
      "eess.SY",
      "cs.AI"
    ],
    "published": "2025-03-17T22:21:25Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13754v2"
  },
  {
    "arxiv_id": "2503.13415v1",
    "entry_id": "http://arxiv.org/abs/2503.13415v1",
    "title": "A Comprehensive Survey on Multi-Agent Cooperative Decision-Making: Scenarios, Approaches, Challenges and Perspectives",
    "summary": "With the rapid development of artificial intelligence, intelligent decision-making techniques have gradually surpassed human levels in various human-machine competitions, especially in complex multi-agent cooperative task scenarios. Multi-agent cooperative decision-making involves multiple agents working together to complete established tasks and achieve specific objectives. These techniques are widely applicable in real-world scenarios such as autonomous driving, drone navigation, disaster rescue, and simulated military confrontations. This paper begins with a comprehensive survey of the leading simulation environments and platforms used for multi-agent cooperative decision-making. Specifically, we provide an in-depth analysis for these simulation environments from various perspectives, including task formats, reward allocation, and the underlying technologies employed. Subsequently, we provide a comprehensive overview of the mainstream intelligent decision-making approaches, algorithms and models for multi-agent systems (MAS). Theseapproaches can be broadly categorized into five types: rule-based (primarily fuzzy logic), game theory-based, evolutionary algorithms-based, deep multi-agent reinforcement learning (MARL)-based, and large language models(LLMs)reasoning-based. Given the significant advantages of MARL andLLMs-baseddecision-making methods over the traditional rule, game theory, and evolutionary algorithms, this paper focuses on these multi-agent methods utilizing MARL and LLMs-based techniques. We provide an in-depth discussion of these approaches, highlighting their methodology taxonomies, advantages, and drawbacks. Further, several prominent research directions in the future and potential challenges of multi-agent cooperative decision-making are also detailed.",
    "authors": [
      "Weiqiang Jin",
      "Hongyang Du",
      "Biao Zhao",
      "Xingwu Tian",
      "Bohang Shi",
      "Guang Yang"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-03-17T17:45:46Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13415v1"
  },
  {
    "arxiv_id": "2503.13299v2",
    "entry_id": "http://arxiv.org/abs/2503.13299v2",
    "title": "A Survey on Transformer Context Extension: Approaches and Evaluation",
    "summary": "Large language models (LLMs) based on Transformer have been widely applied in the filed of natural language processing (NLP), demonstrating strong performance, particularly in handling short text tasks. However, when it comes to long context scenarios, the performance of LLMs degrades due to some challenges. To alleviate this phenomenon, there is a number of work proposed recently. In this survey, we first list the challenges of applying pre-trained LLMs to process long contexts. Then systematically review the approaches related to long context and propose our taxonomy categorizing them into four main types: positional encoding, context compression, retrieval augmented, and attention pattern. In addition to the approaches, we focus on the evaluation of long context, organizing relevant data, tasks, and metrics based on existing long context benchmarks. Finally, we summarize unresolved issues in the long context domain and put forward our views on future developments.",
    "authors": [
      "Yijun Liu",
      "Jinzheng Yu",
      "Yang Xu",
      "Zhongyang Li",
      "Qingfu Zhu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-17T15:44:09Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13299v2"
  },
  {
    "arxiv_id": "2503.12687v1",
    "entry_id": "http://arxiv.org/abs/2503.12687v1",
    "title": "AI Agents: Evolution, Architecture, and Real-World Applications",
    "summary": "This paper examines the evolution, architecture, and practical applications of AI agents from their early, rule-based incarnations to modern sophisticated systems that integrate large language models with dedicated modules for perception, planning, and tool use. Emphasizing both theoretical foundations and real-world deployments, the paper reviews key agent paradigms, discusses limitations of current evaluation benchmarks, and proposes a holistic evaluation framework that balances task effectiveness, efficiency, robustness, and safety. Applications across enterprise, personal assistance, and specialized domains are analyzed, with insights into future research directions for more resilient and adaptive AI agent systems.",
    "authors": [
      "Naveen Krishnan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-16T23:07:48Z",
    "pdf_url": "https://arxiv.org/pdf/2503.12687v1"
  },
  {
    "arxiv_id": "2503.12434v1",
    "entry_id": "http://arxiv.org/abs/2503.12434v1",
    "title": "A Survey on the Optimization of Large Language Model-based Agents",
    "summary": "With the rapid development of Large Language Models (LLMs), LLM-based agents have been widely adopted in various fields, becoming essential for autonomous decision-making and interactive tasks. However, current work typically relies on prompt design or fine-tuning strategies applied to vanilla LLMs, which often leads to limited effectiveness or suboptimal performance in complex agent-related environments. Although LLM optimization techniques can improve model performance across many general tasks, they lack specialized optimization towards critical agent functionalities such as long-term planning, dynamic environmental interaction, and complex decision-making. Although numerous recent studies have explored various strategies to optimize LLM-based agents for complex agent tasks, a systematic review summarizing and comparing these methods from a holistic perspective is still lacking. In this survey, we provide a comprehensive review of LLM-based agent optimization approaches, categorizing them into parameter-driven and parameter-free methods. We first focus on parameter-driven optimization, covering fine-tuning-based optimization, reinforcement learning-based optimization, and hybrid strategies, analyzing key aspects such as trajectory data construction, fine-tuning techniques, reward function design, and optimization algorithms. Additionally, we briefly discuss parameter-free strategies that optimize agent behavior through prompt engineering and external knowledge retrieval. Finally, we summarize the datasets and benchmarks used for evaluation and tuning, review key applications of LLM-based agents, and discuss major challenges and promising future directions. Our repository for related references is available at https://github.com/YoungDubbyDu/LLM-Agent-Optimization.",
    "authors": [
      "Shangheng Du",
      "Jiabao Zhao",
      "Jinxin Shi",
      "Zhentao Xie",
      "Xin Jiang",
      "Yanhong Bai",
      "Liang He"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-03-16T10:09:10Z",
    "pdf_url": "https://arxiv.org/pdf/2503.12434v1"
  },
  {
    "arxiv_id": "2503.16515v1",
    "entry_id": "http://arxiv.org/abs/2503.16515v1",
    "title": "Highlighting Case Studies in LLM Literature Review of Interdisciplinary System Science",
    "summary": "Large Language Models (LLMs) were used to assist four Commonwealth Scientific and Industrial Research Organisation (CSIRO) researchers to perform systematic literature reviews (SLR). We evaluate the performance of LLMs for SLR tasks in these case studies. In each, we explore the impact of changing parameters on the accuracy of LLM responses. The LLM was tasked with extracting evidence from chosen academic papers to answer specific research questions. We evaluate the models' performance in faithfully reproducing quotes from the literature and subject experts were asked to assess the model performance in answering the research questions. We developed a semantic text highlighting tool to facilitate expert review of LLM responses.\n  We found that state of the art LLMs were able to reproduce quotes from texts with greater than 95% accuracy and answer research questions with an accuracy of approximately 83%. We use two methods to determine the correctness of LLM responses; expert review and the cosine similarity of transformer embeddings of LLM and expert answers. The correlation between these methods ranged from 0.48 to 0.77, providing evidence that the latter is a valid metric for measuring semantic similarity.",
    "authors": [
      "Lachlan McGinness",
      "Peter Baumgartner"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-16T05:52:18Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16515v1"
  },
  {
    "arxiv_id": "2503.12344v1",
    "entry_id": "http://arxiv.org/abs/2503.12344v1",
    "title": "EXPRESS: An LLM-Generated Explainable Property Valuation System with Neighbor Imputation",
    "summary": "The demand for property valuation has attracted significant attention from sellers, buyers, and customers applying for loans. Reviews of existing approaches have revealed shortcomings in terms of not being able to handle missing value situations, as well as lacking interpretability, which means they cannot be used in real-world applications. To address these challenges, we propose an LLM-Generated EXplainable PRopErty valuation SyStem with neighbor imputation called EXPRESS, which provides the customizable missing value imputation technique, and addresses the opaqueness of prediction by providing the feature-wise explanation generated by LLM. The dynamic nearest neighbor search finds similar properties depending on different application scenarios by property configuration set by users (e.g., house age as criteria for the house in rural areas, and locations for buildings in urban areas). Motivated by the human appraisal procedure, we generate feature-wise explanations to provide users with a more intuitive understanding of the prediction results.",
    "authors": [
      "Wei-Wei Du",
      "Yung-Chien Wang",
      "Wen-Chih Peng"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-16T03:49:52Z",
    "pdf_url": "https://arxiv.org/pdf/2503.12344v1"
  },
  {
    "arxiv_id": "2503.12016v2",
    "entry_id": "http://arxiv.org/abs/2503.12016v2",
    "title": "A Survey on Federated Fine-tuning of Large Language Models",
    "summary": "Large Language Models (LLMs) have demonstrated impressive success across various tasks. Integrating LLMs with Federated Learning (FL), a paradigm known as FedLLM, offers a promising avenue for collaborative model adaptation while preserving data privacy. This survey provides a systematic and comprehensive review of FedLLM. We begin by tracing the historical development of both LLMs and FL, summarizing relevant prior research to set the context. Subsequently, we delve into an in-depth analysis of the fundamental challenges inherent in deploying FedLLM. Addressing these challenges often requires efficient adaptation strategies; therefore, we conduct an extensive examination of existing Parameter-Efficient Fine-tuning (PEFT) methods and explore their applicability within the FL framework. To rigorously evaluate the performance of FedLLM, we undertake a thorough review of existing fine-tuning datasets and evaluation benchmarks. Furthermore, we discuss FedLLM's diverse real-world applications across multiple domains. Finally, we identify critical open challenges and outline promising research directions to foster future advancements in FedLLM. This survey aims to serve as a foundational resource for researchers and practitioners, offering valuable insights into the rapidly evolving landscape of federated fine-tuning for LLMs. It also establishes a roadmap for future innovations in privacy-preserving AI. We actively maintain a GitHub repo \\href{https://github.com/Clin0212/Awesome-Federated-LLM-Learning}{https://github.com/Clin0212/Awesome-Federated-LLM-Learning} to track cutting-edge advancements in this field.",
    "authors": [
      "Yebo Wu",
      "Chunlin Tian",
      "Jingguang Li",
      "He Sun",
      "Kahou Tam",
      "Zhanting Zhou",
      "Haicheng Liao",
      "Zhijiang Guo",
      "Li Li",
      "Chengzhong Xu"
    ],
    "categories": [
      "cs.LG",
      "cs.DC"
    ],
    "published": "2025-03-15T06:52:10Z",
    "pdf_url": "https://arxiv.org/pdf/2503.12016v2"
  },
  {
    "arxiv_id": "2503.11924v2",
    "entry_id": "http://arxiv.org/abs/2503.11924v2",
    "title": "REGEN: A Dataset and Benchmarks with Natural Language Critiques and Narratives",
    "summary": "This paper introduces a novel dataset REGEN (Reviews Enhanced with GEnerative Narratives), designed to benchmark the conversational capabilities of recommender Large Language Models (LLMs), addressing the limitations of existing datasets that primarily focus on sequential item prediction. REGEN extends the Amazon Product Reviews dataset by inpainting two key natural language features: (1) user critiques, representing user \"steering\" queries that lead to the selection of a subsequent item, and (2) narratives, rich textual outputs associated with each recommended item taking into account prior context. The narratives include product endorsements, purchase explanations, and summaries of user preferences.\n  Further, we establish an end-to-end modeling benchmark for the task of conversational recommendation, where models are trained to generate both recommendations and corresponding narratives conditioned on user history (items and critiques). For this joint task, we introduce a modeling framework LUMEN (LLM-based Unified Multi-task Model with Critiques, Recommendations, and Narratives) which uses an LLM as a backbone for critiquing, retrieval and generation. We also evaluate the dataset's quality using standard auto-rating techniques and benchmark it by training both traditional and LLM-based recommender models. Our results demonstrate that incorporating critiques enhances recommendation quality by enabling the recommender to learn language understanding and integrate it with recommendation signals. Furthermore, LLMs trained on our dataset effectively generate both recommendations and contextual narratives, achieving performance comparable to state-of-the-art recommenders and language models.",
    "authors": [
      "Kun Su",
      "Krishna Sayana",
      "Hubert Pham",
      "James Pine",
      "Yuri Vasilevski",
      "Raghavendra Vasudeva",
      "Marialena Kyriakidi",
      "Liam Hebert",
      "Ambarish Jash",
      "Anushya Subbiah",
      "Sukhdeep Sodhi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-03-14T23:47:46Z",
    "pdf_url": "https://arxiv.org/pdf/2503.11924v2"
  },
  {
    "arxiv_id": "2503.11531v1",
    "entry_id": "http://arxiv.org/abs/2503.11531v1",
    "title": "Potential of large language model-powered nudges for promoting daily water and energy conservation",
    "summary": "The increasing amount of pressure related to water and energy shortages has increased the urgency of cultivating individual conservation behaviors. While the concept of nudging, i.e., providing usage-based feedback, has shown promise in encouraging conservation behaviors, its efficacy is often constrained by the lack of targeted and actionable content. This study investigates the impact of the use of large language models (LLMs) to provide tailored conservation suggestions for conservation intentions and their rationale. Through a survey experiment with 1,515 university participants, we compare three virtual nudging scenarios: no nudging, traditional nudging with usage statistics, and LLM-powered nudging with usage statistics and personalized conservation suggestions. The results of statistical analyses and causal forest modeling reveal that nudging led to an increase in conservation intentions among 86.9%-98.0% of the participants. LLM-powered nudging achieved a maximum increase of 18.0% in conservation intentions, surpassing traditional nudging by 88.6%. Furthermore, structural equation modeling results reveal that exposure to LLM-powered nudges enhances self-efficacy and outcome expectations while diminishing dependence on social norms, thereby increasing intrinsic motivation to conserve. These findings highlight the transformative potential of LLMs in promoting individual water and energy conservation, representing a new frontier in the design of sustainable behavioral interventions and resource management.",
    "authors": [
      "Zonghan Li",
      "Song Tong",
      "Yi Liu",
      "Kaiping Peng",
      "Chunyan Wang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-03-14T15:58:11Z",
    "pdf_url": "https://arxiv.org/pdf/2503.11531v1"
  },
  {
    "arxiv_id": "2503.11486v1",
    "entry_id": "http://arxiv.org/abs/2503.11486v1",
    "title": "A Review of DeepSeek Models' Key Innovative Techniques",
    "summary": "DeepSeek-V3 and DeepSeek-R1 are leading open-source Large Language Models (LLMs) for general-purpose tasks and reasoning, achieving performance comparable to state-of-the-art closed-source models from companies like OpenAI and Anthropic -- while requiring only a fraction of their training costs. Understanding the key innovative techniques behind DeepSeek's success is crucial for advancing LLM research. In this paper, we review the core techniques driving the remarkable effectiveness and efficiency of these models, including refinements to the transformer architecture, innovations such as Multi-Head Latent Attention and Mixture of Experts, Multi-Token Prediction, the co-design of algorithms, frameworks, and hardware, the Group Relative Policy Optimization algorithm, post-training with pure reinforcement learning and iterative training alternating between supervised fine-tuning and reinforcement learning. Additionally, we identify several open questions and highlight potential research opportunities in this rapidly advancing field.",
    "authors": [
      "Chengen Wang",
      "Murat Kantarcioglu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-14T15:11:29Z",
    "pdf_url": "https://arxiv.org/pdf/2503.11486v1"
  },
  {
    "arxiv_id": "2503.16508v1",
    "entry_id": "http://arxiv.org/abs/2503.16508v1",
    "title": "Conversational AI as a Coding Assistant: Understanding Programmers' Interactions with and Expectations from Large Language Models for Coding",
    "summary": "Conversational AI interfaces powered by large language models (LLMs) are increasingly used as coding assistants. However, questions remain about how programmers interact with LLM-based conversational agents, the challenges they encounter, and the factors influencing adoption. This study investigates programmers' usage patterns, perceptions, and interaction strategies when engaging with LLM-driven coding assistants. Through a survey, participants reported both the benefits, such as efficiency and clarity of explanations, and the limitations, including inaccuracies, lack of contextual awareness, and concerns about over-reliance. Notably, some programmers actively avoid LLMs due to a preference for independent learning, distrust in AI-generated code, and ethical considerations. Based on our findings, we propose design guidelines for improving conversational coding assistants, emphasizing context retention, transparency, multimodal support, and adaptability to user preferences. These insights contribute to the broader understanding of how LLM-based conversational agents can be effectively integrated into software development workflows while addressing adoption barriers and enhancing usability.",
    "authors": [
      "Mehmet Akhoroz",
      "Caglar Yildirim"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-03-14T15:06:07Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16508v1"
  },
  {
    "arxiv_id": "2503.11411v1",
    "entry_id": "http://arxiv.org/abs/2503.11411v1",
    "title": "Empowering Time Series Analysis with Synthetic Data: A Survey and Outlook in the Era of Foundation Models",
    "summary": "Time series analysis is crucial for understanding dynamics of complex systems. Recent advances in foundation models have led to task-agnostic Time Series Foundation Models (TSFMs) and Large Language Model-based Time Series Models (TSLLMs), enabling generalized learning and integrating contextual information. However, their success depends on large, diverse, and high-quality datasets, which are challenging to build due to regulatory, diversity, quality, and quantity constraints. Synthetic data emerge as a viable solution, addressing these challenges by offering scalable, unbiased, and high-quality alternatives. This survey provides a comprehensive review of synthetic data for TSFMs and TSLLMs, analyzing data generation strategies, their role in model pretraining, fine-tuning, and evaluation, and identifying future research directions.",
    "authors": [
      "Xu Liu",
      "Taha Aksu",
      "Juncheng Liu",
      "Qingsong Wen",
      "Yuxuan Liang",
      "Caiming Xiong",
      "Silvio Savarese",
      "Doyen Sahoo",
      "Junnan Li",
      "Chenghao Liu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-14T13:53:46Z",
    "pdf_url": "https://arxiv.org/pdf/2503.11411v1"
  },
  {
    "arxiv_id": "2503.11733v1",
    "entry_id": "http://arxiv.org/abs/2503.11733v1",
    "title": "LLM Agents for Education: Advances and Applications",
    "summary": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in automating tasks and driving innovation across diverse educational applications. In this survey, we provide a systematic review of state-of-the-art research on LLM agents in education, categorizing them into two broad classes: (1) \\emph{Pedagogical Agents}, which focus on automating complex pedagogical tasks to support both teachers and students; and (2) \\emph{Domain-Specific Educational Agents}, which are tailored for specialized fields such as science education, language learning, and professional development. We comprehensively examine the technological advancements underlying these LLM agents, including key datasets, benchmarks, and algorithmic frameworks that drive their effectiveness. Furthermore, we discuss critical challenges such as privacy, bias and fairness concerns, hallucination mitigation, and integration with existing educational ecosystems. This survey aims to provide a comprehensive technological overview of LLM agents for education, fostering further research and collaboration to enhance their impact for the greater good of learners and educators alike.",
    "authors": [
      "Zhendong Chu",
      "Shen Wang",
      "Jian Xie",
      "Tinghui Zhu",
      "Yibo Yan",
      "Jinheng Ye",
      "Aoxiao Zhong",
      "Xuming Hu",
      "Jing Liang",
      "Philip S. Yu",
      "Qingsong Wen"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2025-03-14T11:53:44Z",
    "pdf_url": "https://arxiv.org/pdf/2503.11733v1"
  },
  {
    "arxiv_id": "2503.22693v1",
    "entry_id": "http://arxiv.org/abs/2503.22693v1",
    "title": "Bridging Language Models and Financial Analysis",
    "summary": "The rapid advancements in Large Language Models (LLMs) have unlocked transformative possibilities in natural language processing, particularly within the financial sector. Financial data is often embedded in intricate relationships across textual content, numerical tables, and visual charts, posing challenges that traditional methods struggle to address effectively. However, the emergence of LLMs offers new pathways for processing and analyzing this multifaceted data with increased efficiency and insight. Despite the fast pace of innovation in LLM research, there remains a significant gap in their practical adoption within the finance industry, where cautious integration and long-term validation are prioritized. This disparity has led to a slower implementation of emerging LLM techniques, despite their immense potential in financial applications. As a result, many of the latest advancements in LLM technology remain underexplored or not fully utilized in this domain. This survey seeks to bridge this gap by providing a comprehensive overview of recent developments in LLM research and examining their applicability to the financial sector. Building on previous survey literature, we highlight several novel LLM methodologies, exploring their distinctive capabilities and their potential relevance to financial data analysis. By synthesizing insights from a broad range of studies, this paper aims to serve as a valuable resource for researchers and practitioners, offering direction on promising research avenues and outlining future opportunities for advancing LLM applications in finance.",
    "authors": [
      "Alejandro Lopez-Lira",
      "Jihoon Kwon",
      "Sangwoon Yoon",
      "Jy-yong Sohn",
      "Chanyeol Choi"
    ],
    "categories": [
      "q-fin.ST",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-14T01:35:20Z",
    "pdf_url": "https://arxiv.org/pdf/2503.22693v1"
  },
  {
    "arxiv_id": "2503.13505v2",
    "entry_id": "http://arxiv.org/abs/2503.13505v2",
    "title": "Ensemble Learning for Large Language Models in Text and Code Generation: A Survey",
    "summary": "Generative Pretrained Transformers (GPTs) are foundational Large Language Models (LLMs) for text generation. However, individual LLMs often produce inconsistent outputs and exhibit biases, limiting their representation of diverse language patterns. The closed-source nature of many powerful LLMs further restricts industry applications due to data privacy concerns. Inspired by successes in text generation, LLM ensemble techniques are now increasingly explored for code generation. This article reviews these emerging ensemble approaches to enhance understanding, encourage further research, and promote practical implementation in both text and code generation. We categorize LLM ensembles into seven main methods - weight merging, knowledge fusion, mixture-of-experts, reward ensemble, output ensemble, routing, and cascading - analyzing capabilities of those approaches. Our findings highlight key benefits such as improved diversity representation, enhanced output quality, and greater application flexibility. These insights aid model selection for real-world tasks and crucially, lay groundwork for extending ensemble strategies to multimodal LLMs.",
    "authors": [
      "Mari Ashiga",
      "Wei Jie",
      "Fan Wu",
      "Vardan Voskanyan",
      "Fateme Dinmohammadi",
      "Paul Brookes",
      "Jingzhi Gong",
      "Zheng Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-03-13T18:50:57Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13505v2"
  },
  {
    "arxiv_id": "2504.01963v1",
    "entry_id": "http://arxiv.org/abs/2504.01963v1",
    "title": "LLMs Working in Harmony: A Survey on the Technological Aspects of Building Effective LLM-Based Multi Agent Systems",
    "summary": "This survey investigates foundational technologies essential for developing effective Large Language Model (LLM)-based multi-agent systems. Aiming to answer how best to optimize these systems for collaborative, dynamic environments, we focus on four critical areas: Architecture, Memory, Planning, and Technologies/Frameworks. By analyzing recent advancements and their limitations - such as scalability, real-time response challenges, and agent coordination constraints, we provide a detailed view of the technological landscape. Frameworks like the Mixture of Agents architecture and the ReAct planning model exemplify current innovations, showcasing improvements in role assignment and decision-making. This review synthesizes key strengths and persistent challenges, offering practical recommendations to enhance system scalability, agent collaboration, and adaptability. Our findings provide a roadmap for future research, supporting the creation of robust, efficient multi-agent systems that advance both individual agent performance and collective system resilience.",
    "authors": [
      "R. M. Aratchige",
      "W. M. K. S. Ilmini"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-13T06:17:50Z",
    "pdf_url": "https://arxiv.org/pdf/2504.01963v1"
  },
  {
    "arxiv_id": "2503.09956v4",
    "entry_id": "http://arxiv.org/abs/2503.09956v4",
    "title": "DeepSeek-Inspired Exploration of RL-based LLMs and Synergy with Wireless Networks: A Survey",
    "summary": "Reinforcement learning (RL)-based large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, have attracted widespread attention for their remarkable capabilities in multimodal data understanding. Meanwhile, the rapid expansion of information services has led to a growing demand for AI-enabled wireless networks. The open-source DeepSeek models are famous for their innovative designs, such as large-scale pure RL and cost-efficient training, which make them well-suited for practical deployment in wireless networks. By integrating DeepSeek-style LLMs with wireless infrastructures, a synergistic opportunity arises: the DeepSeek-style LLMs enhance network optimization with strong reasoning and decision-making abilities, while wireless infrastructure enables the broad deployment of these models. Motivated by this convergence, this survey presents a comprehensive DeepSeek-inspired exploration of RL-based LLMs in the context of wireless networks. We begin by reviewing key techniques behind network optimization to establish a foundation for understanding DeepSeek-style LLM integration. Next, we examine recent advancements in RL-based LLMs, using DeepSeek models as a representative example. Building on this, we explore the synergy between the two domains, highlighting motivations, challenges, and potential solutions. Finally, we highlight emerging directions for integrating LLMs with wireless networks, such as quantum, on-device, and neural-symbolic LLM models, as well as embodied AI agents. Overall, this survey offers a comprehensive examination of the interplay between DeepSeek-style LLMs and wireless networks, demonstrating how these domains can mutually enhance each other to drive innovation.",
    "authors": [
      "Yu Qiao",
      "Phuong-Nam Tran",
      "Ji Su Yoon",
      "Loc X. Nguyen",
      "Eui-Nam Huh",
      "Dusit Niyato",
      "Choong Seon Hong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "cs.ET"
    ],
    "published": "2025-03-13T01:59:11Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09956v4"
  },
  {
    "arxiv_id": "2503.09829v3",
    "entry_id": "http://arxiv.org/abs/2503.09829v3",
    "title": "SE(3)-Equivariant Robot Learning and Control: A Tutorial Survey",
    "summary": "Recent advances in deep learning and Transformers have driven major breakthroughs in robotics by employing techniques such as imitation learning, reinforcement learning, and LLM-based multimodal perception and decision-making. However, conventional deep learning and Transformer models often struggle to process data with inherent symmetries and invariances, typically relying on large datasets or extensive data augmentation. Equivariant neural networks overcome these limitations by explicitly integrating symmetry and invariance into their architectures, leading to improved efficiency and generalization. This tutorial survey reviews a wide range of equivariant deep learning and control methods for robotics, from classic to state-of-the-art, with a focus on SE(3)-equivariant models that leverage the natural 3D rotational and translational symmetries in visual robotic manipulation and control design. Using unified mathematical notation, we begin by reviewing key concepts from group theory, along with matrix Lie groups and Lie algebras. We then introduce foundational group-equivariant neural network design and show how the group-equivariance can be obtained through their structure. Next, we discuss the applications of SE(3)-equivariant neural networks in robotics in terms of imitation learning and reinforcement learning. The SE(3)-equivariant control design is also reviewed from the perspective of geometric control. Finally, we highlight the challenges and future directions of equivariant methods in developing more robust, sample-efficient, and multi-modal real-world robotic systems.",
    "authors": [
      "Joohwan Seo",
      "Soochul Yoo",
      "Junwoo Chang",
      "Hyunseok An",
      "Hyunwoo Ryu",
      "Soomi Lee",
      "Arvind Kruthiventy",
      "Jongeun Choi",
      "Roberto Horowitz"
    ],
    "categories": [
      "cs.RO",
      "cs.LG",
      "eess.SY"
    ],
    "published": "2025-03-12T20:47:40Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09829v3"
  },
  {
    "arxiv_id": "2503.09743v1",
    "entry_id": "http://arxiv.org/abs/2503.09743v1",
    "title": "Review GIDE -- Restaurant Review Gastrointestinal Illness Detection and Extraction with Large Language Models",
    "summary": "Foodborne gastrointestinal (GI) illness is a common cause of ill health in the UK. However, many cases do not interact with the healthcare system, posing significant challenges for traditional surveillance methods. The growth of publicly available online restaurant reviews and advancements in large language models (LLMs) present potential opportunities to extend disease surveillance by identifying public reports of GI illness. In this study, we introduce a novel annotation schema, developed with experts in GI illness, applied to the Yelp Open Dataset of reviews. Our annotations extend beyond binary disease detection, to include detailed extraction of information on symptoms and foods. We evaluate the performance of open-weight LLMs across these three tasks: GI illness detection, symptom extraction, and food extraction. We compare this performance to RoBERTa-based classification models fine-tuned specifically for these tasks. Our results show that using prompt-based approaches, LLMs achieve micro-F1 scores of over 90% for all three of our tasks. Using prompting alone, we achieve micro-F1 scores that exceed those of smaller fine-tuned models. We further demonstrate the robustness of LLMs in GI illness detection across three bias-focused experiments. Our results suggest that publicly available review text and LLMs offer substantial potential for public health surveillance of GI illness by enabling highly effective extraction of key information. While LLMs appear to exhibit minimal bias in processing, the inherent limitations of restaurant review data highlight the need for cautious interpretation of results.",
    "authors": [
      "Timothy Laurence",
      "Joshua Harris",
      "Leo Loman",
      "Amy Douglas",
      "Yung-Wai Chan",
      "Luke Hounsome",
      "Lesley Larkin",
      "Michael Borowitz"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-03-12T18:42:43Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09743v1"
  },
  {
    "arxiv_id": "2503.09701v3",
    "entry_id": "http://arxiv.org/abs/2503.09701v3",
    "title": "Reassessing Active Learning Adoption in Contemporary NLP: A Community Survey",
    "summary": "Supervised learning relies on data annotation which usually is time-consuming and therefore expensive. A longstanding strategy to reduce annotation costs is active learning, an iterative process, in which a human annotates only data instances deemed informative by a model. Research in active learning has made considerable progress, especially with the rise of large language models (LLMs). However, we still know little about how these remarkable advances have translated into real-world applications, or contributed to removing key barriers to active learning adoption. To fill in this gap, we conduct an online survey in the NLP community to collect previously intangible insights on current implementation practices, common obstacles in application, and future prospects in active learning. We also reassess the perceived relevance of data annotation and active learning as fundamental assumptions. Our findings show that data annotation is expected to remain important and active learning to stay relevant while benefiting from LLMs. Consistent with a community survey from over 15 years ago, three key challenges yet persist -- setup complexity, uncertain cost reduction, and tooling -- for which we propose alleviation strategies. We publish an anonymized version of the dataset.",
    "authors": [
      "Julia Romberg",
      "Christopher Schröder",
      "Julius Gonsior",
      "Katrin Tomanek",
      "Fredrik Olsson"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-03-12T18:00:04Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09701v3"
  },
  {
    "arxiv_id": "2503.09567v5",
    "entry_id": "http://arxiv.org/abs/2503.09567v5",
    "title": "Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models",
    "summary": "Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like \"overthinking\" and \"inference-time scaling.\" This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and inference-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence.",
    "authors": [
      "Qiguang Chen",
      "Libo Qin",
      "Jinhao Liu",
      "Dengyun Peng",
      "Jiannan Guan",
      "Peng Wang",
      "Mengkang Hu",
      "Yuhang Zhou",
      "Te Gao",
      "Wanxiang Che"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-12T17:35:03Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09567v5"
  },
  {
    "arxiv_id": "2503.09326v1",
    "entry_id": "http://arxiv.org/abs/2503.09326v1",
    "title": "A Survey on Enhancing Causal Reasoning Ability of Large Language Models",
    "summary": "Large language models (LLMs) have recently shown remarkable performance in language tasks and beyond. However, due to their limited inherent causal reasoning ability, LLMs still face challenges in handling tasks that require robust causal reasoning ability, such as health-care and economic analysis. As a result, a growing body of research has focused on enhancing the causal reasoning ability of LLMs. Despite the booming research, there lacks a survey to well review the challenges, progress and future directions in this area. To bridge this significant gap, we systematically review literature on how to strengthen LLMs' causal reasoning ability in this paper. We start from the introduction of background and motivations of this topic, followed by the summarisation of key challenges in this area. Thereafter, we propose a novel taxonomy to systematically categorise existing methods, together with detailed comparisons within and between classes of methods. Furthermore, we summarise existing benchmarks and evaluation metrics for assessing LLMs' causal reasoning ability. Finally, we outline future research directions for this emerging field, offering insights and inspiration to researchers and practitioners in the area.",
    "authors": [
      "Xin Li",
      "Zhuo Cai",
      "Shoujin Wang",
      "Kun Yu",
      "Fang Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-12T12:20:31Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09326v1"
  },
  {
    "arxiv_id": "2503.09311v1",
    "entry_id": "http://arxiv.org/abs/2503.09311v1",
    "title": "Adaptive political surveys and GPT-4: Tackling the cold start problem with simulated user interactions",
    "summary": "Adaptive questionnaires dynamically select the next question for a survey participant based on their previous answers. Due to digitalisation, they have become a viable alternative to traditional surveys in application areas such as political science. One limitation, however, is their dependency on data to train the model for question selection. Often, such training data (i.e., user interactions) are unavailable a priori. To address this problem, we (i) test whether Large Language Models (LLM) can accurately generate such interaction data and (ii) explore if these synthetic data can be used to pre-train the statistical model of an adaptive political survey. To evaluate this approach, we utilise existing data from the Swiss Voting Advice Application (VAA) Smartvote in two ways: First, we compare the distribution of LLM-generated synthetic data to the real distribution to assess its similarity. Second, we compare the performance of an adaptive questionnaire that is randomly initialised with one pre-trained on synthetic data to assess their suitability for training. We benchmark these results against an \"oracle\" questionnaire with perfect prior knowledge. We find that an off-the-shelf LLM (GPT-4) accurately generates answers to the Smartvote questionnaire from the perspective of different Swiss parties. Furthermore, we demonstrate that initialising the statistical model with synthetic data can (i) significantly reduce the error in predicting user responses and (ii) increase the candidate recommendation accuracy of the VAA. Our work emphasises the considerable potential of LLMs to create training data to improve the data collection process in adaptive questionnaires in LLM-affine areas such as political surveys.",
    "authors": [
      "Fynn Bachmann",
      "Daan van der Weijden",
      "Lucien Heitz",
      "Cristina Sarasua",
      "Abraham Bernstein"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-03-12T12:02:36Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09311v1"
  },
  {
    "arxiv_id": "2503.13503v3",
    "entry_id": "http://arxiv.org/abs/2503.13503v3",
    "title": "SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models",
    "summary": "In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators Knowledge, Understanding, Reasoning, Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.",
    "authors": [
      "Chuan Qin",
      "Xin Chen",
      "Chengrui Wang",
      "Pengmin Wu",
      "Xi Chen",
      "Yihang Cheng",
      "Jingyi Zhao",
      "Meng Xiao",
      "Xiangchao Dong",
      "Qingqing Long",
      "Boya Pan",
      "Han Wu",
      "Chengzan Li",
      "Yuanchun Zhou",
      "Hui Xiong",
      "Hengshu Zhu"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.DL",
      "cs.IR"
    ],
    "published": "2025-03-12T11:34:41Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13503v3"
  },
  {
    "arxiv_id": "2503.13502v1",
    "entry_id": "http://arxiv.org/abs/2503.13502v1",
    "title": "Foundation Models for Spatio-Temporal Data Science: A Tutorial and Survey",
    "summary": "Spatio-Temporal (ST) data science, which includes sensing, managing, and mining large-scale data across space and time, is fundamental to understanding complex systems in domains such as urban computing, climate science, and intelligent transportation. Traditional deep learning approaches have significantly advanced this field, particularly in the stage of ST data mining. However, these models remain task-specific and often require extensive labeled data. Inspired by the success of Foundation Models (FM), especially large language models, researchers have begun exploring the concept of Spatio-Temporal Foundation Models (STFMs) to enhance adaptability and generalization across diverse ST tasks. Unlike prior architectures, STFMs empower the entire workflow of ST data science, ranging from data sensing, management, to mining, thereby offering a more holistic and scalable approach. Despite rapid progress, a systematic study of STFMs for ST data science remains lacking. This survey aims to provide a comprehensive review of STFMs, categorizing existing methodologies and identifying key research directions to advance ST general intelligence.",
    "authors": [
      "Yuxuan Liang",
      "Haomin Wen",
      "Yutong Xia",
      "Ming Jin",
      "Bin Yang",
      "Flora Salim",
      "Qingsong Wen",
      "Shirui Pan",
      "Gao Cong"
    ],
    "categories": [
      "cs.DB",
      "cs.LG"
    ],
    "published": "2025-03-12T09:42:18Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13502v1"
  },
  {
    "arxiv_id": "2503.11701v1",
    "entry_id": "http://arxiv.org/abs/2503.11701v1",
    "title": "A Survey of Direct Preference Optimization",
    "summary": "Large Language Models (LLMs) have demonstrated unprecedented generative capabilities, yet their alignment with human values remains critical for ensuring helpful and harmless deployments. While Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful paradigm for aligning LLMs with human preferences, its reliance on complex reward modeling introduces inherent trade-offs in computational efficiency and training stability. In this context, Direct Preference Optimization (DPO) has recently gained prominence as a streamlined alternative that directly optimizes LLMs using human preferences, thereby circumventing the need for explicit reward modeling. Owing to its theoretical elegance and computational efficiency, DPO has rapidly attracted substantial research efforts exploring its various implementations and applications. However, this field currently lacks systematic organization and comparative analysis. In this survey, we conduct a comprehensive overview of DPO and introduce a novel taxonomy, categorizing previous works into four key dimensions: data strategy, learning framework, constraint mechanism, and model property. We further present a rigorous empirical analysis of DPO variants across standardized benchmarks. Additionally, we discuss real-world applications, open challenges, and future directions for DPO. This work delivers both a conceptual framework for understanding DPO and practical guidance for practitioners, aiming to advance robust and generalizable alignment paradigms. All collected resources are available and will be continuously updated at https://github.com/liushunyu/awesome-direct-preference-optimization.",
    "authors": [
      "Shunyu Liu",
      "Wenkai Fang",
      "Zetian Hu",
      "Junjie Zhang",
      "Yang Zhou",
      "Kongcheng Zhang",
      "Rongcheng Tu",
      "Ting-En Lin",
      "Fei Huang",
      "Mingli Song",
      "Yongbin Li",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-03-12T08:45:15Z",
    "pdf_url": "https://arxiv.org/pdf/2503.11701v1"
  },
  {
    "arxiv_id": "2503.09648v1",
    "entry_id": "http://arxiv.org/abs/2503.09648v1",
    "title": "A Survey on Trustworthy LLM Agents: Threats and Countermeasures",
    "summary": "With the rapid evolution of Large Language Models (LLMs), LLM-based agents and Multi-agent Systems (MAS) have significantly expanded the capabilities of LLM ecosystems. This evolution stems from empowering LLMs with additional modules such as memory, tools, environment, and even other agents. However, this advancement has also introduced more complex issues of trustworthiness, which previous research focused solely on LLMs could not cover. In this survey, we propose the TrustAgent framework, a comprehensive study on the trustworthiness of agents, characterized by modular taxonomy, multi-dimensional connotations, and technical implementation. By thoroughly investigating and summarizing newly emerged attacks, defenses, and evaluation methods for agents and MAS, we extend the concept of Trustworthy LLM to the emerging paradigm of Trustworthy Agent. In TrustAgent, we begin by deconstructing and introducing various components of the Agent and MAS. Then, we categorize their trustworthiness into intrinsic (brain, memory, and tool) and extrinsic (user, agent, and environment) aspects. Subsequently, we delineate the multifaceted meanings of trustworthiness and elaborate on the implementation techniques of existing research related to these internal and external modules. Finally, we present our insights and outlook on this domain, aiming to provide guidance for future endeavors.",
    "authors": [
      "Miao Yu",
      "Fanci Meng",
      "Xinyun Zhou",
      "Shilong Wang",
      "Junyuan Mao",
      "Linsey Pang",
      "Tianlong Chen",
      "Kun Wang",
      "Xinfeng Li",
      "Yongfeng Zhang",
      "Bo An",
      "Qingsong Wen"
    ],
    "categories": [
      "cs.MA",
      "cs.CY"
    ],
    "published": "2025-03-12T08:42:05Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09648v1"
  },
  {
    "arxiv_id": "2503.09091v2",
    "entry_id": "http://arxiv.org/abs/2503.09091v2",
    "title": "Multi-Modal Foundation Models for Computational Pathology: A Survey",
    "summary": "Foundation models have emerged as a powerful paradigm in computational pathology (CPath), enabling scalable and generalizable analysis of histopathological images. While early developments centered on uni-modal models trained solely on visual data, recent advances have highlighted the promise of multi-modal foundation models that integrate heterogeneous data sources such as textual reports, structured domain knowledge, and molecular profiles. In this survey, we provide a comprehensive and up-to-date review of multi-modal foundation models in CPath, with a particular focus on models built upon hematoxylin and eosin (H&E) stained whole slide images (WSIs) and tile-level representations. We categorize 32 state-of-the-art multi-modal foundation models into three major paradigms: vision-language, vision-knowledge graph, and vision-gene expression. We further divide vision-language models into non-LLM-based and LLM-based approaches. Additionally, we analyze 28 available multi-modal datasets tailored for pathology, grouped into image-text pairs, instruction datasets, and image-other modality pairs. Our survey also presents a taxonomy of downstream tasks, highlights training and evaluation strategies, and identifies key challenges and future directions. We aim for this survey to serve as a valuable resource for researchers and practitioners working at the intersection of pathology and AI.",
    "authors": [
      "Dong Li",
      "Guihong Wan",
      "Xintao Wu",
      "Xinyu Wu",
      "Xiaohui Chen",
      "Yi He",
      "Christine G. Lian",
      "Peter K. Sorger",
      "Yevgeniy R. Semenov",
      "Chen Zhao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-03-12T06:03:33Z",
    "pdf_url": "https://arxiv.org/pdf/2503.09091v2"
  },
  {
    "arxiv_id": "2503.16498v1",
    "entry_id": "http://arxiv.org/abs/2503.16498v1",
    "title": "Llms, Virtual Users, and Bias: Predicting Any Survey Question Without Human Data",
    "summary": "Large Language Models (LLMs) offer a promising alternative to traditional survey methods, potentially enhancing efficiency and reducing costs. In this study, we use LLMs to create virtual populations that answer survey questions, enabling us to predict outcomes comparable to human responses. We evaluate several LLMs-including GPT-4o, GPT-3.5, Claude 3.5-Sonnet, and versions of the Llama and Mistral models-comparing their performance to that of a traditional Random Forests algorithm using demographic data from the World Values Survey (WVS). LLMs demonstrate competitive performance overall, with the significant advantage of requiring no additional training data. However, they exhibit biases when predicting responses for certain religious and population groups, underperforming in these areas. On the other hand, Random Forests demonstrate stronger performance than LLMs when trained with sufficient data. We observe that removing censorship mechanisms from LLMs significantly improves predictive accuracy, particularly for underrepresented demographic segments where censored models struggle. These findings highlight the importance of addressing biases and reconsidering censorship approaches in LLMs to enhance their reliability and fairness in public opinion research.",
    "authors": [
      "Enzo Sinacola",
      "Arnault Pachot",
      "Thierry Petit"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-03-11T16:27:20Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16498v1"
  },
  {
    "arxiv_id": "2503.08569v1",
    "entry_id": "http://arxiv.org/abs/2503.08569v1",
    "title": "DeepReview: Improving LLM-based Paper Review with Human-like Deep Thinking Process",
    "summary": "Large Language Models (LLMs) are increasingly utilized in scientific research assessment, particularly in automated paper review. However, existing LLM-based review systems face significant challenges, including limited domain expertise, hallucinated reasoning, and a lack of structured evaluation. To address these limitations, we introduce DeepReview, a multi-stage framework designed to emulate expert reviewers by incorporating structured analysis, literature retrieval, and evidence-based argumentation. Using DeepReview-13K, a curated dataset with structured annotations, we train DeepReviewer-14B, which outperforms CycleReviewer-70B with fewer tokens. In its best mode, DeepReviewer-14B achieves win rates of 88.21\\% and 80.20\\% against GPT-o1 and DeepSeek-R1 in evaluations. Our work sets a new benchmark for LLM-based paper review, with all resources publicly available. The code, model, dataset and demo have be released in http://ai-researcher.net.",
    "authors": [
      "Minjun Zhu",
      "Yixuan Weng",
      "Linyi Yang",
      "Yue Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-03-11T15:59:43Z",
    "pdf_url": "https://arxiv.org/pdf/2503.08569v1"
  },
  {
    "arxiv_id": "2503.10677v2",
    "entry_id": "http://arxiv.org/abs/2503.10677v2",
    "title": "A Survey on Knowledge-Oriented Retrieval-Augmented Generation",
    "summary": "Retrieval-Augmented Generation (RAG) has gained significant attention in recent years for its potential to enhance natural language understanding and generation by combining large-scale retrieval systems with generative models. RAG leverages external knowledge sources, such as documents, databases, or structured data, to improve model performance and generate more accurate and contextually relevant outputs. This survey aims to provide a comprehensive overview of RAG by examining its fundamental components, including retrieval mechanisms, generation processes, and the integration between the two. We discuss the key characteristics of RAG, such as its ability to augment generative models with dynamic external knowledge, and the challenges associated with aligning retrieved information with generative objectives. We also present a taxonomy that categorizes RAG methods, ranging from basic retrieval-augmented approaches to more advanced models incorporating multi-modal data and reasoning capabilities. Additionally, we review the evaluation benchmarks and datasets commonly used to assess RAG systems, along with a detailed exploration of its applications in fields such as question answering, summarization, and information retrieval. Finally, we highlight emerging research directions and opportunities for improving RAG systems, such as enhanced retrieval efficiency, model interpretability, and domain-specific adaptations. This paper concludes by outlining the prospects for RAG in addressing real-world challenges and its potential to drive further advancements in natural language processing.",
    "authors": [
      "Mingyue Cheng",
      "Yucong Luo",
      "Jie Ouyang",
      "Qi Liu",
      "Huijie Liu",
      "Li Li",
      "Shuo Yu",
      "Bohou Zhang",
      "Jiawei Cao",
      "Jie Ma",
      "Daoyu Wang",
      "Enhong Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-11T01:59:35Z",
    "pdf_url": "https://arxiv.org/pdf/2503.10677v2"
  },
  {
    "arxiv_id": "2503.07556v2",
    "entry_id": "http://arxiv.org/abs/2503.07556v2",
    "title": "Novice Developers' Perspectives on Adopting LLMs for Software Development: A Systematic Literature Review",
    "summary": "Following the rise of large language models (LLMs), many studies have emerged in recent years focusing on exploring the adoption of LLM-based tools for software development by novice developers: computer science/software engineering students and early-career industry developers with two years or less of professional experience. These studies have sought to understand the perspectives of novice developers on using these tools, a critical aspect of the successful adoption of LLMs in software engineering. To systematically collect and summarise these studies, we conducted a systematic literature review (SLR) following the guidelines by Kitchenham et al. on 80 primary studies published between April 2022 and June 2025 to answer four research questions (RQs). In answering RQ1, we categorised the study motivations and methodological approaches. In RQ2, we identified the software development tasks for which novice developers use LLMs. In RQ3, we categorised the advantages, challenges, and recommendations discussed in the studies. Finally, we discuss the study limitations and future research needs suggested in the primary studies in answering RQ4. Throughout the paper, we also indicate directions for future work and implications for software engineering researchers, educators, and developers. Our research artifacts are publicly available at https://github.com/Samuellucas97/SupplementaryInfoPackage-SLR.",
    "authors": [
      "Samuel Ferino",
      "Rashina Hoda",
      "John Grundy",
      "Christoph Treude"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-03-10T17:25:24Z",
    "pdf_url": "https://arxiv.org/pdf/2503.07556v2"
  },
  {
    "arxiv_id": "2503.07545v1",
    "entry_id": "http://arxiv.org/abs/2503.07545v1",
    "title": "Queueing, Predictions, and LLMs: Challenges and Open Problems",
    "summary": "Queueing systems present many opportunities for applying machine-learning predictions, such as estimated service times, to improve system performance. This integration raises numerous open questions about how predictions can be effectively leveraged to improve scheduling decisions. Recent studies explore queues with predicted service times, typically aiming to minimize job time in the system. We review these works, highlight the effectiveness of predictions, and present open questions on queue performance. We then move to consider an important practical example of using predictions in scheduling, namely Large Language Model (LLM) systems, which presents novel scheduling challenges and highlights the potential for predictions to improve performance. In particular, we consider LLMs performing inference. Inference requests (jobs) in LLM systems are inherently complex; they have variable inference times, dynamic memory footprints that are constrained by key-value (KV) store memory limitations, and multiple possible preemption approaches that affect performance differently. We provide background on the important aspects of scheduling in LLM systems, and introduce new models and open problems that arise from them. We argue that there are significant opportunities for applying insights and analysis from queueing theory to scheduling in LLM systems.",
    "authors": [
      "Michael Mitzenmacher",
      "Rana Shahout"
    ],
    "categories": [
      "cs.AI",
      "cs.DS"
    ],
    "published": "2025-03-10T17:12:47Z",
    "pdf_url": "https://arxiv.org/pdf/2503.07545v1"
  },
  {
    "arxiv_id": "2503.07279v1",
    "entry_id": "http://arxiv.org/abs/2503.07279v1",
    "title": "VizTrust: A Visual Analytics Tool for Capturing User Trust Dynamics in Human-AI Communication",
    "summary": "Trust plays a fundamental role in shaping the willingness of users to engage and collaborate with artificial intelligence (AI) systems. Yet, measuring user trust remains challenging due to its complex and dynamic nature. While traditional survey methods provide trust levels for long conversations, they fail to capture its dynamic evolution during ongoing interactions. Here, we present VizTrust, which addresses this challenge by introducing a real-time visual analytics tool that leverages a multi-agent collaboration system to capture and analyze user trust dynamics in human-agent communication. Built on established human-computer trust scales-competence, integrity, benevolence, and predictability-, VizTrust enables stakeholders to observe trust formation as it happens, identify patterns in trust development, and pinpoint specific interaction elements that influence trust. Our tool offers actionable insights into human-agent trust formation and evolution in real time through a dashboard, supporting the design of adaptive conversational agents that responds effectively to user trust signals.",
    "authors": [
      "Xin Wang",
      "Stephanie Tulk Jesso",
      "Sadamori Kojaku",
      "David M Neyens",
      "Min Sun Kim"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-10T13:00:41Z",
    "pdf_url": "https://arxiv.org/pdf/2503.07279v1"
  },
  {
    "arxiv_id": "2503.07137v3",
    "entry_id": "http://arxiv.org/abs/2503.07137v3",
    "title": "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and Applications",
    "summary": "Artificial intelligence (AI) has achieved astonishing successes in many domains, especially with the recent breakthroughs in the development of foundational large models. These large models, leveraging their extensive training data, provide versatile solutions for a wide range of downstream tasks. However, as modern datasets become increasingly diverse and complex, the development of large AI models faces two major challenges: (1) the enormous consumption of computational resources and deployment difficulties, and (2) the difficulty in fitting heterogeneous and complex data, which limits the usability of the models. Mixture of Experts (MoE) models has recently attracted much attention in addressing these challenges, by dynamically selecting and activating the most relevant sub-models to process input data. It has been shown that MoEs can significantly improve model performance and efficiency with fewer resources, particularly excelling in handling large-scale, multimodal data. Given the tremendous potential MoE has demonstrated across various domains, it is urgent to provide a comprehensive summary of recent advancements of MoEs in many important fields. Existing surveys on MoE have their limitations, e.g., being outdated or lacking discussion on certain key areas, and we aim to address these gaps. In this paper, we first introduce the basic design of MoE, including gating functions, expert networks, routing mechanisms, training strategies, and system design. We then explore the algorithm design of MoE in important machine learning paradigms such as continual learning, meta-learning, multi-task learning, and reinforcement learning. Additionally, we summarize theoretical studies aimed at understanding MoE and review its applications in computer vision and natural language processing. Finally, we discuss promising future research directions.",
    "authors": [
      "Siyuan Mu",
      "Sen Lin"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-03-10T10:08:55Z",
    "pdf_url": "https://arxiv.org/pdf/2503.07137v3"
  },
  {
    "arxiv_id": "2504.10489v1",
    "entry_id": "http://arxiv.org/abs/2504.10489v1",
    "title": "Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning",
    "summary": "In this paper, we present Roamify, an Artificial Intelligence powered travel assistant that aims to ease the process of travel planning. We have tested and used multiple Large Language Models like Llama and T5 to generate personalised itineraries per user preferences. Results from user surveys highlight the preference for AI powered mediums over existing methods to help in travel planning across all user age groups. These results firmly validate the potential need of such a travel assistant. We highlight the two primary design considerations for travel assistance: D1) incorporating a web-scraping method to gather up-to-date news articles about destinations from various blog sources, which significantly improves our itinerary suggestions, and D2) utilising user preferences to create customised travel experiences along with a recommendation system which changes the itinerary according to the user needs. Our findings suggest that Roamify has the potential to improve and simplify how users across multiple age groups plan their travel experiences.",
    "authors": [
      "Vikranth Udandarao",
      "Noel Abraham Tiju",
      "Muthuraj Vairamuthu",
      "Harsh Mistry",
      "Dhruv Kumar"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-03-10T03:14:57Z",
    "pdf_url": "https://arxiv.org/pdf/2504.10489v1"
  },
  {
    "arxiv_id": "2503.06797v1",
    "entry_id": "http://arxiv.org/abs/2503.06797v1",
    "title": "Multimodal AI-driven Biomarker for Early Detection of Cancer Cachexia",
    "summary": "Cancer cachexia is a multifactorial syndrome characterized by progressive muscle wasting, metabolic dysfunction, and systemic inflammation, leading to reduced quality of life and increased mortality. Despite extensive research, no single definitive biomarker exists, as cachexia-related indicators such as serum biomarkers, skeletal muscle measurements, and metabolic abnormalities often overlap with other conditions. Existing composite indices, including the Cancer Cachexia Index (CXI), Modified CXI (mCXI), and Cachexia Score (CASCO), integrate multiple biomarkers but lack standardized thresholds, limiting their clinical utility. This study proposes a multimodal AI-based biomarker for early cancer cachexia detection, leveraging open-source large language models (LLMs) and foundation models trained on medical data. The approach integrates heterogeneous patient data, including demographics, disease status, lab reports, radiological imaging (CT scans), and clinical notes, using a machine learning framework that can handle missing data. Unlike previous AI-based models trained on curated datasets, this method utilizes routinely collected clinical data, enhancing real-world applicability. Additionally, the model incorporates confidence estimation, allowing the identification of cases requiring expert review for precise clinical interpretation. Preliminary findings demonstrate that integrating multiple data modalities improves cachexia prediction accuracy at the time of cancer diagnosis. The AI-based biomarker dynamically adapts to patient-specific factors such as age, race, ethnicity, weight, cancer type, and stage, avoiding the limitations of fixed-threshold biomarkers. This multimodal AI biomarker provides a scalable and clinically viable solution for early cancer cachexia detection, facilitating personalized interventions and potentially improving treatment outcomes and patient survival.",
    "authors": [
      "Sabeen Ahmed",
      "Nathan Parker",
      "Margaret Park",
      "Evan W. Davis",
      "Jennifer B. Permuth",
      "Matthew B. Schabath",
      "Yasin Yilmaz",
      "Ghulam Rasool"
    ],
    "categories": [
      "eess.IV",
      "cs.AI",
      "q-bio.QM"
    ],
    "published": "2025-03-09T22:32:37Z",
    "pdf_url": "https://arxiv.org/pdf/2503.06797v1"
  },
  {
    "arxiv_id": "2503.06497v3",
    "entry_id": "http://arxiv.org/abs/2503.06497v3",
    "title": "Evaluation of Safety Cognition Capability in Vision-Language Models for Autonomous Driving",
    "summary": "Ensuring the safety of vision-language models (VLMs) in autonomous driving systems is of paramount importance, yet existing research has largely focused on conventional benchmarks rather than safety-critical evaluation. In this work, we present SCD-Bench (Safety Cognition Driving Benchmark) a novel framework specifically designed to assess the safety cognition capabilities of VLMs within interactive driving scenarios. To address the scalability challenge of data annotation, we introduce ADA (Autonomous Driving Annotation), a semi-automated labeling system, further refined through expert review by professionals with domain-specific knowledge in autonomous driving. To facilitate scalable and consistent evaluation, we also propose an automated assessment pipeline leveraging large language models, which demonstrates over 98% agreement with human expert judgments. In addressing the broader challenge of aligning VLMs with safety cognition in driving environments, we construct SCD-Training, the first large-scale dataset tailored for this task, comprising 324.35K high-quality samples. Through extensive experiments, we show that models trained on SCD-Training exhibit marked improvements not only on SCD-Bench, but also on general and domain-specific benchmarks, offering a new perspective on enhancing safety-aware interactions in vision-language systems for autonomous driving.",
    "authors": [
      "Enming Zhang",
      "Peizhe Gong",
      "Xingyuan Dai",
      "Min Huang",
      "Yisheng Lv",
      "Qinghai Miao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-03-09T07:53:19Z",
    "pdf_url": "https://arxiv.org/pdf/2503.06497v3"
  },
  {
    "arxiv_id": "2503.06475v1",
    "entry_id": "http://arxiv.org/abs/2503.06475v1",
    "title": "SKG-LLM: Developing a Mathematical Model for Stroke Knowledge Graph Construction Using Large Language Models",
    "summary": "The purpose of this study is to introduce SKG-LLM. A knowledge graph (KG) is constructed from stroke-related articles using mathematical and large language models (LLMs). SKG-LLM extracts and organizes complex relationships from the biomedical literature, using it to increase the accuracy and depth of KG in stroke research. In the proposed method, GPT-4 was used for data pre-processing, and the extraction of embeddings was also done by GPT-4 in the whole KG construction process. The performance of the proposed model was tested with two evaluation criteria: Precision and Recall. For further validation of the proposed model, GPT-4 was used. Compared with Wikidata and WN18RR, the proposed KG-LLM approach performs better, especially in precision and recall. By including GPT-4 in the preprocessing process, the SKG-LLM model achieved a precision score of 0.906 and a recall score of 0.923. Expert reviews further improved the results and increased precision to 0.923 and recall to 0.918. The knowledge graph constructed by SKG-LLM contains 2692 nodes and 5012 edges, which are 13 distinct types of nodes and 24 types of edges.",
    "authors": [
      "Ali Sarabadani",
      "Kheirolah Rahsepar Fard",
      "Hamid Dalvand"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-09T06:25:37Z",
    "pdf_url": "https://arxiv.org/pdf/2503.06475v1"
  },
  {
    "arxiv_id": "2503.10658v1",
    "entry_id": "http://arxiv.org/abs/2503.10658v1",
    "title": "LimTopic: LLM-based Topic Modeling and Text Summarization for Analyzing Scientific Articles limitations",
    "summary": "The limitations sections of scientific articles play a crucial role in highlighting the boundaries and shortcomings of research, thereby guiding future studies and improving research methods. Analyzing these limitations benefits researchers, reviewers, funding agencies, and the broader academic community. We introduce LimTopic, a strategy where Topic generation in Limitation sections in scientific articles with Large Language Models (LLMs). Here, each topic contains the title and Topic Summary. This study focuses on effectively extracting and understanding these limitations through topic modeling and text summarization, utilizing the capabilities of LLMs. We extracted limitations from research articles and applied an LLM-based topic modeling integrated with the BERtopic approach to generate a title for each topic and Topic Sentences. To enhance comprehension and accessibility, we employed LLM-based text summarization to create concise and generalizable summaries for each topic Topic Sentences and produce a Topic Summary. Our experimentation involved prompt engineering, fine-tuning LLM and BERTopic, and integrating BERTopic with LLM to generate topics, titles, and a topic summary. We also experimented with various LLMs with BERTopic for topic modeling and various LLMs for text summarization tasks. Our results showed that the combination of BERTopic and GPT 4 performed the best in terms of silhouette and coherence scores in topic modeling, and the GPT4 summary outperformed other LLM tasks as a text summarizer.",
    "authors": [
      "Ibrahim Al Azhar",
      "Venkata Devesh Reddy",
      "Hamed Alhoori",
      "Akhil Pandey Akella"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-03-08T07:59:44Z",
    "pdf_url": "https://arxiv.org/pdf/2503.10658v1"
  },
  {
    "arxiv_id": "2503.06072v3",
    "entry_id": "http://arxiv.org/abs/2503.06072v3",
    "title": "A Survey on Post-training of Large Language Models",
    "summary": "The emergence of Large Language Models (LLMs) has fundamentally transformed natural language processing, making them indispensable across domains ranging from conversational systems to scientific exploration. However, their pre-trained architectures often reveal limitations in specialized contexts, including restricted reasoning capacities, ethical uncertainties, and suboptimal domain-specific performance. These challenges necessitate advanced post-training language models (PoLMs) to address these shortcomings, such as OpenAI-o1/o3 and DeepSeek-R1 (collectively known as Large Reasoning Models, or LRMs). This paper presents the first comprehensive survey of PoLMs, systematically tracing their evolution across five core paradigms: Fine-tuning, which enhances task-specific accuracy; Alignment, which ensures ethical coherence and alignment with human preferences; Reasoning, which advances multi-step inference despite challenges in reward design; Efficiency, which optimizes resource utilization amidst increasing complexity; Integration and Adaptation, which extend capabilities across diverse modalities while addressing coherence issues. Charting progress from ChatGPT's alignment strategies to DeepSeek-R1's innovative reasoning advancements, we illustrate how PoLMs leverage datasets to mitigate biases, deepen reasoning capabilities, and enhance domain adaptability. Our contributions include a pioneering synthesis of PoLM evolution, a structured taxonomy categorizing techniques and datasets, and a strategic agenda emphasizing the role of LRMs in improving reasoning proficiency and domain flexibility. As the first survey of its scope, this work consolidates recent PoLM advancements and establishes a rigorous intellectual framework for future research, fostering the development of LLMs that excel in precision, ethical robustness, and versatility across scientific and societal applications.",
    "authors": [
      "Guiyao Tie",
      "Zeli Zhao",
      "Dingjie Song",
      "Fuyang Wei",
      "Rong Zhou",
      "Yurou Dai",
      "Wen Yin",
      "Zhejian Yang",
      "Jiangyue Yan",
      "Yao Su",
      "Zhenhan Dai",
      "Yifeng Xie",
      "Yihan Cao",
      "Lichao Sun",
      "Pan Zhou",
      "Lifang He",
      "Hechang Chen",
      "Yu Zhang",
      "Qingsong Wen",
      "Tianming Liu",
      "Neil Zhenqiang Gong",
      "Jiliang Tang",
      "Caiming Xiong",
      "Heng Ji",
      "Philip S. Yu",
      "Jianfeng Gao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-08T05:41:42Z",
    "pdf_url": "https://arxiv.org/pdf/2503.06072v3"
  },
  {
    "arxiv_id": "2503.06054v1",
    "entry_id": "http://arxiv.org/abs/2503.06054v1",
    "title": "Fine-Grained Bias Detection in LLM: Enhancing detection mechanisms for nuanced biases",
    "summary": "Recent advancements in Artificial Intelligence, particularly in Large Language Models (LLMs), have transformed natural language processing by improving generative capabilities. However, detecting biases embedded within these models remains a challenge. Subtle biases can propagate misinformation, influence decision-making, and reinforce stereotypes, raising ethical concerns. This study presents a detection framework to identify nuanced biases in LLMs. The approach integrates contextual analysis, interpretability via attention mechanisms, and counterfactual data augmentation to capture hidden biases across linguistic contexts. The methodology employs contrastive prompts and synthetic datasets to analyze model behaviour across cultural, ideological, and demographic scenarios.\n  Quantitative analysis using benchmark datasets and qualitative assessments through expert reviews validate the effectiveness of the framework. Results show improvements in detecting subtle biases compared to conventional methods, which often fail to highlight disparities in model responses to race, gender, and socio-political contexts. The framework also identifies biases arising from imbalances in training data and model architectures. Continuous user feedback ensures adaptability and refinement. This research underscores the importance of proactive bias mitigation strategies and calls for collaboration between policymakers, AI developers, and regulators. The proposed detection mechanisms enhance model transparency and support responsible LLM deployment in sensitive applications such as education, legal systems, and healthcare. Future work will focus on real-time bias monitoring and cross-linguistic generalization to improve fairness and inclusivity in AI-driven communication tools.",
    "authors": [
      "Suvendu Mohanty"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-08T04:43:01Z",
    "pdf_url": "https://arxiv.org/pdf/2503.06054v1"
  },
  {
    "arxiv_id": "2503.05860v2",
    "entry_id": "http://arxiv.org/abs/2503.05860v2",
    "title": "Benchmarking AI Models in Software Engineering: A Review, Search Tool, and Unified Approach for Elevating Benchmark Quality",
    "summary": "Benchmarks are essential for unified evaluation and reproducibility. The rapid rise of Artificial Intelligence for Software Engineering (AI4SE) has produced numerous benchmarks for tasks such as code generation and bug repair. However, this proliferation has led to major challenges: (1) fragmented knowledge across tasks, (2) difficulty in selecting contextually relevant benchmarks, (3) lack of standardization in benchmark creation, and (4) flaws that limit utility. Addressing these requires a dual approach: systematically mapping existing benchmarks for informed selection and defining unified guidelines for robust, adaptable benchmark development.\n  We conduct a review of 247 studies, identifying 273 AI4SE benchmarks since 2014. We categorize them, analyze limitations, and expose gaps in current practices. Building on these insights, we introduce BenchScout, an extensible semantic search tool for locating suitable benchmarks. BenchScout employs automated clustering with contextual embeddings of benchmark-related studies, followed by dimensionality reduction. In a user study with 22 participants, BenchScout achieved usability, effectiveness, and intuitiveness scores of 4.5, 4.0, and 4.1 out of 5.\n  To improve benchmarking standards, we propose BenchFrame, a unified framework for enhancing benchmark quality. Applying BenchFrame to HumanEval yielded HumanEvalNext, featuring corrected errors, improved language conversion, higher test coverage, and greater difficulty. Evaluating 10 state-of-the-art code models on HumanEval, HumanEvalPlus, and HumanEvalNext revealed average pass-at-1 drops of 31.22% and 19.94%, respectively, underscoring the need for continuous benchmark refinement. We further examine BenchFrame's scalability through an agentic pipeline and confirm its generalizability on the MBPP dataset. All review data, user study materials, and enhanced benchmarks are publicly released.",
    "authors": [
      "Roham Koohestani",
      "Philippe de Bekker",
      "Begüm Koç",
      "Maliheh Izadi"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-03-07T18:44:32Z",
    "pdf_url": "https://arxiv.org/pdf/2503.05860v2"
  },
  {
    "arxiv_id": "2503.05613v3",
    "entry_id": "http://arxiv.org/abs/2503.05613v3",
    "title": "A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models",
    "summary": "Large Language Models (LLMs) have transformed natural language processing, yet their internal mechanisms remain largely opaque. Recently, mechanistic interpretability has attracted significant attention from the research community as a means to understand the inner workings of LLMs. Among various mechanistic interpretability approaches, Sparse Autoencoders (SAEs) have emerged as a promising method due to their ability to disentangle the complex, superimposed features within LLMs into more interpretable components. This paper presents a comprehensive survey of SAEs for interpreting and understanding the internal workings of LLMs. Our major contributions include: (1) exploring the technical framework of SAEs, covering basic architecture, design improvements, and effective training strategies; (2) examining different approaches to explaining SAE features, categorized into input-based and output-based explanation methods; (3) discussing evaluation methods for assessing SAE performance, covering both structural and functional metrics; and (4) investigating real-world applications of SAEs in understanding and manipulating LLM behaviors.",
    "authors": [
      "Dong Shu",
      "Xuansheng Wu",
      "Haiyan Zhao",
      "Daking Rai",
      "Ziyu Yao",
      "Ninghao Liu",
      "Mengnan Du"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-03-07T17:38:00Z",
    "pdf_url": "https://arxiv.org/pdf/2503.05613v3"
  },
  {
    "arxiv_id": "2503.10652v3",
    "entry_id": "http://arxiv.org/abs/2503.10652v3",
    "title": "Can Large Language Models Simulate Human Responses? A Case Study of Stated Preference Experiments in the Context of Heating-related Choices",
    "summary": "Stated preference (SP) surveys are a key method to research how individuals make trade-offs in hypothetical, also futuristic, scenarios. In energy context this includes key decarbonisation enablement contexts, such as low-carbon technologies, distributed renewable energy generation, and demand-side response [1,2]. However, they tend to be costly, time-consuming, and can be affected by respondent fatigue and ethical constraints. Large language models (LLMs) have demonstrated remarkable capabilities in generating human-like textual responses, prompting growing interest in their application to survey research. This study investigates the use of LLMs to simulate consumer choices in energy-related SP surveys and explores their integration into data analysis workflows. A series of test scenarios were designed to systematically assess the simulation performance of several LLMs (LLaMA 3.1, Mistral, GPT-3.5 and DeepSeek-R1) at both individual and aggregated levels, considering contexts factors such as prompt design, in-context learning (ICL), chain-of-thought (CoT) reasoning, LLM types, integration with traditional choice models, and potential biases. Cloud-based LLMs do not consistently outperform smaller local models. In this study, the reasoning model DeepSeek-R1 achieves the highest average accuracy (77%) and outperforms non-reasoning LLMs in accuracy, factor identification, and choice distribution alignment. Across models, systematic biases are observed against the gas boiler and no-retrofit options, with a preference for more energy-efficient alternatives. The findings suggest that previous SP choices are the most effective input factor, while longer prompts with additional factors and varied formats can cause LLMs to lose focus, reducing accuracy.",
    "authors": [
      "Han Wang",
      "Jacek Pawlak",
      "Aruna Sivakumar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-03-07T10:37:31Z",
    "pdf_url": "https://arxiv.org/pdf/2503.10652v3"
  },
  {
    "arxiv_id": "2503.05050v2",
    "entry_id": "http://arxiv.org/abs/2503.05050v2",
    "title": "A Unified Framework with Novel Metrics for Evaluating the Effectiveness of XAI Techniques in LLMs",
    "summary": "The increasing complexity of LLMs presents significant challenges to their transparency and interpretability, necessitating the use of eXplainable AI (XAI) techniques to enhance trustworthiness and usability. This study introduces a comprehensive evaluation framework with four novel metrics for assessing the effectiveness of five XAI techniques across five LLMs and two downstream tasks. We apply this framework to evaluate several XAI techniques LIME, SHAP, Integrated Gradients, Layer-wise Relevance Propagation (LRP), and Attention Mechanism Visualization (AMV) using the IMDB Movie Reviews and Tweet Sentiment Extraction datasets. The evaluation focuses on four key metrics: Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. Our results show that LIME consistently achieves high scores across multiple LLMs and evaluation metrics, while AMV demonstrates superior Robustness and near-perfect Consistency. LRP excels in Contrastivity, particularly with more complex models. Our findings provide valuable insights into the strengths and limitations of different XAI methods, offering guidance for developing and selecting appropriate XAI techniques for LLMs.",
    "authors": [
      "Melkamu Abay Mersha",
      "Mesay Gemeda Yigezu",
      "Hassan Shakil",
      "Ali K. AlShami",
      "Sanghyun Byun",
      "Jugal Kalita"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-03-06T23:59:50Z",
    "pdf_url": "https://arxiv.org/pdf/2503.05050v2"
  },
  {
    "arxiv_id": "2503.05012v1",
    "entry_id": "http://arxiv.org/abs/2503.05012v1",
    "title": "LLMs' Reshaping of People, Processes, Products, and Society in Software Development: A Comprehensive Exploration with Early Adopters",
    "summary": "Large language models (LLMs) like OpenAI ChatGPT, Google Gemini, and GitHub Copilot are rapidly gaining traction in the software industry, but their full impact on software engineering remains insufficiently explored. Despite their growing adoption, there is a notable lack of formal, qualitative assessments of how LLMs are applied in real-world software development contexts. To fill this gap, we conducted semi-structured interviews with sixteen early-adopter professional developers to explore their use of LLMs throughout various stages of the software development life cycle. Our investigation examines four dimensions: people - how LLMs affect individual developers and teams; process - how LLMs alter software engineering workflows; product - LLM impact on software quality and innovation; and society - the broader socioeconomic and ethical implications of LLM adoption. Thematic analysis of our data reveals that while LLMs have not fundamentally revolutionized the development process, they have substantially enhanced routine coding tasks, including code generation, refactoring, and debugging. Developers reported the most effective outcomes when providing LLMs with clear, well-defined problem statements, indicating that LLMs excel with decomposed problems and specific requirements. Furthermore, these early-adopters identified that LLMs offer significant value for personal and professional development, aiding in learning new languages and concepts. Early-adopters, highly skilled in software engineering and how LLMs work, identified early and persisting challenges for software engineering, such as inaccuracies in generated content and the need for careful manual review before integrating LLM outputs into production environments. Our study provides a nuanced understanding of how LLMs are shaping the landscape of software development, with their benefits, limitations, and ongoing implications.",
    "authors": [
      "Benyamin Tabarsi",
      "Heidi Reichert",
      "Ally Limke",
      "Sandeep Kuttal",
      "Tiffany Barnes"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-03-06T22:27:05Z",
    "pdf_url": "https://arxiv.org/pdf/2503.05012v1"
  },
  {
    "arxiv_id": "2503.04596v2",
    "entry_id": "http://arxiv.org/abs/2503.04596v2",
    "title": "LLM Applications: Current Paradigms and the Next Frontier",
    "summary": "The development of large language models (LLMs) has given rise to four major application paradigms: LLM app stores, LLM agents, self-hosted LLM services, and LLM-powered devices. Each has its advantages but also shares common challenges. LLM app stores lower the barrier to development but lead to platform lock-in; LLM agents provide autonomy but lack a unified communication mechanism; self-hosted LLM services enhance control but increase deployment complexity; and LLM-powered devices improve privacy and real-time performance but are limited by hardware. This paper reviews and analyzes these paradigms, covering architecture design, application ecosystem, research progress, as well as the challenges and open problems they face. Based on this, we outline the next frontier of LLM applications, characterizing them through three interconnected layers: infrastructure, protocol, and application. We describe their responsibilities and roles of each layer and demonstrate how to mitigate existing fragmentation limitations and improve security and scalability. Finally, we discuss key future challenges, identify opportunities such as protocol-driven cross-platform collaboration and device integration, and propose a research roadmap for openness, security, and sustainability.",
    "authors": [
      "Xinyi Hou",
      "Yanjie Zhao",
      "Haoyu Wang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-03-06T16:38:23Z",
    "pdf_url": "https://arxiv.org/pdf/2503.04596v2"
  },
  {
    "arxiv_id": "2503.04543v1",
    "entry_id": "http://arxiv.org/abs/2503.04543v1",
    "title": "Keeping Yourself is Important in Downstream Tuning Multimodal Large Language Model",
    "summary": "Multi-modal Large Language Models (MLLMs) integrate visual and linguistic reasoning to address complex tasks such as image captioning and visual question answering. While MLLMs demonstrate remarkable versatility, MLLMs appears limited performance on special applications. But tuning MLLMs for downstream tasks encounters two key challenges: Task-Expert Specialization, where distribution shifts between pre-training and target datasets constrain target performance, and Open-World Stabilization, where catastrophic forgetting erases the model general knowledge. In this work, we systematically review recent advancements in MLLM tuning methodologies, classifying them into three paradigms: (I) Selective Tuning, (II) Additive Tuning, and (III) Reparameterization Tuning. Furthermore, we benchmark these tuning strategies across popular MLLM architectures and diverse downstream tasks to establish standardized evaluation analysis and systematic tuning principles. Finally, we highlight several open challenges in this domain and propose future research directions. To facilitate ongoing progress in this rapidly evolving field, we provide a public repository that continuously tracks developments: https://github.com/WenkeHuang/Awesome-MLLM-Tuning.",
    "authors": [
      "Wenke Huang",
      "Jian Liang",
      "Xianda Guo",
      "Yiyang Fang",
      "Guancheng Wan",
      "Xuankun Rong",
      "Chi Wen",
      "Zekun Shi",
      "Qingyun Li",
      "Didi Zhu",
      "Yanbiao Ma",
      "Ke Liang",
      "Bin Yang",
      "He Li",
      "Jiawei Shao",
      "Mang Ye",
      "Bo Du"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-06T15:29:13Z",
    "pdf_url": "https://arxiv.org/pdf/2503.04543v1"
  },
  {
    "arxiv_id": "2503.04110v2",
    "entry_id": "http://arxiv.org/abs/2503.04110v2",
    "title": "InterChat: Enhancing Generative Visual Analytics using Multimodal Interactions",
    "summary": "The rise of Large Language Models (LLMs) and generative visual analytics systems has transformed data-driven insights, yet significant challenges persist in accurately interpreting users' analytical and interaction intents. While language inputs offer flexibility, they often lack precision, making the expression of complex intents inefficient, error-prone, and time-intensive. To address these limitations, we investigate the design space of multimodal interactions for generative visual analytics through a literature review and pilot brainstorming sessions. Building on these insights, we introduce a highly extensible workflow that integrates multiple LLM agents for intent inference and visualization generation. We develop InterChat, a generative visual analytics system that combines direct manipulation of visual elements with natural language inputs. This integration enables precise intent communication and supports progressive, visually driven exploratory data analyses. By employing effective prompt engineering, and contextual interaction linking, alongside intuitive visualization and interaction designs, InterChat bridges the gap between user interactions and LLM-driven visualizations, enhancing both interpretability and usability. Extensive evaluations, including two usage scenarios, a user study, and expert feedback, demonstrate the effectiveness of InterChat. Results show significant improvements in the accuracy and efficiency of handling complex visual analytics tasks, highlighting the potential of multimodal interactions to redefine user engagement and analytical depth in generative visual analytics.",
    "authors": [
      "Juntong Chen",
      "Jiang Wu",
      "Jiajing Guo",
      "Vikram Mohanty",
      "Xueming Li",
      "Jorge Piazentin Ono",
      "Wenbin He",
      "Liu Ren",
      "Dongyu Liu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-03-06T05:35:19Z",
    "pdf_url": "https://arxiv.org/pdf/2503.04110v2"
  },
  {
    "arxiv_id": "2503.03262v3",
    "entry_id": "http://arxiv.org/abs/2503.03262v3",
    "title": "Trajectory Prediction for Autonomous Driving: Progress, Limitations, and Future Directions",
    "summary": "As the potential for autonomous vehicles to be integrated on a large scale into modern traffic systems continues to grow, ensuring safe navigation in dynamic environments is crucial for smooth integration. To guarantee safety and prevent collisions, autonomous vehicles must be capable of accurately predicting the trajectories of surrounding traffic agents. Over the past decade, significant efforts from both academia and industry have been dedicated to designing solutions for precise trajectory forecasting. These efforts have produced a diverse range of approaches, raising questions about the differences between these methods and whether trajectory prediction challenges have been fully addressed. This paper reviews a substantial portion of recent trajectory prediction methods proposing a taxonomy to classify existing solutions. A general overview of the prediction pipeline is also provided, covering input and output modalities, modeling features, and prediction paradigms existing in the literature. In addition, the paper discusses active research areas within trajectory prediction, addresses the posed research questions, and highlights the remaining research gaps and challenges.",
    "authors": [
      "Nadya Abdel Madjid",
      "Abdulrahman Ahmad",
      "Murad Mebrahtu",
      "Yousef Babaa",
      "Abdelmoamen Nasser",
      "Sumbal Malik",
      "Bilal Hassan",
      "Naoufel Werghi",
      "Jorge Dias",
      "Majid Khonji"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-03-05T08:38:51Z",
    "pdf_url": "https://arxiv.org/pdf/2503.03262v3"
  },
  {
    "arxiv_id": "2503.02776v1",
    "entry_id": "http://arxiv.org/abs/2503.02776v1",
    "title": "Implicit Bias in LLMs: A Survey",
    "summary": "Due to the implement of guardrails by developers, Large language models (LLMs) have demonstrated exceptional performance in explicit bias tests. However, bias in LLMs may occur not only explicitly, but also implicitly, much like humans who consciously strive for impartiality yet still harbor implicit bias. The unconscious and automatic nature of implicit bias makes it particularly challenging to study. This paper provides a comprehensive review of the existing literature on implicit bias in LLMs. We begin by introducing key concepts, theories and methods related to implicit bias in psychology, extending them from humans to LLMs. Drawing on the Implicit Association Test (IAT) and other psychological frameworks, we categorize detection methods into three primary approaches: word association, task-oriented text generation and decision-making. We divide our taxonomy of evaluation metrics for implicit bias into two categories: single-value-based metrics and comparison-value-based metrics. We classify datasets into two types: sentences with masked tokens and complete sentences, incorporating datasets from various domains to reflect the broad application of LLMs. Although research on mitigating implicit bias in LLMs is still limited, we summarize existing efforts and offer insights on future challenges. We aim for this work to serve as a clear guide for researchers and inspire innovative ideas to advance exploration in this task.",
    "authors": [
      "Xinru Lin",
      "Luyang Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-03-04T16:49:37Z",
    "pdf_url": "https://arxiv.org/pdf/2503.02776v1"
  },
  {
    "arxiv_id": "2503.05815v1",
    "entry_id": "http://arxiv.org/abs/2503.05815v1",
    "title": "Trust, Experience, and Innovation: Key Factors Shaping American Attitudes About AI",
    "summary": "A large survey of American adults explored the complex landscape of attitudes towards artificial intelligence (AI). It explored the degree of concern regarding specific potential outcomes of the new advances in AI technology and correlates of these concerns. Key variables associated with the direction and intensity of concern include prior experience using a large language model such as ChatGPT, general trust in science, adherence to the precautionary principle versus support for unrestricted innovation, and demographic factors such as gender. By analyzing these relationships, the paper provides valuable insights into the American public's response to AI that are particularly important in the development of policy to regulate or further encourage its development.",
    "authors": [
      "Risa Palm",
      "Justin Kingsland",
      "Toby Bolsen"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN"
    ],
    "published": "2025-03-04T16:08:20Z",
    "pdf_url": "https://arxiv.org/pdf/2503.05815v1"
  },
  {
    "arxiv_id": "2503.02702v2",
    "entry_id": "http://arxiv.org/abs/2503.02702v2",
    "title": "RedChronos: A Large Language Model-Based Log Analysis System for Insider Threat Detection in Enterprises",
    "summary": "Internal threat detection (IDT) aims to address security threats within organizations or enterprises by identifying potential or already occurring malicious threats within vast amounts of logs. Although organizations or enterprises have dedicated personnel responsible for reviewing these logs, it is impossible to manually examine all logs entirely.In response to the vast number of logs, we propose a system called RedChronos, which is a Large Language Model-Based Log Analysis System. This system incorporates innovative improvements over previous research by employing Query-Aware Weighted Voting and a Semantic Expansion-based Genetic Algorithm with LLM-driven Mutations. On the public datasets CERT 4.2 and 5.2, RedChronos outperforms or matches existing approaches in terms of accuracy, precision, and detection rate. Moreover, RedChronos reduces the need for manual intervention in security log reviews by approximately 90% in the Xiaohongshu Security Operation Center. Therefore, our RedChronos system demonstrates exceptional performance in handling IDT tasks, providing innovative solutions for these challenges. We believe that future research can continue to enhance the system's performance in IDT tasks while also reducing the response time to internal risk events.",
    "authors": [
      "Chenyu Li",
      "Zhengjia Zhu",
      "Jiyan He",
      "Xiu Zhang"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-03-04T15:18:40Z",
    "pdf_url": "https://arxiv.org/pdf/2503.02702v2"
  },
  {
    "arxiv_id": "2503.02631v2",
    "entry_id": "http://arxiv.org/abs/2503.02631v2",
    "title": "Reflection on Data Storytelling Tools in the Generative AI Era from the Human-AI Collaboration Perspective",
    "summary": "Human-AI collaborative tools attract attentions from the data storytelling community to lower the expertise barrier and streamline the workflow. The recent advance in large-scale generative AI techniques, e.g., large language models (LLMs) and text-to-image models, has the potential to enhance data storytelling with their power in visual and narration generation. After two years since these techniques were publicly available, it is important to reflect our progress of applying them and have an outlook for future opportunities. To achieve the goal, we compare the collaboration patterns of the latest tools with those of earlier ones using a dedicated framework for understanding human-AI collaboration in data storytelling. Through comparison, we identify consistently widely studied patterns, e.g., human-creator + AI-assistant, and newly explored or emerging ones, e.g., AI-creator + human-reviewer. The benefits of these AI techniques and implications to human-AI collaboration are also revealed. We further propose future directions to hopefully ignite innovations.",
    "authors": [
      "Haotian Li",
      "Yun Wang",
      "Huamin Qu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-03-04T13:56:18Z",
    "pdf_url": "https://arxiv.org/pdf/2503.02631v2"
  },
  {
    "arxiv_id": "2503.02144v1",
    "entry_id": "http://arxiv.org/abs/2503.02144v1",
    "title": "Malware Classification from Memory Dumps Using Machine Learning, Transformers, and Large Language Models",
    "summary": "This study investigates the performance of various classification models for a malware classification task using different feature sets and data configurations. Six models-Logistic Regression, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), Decision Trees, Random Forest (RF), and Extreme Gradient Boosting (XGB)-were evaluated alongside two deep learning models, Recurrent Neural Networks (RNN) and Transformers, as well as the Gemini zero-shot and few-shot learning methods. Four feature sets were tested including All Features, Literature Review Features, the Top 45 Features from RF, and Down-Sampled with Top 45 Features. XGB achieved the highest accuracy of 87.42% using the Top 45 Features, outperforming all other models. RF followed closely with 87.23% accuracy on the same feature set. In contrast, deep learning models underperformed, with RNN achieving 66.71% accuracy and Transformers reaching 71.59%. Down-sampling reduced performance across all models, with XGB dropping to 81.31%. Gemini zero-shot and few-shot learning approaches showed the lowest performance, with accuracies of 40.65% and 48.65%, respectively. The results highlight the importance of feature selection in improving model performance while reducing computational complexity. Traditional models like XGB and RF demonstrated superior performance, while deep learning and few-shot methods struggled to match their accuracy. This study underscores the effectiveness of traditional machine learning models for structured datasets and provides a foundation for future research into hybrid approaches and larger datasets.",
    "authors": [
      "Areej Dweib",
      "Montaser Tanina",
      "Shehab Alawi",
      "Mohammad Dyab",
      "Huthaifa I. Ashqar"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CR"
    ],
    "published": "2025-03-04T00:24:21Z",
    "pdf_url": "https://arxiv.org/pdf/2503.02144v1"
  },
  {
    "arxiv_id": "2503.02104v1",
    "entry_id": "http://arxiv.org/abs/2503.02104v1",
    "title": "Biomedical Foundation Model: A Survey",
    "summary": "Foundation models, first introduced in 2021, are large-scale pre-trained models (e.g., large language models (LLMs) and vision-language models (VLMs)) that learn from extensive unlabeled datasets through unsupervised methods, enabling them to excel in diverse downstream tasks. These models, like GPT, can be adapted to various applications such as question answering and visual understanding, outperforming task-specific AI models and earning their name due to broad applicability across fields. The development of biomedical foundation models marks a significant milestone in leveraging artificial intelligence (AI) to understand complex biological phenomena and advance medical research and practice. This survey explores the potential of foundation models across diverse domains within biomedical fields, including computational biology, drug discovery and development, clinical informatics, medical imaging, and public health. The purpose of this survey is to inspire ongoing research in the application of foundation models to health science.",
    "authors": [
      "Xiangrui Liu",
      "Yuanyuan Zhang",
      "Yingzhou Lu",
      "Changchang Yin",
      "Xiaoling Hu",
      "Xiaoou Liu",
      "Lulu Chen",
      "Sheng Wang",
      "Alexander Rodriguez",
      "Huaxiu Yao",
      "Yezhou Yang",
      "Ping Zhang",
      "Jintai Chen",
      "Tianfan Fu",
      "Xiao Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-03-03T22:42:00Z",
    "pdf_url": "https://arxiv.org/pdf/2503.02104v1"
  },
  {
    "arxiv_id": "2503.02068v1",
    "entry_id": "http://arxiv.org/abs/2503.02068v1",
    "title": "Interactive Debugging and Steering of Multi-Agent AI Systems",
    "summary": "Fully autonomous teams of LLM-powered AI agents are emerging that collaborate to perform complex tasks for users. What challenges do developers face when trying to build and debug these AI agent teams? In formative interviews with five AI agent developers, we identify core challenges: difficulty reviewing long agent conversations to localize errors, lack of support in current tools for interactive debugging, and the need for tool support to iterate on agent configuration. Based on these needs, we developed an interactive multi-agent debugging tool, AGDebugger, with a UI for browsing and sending messages, the ability to edit and reset prior agent messages, and an overview visualization for navigating complex message histories. In a two-part user study with 14 participants, we identify common user strategies for steering agents and highlight the importance of interactive message resets for debugging. Our studies deepen understanding of interfaces for debugging increasingly important agentic workflows.",
    "authors": [
      "Will Epperson",
      "Gagan Bansal",
      "Victor Dibia",
      "Adam Fourney",
      "Jack Gerrits",
      "Erkang Zhu",
      "Saleema Amershi"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-03-03T21:42:54Z",
    "pdf_url": "https://arxiv.org/pdf/2503.02068v1"
  },
  {
    "arxiv_id": "2503.01245v2",
    "entry_id": "http://arxiv.org/abs/2503.01245v2",
    "title": "Large Language Models for Code Generation: A Comprehensive Survey of Challenges, Techniques, Evaluation, and Applications",
    "summary": "Large Language Models (LLMs) have demonstrated their remarkable capabilities in numerous fields. This survey focuses on how LLMs empower users, regardless of their technical background, to use human languages to automatically generate executable code. We begin with understanding LLMs' limitations and challenges in automated code generation. Subsequently, we review various fine-tuning techniques designed to enhance both the performance and adaptability of LLMs in code generation tasks. We then review the existing metrics and benchmarks for evaluations to assess model performance based on fine-tuning techniques. Finally, we explore the applications of LLMs (e.g. CodeLlama, GitHub Copilot, ToolGen) in code generation tasks to illustrate their roles and functionalities. This survey provides a comprehensive overview of LLMs for code generation, helps researchers in diverse fields better understand the current state-of-the-art technologies, and offers the potential of effectively leveraging LLMs for code generation tasks.",
    "authors": [
      "Nam Huynh",
      "Beiyu Lin"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2025-03-03T07:17:30Z",
    "pdf_url": "https://arxiv.org/pdf/2503.01245v2"
  },
  {
    "arxiv_id": "2503.00863v1",
    "entry_id": "http://arxiv.org/abs/2503.00863v1",
    "title": "Systematic Literature Review on Clinical Trial Eligibility Matching",
    "summary": "Clinical trial eligibility matching is a critical yet often labor-intensive and error-prone step in medical research, as it ensures that participants meet precise criteria for safe and reliable study outcomes. Recent advances in Natural Language Processing (NLP) have shown promise in automating and improving this process by rapidly analyzing large volumes of unstructured clinical text and structured electronic health record (EHR) data. In this paper, we present a systematic overview of current NLP methodologies applied to clinical trial eligibility screening, focusing on data sources, annotation practices, machine learning approaches, and real-world implementation challenges. A comprehensive literature search (spanning Google Scholar, Mendeley, and PubMed from 2015 to 2024) yielded high-quality studies, each demonstrating the potential of techniques such as rule-based systems, named entity recognition, contextual embeddings, and ontology-based normalization to enhance patient matching accuracy. While results indicate substantial improvements in screening efficiency and precision, limitations persist regarding data completeness, annotation consistency, and model scalability across diverse clinical domains. The review highlights how explainable AI and standardized ontologies can bolster clinician trust and broaden adoption. Looking ahead, further research into advanced semantic and temporal representations, expanded data integration, and rigorous prospective evaluations is necessary to fully realize the transformative potential of NLP in clinical trial recruitment.",
    "authors": [
      "Muhammad Talha Sharif",
      "Abdul Rehman"
    ],
    "categories": [
      "cs.LG",
      "cs.IR"
    ],
    "published": "2025-03-02T11:45:50Z",
    "pdf_url": "https://arxiv.org/pdf/2503.00863v1"
  },
  {
    "arxiv_id": "2503.00673v1",
    "entry_id": "http://arxiv.org/abs/2503.00673v1",
    "title": "Towards Refining Developer Questions using LLM-Based Named Entity Recognition for Developer Chatroom Conversations",
    "summary": "In software engineering chatrooms, communication is often hindered by imprecise questions that cannot be answered. Recognizing key entities can be essential for improving question clarity and facilitating better exchange. However, existing research using natural language processing techniques often overlooks these software-specific nuances. In this paper, we introduce Software-specific Named Entity Recognition, Intent Detection, and Resolution Classification (SENIR), a labeling approach that leverages a Large Language Model to annotate entities, intents, and resolution status in developer chatroom conversations. To offer quantitative guidance for improving question clarity and resolvability, we build a resolution prediction model that leverages SENIR's entity and intent labels along with additional predictive features. We evaluate SENIR on the DISCO dataset using a subset of annotated chatroom dialogues. SENIR achieves an 86% F-score for entity recognition, a 71% F-score for intent detection, and an 89% F-score for resolution status classification. Furthermore, our resolution prediction model, tested with various sampling strategies (random undersampling and oversampling with SMOTE) and evaluation methods (5-fold cross-validation, 10-fold cross-validation, and bootstrapping), demonstrates AUC values ranging from 0.7 to 0.8. Key factors influencing resolution include positive sentiment and entities such as Programming Language and User Variable across multiple intents, while diagnostic entities are more relevant in error-related questions. Moreover, resolution rates vary significantly by intent: questions about API Usage and API Change achieve higher resolution rates, whereas Discrepancy and Review have lower resolution rates. A Chi-Square analysis confirms the statistical significance of these differences.",
    "authors": [
      "Pouya Fathollahzadeh",
      "Mariam El Mezouar",
      "Hao Li",
      "Ying Zou",
      "Ahmed E. Hassan"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2025-03-02T00:20:24Z",
    "pdf_url": "https://arxiv.org/pdf/2503.00673v1"
  },
  {
    "arxiv_id": "2503.00096v3",
    "entry_id": "http://arxiv.org/abs/2503.00096v3",
    "title": "BixBench: a Comprehensive Benchmark for LLM-based Agents in Computational Biology",
    "summary": "Large Language Models (LLMs) and LLM-based agents show great promise in accelerating scientific research. Existing benchmarks for measuring this potential and guiding future development continue to evolve from pure recall and rote knowledge tasks, towards more practical work such as literature review and experimental planning. Bioinformatics is a domain where fully autonomous AI-driven discovery may be near, but no extensive benchmarks for measuring progress have been introduced to date. We therefore present the Bioinformatics Benchmark (BixBench), a dataset comprising over 50 real-world scenarios of practical biological data analysis with nearly 300 associated open-answer questions designed to measure the ability of LLM-based agents to explore biological datasets, perform long, multi-step analytical trajectories, and interpret the nuanced results of those analyses. We evaluate the performance of two frontier LLMs (GPT-4o and Claude 3.5 Sonnet) using a custom agent framework we open source. We find that even the latest frontier models only achieve 17% accuracy in the open-answer regime, and no better than random in a multiple-choice setting. By exposing the current limitations of frontier models, we hope BixBench can spur the development of agents capable of conducting rigorous bioinformatic analysis and accelerate scientific discovery.",
    "authors": [
      "Ludovico Mitchener",
      "Jon M Laurent",
      "Alex Andonian",
      "Benjamin Tenmann",
      "Siddharth Narayanan",
      "Geemi P Wellawatte",
      "Andrew White",
      "Lorenzo Sani",
      "Samuel G Rodriques"
    ],
    "categories": [
      "q-bio.QM",
      "cs.AI"
    ],
    "published": "2025-02-28T18:47:57Z",
    "pdf_url": "https://arxiv.org/pdf/2503.00096v3"
  },
  {
    "arxiv_id": "2502.20988v2",
    "entry_id": "http://arxiv.org/abs/2502.20988v2",
    "title": "Reviewing Clinical Knowledge in Medical Large Language Models: Training and Beyond",
    "summary": "The large-scale development of large language models (LLMs) in medical contexts, such as diagnostic assistance and treatment recommendations, necessitates that these models possess accurate medical knowledge and deliver traceable decision-making processes. Clinical knowledge, encompassing the insights gained from research on the causes, prognosis, diagnosis, and treatment of diseases, has been extensively examined within real-world medical practices. Recently, there has been a notable increase in research efforts aimed at integrating this type of knowledge into LLMs, encompassing not only traditional text and multimodal data integration but also technologies such as knowledge graphs (KGs) and retrieval-augmented generation (RAG). In this paper, we review the various initiatives to embed clinical knowledge into training-based, KG-supported, and RAG-assisted LLMs. We begin by gathering reliable knowledge sources from the medical domain, including databases and datasets. Next, we evaluate implementations for integrating clinical knowledge through specialized datasets and collaborations with external knowledge sources such as KGs and relevant documentation. Furthermore, we discuss the applications of the developed medical LLMs in the industrial sector to assess the disparity between models developed in academic settings and those in industry. We conclude the survey by presenting evaluation systems applicable to relevant tasks and identifying potential challenges facing this field. In this review, we do not aim for completeness, since any ostensibly complete review would soon be outdated. Our goal is to illustrate diversity by selecting representative and accessible items from current research and industry practices, reflecting real-world situations rather than claiming completeness. Thus, we emphasize showcasing diverse approaches.",
    "authors": [
      "Qiyuan Li",
      "Haijiang Liu",
      "Caicai Guo",
      "Chao Gao",
      "Deyu Chen",
      "Meng Wang",
      "Feng Gao",
      "Frank van Harmelen",
      "Jinguang Gu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-28T12:00:51Z",
    "pdf_url": "https://arxiv.org/pdf/2502.20988v2"
  },
  {
    "arxiv_id": "2503.13467v2",
    "entry_id": "http://arxiv.org/abs/2503.13467v2",
    "title": "How Metacognitive Architectures Remember Their Own Thoughts: A Systematic Review",
    "summary": "Background: Metacognition has gained significant attention for its potential to enhance autonomy and adaptability of artificial agents but remains a fragmented field: diverse theories, terminologies, and design choices have led to disjointed developments and limited comparability across systems. Existing overviews remain at a conceptual level that is undiscerning to the underlying algorithms, representations, and their respective success.\n  Methods: We address this gap by performing an explorative systematic review. Reports were included if they described techniques enabling Computational Metacognitive Architectures (CMAs) to model, store, remember, and process their episodic metacognitive experiences, one of Flavell's (1979a) three foundational components of metacognition. Searches were conducted in 16 databases, consulted between December 2023 and June 2024. Data were extracted using a 20-item framework considering pertinent aspects.\n  Results: A total of 101 reports on 35 distinct CMAs were included. Our findings show that metacognitive experiences may boost system performance and explainability, e.g., via self-repair. However, lack of standardization and limited evaluations may hinder progress: only 17% of CMAs were quantitatively evaluated regarding this review's focus, and significant terminological inconsistency limits cross-architecture synthesis. Systems also varied widely in memory content, data types, and employed algorithms.\n  Discussion: Limitations include the non-iterative nature of the search query, heterogeneous data availability, and an under-representation of emergent, sub-symbolic CMAs. Future research should focus on standardization and evaluation, e.g., via community-driven challenges, and on transferring promising principles to emergent architectures.",
    "authors": [
      "Robin Nolte",
      "Mihai Pomarlan",
      "Ayden Janssen",
      "Daniel Beßler",
      "Kamyar Javanmardi",
      "Sascha Jongebloed",
      "Robert Porzel",
      "John Bateman",
      "Michael Beetz",
      "Rainer Malaka"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI"
    ],
    "published": "2025-02-28T08:48:41Z",
    "pdf_url": "https://arxiv.org/pdf/2503.13467v2"
  },
  {
    "arxiv_id": "2502.20689v2",
    "entry_id": "http://arxiv.org/abs/2502.20689v2",
    "title": "WiseMind: Recontextualizing AI with a Knowledge-Guided, Theory-Informed Multi-Agent Framework for Instrumental and Humanistic Benefits",
    "summary": "Translating state-of-the-art NLP into practice often stalls at the \"last mile\" owing to insufficient contextualization of the target domain's knowledge, processes, and evaluation. Psychiatric differential diagnosis exemplifies this challenge: accurate assessments depend on nuanced clinical knowledge, a delicate cognitive-affective interview process, and downstream outcomes that extend far beyond benchmark accuracy. We present WiseMind, a systematic interdisciplinary contextualization framework that delivers both instrumental (diagnostic precision) and humanistic (empathy) gains. WiseMind comprises three components:(i) structured knowledge-guided proactive reasoning, which embeds DSM-5 criteria in a knowledge graph to steer questioning; (ii) a theory-informed dual-agent architecture that coordinates a \"reasonable-mind\" reasoning agent and an \"emotional-mind\" empathy agent, inspired by Dialectical Behavior Therapy; and (iii) a multi-faceted evaluation strategy covering simulated patients, user studies, clinician review, and ethical assessment. Tested on depression, anxiety, and bipolar disorder, WiseMind attains up to 84.2% diagnostic accuracy, which is comparable to human experts, while outperforming single-agent baselines in perceived empathy and trustworthiness. These results show that deep contextualization-across knowledge, process, and evaluation layers-can transform benchmark-driven NLP into clinically meaningful impact.",
    "authors": [
      "Yuqi Wu",
      "Guangya Wan",
      "Jingjing Li",
      "Shengming Zhao",
      "Lingfeng Ma",
      "Tianyi Ye",
      "Ion Pop",
      "Yanbo Zhang",
      "Jie Chen"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-28T03:45:39Z",
    "pdf_url": "https://arxiv.org/pdf/2502.20689v2"
  },
  {
    "arxiv_id": "2502.20682v1",
    "entry_id": "http://arxiv.org/abs/2502.20682v1",
    "title": "Fine-tuning BERT with Bidirectional LSTM for Fine-grained Movie Reviews Sentiment Analysis",
    "summary": "Sentiment Analysis (SA) is instrumental in understanding peoples viewpoints facilitating social media monitoring recognizing products and brands and gauging customer satisfaction. Consequently SA has evolved into an active research domain within Natural Language Processing (NLP). Many approaches outlined in the literature devise intricate frameworks aimed at achieving high accuracy, focusing exclusively on either binary sentiment classification or fine-grained sentiment classification. In this paper our objective is to fine-tune the pre-trained BERT model with Bidirectional LSTM (BiLSTM) to enhance both binary and fine-grained SA specifically for movie reviews. Our approach involves conducting sentiment classification for each review followed by computing the overall sentiment polarity across all reviews. We present our findings on binary classification as well as fine-grained classification utilizing benchmark datasets. Additionally we implement and assess two accuracy improvement techniques Synthetic Minority Oversampling Technique (SMOTE) and NLP Augmenter (NLPAUG) to bolster the models generalization in fine-grained sentiment classification. Finally a heuristic algorithm is employed to calculate the overall polarity of predicted reviews from the BERT+BiLSTM output vector. Our approach performs comparably with state-of-the-art (SOTA) techniques in both classifications. For instance in binary classification we achieve 97.67% accuracy surpassing the leading SOTA model NB-weighted-BON+dv-cosine by 0.27% on the renowned IMDb dataset. Conversely for five-class classification on SST-5 while the top SOTA model RoBERTa+large+Self-explaining attains 55.5% accuracy our model achieves 59.48% accuracy surpassing the BERT-large baseline by 3.6%.",
    "authors": [
      "Gibson Nkhata",
      "Susan Gauch",
      "Usman Anjum",
      "Justin Zhan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-28T03:30:48Z",
    "pdf_url": "https://arxiv.org/pdf/2502.20682v1"
  },
  {
    "arxiv_id": "2503.05788v2",
    "entry_id": "http://arxiv.org/abs/2503.05788v2",
    "title": "Emergent Abilities in Large Language Models: A Survey",
    "summary": "Large Language Models (LLMs) are leading a new technological revolution as one of the most promising research streams toward artificial general intelligence. The scaling of these models, accomplished by increasing the number of parameters and the magnitude of the training datasets, has been linked to various so-called emergent abilities that were previously unobserved. These emergent abilities, ranging from advanced reasoning and in-context learning to coding and problem-solving, have sparked an intense scientific debate: Are they truly emergent, or do they simply depend on external factors, such as training dynamics, the type of problems, or the chosen metric? What underlying mechanism causes them? Despite their transformative potential, emergent abilities remain poorly understood, leading to misconceptions about their definition, nature, predictability, and implications. In this work, we shed light on emergent abilities by conducting a comprehensive review of the phenomenon, addressing both its scientific underpinnings and real-world consequences. We first critically analyze existing definitions, exposing inconsistencies in conceptualizing emergent abilities. We then explore the conditions under which these abilities appear, evaluating the role of scaling laws, task complexity, pre-training loss, quantization, and prompting strategies. Our review extends beyond traditional LLMs and includes Large Reasoning Models (LRMs), which leverage reinforcement learning and inference-time search to amplify reasoning and self-reflection. However, emergence is not inherently positive. As AI systems gain autonomous reasoning capabilities, they also develop harmful behaviors, including deception, manipulation, and reward hacking. We highlight growing concerns about safety and governance, emphasizing the need for better evaluation frameworks and regulatory oversight.",
    "authors": [
      "Leonardo Berti",
      "Flavio Giorgi",
      "Gjergji Kasneci"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-28T01:20:01Z",
    "pdf_url": "https://arxiv.org/pdf/2503.05788v2"
  },
  {
    "arxiv_id": "2502.20541v1",
    "entry_id": "http://arxiv.org/abs/2502.20541v1",
    "title": "NANOGPT: A Query-Driven Large Language Model Retrieval-Augmented Generation System for Nanotechnology Research",
    "summary": "This paper presents the development and application of a Large Language Model Retrieval-Augmented Generation (LLM-RAG) system tailored for nanotechnology research. The system leverages the capabilities of a sophisticated language model to serve as an intelligent research assistant, enhancing the efficiency and comprehensiveness of literature reviews in the nanotechnology domain. Central to this LLM-RAG system is its advanced query backend retrieval mechanism, which integrates data from multiple reputable sources. The system retrieves relevant literature by utilizing Google Scholar's advanced search, and scraping open-access papers from Elsevier, Springer Nature, and ACS Publications. This multifaceted approach ensures a broad and diverse collection of up-to-date scholarly articles and papers. The proposed system demonstrates significant potential in aiding researchers by providing a streamlined, accurate, and exhaustive literature retrieval process, thereby accelerating research advancements in nanotechnology. The effectiveness of the LLM-RAG system is validated through rigorous testing, illustrating its capability to significantly reduce the time and effort required for comprehensive literature reviews, while maintaining high accuracy, query relevance and outperforming standard, publicly available LLMS.",
    "authors": [
      "Achuth Chandrasekhar",
      "Omid Barati Farimani",
      "Olabode T. Ajenifujah",
      "Janghoon Ock",
      "Amir Barati Farimani"
    ],
    "categories": [
      "cs.CL",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2025-02-27T21:40:22Z",
    "pdf_url": "https://arxiv.org/pdf/2502.20541v1"
  },
  {
    "arxiv_id": "2503.01887v1",
    "entry_id": "http://arxiv.org/abs/2503.01887v1",
    "title": "When Continue Learning Meets Multimodal Large Language Model: A Survey",
    "summary": "Recent advancements in Artificial Intelligence have led to the development of Multimodal Large Language Models (MLLMs). However, adapting these pre-trained models to dynamic data distributions and various tasks efficiently remains a challenge. Fine-tuning MLLMs for specific tasks often causes performance degradation in the model's prior knowledge domain, a problem known as 'Catastrophic Forgetting'. While this issue has been well-studied in the Continual Learning (CL) community, it presents new challenges for MLLMs. This review paper, the first of its kind in MLLM continual learning, presents an overview and analysis of 440 research papers in this area.The review is structured into four sections. First, it discusses the latest research on MLLMs, covering model innovations, benchmarks, and applications in various fields. Second, it categorizes and overviews the latest studies on continual learning, divided into three parts: non-large language models unimodal continual learning (Non-LLM Unimodal CL), non-large language models multimodal continual learning (Non-LLM Multimodal CL), and continual learning in large language models (CL in LLM). The third section provides a detailed analysis of the current state of MLLM continual learning research, including benchmark evaluations, architectural innovations, and a summary of theoretical and empirical studies.Finally, the paper discusses the challenges and future directions of continual learning in MLLMs, aiming to inspire future research and development in the field. This review connects the foundational concepts, theoretical insights, method innovations, and practical applications of continual learning for multimodal large models, providing a comprehensive understanding of the research progress and challenges in this field, aiming to inspire researchers in the field and promote the advancement of related technologies.",
    "authors": [
      "Yukang Huo",
      "Hao Tang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-27T03:39:10Z",
    "pdf_url": "https://arxiv.org/pdf/2503.01887v1"
  },
  {
    "arxiv_id": "2502.19649v5",
    "entry_id": "http://arxiv.org/abs/2502.19649v5",
    "title": "Taxonomy, Opportunities, and Challenges of Representation Engineering for Large Language Models",
    "summary": "Representation Engineering (RepE) is a novel paradigm for controlling the behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune the model, RepE directly manipulates the model's internal representations. As a result, it may offer more effective, interpretable, data-efficient, and flexible control over models' behavior. We present the first comprehensive survey of RepE for LLMs, reviewing the rapidly growing literature to address key questions: What RepE methods exist and how do they differ? For what concepts and problems has RepE been applied? What are the strengths and weaknesses of RepE compared to other methods? To answer these, we propose a unified framework describing RepE as a pipeline comprising representation identification, operationalization, and control. We posit that while RepE methods offer significant potential, challenges remain, including managing multiple concepts, ensuring reliability, and preserving models' performance. Towards improving RepE, we identify opportunities for experimental and methodological improvements and construct a guide for best practices.",
    "authors": [
      "Jan Wehner",
      "Sahar Abdelnabi",
      "Daniel Tan",
      "David Krueger",
      "Mario Fritz"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-02-27T00:40:01Z",
    "pdf_url": "https://arxiv.org/pdf/2502.19649v5"
  },
  {
    "arxiv_id": "2502.19614v2",
    "entry_id": "http://arxiv.org/abs/2502.19614v2",
    "title": "Is Your Paper Being Reviewed by an LLM? Benchmarking AI Text Detection in Peer Review",
    "summary": "Peer review is a critical process for ensuring the integrity of published scientific research. Confidence in this process is predicated on the assumption that experts in the relevant domain give careful consideration to the merits of manuscripts which are submitted for publication. With the recent rapid advancements in large language models (LLMs), a new risk to the peer review process is that negligent reviewers will rely on LLMs to perform the often time consuming process of reviewing a paper. However, there is a lack of existing resources for benchmarking the detectability of AI text in the domain of peer review. To address this deficiency, we introduce a comprehensive dataset containing a total of 788,984 AI-written peer reviews paired with corresponding human reviews, covering 8 years of papers submitted to each of two leading AI research conferences (ICLR and NeurIPS). We use this new resource to evaluate the ability of 18 existing AI text detection algorithms to distinguish between peer reviews fully written by humans and different state-of-the-art LLMs. Additionally, we explore a context-aware detection method called Anchor, which leverages manuscript content to detect AI-generated reviews, and analyze the sensitivity of detection models to LLM-assisted editing of human-written text. Our work reveals the difficulty of identifying AI-generated text at the individual peer review level, highlighting the urgent need for new tools and methods to detect this unethical use of generative AI. Our dataset is publicly available at: https://huggingface.co/datasets/IntelLabs/AI-Peer-Review-Detection-Benchmark.",
    "authors": [
      "Sungduk Yu",
      "Man Luo",
      "Avinash Madusu",
      "Vasudev Lal",
      "Phillip Howard"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-26T23:04:05Z",
    "pdf_url": "https://arxiv.org/pdf/2502.19614v2"
  },
  {
    "arxiv_id": "2502.19411v1",
    "entry_id": "http://arxiv.org/abs/2502.19411v1",
    "title": "Code to Think, Think to Code: A Survey on Code-Enhanced Reasoning and Reasoning-Driven Code Intelligence in LLMs",
    "summary": "In large language models (LLMs), code and reasoning reinforce each other: code offers an abstract, modular, and logic-driven structure that supports reasoning, while reasoning translates high-level goals into smaller, executable steps that drive more advanced code intelligence. In this study, we examine how code serves as a structured medium for enhancing reasoning: it provides verifiable execution paths, enforces logical decomposition, and enables runtime validation. We also explore how improvements in reasoning have transformed code intelligence from basic completion to advanced capabilities, enabling models to address complex software engineering tasks through planning and debugging. Finally, we identify key challenges and propose future research directions to strengthen this synergy, ultimately improving LLM's performance in both areas.",
    "authors": [
      "Dayu Yang",
      "Tianyang Liu",
      "Daoan Zhang",
      "Antoine Simoulin",
      "Xiaoyi Liu",
      "Yuwei Cao",
      "Zhaopu Teng",
      "Xin Qian",
      "Grey Yang",
      "Jiebo Luo",
      "Julian McAuley"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.SE"
    ],
    "published": "2025-02-26T18:55:42Z",
    "pdf_url": "https://arxiv.org/pdf/2502.19411v1"
  },
  {
    "arxiv_id": "2502.19312v1",
    "entry_id": "http://arxiv.org/abs/2502.19312v1",
    "title": "FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users",
    "summary": "Effective personalization of LLMs is critical for a broad range of user-interfacing applications such as virtual assistants and content curation. Inspired by the strong in-context learning capabilities of LLMs, we propose Few-Shot Preference Optimization (FSPO), which reframes reward modeling as a meta-learning problem. Under this framework, an LLM learns to quickly adapt to a user via a few labeled preferences from that user, constructing a personalized reward function for them. Additionally, since real-world preference data is scarce and challenging to collect at scale, we propose careful design choices to construct synthetic preference datasets for personalization, generating over 1M synthetic personalized preferences using publicly available LLMs. In particular, to successfully transfer from synthetic data to real users, we find it crucial for the data to exhibit both high diversity and coherent, self-consistent structure. We evaluate FSPO on personalized open-ended generation for up to 1,500 synthetic users across across three domains: movie reviews, pedagogical adaptation based on educational background, and general question answering, along with a controlled human study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average in generating responses that are personalized to synthetic users and a 72% winrate with real human users in open-ended question answering.",
    "authors": [
      "Anikait Singh",
      "Sheryl Hsu",
      "Kyle Hsu",
      "Eric Mitchell",
      "Stefano Ermon",
      "Tatsunori Hashimoto",
      "Archit Sharma",
      "Chelsea Finn"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC",
      "stat.ML"
    ],
    "published": "2025-02-26T17:08:46Z",
    "pdf_url": "https://arxiv.org/pdf/2502.19312v1"
  },
  {
    "arxiv_id": "2502.19008v1",
    "entry_id": "http://arxiv.org/abs/2502.19008v1",
    "title": "Binary Neural Networks for Large Language Model: A Survey",
    "summary": "Large language models (LLMs) have wide applications in the field of natural language processing(NLP), such as GPT-4 and Llama. However, with the exponential growth of model parameter sizes, LLMs bring significant resource overheads. Low-bit quantization, as a key technique, reduces memory usage and computational demands by decreasing the bit-width of model parameters, activations, and gradients. Previous quantization methods for LLMs have largely employed Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). PTQ does not require any retraining of the original model, while QAT involves optimizing precision during training to achieve the best quantization parameters. The BitNet team proposed a radically different approach, where quantization is performed from the start of model training, utilizing low-precision binary weights during the training process. This approach has led to the emergence of many binary quantization techniques for large language models. This paper provides a comprehensive review of these binary quantization techniques. Specifically, we will introduce binary quantization techniques in deep neural networks and further explore their application to LLMs, reviewing their various contributions, implementations, and applications.",
    "authors": [
      "Liangdong Liu",
      "Zhitong Zheng",
      "Cong Wang",
      "Tianhuang Su",
      "Zhenyu Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-26T10:14:19Z",
    "pdf_url": "https://arxiv.org/pdf/2502.19008v1"
  },
  {
    "arxiv_id": "2502.18791v3",
    "entry_id": "http://arxiv.org/abs/2502.18791v3",
    "title": "Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs",
    "summary": "The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEvalDB. We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches. We validate LLMEvalDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding & multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through LLMEvalDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.",
    "authors": [
      "Jungsoo Park",
      "Junmo Kang",
      "Gabriel Stanovsky",
      "Alan Ritter"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-26T03:56:34Z",
    "pdf_url": "https://arxiv.org/pdf/2502.18791v3"
  },
  {
    "arxiv_id": "2503.01870v1",
    "entry_id": "http://arxiv.org/abs/2503.01870v1",
    "title": "Can Large Language Models Extract Customer Needs as well as Professional Analysts?",
    "summary": "Identifying customer needs (CNs) is important for product management, product development, and marketing. Applications rely on professional analysts interpreting textual data (e.g., interview transcripts, online reviews) to understand the nuances of customer experience and concisely formulate \"jobs to be done.\" The task is cognitively complex and time-consuming. Current practice facilitates the process with keyword search and machine learning but relies on human judgment to formulate CNs. We examine whether Large Language Models (LLMs) can automatically extract CNs. Because evaluating CNs requires professional judgment, we partnered with a marketing consulting firm to conduct a blind study of CNs extracted by: (1) a foundational LLM with prompt engineering only (Base LLM), (2) an LLM fine-tuned with professionally identified CNs (SFT LLM), and (3) professional analysts. The SFT LLM performs as well as or better than professional analysts when extracting CNs. The extracted CNs are well-formulated, sufficiently specific to identify opportunities, and justified by source content (no hallucinations). The SFT LLM is efficient and provides more complete coverage of CNs. The Base LLM was not sufficiently accurate or specific. Organizations can rely on SFT LLMs to reduce manual effort, enhance the precision of CN articulation, and provide improved insight for innovation and marketing strategy.",
    "authors": [
      "Artem Timoshenko",
      "Chengfeng Mao",
      "John R. Hauser"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "econ.GN"
    ],
    "published": "2025-02-25T21:55:35Z",
    "pdf_url": "https://arxiv.org/pdf/2503.01870v1"
  },
  {
    "arxiv_id": "2503.01869v2",
    "entry_id": "http://arxiv.org/abs/2503.01869v2",
    "title": "From Small to Large Language Models: Revisiting the Federalist Papers",
    "summary": "For a long time, the authorship of the Federalist Papers had been a subject of inquiry and debate, not only by linguists and historians but also by statisticians. In what was arguably the first Bayesian case study, Mosteller and Wallace (1963) provided the first statistical evidence for attributing all disputed papers to Madison. Our paper revisits this historical dataset but from a lens of modern language models, both small and large. We review some of the more popular Large Language Model (LLM) tools and examine them from a statistical point of view in the context of text classification. We investigate whether, without any attempt to fine-tune, the general embedding constructs can be useful for stylometry and attribution. We explain differences between various word/phrase embeddings and discuss how to aggregate them in a document. Contrary to our expectations, we exemplify that dimension expansion with word embeddings may not always be beneficial for attribution relative to dimension reduction with topic embeddings. Our experiments demonstrate that default LLM embeddings (even after manual fine-tuning) may not consistently improve authorship attribution accuracy. Instead, Bayesian analysis with topic embeddings trained on ``function words\" yields superior out-of-sample classification performance. This suggests that traditional (small) statistical language models, with their interpretability and solid theoretical foundation, can offer significant advantages in authorship attribution tasks. The code used in this analysis is available at github.com/sowonjeong/slm-to-llm",
    "authors": [
      "So Won Jeong",
      "Veronika Ročková"
    ],
    "categories": [
      "cs.CL",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-02-25T21:50:46Z",
    "pdf_url": "https://arxiv.org/pdf/2503.01869v2"
  },
  {
    "arxiv_id": "2502.18357v1",
    "entry_id": "http://arxiv.org/abs/2502.18357v1",
    "title": "Which Contributions Deserve Credit? Perceptions of Attribution in Human-AI Co-Creation",
    "summary": "AI systems powered by large language models can act as capable assistants for writing and editing. In these tasks, the AI system acts as a co-creative partner, making novel contributions to an artifact-under-creation alongside its human partner(s). One question that arises in these scenarios is the extent to which AI should be credited for its contributions. We examined knowledge workers' views of attribution through a survey study (N=155) and found that they assigned different levels of credit across different contribution types, amounts, and initiative. Compared to a human partner, we observed a consistent pattern in which AI was assigned less credit for equivalent contributions. Participants felt that disclosing AI involvement was important and used a variety of criteria to make attribution judgments, including the quality of contributions, personal values, and technology considerations. Our results motivate and inform new approaches for crediting AI contributions to co-created work.",
    "authors": [
      "Jessica He",
      "Stephanie Houde",
      "Justin D. Weisz"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-02-25T16:48:10Z",
    "pdf_url": "https://arxiv.org/pdf/2502.18357v1"
  },
  {
    "arxiv_id": "2502.17773v3",
    "entry_id": "http://arxiv.org/abs/2502.17773v3",
    "title": "Uncertainty Quantification for LLM-Based Survey Simulations",
    "summary": "We investigate the use of large language models (LLMs) to simulate human responses to survey questions, and perform uncertainty quantification to gain reliable insights. Our approach converts imperfect LLM-simulated responses into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.",
    "authors": [
      "Chengpiao Huang",
      "Yuhang Wu",
      "Kaizheng Wang"
    ],
    "categories": [
      "stat.ME",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-25T02:07:29Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17773v3"
  },
  {
    "arxiv_id": "2503.01863v1",
    "entry_id": "http://arxiv.org/abs/2503.01863v1",
    "title": "Vision Language Models in Medicine",
    "summary": "With the advent of Vision-Language Models (VLMs), medical artificial intelligence (AI) has experienced significant technological progress and paradigm shifts. This survey provides an extensive review of recent advancements in Medical Vision-Language Models (Med-VLMs), which integrate visual and textual data to enhance healthcare outcomes. We discuss the foundational technology behind Med-VLMs, illustrating how general models are adapted for complex medical tasks, and examine their applications in healthcare. The transformative impact of Med-VLMs on clinical practice, education, and patient care is highlighted, alongside challenges such as data scarcity, narrow task generalization, interpretability issues, and ethical concerns like fairness, accountability, and privacy. These limitations are exacerbated by uneven dataset distribution, computational demands, and regulatory hurdles. Rigorous evaluation methods and robust regulatory frameworks are essential for safe integration into healthcare workflows. Future directions include leveraging large-scale, diverse datasets, improving cross-modal generalization, and enhancing interpretability. Innovations like federated learning, lightweight architectures, and Electronic Health Record (EHR) integration are explored as pathways to democratize access and improve clinical relevance. This review aims to provide a comprehensive understanding of Med-VLMs' strengths and limitations, fostering their ethical and balanced adoption in healthcare.",
    "authors": [
      "Beria Chingnabe Kalpelbe",
      "Angel Gabriel Adaambiik",
      "Wei Peng"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "eess.IV"
    ],
    "published": "2025-02-24T22:53:22Z",
    "pdf_url": "https://arxiv.org/pdf/2503.01863v1"
  },
  {
    "arxiv_id": "2502.17701v2",
    "entry_id": "http://arxiv.org/abs/2502.17701v2",
    "title": "From Perceptions to Decisions: Wildfire Evacuation Decision Prediction with Behavioral Theory-informed LLMs",
    "summary": "Evacuation decision prediction is critical for efficient and effective wildfire response by helping emergency management anticipate traffic congestion and bottlenecks, allocate resources, and minimize negative impacts. Traditional statistical methods for evacuation decision prediction fail to capture the complex and diverse behavioral logic of different individuals. In this work, for the first time, we introduce FLARE, short for facilitating LLM for advanced reasoning on wildfire evacuation decision prediction, a Large Language Model (LLM)-based framework that integrates behavioral theories and models to streamline the Chain-of-Thought (CoT) reasoning and subsequently integrate with memory-based Reinforcement Learning (RL) module to provide accurate evacuation decision prediction and understanding. Our proposed method addresses the limitations of using existing LLMs for evacuation behavioral predictions, such as limited survey data, mismatching with behavioral theory, conflicting individual preferences, implicit and complex mental states, and intractable mental state-behavior mapping. Experiments on three post-wildfire survey datasets show an average of 20.47% performance improvement over traditional theory-informed behavioral models, with strong cross-event generalizability. Our complete code is publicly available at https://github.com/SusuXu-s-Lab/FLARE",
    "authors": [
      "Ruxiao Chen",
      "Chenguang Wang",
      "Yuran Sun",
      "Xilei Zhao",
      "Susu Xu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-02-24T22:47:33Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17701v2"
  },
  {
    "arxiv_id": "2502.17601v1",
    "entry_id": "http://arxiv.org/abs/2502.17601v1",
    "title": "Representation Engineering for Large-Language Models: Survey and Research Challenges",
    "summary": "Large-language models are capable of completing a variety of tasks, but remain unpredictable and intractable. Representation engineering seeks to resolve this problem through a new approach utilizing samples of contrasting inputs to detect and edit high-level representations of concepts such as honesty, harmfulness or power-seeking. We formalize the goals and methods of representation engineering to present a cohesive picture of work in this emerging discipline. We compare it with alternative approaches, such as mechanistic interpretability, prompt-engineering and fine-tuning. We outline risks such as performance decrease, compute time increases and steerability issues. We present a clear agenda for future research to build predictable, dynamic, safe and personalizable LLMs.",
    "authors": [
      "Lukasz Bartoszcze",
      "Sarthak Munshi",
      "Bryan Sukidi",
      "Jennifer Yen",
      "Zejia Yang",
      "David Williams-King",
      "Linh Le",
      "Kosi Asuzu",
      "Carsten Maple"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-02-24T19:36:26Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17601v1"
  },
  {
    "arxiv_id": "2502.17425v1",
    "entry_id": "http://arxiv.org/abs/2502.17425v1",
    "title": "Introducing Visual Perception Token into Multimodal Large Language Model",
    "summary": "To utilize visual information, Multimodal Large Language Model (MLLM) relies on the perception process of its vision encoder. The completeness and accuracy of visual perception significantly influence the precision of spatial reasoning, fine-grained understanding, and other tasks. However, MLLM still lacks the autonomous capability to control its own visual perception processes, for example, selectively reviewing specific regions of an image or focusing on information related to specific object categories. In this work, we propose the concept of Visual Perception Token, aiming to empower MLLM with a mechanism to control its visual perception processes. We design two types of Visual Perception Tokens, termed the Region Selection Token and the Vision Re-Encoding Token. MLLMs autonomously generate these tokens, just as they generate text, and use them to trigger additional visual perception actions. The Region Selection Token explicitly identifies specific regions in an image that require further perception, while the Vision Re-Encoding Token uses its hidden states as control signals to guide additional visual perception processes. Extensive experiments demonstrate the advantages of these tokens in handling spatial reasoning, improving fine-grained understanding, and other tasks. On average, the introduction of Visual Perception Tokens improves the performance of a 2B model by 23.6\\%, increasing its score from 0.572 to 0.708, and even outperforms a 7B parameter model by 13.4\\% (from 0.624). Please check out our repo https://github.com/yu-rp/VisualPerceptionToken",
    "authors": [
      "Runpeng Yu",
      "Xinyin Ma",
      "Xinchao Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-02-24T18:56:12Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17425v1"
  },
  {
    "arxiv_id": "2502.17419v6",
    "entry_id": "http://arxiv.org/abs/2502.17419v6",
    "title": "From System 1 to System 2: A Survey of Reasoning Large Language Models",
    "summary": "Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.",
    "authors": [
      "Zhong-Zhi Li",
      "Duzhen Zhang",
      "Ming-Liang Zhang",
      "Jiaxin Zhang",
      "Zengyan Liu",
      "Yuxuan Yao",
      "Haotian Xu",
      "Junhao Zheng",
      "Pei-Jie Wang",
      "Xiuyi Chen",
      "Yingying Zhang",
      "Fei Yin",
      "Jiahua Dong",
      "Zhiwei Li",
      "Bao-Long Bi",
      "Ling-Rui Mei",
      "Junfeng Fang",
      "Xiao Liang",
      "Zhijiang Guo",
      "Le Song",
      "Cheng-Lin Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-02-24T18:50:52Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17419v6"
  },
  {
    "arxiv_id": "2502.17307v2",
    "entry_id": "http://arxiv.org/abs/2502.17307v2",
    "title": "Survey on Strategic Mining in Blockchain: A Reinforcement Learning Approach",
    "summary": "Strategic mining attacks, such as selfish mining, exploit blockchain consensus protocols by deviating from honest behavior to maximize rewards. Markov Decision Process (MDP) analysis faces scalability challenges in modern digital economics, including blockchain. To address these limitations, reinforcement learning (RL) provides a scalable alternative, enabling adaptive strategy optimization in complex dynamic environments.\n  In this survey, we examine RL's role in strategic mining analysis, comparing it to MDP-based approaches. We begin by reviewing foundational MDP models and their limitations, before exploring RL frameworks that can learn near-optimal strategies across various protocols. Building on this analysis, we compare RL techniques and their effectiveness in deriving security thresholds, such as the minimum attacker power required for profitable attacks. Expanding the discussion further, we classify consensus protocols and propose open challenges, such as multi-agent dynamics and real-world validation.\n  This survey highlights the potential of reinforcement learning (RL) to address the challenges of selfish mining, including protocol design, threat detection, and security analysis, while offering a strategic roadmap for researchers in decentralized systems and AI-driven analytics.",
    "authors": [
      "Jichen Li",
      "Lijia Xie",
      "Hanting Huang",
      "Bo Zhou",
      "Binfeng Song",
      "Wanying Zeng",
      "Xiaotie Deng",
      "Xiao Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.GT",
      "cs.MA"
    ],
    "published": "2025-02-24T16:42:51Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17307v2"
  },
  {
    "arxiv_id": "2502.17535v1",
    "entry_id": "http://arxiv.org/abs/2502.17535v1",
    "title": "The Lottery LLM Hypothesis, Rethinking What Abilities Should LLM Compression Preserve?",
    "summary": "Motivated by reducing the computational and storage costs of LLMs, model compression and KV cache compression have attracted much attention from researchers. However, current methods predominantly emphasize maintaining the performance of compressed LLMs, as measured by perplexity or simple accuracy on tasks of common sense knowledge QA and basic arithmetic reasoning. In this blog, we present a brief review of recent advancements in LLMs related to retrieval-augmented generation, multi-step reasoning, external tools, and computational expressivity, all of which substantially enhance LLM performance. Then, we propose a lottery LLM hypothesis suggesting that for a given LLM and task, there exists a smaller lottery LLM capable of producing the same performance as the original LLM with the assistance of multi-step reasoning and external tools. Based on the review of current progress in LLMs, we discuss and summarize the essential capabilities that the lottery LLM and KV cache compression must possess, which are currently overlooked in existing methods.",
    "authors": [
      "Zhenheng Tang",
      "Xiang Liu",
      "Qian Wang",
      "Peijie Dong",
      "Bingsheng He",
      "Xiaowen Chu",
      "Bo Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.FL"
    ],
    "published": "2025-02-24T15:39:35Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17535v1"
  },
  {
    "arxiv_id": "2502.17100v3",
    "entry_id": "http://arxiv.org/abs/2502.17100v3",
    "title": "Generative Models in Decision Making: A Survey",
    "summary": "In recent years, the exceptional performance of generative models in generative tasks has sparked significant interest in their integration into decision-making processes. Due to their ability to handle complex data distributions and their strong model capacity, generative models can be effectively incorporated into decision-making systems by generating trajectories that guide agents toward high-reward state-action regions or intermediate sub-goals. This paper presents a comprehensive review of the application of generative models in decision-making tasks. We classify seven fundamental types of generative models: energy-based models, generative adversarial networks, variational autoencoders, normalizing flows, diffusion models, generative flow networks, and autoregressive models. Regarding their applications, we categorize their functions into three main roles: controllers, modelers and optimizers, and discuss how each role contributes to decision-making. Furthermore, we examine the deployment of these models across five critical real-world decision-making scenarios. Finally, we summarize the strengths and limitations of current approaches and propose three key directions for advancing next-generation generative directive models: high-performance algorithms, large-scale generalized decision-making models, and self-evolving and adaptive models.",
    "authors": [
      "Yinchuan Li",
      "Xinyu Shao",
      "Jianping Zhang",
      "Haozhi Wang",
      "Leo Maxime Brunswic",
      "Kaiwen Zhou",
      "Jiqian Dong",
      "Kaiyang Guo",
      "Xiu Li",
      "Zhitang Chen",
      "Jun Wang",
      "Jianye Hao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-24T12:31:28Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17100v3"
  },
  {
    "arxiv_id": "2502.16923v2",
    "entry_id": "http://arxiv.org/abs/2502.16923v2",
    "title": "A Systematic Survey of Automatic Prompt Optimization Techniques",
    "summary": "Since the advent of large language models (LLMs), prompt engineering has been a crucial step for eliciting desired responses for various Natural Language Processing (NLP) tasks. However, prompt engineering remains an impediment for end users due to rapid advances in models, tasks, and associated best practices. To mitigate this, Automatic Prompt Optimization (APO) techniques have recently emerged that use various automated techniques to help improve the performance of LLMs on various tasks. In this paper, we present a comprehensive survey summarizing the current progress and remaining challenges in this field. We provide a formal definition of APO, a 5-part unifying framework, and then proceed to rigorously categorize all relevant works based on their salient features therein. We hope to spur further research guided by our framework.",
    "authors": [
      "Kiran Ramnath",
      "Kang Zhou",
      "Sheng Guan",
      "Soumya Smruti Mishra",
      "Xuan Qi",
      "Zhengyuan Shen",
      "Shuai Wang",
      "Sangmin Woo",
      "Sullam Jeoung",
      "Yawei Wang",
      "Haozhu Wang",
      "Han Ding",
      "Yuzhe Lu",
      "Zhichao Xu",
      "Yun Zhou",
      "Balasubramaniam Srinivasan",
      "Qiaojing Yan",
      "Yueyan Chen",
      "Haibo Ding",
      "Panpan Xu",
      "Lin Lee Cheong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-24T07:29:13Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16923v2"
  },
  {
    "arxiv_id": "2502.16902v2",
    "entry_id": "http://arxiv.org/abs/2502.16902v2",
    "title": "Culture-TRIP: Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement",
    "summary": "Text-to-Image models, including Stable Diffusion, have significantly improved in generating images that are highly semantically aligned with the given prompts. However, existing models may fail to produce appropriate images for the cultural concepts or objects that are not well known or underrepresented in western cultures, such as `hangari' (Korean utensil). In this paper, we propose a novel approach, Culturally-Aware Text-to-Image Generation with Iterative Prompt Refinement (Culture-TRIP), which refines the prompt in order to improve the alignment of the image with such culture nouns in text-to-image models. Our approach (1) retrieves cultural contexts and visual details related to the culture nouns in the prompt and (2) iteratively refines and evaluates the prompt based on a set of cultural criteria and large language models. The refinement process utilizes the information retrieved from Wikipedia and the Web. Our user survey, conducted with 66 participants from eight different countries demonstrates that our proposed approach enhances the alignment between the images and the prompts. In particular, C-TRIP demonstrates improved alignment between the generated images and underrepresented culture nouns. Resource can be found at https://shane3606.github.io/Culture-TRIP.",
    "authors": [
      "Suchae Jeong",
      "Inseong Choi",
      "Youngsik Yun",
      "Jihie Kim"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-02-24T06:56:56Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16902v2"
  },
  {
    "arxiv_id": "2502.16868v1",
    "entry_id": "http://arxiv.org/abs/2502.16868v1",
    "title": "Graphy'our Data: Towards End-to-End Modeling, Exploring and Generating Report from Raw Data",
    "summary": "Large Language Models (LLMs) have recently demonstrated remarkable performance in tasks such as Retrieval-Augmented Generation (RAG) and autonomous AI agent workflows. Yet, when faced with large sets of unstructured documents requiring progressive exploration, analysis, and synthesis, such as conducting literature survey, existing approaches often fall short. We address this challenge -- termed Progressive Document Investigation -- by introducing Graphy, an end-to-end platform that automates data modeling, exploration and high-quality report generation in a user-friendly manner. Graphy comprises an offline Scrapper that transforms raw documents into a structured graph of Fact and Dimension nodes, and an online Surveyor that enables iterative exploration and LLM-driven report generation. We showcase a pre-scrapped graph of over 50,000 papers -- complete with their references -- demonstrating how Graphy facilitates the literature-survey scenario. The demonstration video can be found at https://youtu.be/uM4nzkAdGlM.",
    "authors": [
      "Longbin Lai",
      "Changwei Luo",
      "Yunkai Lou",
      "Mingchen Ju",
      "Zhengyi Yang"
    ],
    "categories": [
      "cs.DB",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-02-24T06:10:49Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16868v1"
  },
  {
    "arxiv_id": "2502.16866v1",
    "entry_id": "http://arxiv.org/abs/2502.16866v1",
    "title": "Toward Agentic AI: Generative Information Retrieval Inspired Intelligent Communications and Networking",
    "summary": "The increasing complexity and scale of modern telecommunications networks demand intelligent automation to enhance efficiency, adaptability, and resilience. Agentic AI has emerged as a key paradigm for intelligent communications and networking, enabling AI-driven agents to perceive, reason, decide, and act within dynamic networking environments. However, effective decision-making in telecom applications, such as network planning, management, and resource allocation, requires integrating retrieval mechanisms that support multi-hop reasoning, historical cross-referencing, and compliance with evolving 3GPP standards. This article presents a forward-looking perspective on generative information retrieval-inspired intelligent communications and networking, emphasizing the role of knowledge acquisition, processing, and retrieval in agentic AI for telecom systems. We first provide a comprehensive review of generative information retrieval strategies, including traditional retrieval, hybrid retrieval, semantic retrieval, knowledge-based retrieval, and agentic contextual retrieval. We then analyze their advantages, limitations, and suitability for various networking scenarios. Next, we present a survey about their applications in communications and networking. Additionally, we introduce an agentic contextual retrieval framework to enhance telecom-specific planning by integrating multi-source retrieval, structured reasoning, and self-reflective validation. Experimental results demonstrate that our framework significantly improves answer accuracy, explanation consistency, and retrieval efficiency compared to traditional and semantic retrieval methods. Finally, we outline future research directions.",
    "authors": [
      "Ruichen Zhang",
      "Shunpu Tang",
      "Yinqiu Liu",
      "Dusit Niyato",
      "Zehui Xiong",
      "Sumei Sun",
      "Shiwen Mao",
      "Zhu Han"
    ],
    "categories": [
      "cs.NI",
      "cs.AI"
    ],
    "published": "2025-02-24T06:02:25Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16866v1"
  },
  {
    "arxiv_id": "2502.16804v2",
    "entry_id": "http://arxiv.org/abs/2502.16804v2",
    "title": "Multi-Agent Autonomous Driving Systems with Large Language Models: A Survey of Recent Advances",
    "summary": "Autonomous Driving Systems (ADSs) are revolutionizing transportation by reducing human intervention, improving operational efficiency, and enhancing safety. Large Language Models (LLMs) have been integrated into ADSs to support high-level decision-making through their powerful reasoning, instruction-following, and communication abilities. However, LLM-based single-agent ADSs face three major challenges: limited perception, insufficient collaboration, and high computational demands. To address these issues, recent advances in LLM-based multi-agent ADSs leverage language-driven communication and coordination to enhance inter-agent collaboration. This paper provides a frontier survey of this emerging intersection between NLP and multi-agent ADSs. We begin with a background introduction to related concepts, followed by a categorization of existing LLM-based methods based on different agent interaction modes. We then discuss agent-human interactions in scenarios where LLM-based agents engage with humans. Finally, we summarize key applications, datasets, and challenges to support future research.",
    "authors": [
      "Yaozu Wu",
      "Dongyuan Li",
      "Yankai Chen",
      "Renhe Jiang",
      "Henry Peng Zou",
      "Wei-Chieh Huang",
      "Yangning Li",
      "Liancheng Fang",
      "Zhen Wang",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-02-24T03:26:13Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16804v2"
  },
  {
    "arxiv_id": "2502.16534v2",
    "entry_id": "http://arxiv.org/abs/2502.16534v2",
    "title": "Multilingual != Multicultural: Evaluating Gaps Between Multilingual Capabilities and Cultural Alignment in LLMs",
    "summary": "Large Language Models (LLMs) are becoming increasingly capable across global languages. However, the ability to communicate across languages does not necessarily translate to appropriate cultural representations. A key concern is US-centric bias, where LLMs reflect US rather than local cultural values. We propose a novel methodology that compares LLM-generated response distributions against population-level opinion data from the World Value Survey across four languages (Danish, Dutch, English, and Portuguese). Using a rigorous linear mixed-effects regression framework, we compare two families of models: Google's Gemma models (2B--27B parameters) and successive iterations of OpenAI's turbo-series. Across the families of models, we find no consistent relationships between language capabilities and cultural alignment. While the Gemma models have a positive correlation between language capability and cultural alignment across languages, the OpenAI models do not. Importantly, we find that self-consistency is a stronger predictor of multicultural alignment than multilingual capabilities. Our results demonstrate that achieving meaningful cultural alignment requires dedicated effort beyond improving general language capabilities.",
    "authors": [
      "Jonathan Rystrøm",
      "Hannah Rose Kirk",
      "Scott Hale"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-02-23T11:02:41Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16534v2"
  },
  {
    "arxiv_id": "2502.17521v2",
    "entry_id": "http://arxiv.org/abs/2502.17521v2",
    "title": "Recent Advances in Large Langauge Model Benchmarks against Data Contamination: From Static to Dynamic Evaluation",
    "summary": "Data contamination has received increasing attention in the era of large language models (LLMs) due to their reliance on vast Internet-derived training corpora. To mitigate the risk of potential data contamination, LLM benchmarking has undergone a transformation from static to dynamic benchmarking. In this work, we conduct an in-depth analysis of existing static to dynamic benchmarking methods aimed at reducing data contamination risks. We first examine methods that enhance static benchmarks and identify their inherent limitations. We then highlight a critical gap-the lack of standardized criteria for evaluating dynamic benchmarks. Based on this observation, we propose a series of optimal design principles for dynamic benchmarking and analyze the limitations of existing dynamic benchmarks. This survey provides a concise yet comprehensive overview of recent advancements in data contamination research, offering valuable insights and a clear guide for future research efforts. We maintain a GitHub repository to continuously collect both static and dynamic benchmarking methods for LLMs. The repository can be found at this link.",
    "authors": [
      "Simin Chen",
      "Yiming Chen",
      "Zexin Li",
      "Yifan Jiang",
      "Zhongwei Wan",
      "Yixin He",
      "Dezhi Ran",
      "Tianle Gu",
      "Haizhou Li",
      "Tao Xie",
      "Baishakhi Ray"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-23T08:18:37Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17521v2"
  },
  {
    "arxiv_id": "2502.16395v3",
    "entry_id": "http://arxiv.org/abs/2502.16395v3",
    "title": "AIRepr: An Analyst-Inspector Framework for Evaluating Reproducibility of LLMs in Data Science",
    "summary": "Large language models (LLMs) are increasingly used to automate data analysis through executable code generation. Yet, data science tasks often admit multiple statistically valid solutions, e.g. different modeling strategies, making it critical to understand the reasoning behind analyses, not just their outcomes. While manual review of LLM-generated code can help ensure statistical soundness, it is labor-intensive and requires expertise. A more scalable approach is to evaluate the underlying workflows-the logical plans guiding code generation. However, it remains unclear how to assess whether an LLM-generated workflow supports reproducible implementations.\n  To address this, we present AIRepr, an Analyst-Inspector framework for automatically evaluating and improving the reproducibility of LLM-generated data analysis workflows. Our framework is grounded in statistical principles and supports scalable, automated assessment. We introduce two novel reproducibility-enhancing prompting strategies and benchmark them against standard prompting across 15 analyst-inspector LLM pairs and 1,032 tasks from three public benchmarks. Our findings show that workflows with higher reproducibility also yield more accurate analyses, and that reproducibility-enhancing prompts substantially improve both metrics. This work provides a foundation for transparent, reliable, and efficient human-AI collaboration in data science. Our code is publicly available.",
    "authors": [
      "Qiuhai Zeng",
      "Claire Jin",
      "Xinyue Wang",
      "Yuhan Zheng",
      "Qunhua Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2025-02-23T01:15:50Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16395v3"
  },
  {
    "arxiv_id": "2502.17516v1",
    "entry_id": "http://arxiv.org/abs/2502.17516v1",
    "title": "A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models",
    "summary": "The rise of foundation models has transformed machine learning research, prompting efforts to uncover their inner workings and develop more efficient and reliable applications for better control. While significant progress has been made in interpreting Large Language Models (LLMs), multimodal foundation models (MMFMs) - such as contrastive vision-language models, generative vision-language models, and text-to-image models - pose unique interpretability challenges beyond unimodal frameworks. Despite initial studies, a substantial gap remains between the interpretability of LLMs and MMFMs. This survey explores two key aspects: (1) the adaptation of LLM interpretability methods to multimodal models and (2) understanding the mechanistic differences between unimodal language models and crossmodal systems. By systematically reviewing current MMFM analysis techniques, we propose a structured taxonomy of interpretability methods, compare insights across unimodal and multimodal architectures, and highlight critical research gaps.",
    "authors": [
      "Zihao Lin",
      "Samyadeep Basu",
      "Mohammad Beigi",
      "Varun Manjunatha",
      "Ryan A. Rossi",
      "Zichao Wang",
      "Yufan Zhou",
      "Sriram Balasubramanian",
      "Arman Zarei",
      "Keivan Rezaei",
      "Ying Shen",
      "Barry Menglong Yao",
      "Zhiyang Xu",
      "Qin Liu",
      "Yuxiang Zhang",
      "Yan Sun",
      "Shilong Liu",
      "Li Shen",
      "Hongxuan Li",
      "Soheil Feizi",
      "Lifu Huang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-22T20:55:26Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17516v1"
  },
  {
    "arxiv_id": "2502.16291v2",
    "entry_id": "http://arxiv.org/abs/2502.16291v2",
    "title": "The Design Space of Recent AI-assisted Research Tools for Ideation, Sensemaking, and Scientific Creativity",
    "summary": "Generative AI (GenAI) tools are radically expanding the scope and capability of automation in knowledge work such as academic research. While promising for augmenting cognition and streamlining processes, AI-assisted research tools may also increase automation bias and hinder critical thinking. To examine recent developments, we surveyed publications from leading HCI venues over the past three years, closely analyzing thirteen tools to better understand the novel capabilities of these AI-assisted systems and the design spaces they enable: seven employing traditional AI or customized transformer-based approaches, and six integrating open-access large language models (LLMs). Our analysis characterizes the emerging design space, distinguishes between tools focused on workflow mimicry versus generative exploration, and yields four critical design recommendations to guide the development of future systems that foster meaningful cognitive engagement: providing user agency and control, differentiating divergent/convergent thinking support, ensuring adaptability, and prioritizing transparency/accuracy. This work discusses how these insights signal a shift from mere workflow replication towards generative co-creation, presenting new opportunities for the community to craft intuitive, AI-driven research interfaces and interactions.",
    "authors": [
      "Runlong Ye",
      "Matthew Varona",
      "Oliver Huang",
      "Patrick Yung Kang Lee",
      "Michael Liut",
      "Carolina Nobre"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-02-22T16:42:11Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16291v2"
  },
  {
    "arxiv_id": "2502.16280v1",
    "entry_id": "http://arxiv.org/abs/2502.16280v1",
    "title": "Human Preferences in Large Language Model Latent Space: A Technical Analysis on the Reliability of Synthetic Data in Voting Outcome Prediction",
    "summary": "Generative AI (GenAI) is increasingly used in survey contexts to simulate human preferences. While many research endeavors evaluate the quality of synthetic GenAI data by comparing model-generated responses to gold-standard survey results, fundamental questions about the validity and reliability of using LLMs as substitutes for human respondents remain. Our study provides a technical analysis of how demographic attributes and prompt variations influence latent opinion mappings in large language models (LLMs) and evaluates their suitability for survey-based predictions. Using 14 different models, we find that LLM-generated data fails to replicate the variance observed in real-world human responses, particularly across demographic subgroups. In the political space, persona-to-party mappings exhibit limited differentiation, resulting in synthetic data that lacks the nuanced distribution of opinions found in survey data. Moreover, we show that prompt sensitivity can significantly alter outputs for some models, further undermining the stability and predictiveness of LLM-based simulations. As a key contribution, we adapt a probe-based methodology that reveals how LLMs encode political affiliations in their latent space, exposing the systematic distortions introduced by these models. Our findings highlight critical limitations in AI-generated survey data, urging caution in its use for public opinion research, social science experimentation, and computational behavioral modeling.",
    "authors": [
      "Sarah Ball",
      "Simeon Allmendinger",
      "Frauke Kreuter",
      "Niklas Kühl"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-22T16:25:33Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16280v1"
  },
  {
    "arxiv_id": "2503.01854v2",
    "entry_id": "http://arxiv.org/abs/2503.01854v2",
    "title": "A Comprehensive Survey of Machine Unlearning Techniques for Large Language Models",
    "summary": "This study investigates the machine unlearning techniques within the context of large language models (LLMs), referred to as \\textit{LLM unlearning}. LLM unlearning offers a principled approach to removing the influence of undesirable data (e.g., sensitive or illegal information) from LLMs, while preserving their overall utility without requiring full retraining. Despite growing research interest, there is no comprehensive survey that systematically organizes existing work and distills key insights; here, we aim to bridge this gap. We begin by introducing the definition and the paradigms of LLM unlearning, followed by a comprehensive taxonomy of existing unlearning studies. Next, we categorize current unlearning approaches, summarizing their strengths and limitations. Additionally, we review evaluation metrics and benchmarks, providing a structured overview of current assessment methodologies. Finally, we outline promising directions for future research, highlighting key challenges and opportunities in the field.",
    "authors": [
      "Jiahui Geng",
      "Qing Li",
      "Herbert Woisetschlaeger",
      "Zongxiong Chen",
      "Fengyu Cai",
      "Yuxia Wang",
      "Preslav Nakov",
      "Hans-Arno Jacobsen",
      "Fakhri Karray"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-22T12:46:14Z",
    "pdf_url": "https://arxiv.org/pdf/2503.01854v2"
  },
  {
    "arxiv_id": "2502.16195v2",
    "entry_id": "http://arxiv.org/abs/2502.16195v2",
    "title": "Statistical Inference in Reinforcement Learning: A Selective Survey",
    "summary": "Reinforcement learning (RL) is concerned with how intelligence agents take actions in a given environment to maximize the cumulative reward they receive. In healthcare, applying RL algorithms could assist patients in improving their health status. In ride-sharing platforms, applying RL algorithms could increase drivers' income and customer satisfaction. For large language models, applying RL algorithms could align their outputs with human preferences. Over the past decade, RL has been arguably one of the most vibrant research frontiers in machine learning. Nevertheless, statistics as a field, as opposed to computer science, has only recently begun to engage with RL both in depth and in breadth. This chapter presents a selective review of statistical inferential tools for RL, covering both hypothesis testing and confidence interval construction. Our goal is to highlight the value of statistical inference in RL for both the statistics and machine learning communities, and to promote the broader application of classical statistical inference tools in this vibrant area of research.",
    "authors": [
      "Chengchun Shi"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2025-02-22T11:49:20Z",
    "pdf_url": "https://arxiv.org/pdf/2502.16195v2"
  },
  {
    "arxiv_id": "2503.16457v1",
    "entry_id": "http://arxiv.org/abs/2503.16457v1",
    "title": "Integrating Personality into Digital Humans: A Review of LLM-Driven Approaches for Virtual Reality",
    "summary": "The integration of large language models (LLMs) into virtual reality (VR) environments has opened new pathways for creating more immersive and interactive digital humans. By leveraging the generative capabilities of LLMs alongside multimodal outputs such as facial expressions and gestures, virtual agents can simulate human-like personalities and emotions, fostering richer and more engaging user experiences. This paper provides a comprehensive review of methods for enabling digital humans to adopt nuanced personality traits, exploring approaches such as zero-shot, few-shot, and fine-tuning. Additionally, it highlights the challenges of integrating LLM-driven personality traits into VR, including computational demands, latency issues, and the lack of standardized evaluation frameworks for multimodal interactions. By addressing these gaps, this work lays a foundation for advancing applications in education, therapy, and gaming, while fostering interdisciplinary collaboration to redefine human-computer interaction in VR.",
    "authors": [
      "Iago Alves Brito",
      "Julia Soares Dollis",
      "Fernanda Bufon Färber",
      "Pedro Schindler Freire Brasil Ribeiro",
      "Rafael Teixeira Sousa",
      "Arlindo Rodrigues Galvão Filho"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-22T01:33:05Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16457v1"
  },
  {
    "arxiv_id": "2502.17504v2",
    "entry_id": "http://arxiv.org/abs/2502.17504v2",
    "title": "Protein Large Language Models: A Comprehensive Survey",
    "summary": "Protein-specific large language models (Protein LLMs) are revolutionizing protein science by enabling more efficient protein structure prediction, function annotation, and design. While existing surveys focus on specific aspects or applications, this work provides the first comprehensive overview of Protein LLMs, covering their architectures, training datasets, evaluation metrics, and diverse applications. Through a systematic analysis of over 100 articles, we propose a structured taxonomy of state-of-the-art Protein LLMs, analyze how they leverage large-scale protein sequence data for improved accuracy, and explore their potential in advancing protein engineering and biomedical research. Additionally, we discuss key challenges and future directions, positioning Protein LLMs as essential tools for scientific discovery in protein science. Resources are maintained at https://github.com/Yijia-Xiao/Protein-LLM-Survey.",
    "authors": [
      "Yijia Xiao",
      "Wanjia Zhao",
      "Junkai Zhang",
      "Yiqiao Jin",
      "Han Zhang",
      "Zhicheng Ren",
      "Renliang Sun",
      "Haixin Wang",
      "Guancheng Wan",
      "Pan Lu",
      "Xiao Luo",
      "Yu Zhang",
      "James Zou",
      "Yizhou Sun",
      "Wei Wang"
    ],
    "categories": [
      "q-bio.BM",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-02-21T19:22:10Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17504v2"
  },
  {
    "arxiv_id": "2502.15871v2",
    "entry_id": "http://arxiv.org/abs/2502.15871v2",
    "title": "A Comprehensive Survey on the Trustworthiness of Large Language Models in Healthcare",
    "summary": "The application of large language models (LLMs) in healthcare holds significant promise for enhancing clinical decision-making, medical research, and patient care. However, their integration into real-world clinical settings raises critical concerns around trustworthiness, particularly around dimensions of truthfulness, privacy, safety, robustness, fairness, and explainability. These dimensions are essential for ensuring that LLMs generate reliable, unbiased, and ethically sound outputs. While researchers have recently begun developing benchmarks and evaluation frameworks to assess LLM trustworthiness, the trustworthiness of LLMs in healthcare remains underexplored, lacking a systematic review that provides a comprehensive understanding and future insights. This survey addresses that gap by providing a comprehensive review of current methodologies and solutions aimed at mitigating risks across key trust dimensions. We analyze how each dimension affects the reliability and ethical deployment of healthcare LLMs, synthesize ongoing research efforts, and identify critical gaps in existing approaches. We also identify emerging challenges posed by evolving paradigms, such as multi-agent collaboration, multi-modal reasoning, and the development of small open-source medical models. Our goal is to guide future research toward more trustworthy, transparent, and clinically viable LLMs.",
    "authors": [
      "Manar Aljohani",
      "Jun Hou",
      "Sindhura Kommu",
      "Xuan Wang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-21T18:43:06Z",
    "pdf_url": "https://arxiv.org/pdf/2502.15871v2"
  },
  {
    "arxiv_id": "2502.15652v4",
    "entry_id": "http://arxiv.org/abs/2502.15652v4",
    "title": "Empowering LLMs with Logical Reasoning: A Comprehensive Survey",
    "summary": "Large language models (LLMs) have achieved remarkable successes on various tasks. However, recent studies have found that there are still significant challenges to the logical reasoning abilities of LLMs, which can be categorized into the following two aspects: (1) Logical question answering: LLMs often fail to generate the correct answer within a complex logical problem which requires sophisticated deductive, inductive or abductive reasoning given a collection of premises. (2) Logical consistency: LLMs are prone to producing responses contradicting themselves across different questions. For example, a state-of-the-art question-answering LLM Macaw, answers Yes to both questions Is a magpie a bird? and Does a bird have wings? but answers No to Does a magpie have wings?. To facilitate this research direction, we comprehensively investigate the most cutting-edge methods and propose a detailed taxonomy. Specifically, to accurately answer complex logic questions, previous methods can be categorized based on reliance on external solvers, prompts, and fine-tuning. To avoid logical contradictions, we discuss concepts and solutions of various logical consistencies, including implication, negation, transitivity, factuality consistencies, and their composites. In addition, we review commonly used benchmark datasets and evaluation metrics, and discuss promising research directions, such as extending to modal logic to account for uncertainty and developing efficient algorithms that simultaneously satisfy multiple logical consistencies.",
    "authors": [
      "Fengxiang Cheng",
      "Haoxuan Li",
      "Fenrong Liu",
      "Robert van Rooij",
      "Kun Zhang",
      "Zhouchen Lin"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-21T18:20:35Z",
    "pdf_url": "https://arxiv.org/pdf/2502.15652v4"
  },
  {
    "arxiv_id": "2502.15365v2",
    "entry_id": "http://arxiv.org/abs/2502.15365v2",
    "title": "Identifying Features that Shape Perceived Consciousness in Large Language Model-based AI: A Quantitative Study of Human Responses",
    "summary": "This study quantitively examines which features of AI-generated text lead humans to perceive subjective consciousness in large language model (LLM)-based AI systems. Drawing on 99 passages from conversations with Claude 3 Opus and focusing on eight features -- metacognitive self-reflection, logical reasoning, empathy, emotionality, knowledge, fluency, unexpectedness, and subjective expressiveness -- we conducted a survey with 123 participants. Using regression and clustering analyses, we investigated how these features influence participants' perceptions of AI consciousness. The results reveal that metacognitive self-reflection and the AI's expression of its own emotions significantly increased perceived consciousness, while a heavy emphasis on knowledge reduced it. Participants clustered into seven subgroups, each showing distinct feature-weighting patterns. Additionally, higher prior knowledge of LLMs and more frequent usage of LLM-based chatbots were associated with greater overall likelihood assessments of AI consciousness. This study underscores the multidimensional and individualized nature of perceived AI consciousness and provides a foundation for better understanding the psychosocial implications of human-AI interaction.",
    "authors": [
      "Bongsu Kang",
      "Jundong Kim",
      "Tae-Rim Yun",
      "Hyojin Bae",
      "Chang-Eop Kim"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-02-21T10:27:28Z",
    "pdf_url": "https://arxiv.org/pdf/2502.15365v2"
  },
  {
    "arxiv_id": "2502.15336v1",
    "entry_id": "http://arxiv.org/abs/2502.15336v1",
    "title": "Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions",
    "summary": "Embodied multimodal large models (EMLMs) have gained significant attention in recent years due to their potential to bridge the gap between perception, cognition, and action in complex, real-world environments. This comprehensive review explores the development of such models, including Large Language Models (LLMs), Large Vision Models (LVMs), and other models, while also examining other emerging architectures. We discuss the evolution of EMLMs, with a focus on embodied perception, navigation, interaction, and simulation. Furthermore, the review provides a detailed analysis of the datasets used for training and evaluating these models, highlighting the importance of diverse, high-quality data for effective learning. The paper also identifies key challenges faced by EMLMs, including issues of scalability, generalization, and real-time decision-making. Finally, we outline future directions, emphasizing the integration of multimodal sensing, reasoning, and action to advance the development of increasingly autonomous systems. By providing an in-depth analysis of state-of-the-art methods and identifying critical gaps, this paper aims to inspire future advancements in EMLMs and their applications across diverse domains.",
    "authors": [
      "Shoubin Chen",
      "Zehao Wu",
      "Kai Zhang",
      "Chunyu Li",
      "Baiyang Zhang",
      "Fei Ma",
      "Fei Richard Yu",
      "Qingquan Li"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-02-21T09:41:27Z",
    "pdf_url": "https://arxiv.org/pdf/2502.15336v1"
  },
  {
    "arxiv_id": "2502.15214v1",
    "entry_id": "http://arxiv.org/abs/2502.15214v1",
    "title": "The Evolving Landscape of LLM- and VLM-Integrated Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has shown impressive results in sequential decision-making tasks. Meanwhile, Large Language Models (LLMs) and Vision-Language Models (VLMs) have emerged, exhibiting impressive capabilities in multimodal understanding and reasoning. These advances have led to a surge of research integrating LLMs and VLMs into RL. In this survey, we review representative works in which LLMs and VLMs are used to overcome key challenges in RL, such as lack of prior knowledge, long-horizon planning, and reward design. We present a taxonomy that categorizes these LLM/VLM-assisted RL approaches into three roles: agent, planner, and reward. We conclude by exploring open problems, including grounding, bias mitigation, improved representations, and action advice. By consolidating existing research and identifying future directions, this survey establishes a framework for integrating LLMs and VLMs into RL, advancing approaches that unify natural language and visual understanding with sequential decision-making.",
    "authors": [
      "Sheila Schoepp",
      "Masoud Jafaripour",
      "Yingyue Cao",
      "Tianpei Yang",
      "Fatemeh Abdollahi",
      "Shadan Golestan",
      "Zahin Sufiyan",
      "Osmar R. Zaiane",
      "Matthew E. Taylor"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-21T05:01:30Z",
    "pdf_url": "https://arxiv.org/pdf/2502.15214v1"
  },
  {
    "arxiv_id": "2502.14820v1",
    "entry_id": "http://arxiv.org/abs/2502.14820v1",
    "title": "eC-Tab2Text: Aspect-Based Text Generation from e-Commerce Product Tables",
    "summary": "Large Language Models (LLMs) have demonstrated exceptional versatility across diverse domains, yet their application in e-commerce remains underexplored due to a lack of domain-specific datasets. To address this gap, we introduce eC-Tab2Text, a novel dataset designed to capture the intricacies of e-commerce, including detailed product attributes and user-specific queries. Leveraging eC-Tab2Text, we focus on text generation from product tables, enabling LLMs to produce high-quality, attribute-specific product reviews from structured tabular data. Fine-tuned models were rigorously evaluated using standard Table2Text metrics, alongside correctness, faithfulness, and fluency assessments. Our results demonstrate substantial improvements in generating contextually accurate reviews, highlighting the transformative potential of tailored datasets and fine-tuning methodologies in optimizing e-commerce workflows. This work highlights the potential of LLMs in e-commerce workflows and the essential role of domain-specific datasets in tailoring them to industry-specific challenges.",
    "authors": [
      "Luis Antonio Gutiérrez Guanilo",
      "Mir Tafseer Nayeem",
      "Cristian López",
      "Davood Rafiei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB",
      "cs.HC"
    ],
    "published": "2025-02-20T18:41:48Z",
    "pdf_url": "https://arxiv.org/pdf/2502.14820v1"
  },
  {
    "arxiv_id": "2502.14767v2",
    "entry_id": "http://arxiv.org/abs/2502.14767v2",
    "title": "Tree-of-Debate: Multi-Persona Debate Trees Elicit Critical Thinking for Scientific Comparative Analysis",
    "summary": "With the exponential growth of research facilitated by modern technology and improved accessibility, scientific discoveries have become increasingly fragmented within and across fields. This makes it challenging to assess the significance, novelty, incremental findings, and equivalent ideas between related works, particularly those from different research communities. Large language models (LLMs) have recently demonstrated strong quantitative and qualitative reasoning abilities, and multi-agent LLM debates have shown promise in handling complex reasoning tasks by exploring diverse perspectives and reasoning paths. Inspired by this, we introduce Tree-of-Debate (ToD), a framework which converts scientific papers into LLM personas that debate their respective novelties. To emphasize structured, critical reasoning rather than focusing solely on outcomes, ToD dynamically constructs a debate tree, enabling fine-grained analysis of independent novelty arguments within scholarly articles. Through experiments on scientific literature across various domains, evaluated by expert researchers, we demonstrate that ToD generates informative arguments, effectively contrasts papers, and supports researchers in their literature review.",
    "authors": [
      "Priyanka Kargupta",
      "Ishika Agarwal",
      "Tal August",
      "Jiawei Han"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-20T17:43:40Z",
    "pdf_url": "https://arxiv.org/pdf/2502.14767v2"
  },
  {
    "arxiv_id": "2502.14743v2",
    "entry_id": "http://arxiv.org/abs/2502.14743v2",
    "title": "Multi-Agent Coordination across Diverse Applications: A Survey",
    "summary": "Multi-agent coordination studies the underlying mechanism enabling the trending spread of diverse multi-agent systems (MAS) and has received increasing attention, driven by the expansion of emerging applications and rapid AI advances. This survey outlines the current state of coordination research across applications through a unified understanding that answers four fundamental coordination questions: (1) what is coordination; (2) why coordination; (3) who to coordinate with; and (4) how to coordinate. Our purpose is to explore existing ideas and expertise in coordination and their connections across diverse applications, while identifying and highlighting emerging and promising research directions. First, general coordination problems that are essential to varied applications are identified and analyzed. Second, a number of MAS applications are surveyed, ranging from widely studied domains, e.g., search and rescue, warehouse automation and logistics, and transportation systems, to emerging fields including humanoid and anthropomorphic robots, satellite systems, and large language models (LLMs). Finally, open challenges about the scalability, heterogeneity, and learning mechanisms of MAS are analyzed and discussed. In particular, we identify the hybridization of hierarchical and decentralized coordination, human-MAS coordination, and LLM-based MAS as promising future directions.",
    "authors": [
      "Lijun Sun",
      "Yijun Yang",
      "Qiqi Duan",
      "Yuhui Shi",
      "Chao Lyu",
      "Yu-Cheng Chang",
      "Chin-Teng Lin",
      "Yang Shen"
    ],
    "categories": [
      "cs.MA",
      "cs.AI"
    ],
    "published": "2025-02-20T17:12:45Z",
    "pdf_url": "https://arxiv.org/pdf/2502.14743v2"
  },
  {
    "arxiv_id": "2502.14333v1",
    "entry_id": "http://arxiv.org/abs/2502.14333v1",
    "title": "A Survey on Feedback-based Multi-step Reasoning for Large Language Models on Mathematics",
    "summary": "Recent progress in large language models (LLM) found chain-of-thought prompting strategies to improve the reasoning ability of LLMs by encouraging problem solving through multiple steps. Therefore, subsequent research aimed to integrate the multi-step reasoning process into the LLM itself through process rewards as feedback and achieved improvements over prompting strategies. Due to the cost of step-level annotation, some turn to outcome rewards as feedback. Aside from these training-based approaches, training-free techniques leverage frozen LLMs or external tools for feedback at each step to enhance the reasoning process. With the abundance of work in mathematics due to its logical nature, we present a survey of strategies utilizing feedback at the step and outcome levels to enhance multi-step math reasoning for LLMs. As multi-step reasoning emerges a crucial component in scaling LLMs, we hope to establish its foundation for easier understanding and empower further research.",
    "authors": [
      "Ting-Ruen Wei",
      "Haowei Liu",
      "Xuyang Wu",
      "Yi Fang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-20T07:31:00Z",
    "pdf_url": "https://arxiv.org/pdf/2502.14333v1"
  },
  {
    "arxiv_id": "2502.14321v2",
    "entry_id": "http://arxiv.org/abs/2502.14321v2",
    "title": "Beyond Self-Talk: A Communication-Centric Survey of LLM-Based Multi-Agent Systems",
    "summary": "Large language model-based multi-agent systems have recently gained significant attention due to their potential for complex, collaborative, and intelligent problem-solving capabilities. Existing surveys typically categorize LLM-based multi-agent systems (LLM-MAS) according to their application domains or architectures, overlooking the central role of communication in coordinating agent behaviors and interactions. To address this gap, this paper presents a comprehensive survey of LLM-MAS from a communication-centric perspective. Specifically, we propose a structured framework that integrates system-level communication (architecture, goals, and protocols) with system internal communication (strategies, paradigms, objects, and content), enabling a detailed exploration of how agents interact, negotiate, and achieve collective intelligence. Through an extensive analysis of recent literature, we identify key components in multiple dimensions and summarize their strengths and limitations. In addition, we highlight current challenges, including communication efficiency, security vulnerabilities, inadequate benchmarking, and scalability issues, and outline promising future research directions. This review aims to help researchers and practitioners gain a clear understanding of the communication mechanisms in LLM-MAS, thereby facilitating the design and deployment of robust, scalable, and secure multi-agent systems.",
    "authors": [
      "Bingyu Yan",
      "Zhibo Zhou",
      "Litian Zhang",
      "Lian Zhang",
      "Ziyi Zhou",
      "Dezhuang Miao",
      "Zhoujun Li",
      "Chaozhuo Li",
      "Xiaoming Zhang"
    ],
    "categories": [
      "cs.MA",
      "cs.CL"
    ],
    "published": "2025-02-20T07:18:34Z",
    "pdf_url": "https://arxiv.org/pdf/2502.14321v2"
  },
  {
    "arxiv_id": "2502.13499v1",
    "entry_id": "http://arxiv.org/abs/2502.13499v1",
    "title": "Hidden Darkness in LLM-Generated Designs: Exploring Dark Patterns in Ecommerce Web Components Generated by LLMs",
    "summary": "Recent work has highlighted the risks of LLM-generated content for a wide range of harmful behaviors, including incorrect and harmful code. In this work, we extend this by studying whether LLM-generated web design contains dark patterns. This work evaluated designs of ecommerce web components generated by four popular LLMs: Claude, GPT, Gemini, and Llama. We tested 13 commonly used ecommerce components (e.g., search, product reviews) and used them as prompts to generate a total of 312 components across all models. Over one-third of generated components contain at least one dark pattern. The majority of dark pattern strategies involve hiding crucial information, limiting users' actions, and manipulating them into making decisions through a sense of urgency. Dark patterns are also more frequently produced in components that are related to company interests. These findings highlight the need for interventions to prevent dark patterns during front-end code generation with LLMs and emphasize the importance of expanding ethical design education to a broader audience.",
    "authors": [
      "Ziwei Chen",
      "Jiawen Shen",
      "Luna",
      "Kristen Vaccaro"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-19T07:35:07Z",
    "pdf_url": "https://arxiv.org/pdf/2502.13499v1"
  },
  {
    "arxiv_id": "2502.17487v2",
    "entry_id": "http://arxiv.org/abs/2502.17487v2",
    "title": "User Intent to Use DeepSeek for Healthcare Purposes and their Trust in the Large Language Model: Multinational Survey Study",
    "summary": "Large language models (LLMs) increasingly serve as interactive healthcare resources, yet user acceptance remains underexplored. This study examines how ease of use, perceived usefulness, trust, and risk perception interact to shape intentions to adopt DeepSeek, an emerging LLM-based platform, for healthcare purposes. A cross-sectional survey of 556 participants from India, the United Kingdom, and the United States was conducted to measure perceptions and usage patterns. Structural equation modeling assessed both direct and indirect effects, including potential quadratic relationships. Results revealed that trust plays a pivotal mediating role: ease of use exerts a significant indirect effect on usage intentions through trust, while perceived usefulness contributes to both trust development and direct adoption. By contrast, risk perception negatively affects usage intent, emphasizing the importance of robust data governance and transparency. Notably, significant non-linear paths were observed for ease of use and risk, indicating threshold or plateau effects. The measurement model demonstrated strong reliability and validity, supported by high composite reliabilities, average variance extracted, and discriminant validity measures. These findings extend technology acceptance and health informatics research by illuminating the multifaceted nature of user adoption in sensitive domains. Stakeholders should invest in trust-building strategies, user-centric design, and risk mitigation measures to encourage sustained and safe uptake of LLMs in healthcare. Future work can employ longitudinal designs or examine culture-specific variables to further clarify how user perceptions evolve over time and across different regulatory environments. Such insights are critical for harnessing AI to enhance outcomes.",
    "authors": [
      "Avishek Choudhury",
      "Yeganeh Shahsavar",
      "Hamid Shamszare"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2025-02-18T16:49:36Z",
    "pdf_url": "https://arxiv.org/pdf/2502.17487v2"
  },
  {
    "arxiv_id": "2502.13187v3",
    "entry_id": "http://arxiv.org/abs/2502.13187v3",
    "title": "A Survey of Sim-to-Real Methods in RL: Progress, Prospects and Challenges with Foundation Models",
    "summary": "Deep Reinforcement Learning (RL) has been explored and verified to be effective in solving decision-making tasks in various domains, such as robotics, transportation, recommender systems, etc. It learns from the interaction with environments and updates the policy using the collected experience. However, due to the limited real-world data and unbearable consequences of taking detrimental actions, the learning of RL policy is mainly restricted within the simulators. This practice guarantees safety in learning but introduces an inevitable sim-to-real gap in terms of deployment, thus causing degraded performance and risks in execution. There are attempts to solve the sim-to-real problems from different domains with various techniques, especially in the era with emerging techniques such as large foundations or language models that have cast light on the sim-to-real. This survey paper, to the best of our knowledge, is the first taxonomy that formally frames the sim-to-real techniques from key elements of the Markov Decision Process (State, Action, Transition, and Reward). Based on the framework, we cover comprehensive literature from the classic to the most advanced methods including the sim-to-real techniques empowered by foundation models, and we also discuss the specialties that are worth attention in different domains of sim-to-real problems. Then we summarize the formal evaluation process of sim-to-real performance with accessible code or benchmarks. The challenges and opportunities are also presented to encourage future exploration of this direction. We are actively maintaining a repository to include the most up-to-date sim-to-real research work to help domain researchers.",
    "authors": [
      "Longchao Da",
      "Justin Turnau",
      "Thirulogasankar Pranav Kutralingam",
      "Alvaro Velasquez",
      "Paulo Shakarian",
      "Hua Wei"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-02-18T12:57:29Z",
    "pdf_url": "https://arxiv.org/pdf/2502.13187v3"
  },
  {
    "arxiv_id": "2503.16449v1",
    "entry_id": "http://arxiv.org/abs/2503.16449v1",
    "title": "Mitigating the Uncanny Valley Effect in Hyper-Realistic Robots: A Student-Centered Study on LLM-Driven Conversations",
    "summary": "The uncanny valley effect poses a significant challenge in the development and acceptance of hyper-realistic social robots. This study investigates whether advanced conversational capabilities powered by large language models (LLMs) can mitigate this effect in highly anthropomorphic robots. We conducted a user study with 80 participants interacting with Nadine, a hyper-realistic humanoid robot equipped with LLM-driven communication skills. Through pre- and post-interaction surveys, we assessed changes in perceptions of uncanniness, conversational quality, and overall user experience. Our findings reveal that LLM-enhanced interactions significantly reduce feelings of eeriness while fostering more natural and engaging conversations. Additionally, we identify key factors influencing user acceptance, including conversational naturalness, human-likeness, and interestingness. Based on these insights, we propose design recommendations to enhance the appeal and acceptability of hyper-realistic robots in social contexts. This research contributes to the growing field of human-robot interaction by offering empirical evidence on the potential of LLMs to bridge the uncanny valley, with implications for the future development of social robots.",
    "authors": [
      "Hangyeol Kang",
      "Thiago Freitas dos Santos",
      "Maher Ben Moussa",
      "Nadia Magnenat-Thalmann"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-02-18T12:53:41Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16449v1"
  },
  {
    "arxiv_id": "2502.12669v3",
    "entry_id": "http://arxiv.org/abs/2502.12669v3",
    "title": "Perovskite-LLM: Knowledge-Enhanced Large Language Models for Perovskite Solar Cell Research",
    "summary": "The rapid advancement of perovskite solar cells (PSCs) has led to an exponential growth in research publications, creating an urgent need for efficient knowledge management and reasoning systems in this domain. We present a comprehensive knowledge-enhanced system for PSCs that integrates three key components. First, we develop Perovskite-KG, a domain-specific knowledge graph constructed from 1,517 research papers, containing 23,789 entities and 22,272 relationships. Second, we create two complementary datasets: Perovskite-Chat, comprising 55,101 high-quality question-answer pairs generated through a novel multi-agent framework, and Perovskite-Reasoning, containing 2,217 carefully curated materials science problems. Third, we introduce two specialized large language models: Perovskite-Chat-LLM for domain-specific knowledge assistance and Perovskite-Reasoning-LLM for scientific reasoning tasks. Experimental results demonstrate that our system significantly outperforms existing models in both domain-specific knowledge retrieval and scientific reasoning tasks, providing researchers with effective tools for literature review, experimental design, and complex problem-solving in PSC research.",
    "authors": [
      "Xiang Liu",
      "Penglei Sun",
      "Shuyan Chen",
      "Longhan Zhang",
      "Peijie Dong",
      "Huajie You",
      "Yongqi Zhang",
      "Chang Yan",
      "Xiaowen Chu",
      "Tong-yi Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-02-18T09:19:24Z",
    "pdf_url": "https://arxiv.org/pdf/2502.12669v3"
  },
  {
    "arxiv_id": "2502.12568v3",
    "entry_id": "http://arxiv.org/abs/2502.12568v3",
    "title": "A Cognitive Writing Perspective for Constrained Long-Form Text Generation",
    "summary": "Like humans, Large Language Models (LLMs) struggle to generate high-quality long-form text that adheres to strict requirements in a single pass. This challenge is unsurprising, as successful human writing, according to the Cognitive Writing Theory, is a complex cognitive process involving iterative planning, translating, reviewing, and monitoring. Motivated by these cognitive principles, we aim to equip LLMs with human-like cognitive writing capabilities through CogWriter, a novel training-free framework that transforms LLM constrained long-form text generation into a systematic cognitive writing paradigm. Our framework consists of two key modules: (1) a Planning Agent that performs hierarchical planning to decompose the task, and (2) multiple Generation Agents that execute these plans in parallel. The system maintains quality via continuous monitoring and reviewing mechanisms, which evaluate outputs against specified requirements and trigger necessary revisions. CogWriter demonstrates exceptional performance on LongGenBench, a benchmark for complex constrained long-form text generation. Even when using Qwen-2.5-14B as its backbone, CogWriter surpasses GPT-4o by 22% in complex instruction completion accuracy while reliably generating texts exceeding 10,000 words. We hope this cognitive science-inspired approach provides a paradigm for LLM writing advancements: \\href{https://github.com/KaiyangWan/CogWriter}{CogWriter}.",
    "authors": [
      "Kaiyang Wan",
      "Honglin Mu",
      "Rui Hao",
      "Haoran Luo",
      "Tianle Gu",
      "Xiuying Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-18T06:12:14Z",
    "pdf_url": "https://arxiv.org/pdf/2502.12568v3"
  },
  {
    "arxiv_id": "2502.13175v2",
    "entry_id": "http://arxiv.org/abs/2502.13175v2",
    "title": "Towards Robust and Secure Embodied AI: A Survey on Vulnerabilities and Attacks",
    "summary": "Embodied AI systems, including robots and autonomous vehicles, are increasingly integrated into real-world applications, where they encounter a range of vulnerabilities stemming from both environmental and system-level factors. These vulnerabilities manifest through sensor spoofing, adversarial attacks, and failures in task and motion planning, posing significant challenges to robustness and safety. Despite the growing body of research, existing reviews rarely focus specifically on the unique safety and security challenges of embodied AI systems. Most prior work either addresses general AI vulnerabilities or focuses on isolated aspects, lacking a dedicated and unified framework tailored to embodied AI. This survey fills this critical gap by: (1) categorizing vulnerabilities specific to embodied AI into exogenous (e.g., physical attacks, cybersecurity threats) and endogenous (e.g., sensor failures, software flaws) origins; (2) systematically analyzing adversarial attack paradigms unique to embodied AI, with a focus on their impact on perception, decision-making, and embodied interaction; (3) investigating attack vectors targeting large vision-language models (LVLMs) and large language models (LLMs) within embodied systems, such as jailbreak attacks and instruction misinterpretation; (4) evaluating robustness challenges in algorithms for embodied perception, decision-making, and task planning; and (5) proposing targeted strategies to enhance the safety and reliability of embodied AI systems. By integrating these dimensions, we provide a comprehensive framework for understanding the interplay between vulnerabilities and safety in embodied AI.",
    "authors": [
      "Wenpeng Xing",
      "Minghao Li",
      "Mohan Li",
      "Meng Han"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2025-02-18T03:38:07Z",
    "pdf_url": "https://arxiv.org/pdf/2502.13175v2"
  },
  {
    "arxiv_id": "2502.12435v1",
    "entry_id": "http://arxiv.org/abs/2502.12435v1",
    "title": "A Survey on Large Language Models for Automated Planning",
    "summary": "The planning ability of Large Language Models (LLMs) has garnered increasing attention in recent years due to their remarkable capacity for multi-step reasoning and their ability to generalize across a wide range of domains. While some researchers emphasize the potential of LLMs to perform complex planning tasks, others highlight significant limitations in their performance, particularly when these models are tasked with handling the intricacies of long-horizon reasoning. In this survey, we critically investigate existing research on the use of LLMs in automated planning, examining both their successes and shortcomings in detail. We illustrate that although LLMs are not well-suited to serve as standalone planners because of these limitations, they nonetheless present an enormous opportunity to enhance planning applications when combined with other approaches. Thus, we advocate for a balanced methodology that leverages the inherent flexibility and generalized knowledge of LLMs alongside the rigor and cost-effectiveness of traditional planning methods.",
    "authors": [
      "Mohamed Aghzal",
      "Erion Plaku",
      "Gregory J. Stein",
      "Ziyu Yao"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-18T02:11:03Z",
    "pdf_url": "https://arxiv.org/pdf/2502.12435v1"
  },
  {
    "arxiv_id": "2502.12058v1",
    "entry_id": "http://arxiv.org/abs/2502.12058v1",
    "title": "A survey about perceptions of mobility to inform an agent-based simulator of subjective modal choice",
    "summary": "In order to adapt to the issues of climate change and public health, urban policies are trying to encourage soft mobility, but the share of the car remains significant. Beyond known constraints, we study here the impact of perception biases on individual choices. We designed a multi-criteria decision model, integrating the influence of habits and biases. We then conducted an online survey, which received 650 responses. We used these to calculate realistic mobility perception values, in order to initialise the environment and the population of a modal choice simulator, implemented in Netlogo. This allows us to visualize the adaptation of the modal distribution in reaction to the evolution of urban planning, depending on whether or not we activate biases and habits in individual reasoning.\n  This is an extended and translated version of a demo paper published in French at JFSMA-JFMS 2024 \"Un simulateur multi-agent de choix modal subjectif\"",
    "authors": [
      "Carole Adam",
      "Benoit Gaudou"
    ],
    "categories": [
      "cs.MA",
      "cs.CY"
    ],
    "published": "2025-02-17T17:25:18Z",
    "pdf_url": "https://arxiv.org/pdf/2502.12058v1"
  },
  {
    "arxiv_id": "2502.12048v2",
    "entry_id": "http://arxiv.org/abs/2502.12048v2",
    "title": "A Survey on Bridging EEG Signals and Generative AI: From Image and Text to Beyond",
    "summary": "Integration of Brain-Computer Interfaces (BCIs) and Generative Artificial Intelligence (GenAI) has opened new frontiers in brain signal decoding, enabling assistive communication, neural representation learning, and multimodal integration. BCIs, particularly those leveraging Electroencephalography (EEG), provide a non-invasive means of translating neural activity into meaningful outputs. Recent advances in deep learning, including Generative Adversarial Networks (GANs) and Transformer-based Large Language Models (LLMs), have significantly improved EEG-based generation of images, text, and speech. This paper provides a literature review of the state-of-the-art in EEG-based multimodal generation, focusing on (i) EEG-to-image generation through GANs, Variational Autoencoders (VAEs), and Diffusion Models, and (ii) EEG-to-text generation leveraging Transformer based language models and contrastive learning methods. Additionally, we discuss the emerging domain of EEG-to-speech synthesis, an evolving multimodal frontier. We highlight key datasets, use cases, challenges, and EEG feature encoding methods that underpin generative approaches. By providing a structured overview of EEG-based generative AI, this survey aims to equip researchers and practitioners with insights to advance neural decoding, enhance assistive technologies, and expand the frontiers of brain-computer interaction.",
    "authors": [
      "Shreya Shukla",
      "Jose Torres",
      "Abhijit Mishra",
      "Jacek Gwizdka",
      "Shounak Roychowdhury"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2025-02-17T17:16:41Z",
    "pdf_url": "https://arxiv.org/pdf/2502.12048v2"
  },
  {
    "arxiv_id": "2502.11995v2",
    "entry_id": "http://arxiv.org/abs/2502.11995v2",
    "title": "Presumed Cultural Identity: How Names Shape LLM Responses",
    "summary": "Names are deeply tied to human identity. They can serve as markers of individuality, cultural heritage, and personal history. However, using names as a core indicator of identity can lead to over-simplification of complex identities. When interacting with LLMs, user names are an important point of information for personalisation. Names can enter chatbot conversations through direct user input (requested by chatbots), as part of task contexts such as CV reviews, or as built-in memory features that store user information for personalisation. We study biases associated with names by measuring cultural presumptions in the responses generated by LLMs when presented with common suggestion-seeking queries, which might involve making assumptions about the user. Our analyses demonstrate strong assumptions about cultural identity associated with names present in LLM generations across multiple cultures. Our work has implications for designing more nuanced personalisation systems that avoid reinforcing stereotypes while maintaining meaningful customisation.",
    "authors": [
      "Siddhesh Pawar",
      "Arnav Arora",
      "Lucie-Aimée Kaffee",
      "Isabelle Augenstein"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-17T16:35:15Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11995v2"
  },
  {
    "arxiv_id": "2502.11767v2",
    "entry_id": "http://arxiv.org/abs/2502.11767v2",
    "title": "From Selection to Generation: A Survey of LLM-based Active Learning",
    "summary": "Active Learning (AL) has been a powerful paradigm for improving model efficiency and performance by selecting the most informative data points for labeling and training. In recent active learning frameworks, Large Language Models (LLMs) have been employed not only for selection but also for generating entirely new data instances and providing more cost-effective annotations. Motivated by the increasing importance of high-quality data and efficient model training in the era of LLMs, we present a comprehensive survey on LLM-based Active Learning. We introduce an intuitive taxonomy that categorizes these techniques and discuss the transformative roles LLMs can play in the active learning loop. We further examine the impact of AL on LLM learning paradigms and its applications across various domains. Finally, we identify open challenges and propose future research directions. This survey aims to serve as an up-to-date resource for researchers and practitioners seeking to gain an intuitive understanding of LLM-based AL techniques and deploy them to new applications.",
    "authors": [
      "Yu Xia",
      "Subhojyoti Mukherjee",
      "Zhouhang Xie",
      "Junda Wu",
      "Xintong Li",
      "Ryan Aponte",
      "Hanjia Lyu",
      "Joe Barrow",
      "Hongjie Chen",
      "Franck Dernoncourt",
      "Branislav Kveton",
      "Tong Yu",
      "Ruiyi Zhang",
      "Jiuxiang Gu",
      "Nesreen K. Ahmed",
      "Yu Wang",
      "Xiang Chen",
      "Hanieh Deilamsalehy",
      "Sungchul Kim",
      "Zhengmian Hu",
      "Yue Zhao",
      "Nedim Lipka",
      "Seunghyun Yoon",
      "Ting-Hao Kenneth Huang",
      "Zichao Wang",
      "Puneet Mathur",
      "Soumyabrata Pal",
      "Koyel Mukherjee",
      "Zhehao Zhang",
      "Namyong Park",
      "Thien Huu Nguyen",
      "Jiebo Luo",
      "Ryan A. Rossi",
      "Julian McAuley"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2025-02-17T12:58:17Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11767v2"
  },
  {
    "arxiv_id": "2502.11736v3",
    "entry_id": "http://arxiv.org/abs/2502.11736v3",
    "title": "ReviewEval: An Evaluation Framework for AI-Generated Reviews",
    "summary": "The escalating volume of academic research, coupled with a shortage of qualified reviewers, necessitates innovative approaches to peer review. In this work, we propose: 1. ReviewEval, a comprehensive evaluation framework for AI-generated reviews that measures alignment with human assessments, verifies factual accuracy, assesses analytical depth, identifies degree of constructiveness and adherence to reviewer guidelines; and 2. ReviewAgent, an LLM-based review generation agent featuring a novel alignment mechanism to tailor feedback to target conferences and journals, along with a self-refinement loop that iteratively optimizes its intermediate outputs and an external improvement loop using ReviewEval to improve upon the final reviews. ReviewAgent improves actionable insights by 6.78% and 47.62% over existing AI baselines and expert reviews respectively. Further, it boosts analytical depth by 3.97% and 12.73%, enhances adherence to guidelines by 10.11% and 47.26% respectively. This paper establishes essential metrics for AIbased peer review and substantially enhances the reliability and impact of AI-generated reviews in academic research.",
    "authors": [
      "Madhav Krishan Garg",
      "Tejash Prasad",
      "Tanmay Singhal",
      "Chhavi Kirtani",
      "Murari Mandal",
      "Dhruv Kumar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-17T12:22:11Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11736v3"
  },
  {
    "arxiv_id": "2502.11588v1",
    "entry_id": "http://arxiv.org/abs/2502.11588v1",
    "title": "A Unified Modeling Framework for Automated Penetration Testing",
    "summary": "The integration of artificial intelligence into automated penetration testing (AutoPT) has highlighted the necessity of simulation modeling for the training of intelligent agents, due to its cost-efficiency and swift feedback capabilities. Despite the proliferation of AutoPT research, there is a recognized gap in the availability of a unified framework for simulation modeling methods. This paper presents a systematic review and synthesis of existing techniques, introducing MDCPM to categorize studies based on literature objectives, network simulation complexity, dependency of technical and tactical operations, and scenario feedback and variation. To bridge the gap in unified method for multi-dimensional and multi-level simulation modeling, dynamic environment modeling, and the scarcity of public datasets, we introduce AutoPT-Sim, a novel modeling framework that based on policy automation and encompasses the combination of all sub dimensions. AutoPT-Sim offers a comprehensive approach to modeling network environments, attackers, and defenders, transcending the constraints of static modeling and accommodating networks of diverse scales. We publicly release a generated standard network environment dataset and the code of Network Generator. By integrating publicly available datasets flexibly, support is offered for various simulation modeling levels focused on policy automation in MDCPM and the network generator help researchers output customized target network data by adjusting parameters or fine-tuning the network generator.",
    "authors": [
      "Yunfei Wang",
      "Shixuan Liu",
      "Wenhao Wang",
      "Changling Zhou",
      "Chao Zhang",
      "Jiandong Jin",
      "Cheng Zhu"
    ],
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "published": "2025-02-17T09:21:53Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11588v1"
  },
  {
    "arxiv_id": "2502.11560v1",
    "entry_id": "http://arxiv.org/abs/2502.11560v1",
    "title": "A Survey of Automatic Prompt Engineering: An Optimization Perspective",
    "summary": "The rise of foundation models has shifted focus from resource-intensive fine-tuning to prompt engineering, a paradigm that steers model behavior through input design rather than weight updates. While manual prompt engineering faces limitations in scalability, adaptability, and cross-modal alignment, automated methods, spanning foundation model (FM) based optimization, evolutionary methods, gradient-based optimization, and reinforcement learning, offer promising solutions. Existing surveys, however, remain fragmented across modalities and methodologies. This paper presents the first comprehensive survey on automated prompt engineering through a unified optimization-theoretic lens. We formalize prompt optimization as a maximization problem over discrete, continuous, and hybrid prompt spaces, systematically organizing methods by their optimization variables (instructions, soft prompts, exemplars), task-specific objectives, and computational frameworks. By bridging theoretical formulation with practical implementations across text, vision, and multimodal domains, this survey establishes a foundational framework for both researchers and practitioners, while highlighting underexplored frontiers in constrained optimization and agent-oriented prompt design.",
    "authors": [
      "Wenwu Li",
      "Xiangfeng Wang",
      "Wenhao Li",
      "Bo Jin"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-17T08:48:07Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11560v1"
  },
  {
    "arxiv_id": "2502.11528v2",
    "entry_id": "http://arxiv.org/abs/2502.11528v2",
    "title": "A Survey of Personalized Large Language Models: Progress and Future Directions",
    "summary": "Large Language Models (LLMs) excel in handling general knowledge tasks, yet they struggle with user-specific personalization, such as understanding individual emotions, writing styles, and preferences. Personalized Large Language Models (PLLMs) tackle these challenges by leveraging individual user data, such as user profiles, historical dialogues, content, and interactions, to deliver responses that are contextually relevant and tailored to each user's specific needs. This is a highly valuable research topic, as PLLMs can significantly enhance user satisfaction and have broad applications in conversational agents, recommendation systems, emotion recognition, medical assistants, and more. This survey reviews recent advancements in PLLMs from three technical perspectives: prompting for personalized context (input level), finetuning for personalized adapters (model level), and alignment for personalized preferences (objective level). To provide deeper insights, we also discuss current limitations and outline several promising directions for future research. Updated information about this survey can be found at the https://github.com/JiahongLiu21/Awesome-Personalized-Large-Language-Models.",
    "authors": [
      "Jiahong Liu",
      "Zexuan Qiu",
      "Zhongyang Li",
      "Quanyu Dai",
      "Wenhao Yu",
      "Jieming Zhu",
      "Minda Hu",
      "Menglin Yang",
      "Tat-Seng Chua",
      "Irwin King"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-02-17T07:58:31Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11528v2"
  },
  {
    "arxiv_id": "2502.11518v1",
    "entry_id": "http://arxiv.org/abs/2502.11518v1",
    "title": "Generative Multi-Agent Collaboration in Embodied AI: A Systematic Review",
    "summary": "Embodied multi-agent systems (EMAS) have attracted growing attention for their potential to address complex, real-world challenges in areas such as logistics and robotics. Recent advances in foundation models pave the way for generative agents capable of richer communication and adaptive problem-solving. This survey provides a systematic examination of how EMAS can benefit from these generative capabilities. We propose a taxonomy that categorizes EMAS by system architectures and embodiment modalities, emphasizing how collaboration spans both physical and virtual contexts. Central building blocks, perception, planning, communication, and feedback, are then analyzed to illustrate how generative techniques bolster system robustness and flexibility. Through concrete examples, we demonstrate the transformative effects of integrating foundation models into embodied, multi-agent frameworks. Finally, we discuss challenges and future directions, underlining the significant promise of EMAS to reshape the landscape of AI-driven collaboration.",
    "authors": [
      "Di Wu",
      "Xian Wei",
      "Guang Chen",
      "Hao Shen",
      "Xiangfeng Wang",
      "Wenhao Li",
      "Bo Jin"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-17T07:39:34Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11518v1"
  },
  {
    "arxiv_id": "2502.11508v1",
    "entry_id": "http://arxiv.org/abs/2502.11508v1",
    "title": "Chinese Spelling Correction: A Comprehensive Survey of Progress, Challenges, and Opportunities",
    "summary": "Chinese Spelling Correction (CSC) is a critical task in natural language processing, aimed at detecting and correcting spelling errors in Chinese text. This survey provides a comprehensive overview of CSC, tracing its evolution from pre-trained language models to large language models, and critically analyzing their respective strengths and weaknesses in this domain. Moreover, we further present a detailed examination of existing benchmark datasets, highlighting their inherent challenges and limitations. Finally, we propose promising future research directions, particularly focusing on leveraging the potential of LLMs and their reasoning capabilities for improved CSC performance. To the best of our knowledge, this is the first comprehensive survey dedicated to the field of CSC. We believe this work will serve as a valuable resource for researchers, fostering a deeper understanding of the field and inspiring future advancements.",
    "authors": [
      "Changchun Liu",
      "Kai Zhang",
      "Junzhe Jiang",
      "Zixiao Kong",
      "Qi Liu",
      "Enhong Chen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-17T07:17:27Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11508v1"
  },
  {
    "arxiv_id": "2502.11453v1",
    "entry_id": "http://arxiv.org/abs/2502.11453v1",
    "title": "Connector-S: A Survey of Connectors in Multi-modal Large Language Models",
    "summary": "With the rapid advancements in multi-modal large language models (MLLMs), connectors play a pivotal role in bridging diverse modalities and enhancing model performance. However, the design and evolution of connectors have not been comprehensively analyzed, leaving gaps in understanding how these components function and hindering the development of more powerful connectors. In this survey, we systematically review the current progress of connectors in MLLMs and present a structured taxonomy that categorizes connectors into atomic operations (mapping, compression, mixture of experts) and holistic designs (multi-layer, multi-encoder, multi-modal scenarios), highlighting their technical contributions and advancements. Furthermore, we discuss several promising research frontiers and challenges, including high-resolution input, dynamic compression, guide information selection, combination strategy, and interpretability. This survey is intended to serve as a foundational reference and a clear roadmap for researchers, providing valuable insights into the design and optimization of next-generation connectors to enhance the performance and adaptability of MLLMs.",
    "authors": [
      "Xun Zhu",
      "Zheng Zhang",
      "Xi Chen",
      "Yiming Shi",
      "Miao Li",
      "Ji Wu"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-17T05:28:04Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11453v1"
  },
  {
    "arxiv_id": "2502.11368v2",
    "entry_id": "http://arxiv.org/abs/2502.11368v2",
    "title": "LLMs can Perform Multi-Dimensional Analytic Writing Assessments: A Case Study of L2 Graduate-Level Academic English Writing",
    "summary": "The paper explores the performance of LLMs in the context of multi-dimensional analytic writing assessments, i.e. their ability to provide both scores and comments based on multiple assessment criteria. Using a corpus of literature reviews written by L2 graduate students and assessed by human experts against 9 analytic criteria, we prompt several popular LLMs to perform the same task under various conditions. To evaluate the quality of feedback comments, we apply a novel feedback comment quality evaluation framework. This framework is interpretable, cost-efficient, scalable, and reproducible, compared to existing methods that rely on manual judgments. We find that LLMs can generate reasonably good and generally reliable multi-dimensional analytic assessments. We release our corpus and code for reproducibility.",
    "authors": [
      "Zhengxiang Wang",
      "Veronika Makarova",
      "Zhi Li",
      "Jordan Kodner",
      "Owen Rambow"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-17T02:31:56Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11368v2"
  },
  {
    "arxiv_id": "2502.11221v3",
    "entry_id": "http://arxiv.org/abs/2502.11221v3",
    "title": "PlanGenLLMs: A Modern Survey of LLM Planning Capabilities",
    "summary": "LLMs have immense potential for generating plans, transforming an initial world state into a desired goal state. A large body of research has explored the use of LLMs for various planning tasks, from web navigation to travel planning and database querying. However, many of these systems are tailored to specific problems, making it challenging to compare them or determine the best approach for new tasks. There is also a lack of clear and consistent evaluation criteria. Our survey aims to offer a comprehensive overview of current LLM planners to fill this gap. It builds on foundational work by Kartam and Wilkins (1990) and examines six key performance criteria: completeness, executability, optimality, representation, generalization, and efficiency. For each, we provide a thorough analysis of representative works and highlight their strengths and weaknesses. Our paper also identifies crucial future directions, making it a valuable resource for both practitioners and newcomers interested in leveraging LLM planning to support agentic workflows.",
    "authors": [
      "Hui Wei",
      "Zihao Zhang",
      "Shenghua He",
      "Tian Xia",
      "Shijia Pan",
      "Fei Liu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-16T17:54:57Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11221v3"
  },
  {
    "arxiv_id": "2502.11211v2",
    "entry_id": "http://arxiv.org/abs/2502.11211v2",
    "title": "A Survey of LLM-based Agents in Medicine: How far are we from Baymax?",
    "summary": "Large Language Models (LLMs) are transforming healthcare through the development of LLM-based agents that can understand, reason about, and assist with medical tasks. This survey provides a comprehensive review of LLM-based agents in medicine, examining their architectures, applications, and challenges. We analyze the key components of medical agent systems, including system profiles, clinical planning mechanisms, medical reasoning frameworks, and external capacity enhancement. The survey covers major application scenarios such as clinical decision support, medical documentation, training simulations, and healthcare service optimization. We discuss evaluation frameworks and metrics used to assess these agents' performance in healthcare settings. While LLM-based agents show promise in enhancing healthcare delivery, several challenges remain, including hallucination management, multimodal integration, implementation barriers, and ethical considerations. The survey concludes by highlighting future research directions, including advances in medical reasoning inspired by recent developments in LLM architectures, integration with physical systems, and improvements in training simulations. This work provides researchers and practitioners with a structured overview of the current state and future prospects of LLM-based agents in medicine.",
    "authors": [
      "Wenxuan Wang",
      "Zizhan Ma",
      "Zheng Wang",
      "Chenghan Wu",
      "Jiaming Ji",
      "Wenting Chen",
      "Xiang Li",
      "Yixuan Yuan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-02-16T17:21:05Z",
    "pdf_url": "https://arxiv.org/pdf/2502.11211v2"
  },
  {
    "arxiv_id": "2502.15770v2",
    "entry_id": "http://arxiv.org/abs/2502.15770v2",
    "title": "Performance Review on LLM for solving leetcode problems",
    "summary": "This paper presents a comprehensive performance evaluation of Large Language Models (LLMs) in solving programming challenges from Leetcode, a widely used platform for algorithm practice and technical interviews. We began by crawling the Leetcode website to collect a diverse set of problems encompassing various difficulty levels and topics. Using this dataset, we generated solutions with multiple LLMs, including GPT-4 and GPT-3.5-turbo (ChatGPT-turbo). The generated solutions were systematically evaluated for correctness and efficiency. We employed the pass@k metric to assess the success rates within a given number of attempts and analyzed the runtime performance of the solutions. Our results highlight the strengths and limitations of current LLMs [10] in code generation and problem-solving tasks, providing insights into their potential applications and areas for improvement in automated programming assistance.",
    "authors": [
      "Lun Wang",
      "Chuanqi Shi",
      "Shaoshui Du",
      "Yiyi Tao",
      "Yixian Shen",
      "Hang Zheng",
      "Yanxin Shen",
      "Xinyu Qiu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-02-16T08:52:45Z",
    "pdf_url": "https://arxiv.org/pdf/2502.15770v2"
  },
  {
    "arxiv_id": "2502.10914v1",
    "entry_id": "http://arxiv.org/abs/2502.10914v1",
    "title": "LLM-driven Knowledge Distillation for Dynamic Text-Attributed Graphs",
    "summary": "Dynamic Text-Attributed Graphs (DyTAGs) have numerous real-world applications, e.g. social, collaboration, citation, communication, and review networks. In these networks, nodes and edges often contain text descriptions, and the graph structure can evolve over time. Future link prediction, edge classification, relation generation, and other downstream tasks on DyTAGs require powerful representations that encode structural, temporal, and textual information. Although graph neural networks (GNNs) excel at handling structured data, encoding temporal information within dynamic graphs remains a significant challenge. In this work, we propose LLM-driven Knowledge Distillation for Dynamic Text Attributed Graph (LKD4DyTAG) with temporal encoding to address these challenges. We use a simple, yet effective approach to encode temporal information in edges so that graph convolution can simultaneously capture both temporal and structural information in the hidden representations. To leverage LLM's text processing capabilities for learning richer representations on DyTAGs, we distill knowledge from LLM-driven edge representations (based on a neighborhood's text attributes) into saptio-temporal representations using a lightweight GNN model that encodes temporal and structural information. The objective of knowledge distillation enables the GNN to learn representations that more effectively encode the available structural, temporal, and textual information in DyTAG. We conducted extensive experimentation on six real-world DyTAG datasets to verify the effectiveness of our approach LKD4DyTAG for future link prediction and edge classification task. The results show that our approach significantly improves the performance of downstream tasks compared to the baseline models.",
    "authors": [
      "Amit Roy",
      "Ning Yan",
      "Masood Mortazavi"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-02-15T21:48:46Z",
    "pdf_url": "https://arxiv.org/pdf/2502.10914v1"
  },
  {
    "arxiv_id": "2502.10732v1",
    "entry_id": "http://arxiv.org/abs/2502.10732v1",
    "title": "Rule-Bottleneck Reinforcement Learning: Joint Explanation and Decision Optimization for Resource Allocation with Language Agents",
    "summary": "Deep Reinforcement Learning (RL) is remarkably effective in addressing sequential resource allocation problems in domains such as healthcare, public policy, and resource management. However, deep RL policies often lack transparency and adaptability, challenging their deployment alongside human decision-makers. In contrast, Language Agents, powered by large language models (LLMs), provide human-understandable reasoning but may struggle with effective decision making. To bridge this gap, we propose Rule-Bottleneck Reinforcement Learning (RBRL), a novel framework that jointly optimizes decision and explanations. At each step, RBRL generates candidate rules with an LLM, selects among them using an attention-based RL policy, and determines the environment action with an explanation via chain-of-thought reasoning. The RL rule selection is optimized using the environment rewards and an explainability metric judged by the LLM. Evaluations in real-world scenarios highlight RBRL's competitive performance with deep RL and efficiency gains over LLM fine-tuning. A survey further confirms the enhanced quality of its explanations.",
    "authors": [
      "Mauricio Tec",
      "Guojun Xiong",
      "Haichuan Wang",
      "Francesca Dominici",
      "Milind Tambe"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-15T09:01:31Z",
    "pdf_url": "https://arxiv.org/pdf/2502.10732v1"
  },
  {
    "arxiv_id": "2502.10525v1",
    "entry_id": "http://arxiv.org/abs/2502.10525v1",
    "title": "Towards Watermarking of Open-Source LLMs",
    "summary": "While watermarks for closed LLMs have matured and have been included in large-scale deployments, these methods are not applicable to open-source models, which allow users full control over the decoding process. This setting is understudied yet critical, given the rising performance of open-source models. In this work, we lay the foundation for systematic study of open-source LLM watermarking. For the first time, we explicitly formulate key requirements, including durability against common model modifications such as model merging, quantization, or finetuning, and propose a concrete evaluation setup. Given the prevalence of these modifications, durability is crucial for an open-source watermark to be effective. We survey and evaluate existing methods, showing that they are not durable. We also discuss potential ways to improve their durability and highlight remaining challenges. We hope our work enables future progress on this important problem.",
    "authors": [
      "Thibaud Gloaguen",
      "Nikola Jovanović",
      "Robin Staab",
      "Martin Vechev"
    ],
    "categories": [
      "cs.CR",
      "cs.LG"
    ],
    "published": "2025-02-14T19:41:23Z",
    "pdf_url": "https://arxiv.org/pdf/2502.10525v1"
  },
  {
    "arxiv_id": "2502.10303v1",
    "entry_id": "http://arxiv.org/abs/2502.10303v1",
    "title": "Reinforcement Learning in Strategy-Based and Atari Games: A Review of Google DeepMinds Innovations",
    "summary": "Reinforcement Learning (RL) has been widely used in many applications, particularly in gaming, which serves as an excellent training ground for AI models. Google DeepMind has pioneered innovations in this field, employing reinforcement learning algorithms, including model-based, model-free, and deep Q-network approaches, to create advanced AI models such as AlphaGo, AlphaGo Zero, and MuZero. AlphaGo, the initial model, integrates supervised learning and reinforcement learning to master the game of Go, surpassing professional human players. AlphaGo Zero refines this approach by eliminating reliance on human gameplay data, instead utilizing self-play for enhanced learning efficiency. MuZero further extends these advancements by learning the underlying dynamics of game environments without explicit knowledge of the rules, achieving adaptability across various games, including complex Atari games. This paper reviews the significance of reinforcement learning applications in Atari and strategy-based games, analyzing these three models, their key innovations, training processes, challenges encountered, and improvements made. Additionally, we discuss advancements in the field of gaming, including MiniZero and multi-agent models, highlighting future directions and emerging AI models from Google DeepMind.",
    "authors": [
      "Abdelrhman Shaheen",
      "Anas Badr",
      "Ali Abohendy",
      "Hatem Alsaadawy",
      "Nadine Alsayad"
    ],
    "categories": [
      "cs.AI",
      "cs.GT"
    ],
    "published": "2025-02-14T17:06:34Z",
    "pdf_url": "https://arxiv.org/pdf/2502.10303v1"
  },
  {
    "arxiv_id": "2503.05712v1",
    "entry_id": "http://arxiv.org/abs/2503.05712v1",
    "title": "Automatic Evaluation Metrics for Artificially Generated Scientific Research",
    "summary": "Foundation models are increasingly used in scientific research, but evaluating AI-generated scientific work remains challenging. While expert reviews are costly, large language models (LLMs) as proxy reviewers have proven to be unreliable. To address this, we investigate two automatic evaluation metrics, specifically citation count prediction and review score prediction. We parse all papers of OpenReview and augment each submission with its citation count, reference, and research hypothesis. Our findings reveal that citation count prediction is more viable than review score prediction, and predicting scores is more difficult purely from the research hypothesis than from the full paper. Furthermore, we show that a simple prediction model based solely on title and abstract outperforms LLM-based reviewers, though it still falls short of human-level consistency.",
    "authors": [
      "Niklas Höpner",
      "Leon Eshuijs",
      "Dimitrios Alivanistos",
      "Giacomo Zamprogno",
      "Ilaria Tiddi"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-14T14:56:14Z",
    "pdf_url": "https://arxiv.org/pdf/2503.05712v1"
  },
  {
    "arxiv_id": "2502.10050v1",
    "entry_id": "http://arxiv.org/abs/2502.10050v1",
    "title": "A Survey on LLM-powered Agents for Recommender Systems",
    "summary": "Recommender systems are essential components of many online platforms, yet traditional approaches still struggle with understanding complex user preferences and providing explainable recommendations. The emergence of Large Language Model (LLM)-powered agents offers a promising approach by enabling natural language interactions and interpretable reasoning, potentially transforming research in recommender systems. This survey provides a systematic review of the emerging applications of LLM-powered agents in recommender systems. We identify and analyze three key paradigms in current research: (1) Recommender-oriented approaches, which leverage intelligent agents to enhance the fundamental recommendation mechanisms; (2) Interaction-oriented approaches, which facilitate dynamic user engagement through natural dialogue and interpretable suggestions; and (3) Simulation-oriented approaches, which employ multi-agent frameworks to model complex user-item interactions and system dynamics. Beyond paradigm categorization, we analyze the architectural foundations of LLM-powered recommendation agents, examining their essential components: profile construction, memory management, strategic planning, and action execution. Our investigation extends to a comprehensive analysis of benchmark datasets and evaluation frameworks in this domain. This systematic examination not only illuminates the current state of LLM-powered agent recommender systems but also charts critical challenges and promising research directions in this transformative field.",
    "authors": [
      "Qiyao Peng",
      "Hongtao Liu",
      "Hua Huang",
      "Qing Yang",
      "Minglai Shao"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-02-14T09:57:07Z",
    "pdf_url": "https://arxiv.org/pdf/2502.10050v1"
  },
  {
    "arxiv_id": "2502.09797v2",
    "entry_id": "http://arxiv.org/abs/2502.09797v2",
    "title": "A Survey on LLM-based News Recommender Systems",
    "summary": "News recommender systems play a critical role in mitigating the information overload problem. In recent years, due to the successful applications of large language model technologies, researchers have utilized Discriminative Large Language Models (DLLMs) or Generative Large Language Models (GLLMs) to improve the performance of news recommender systems. Although several recent surveys review significant challenges for deep learning-based news recommender systems, such as fairness, privacy-preserving, and responsibility, there is a lack of a systematic survey on Large Language Model (LLM)-based news recommender systems. In order to review different core methodologies and explore potential issues systematically, we categorize DLLM-based and GLLM-based news recommender systems under the umbrella of LLM-based news recommender systems. In this survey, we first overview the development of deep learning-based news recommender systems. Then, we review LLM-based news recommender systems based on three aspects: news-oriented modeling, user-oriented modeling, and prediction-oriented modeling. Next, we examine the challenges from various perspectives, including datasets, benchmarking tools, and methodologies. Furthermore, we conduct extensive experiments to analyze how large language model technologies affect the performance of different news recommender systems. Finally, we comprehensively explore the future directions for LLM-based news recommendations in the era of LLMs.",
    "authors": [
      "Rongyao Wang",
      "Veronica Liesaputra",
      "Zhiyi Huang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-02-13T22:13:59Z",
    "pdf_url": "https://arxiv.org/pdf/2502.09797v2"
  },
  {
    "arxiv_id": "2502.09242v1",
    "entry_id": "http://arxiv.org/abs/2502.09242v1",
    "title": "From large language models to multimodal AI: A scoping review on the potential of generative AI in medicine",
    "summary": "Generative artificial intelligence (AI) models, such as diffusion models and OpenAI's ChatGPT, are transforming medicine by enhancing diagnostic accuracy and automating clinical workflows. The field has advanced rapidly, evolving from text-only large language models for tasks such as clinical documentation and decision support to multimodal AI systems capable of integrating diverse data modalities, including imaging, text, and structured data, within a single model. The diverse landscape of these technologies, along with rising interest, highlights the need for a comprehensive review of their applications and potential. This scoping review explores the evolution of multimodal AI, highlighting its methods, applications, datasets, and evaluation in clinical settings. Adhering to PRISMA-ScR guidelines, we systematically queried PubMed, IEEE Xplore, and Web of Science, prioritizing recent studies published up to the end of 2024. After rigorous screening, 144 papers were included, revealing key trends and challenges in this dynamic field. Our findings underscore a shift from unimodal to multimodal approaches, driving innovations in diagnostic support, medical report generation, drug discovery, and conversational AI. However, critical challenges remain, including the integration of heterogeneous data types, improving model interpretability, addressing ethical concerns, and validating AI systems in real-world clinical settings. This review summarizes the current state of the art, identifies critical gaps, and provides insights to guide the development of scalable, trustworthy, and clinically impactful multimodal AI solutions in healthcare.",
    "authors": [
      "Lukas Buess",
      "Matthias Keicher",
      "Nassir Navab",
      "Andreas Maier",
      "Soroosh Tayebi Arasteh"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-02-13T11:57:51Z",
    "pdf_url": "https://arxiv.org/pdf/2502.09242v1"
  },
  {
    "arxiv_id": "2502.09104v1",
    "entry_id": "http://arxiv.org/abs/2502.09104v1",
    "title": "One-shot Federated Learning Methods: A Practical Guide",
    "summary": "One-shot Federated Learning (OFL) is a distributed machine learning paradigm that constrains client-server communication to a single round, addressing privacy and communication overhead issues associated with multiple rounds of data exchange in traditional Federated Learning (FL). OFL demonstrates the practical potential for integration with future approaches that require collaborative training models, such as large language models (LLMs). However, current OFL methods face two major challenges: data heterogeneity and model heterogeneity, which result in subpar performance compared to conventional FL methods. Worse still, despite numerous studies addressing these limitations, a comprehensive summary is still lacking. To address these gaps, this paper presents a systematic analysis of the challenges faced by OFL and thoroughly reviews the current methods. We also offer an innovative categorization method and analyze the trade-offs of various techniques. Additionally, we discuss the most promising future directions and the technologies that should be integrated into the OFL field. This work aims to provide guidance and insights for future research.",
    "authors": [
      "Xiang Liu",
      "Zhenheng Tang",
      "Xia Li",
      "Yijun Song",
      "Sijie Ji",
      "Zemin Liu",
      "Bo Han",
      "Linshan Jiang",
      "Jialin Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-13T09:26:44Z",
    "pdf_url": "https://arxiv.org/pdf/2502.09104v1"
  },
  {
    "arxiv_id": "2502.09100v1",
    "entry_id": "http://arxiv.org/abs/2502.09100v1",
    "title": "Logical Reasoning in Large Language Models: A Survey",
    "summary": "With the emergence of advanced reasoning models like OpenAI o3 and DeepSeek-R1, large language models (LLMs) have demonstrated remarkable reasoning capabilities. However, their ability to perform rigorous logical reasoning remains an open question. This survey synthesizes recent advancements in logical reasoning within LLMs, a critical area of AI research. It outlines the scope of logical reasoning in LLMs, its theoretical foundations, and the benchmarks used to evaluate reasoning proficiency. We analyze existing capabilities across different reasoning paradigms - deductive, inductive, abductive, and analogical - and assess strategies to enhance reasoning performance, including data-centric tuning, reinforcement learning, decoding strategies, and neuro-symbolic approaches. The review concludes with future directions, emphasizing the need for further exploration to strengthen logical reasoning in AI systems.",
    "authors": [
      "Hanmeng Liu",
      "Zhizhang Fu",
      "Mengru Ding",
      "Ruoxi Ning",
      "Chaoli Zhang",
      "Xiaozhang Liu",
      "Yue Zhang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-13T09:19:14Z",
    "pdf_url": "https://arxiv.org/pdf/2502.09100v1"
  },
  {
    "arxiv_id": "2502.09053v2",
    "entry_id": "http://arxiv.org/abs/2502.09053v2",
    "title": "Game Theory Meets Large Language Models: A Systematic Survey with Taxonomy and New Frontiers",
    "summary": "Game theory is a foundational framework for analyzing strategic interactions, and its intersection with large language models (LLMs) is a rapidly growing field. However, existing surveys mainly focus narrowly on using game theory to evaluate LLM behavior. This paper provides the first comprehensive survey of the bidirectional relationship between Game Theory and LLMs. We propose a novel taxonomy that categorizes the research in this intersection into four distinct perspectives: (1) evaluating LLMs in game-based scenarios; (2) improving LLMs using game-theoretic concepts for better interpretability and alignment; (3) modeling the competitive landscape of LLM development and its societal impact; and (4) leveraging LLMs to advance game models and to solve corresponding game theory problems. Furthermore, we identify key challenges and outline future research directions. By systematically investigating this interdisciplinary landscape, our survey highlights the mutual influence of game theory and LLMs, fostering progress at the intersection of these fields.",
    "authors": [
      "Haoran Sun",
      "Yusen Wu",
      "Peng Wang",
      "Wei Chen",
      "Yukun Cheng",
      "Xiaotie Deng",
      "Xu Chu"
    ],
    "categories": [
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "published": "2025-02-13T08:08:27Z",
    "pdf_url": "https://arxiv.org/pdf/2502.09053v2"
  },
  {
    "arxiv_id": "2502.08869v2",
    "entry_id": "http://arxiv.org/abs/2502.08869v2",
    "title": "Harnessing Vision Models for Time Series Analysis: A Survey",
    "summary": "Time series analysis has witnessed the inspiring development from traditional autoregressive models, deep learning models, to recent Transformers and Large Language Models (LLMs). Efforts in leveraging vision models for time series analysis have also been made along the way but are less visible to the community due to the predominant research on sequence modeling in this domain. However, the discrepancy between continuous time series and the discrete token space of LLMs, and the challenges in explicitly modeling the correlations of variates in multivariate time series have shifted some research attentions to the equally successful Large Vision Models (LVMs) and Vision Language Models (VLMs). To fill the blank in the existing literature, this survey discusses the advantages of vision models over LLMs in time series analysis. It provides a comprehensive and in-depth overview of the existing methods, with dual views of detailed taxonomy that answer the key research questions including how to encode time series as images and how to model the imaged time series for various tasks. Additionally, we address the challenges in the pre- and post-processing steps involved in this framework and outline future directions to further advance time series analysis with vision models.",
    "authors": [
      "Jingchao Ni",
      "Ziming Zhao",
      "ChengAo Shen",
      "Hanghang Tong",
      "Dongjin Song",
      "Wei Cheng",
      "Dongsheng Luo",
      "Haifeng Chen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-02-13T00:42:11Z",
    "pdf_url": "https://arxiv.org/pdf/2502.08869v2"
  },
  {
    "arxiv_id": "2502.09670v1",
    "entry_id": "http://arxiv.org/abs/2502.09670v1",
    "title": "The Science of Evaluating Foundation Models",
    "summary": "The emergent phenomena of large foundation models have revolutionized natural language processing. However, evaluating these models presents significant challenges due to their size, capabilities, and deployment across diverse applications. Existing literature often focuses on individual aspects, such as benchmark performance or specific tasks, but fails to provide a cohesive process that integrates the nuances of diverse use cases with broader ethical and operational considerations. This work focuses on three key aspects: (1) Formalizing the Evaluation Process by providing a structured framework tailored to specific use-case contexts, (2) Offering Actionable Tools and Frameworks such as checklists and templates to ensure thorough, reproducible, and practical evaluations, and (3) Surveying Recent Work with a targeted review of advancements in LLM evaluation, emphasizing real-world applications.",
    "authors": [
      "Jiayi Yuan",
      "Jiamu Zhang",
      "Andrew Wen",
      "Xia Hu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-12T22:55:43Z",
    "pdf_url": "https://arxiv.org/pdf/2502.09670v1"
  },
  {
    "arxiv_id": "2502.08826v3",
    "entry_id": "http://arxiv.org/abs/2502.08826v3",
    "title": "Ask in Any Modality: A Comprehensive Survey on Multimodal Retrieval-Augmented Generation",
    "summary": "Large Language Models (LLMs) suffer from hallucinations and outdated knowledge due to their reliance on static training data. Retrieval-Augmented Generation (RAG) mitigates these issues by integrating external dynamic information for improved factual grounding. With advances in multimodal learning, Multimodal RAG extends this approach by incorporating multiple modalities such as text, images, audio, and video to enhance the generated outputs. However, cross-modal alignment and reasoning introduce unique challenges beyond those in unimodal RAG. This survey offers a structured and comprehensive analysis of Multimodal RAG systems, covering datasets, benchmarks, metrics, evaluation, methodologies, and innovations in retrieval, fusion, augmentation, and generation. We review training strategies, robustness enhancements, loss functions, and agent-based approaches, while also exploring the diverse Multimodal RAG scenarios. In addition, we outline open challenges and future directions to guide research in this evolving field. This survey lays the foundation for developing more capable and reliable AI systems that effectively leverage multimodal dynamic external knowledge bases. All resources are publicly available at https://github.com/llm-lab-org/Multimodal-RAG-Survey.",
    "authors": [
      "Mohammad Mahdi Abootorabi",
      "Amirhosein Zobeiri",
      "Mahdi Dehghani",
      "Mohammadali Mohammadkhani",
      "Bardia Mohammadi",
      "Omid Ghahroodi",
      "Mahdieh Soleymani Baghshah",
      "Ehsaneddin Asgari"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-02-12T22:33:41Z",
    "pdf_url": "https://arxiv.org/pdf/2502.08826v3"
  },
  {
    "arxiv_id": "2502.08576v2",
    "entry_id": "http://arxiv.org/abs/2502.08576v2",
    "title": "Mapping the Landscape of Generative AI in Network Monitoring and Management",
    "summary": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management.",
    "authors": [
      "Giampaolo Bovenzi",
      "Francesco Cerasuolo",
      "Domenico Ciuonzo",
      "Davide Di Monda",
      "Idio Guarino",
      "Antonio Montieri",
      "Valerio Persico",
      "Antonio Pescapè"
    ],
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-12T17:10:34Z",
    "pdf_url": "https://arxiv.org/pdf/2502.08576v2"
  },
  {
    "arxiv_id": "2502.08556v1",
    "entry_id": "http://arxiv.org/abs/2502.08556v1",
    "title": "Human-Centric Foundation Models: Perception, Generation and Agentic Modeling",
    "summary": "Human understanding and generation are critical for modeling digital humans and humanoid embodiments. Recently, Human-centric Foundation Models (HcFMs) inspired by the success of generalist models, such as large language and vision models, have emerged to unify diverse human-centric tasks into a single framework, surpassing traditional task-specific approaches. In this survey, we present a comprehensive overview of HcFMs by proposing a taxonomy that categorizes current approaches into four groups: (1) Human-centric Perception Foundation Models that capture fine-grained features for multi-modal 2D and 3D understanding. (2) Human-centric AIGC Foundation Models that generate high-fidelity, diverse human-related content. (3) Unified Perception and Generation Models that integrate these capabilities to enhance both human understanding and synthesis. (4) Human-centric Agentic Foundation Models that extend beyond perception and generation to learn human-like intelligence and interactive behaviors for humanoid embodied tasks. We review state-of-the-art techniques, discuss emerging challenges and future research directions. This survey aims to serve as a roadmap for researchers and practitioners working towards more robust, versatile, and intelligent digital human and embodiments modeling.",
    "authors": [
      "Shixiang Tang",
      "Yizhou Wang",
      "Lu Chen",
      "Yuan Wang",
      "Sida Peng",
      "Dan Xu",
      "Wanli Ouyang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2025-02-12T16:38:40Z",
    "pdf_url": "https://arxiv.org/pdf/2502.08556v1"
  },
  {
    "arxiv_id": "2502.08691v1",
    "entry_id": "http://arxiv.org/abs/2502.08691v1",
    "title": "AgentSociety: Large-Scale Simulation of LLM-Driven Generative Agents Advances Understanding of Human Behaviors and Society",
    "summary": "Understanding human behavior and society is a central focus in social sciences, with the rise of generative social science marking a significant paradigmatic shift. By leveraging bottom-up simulations, it replaces costly and logistically challenging traditional experiments with scalable, replicable, and systematic computational approaches for studying complex social dynamics. Recent advances in large language models (LLMs) have further transformed this research paradigm, enabling the creation of human-like generative social agents and realistic simulacra of society. In this paper, we propose AgentSociety, a large-scale social simulator that integrates LLM-driven agents, a realistic societal environment, and a powerful large-scale simulation engine. Based on the proposed simulator, we generate social lives for over 10k agents, simulating their 5 million interactions both among agents and between agents and their environment. Furthermore, we explore the potential of AgentSociety as a testbed for computational social experiments, focusing on four key social issues: polarization, the spread of inflammatory messages, the effects of universal basic income policies, and the impact of external shocks such as hurricanes. These four issues serve as valuable cases for assessing AgentSociety's support for typical research methods -- such as surveys, interviews, and interventions -- as well as for investigating the patterns, causes, and underlying mechanisms of social issues. The alignment between AgentSociety's outcomes and real-world experimental results not only demonstrates its ability to capture human behaviors and their underlying mechanisms, but also underscores its potential as an important platform for social scientists and policymakers.",
    "authors": [
      "Jinghua Piao",
      "Yuwei Yan",
      "Jun Zhang",
      "Nian Li",
      "Junbo Yan",
      "Xiaochong Lan",
      "Zhihong Lu",
      "Zhiheng Zheng",
      "Jing Yi Wang",
      "Di Zhou",
      "Chen Gao",
      "Fengli Xu",
      "Fang Zhang",
      "Ke Rong",
      "Jun Su",
      "Yong Li"
    ],
    "categories": [
      "cs.SI",
      "cs.AI"
    ],
    "published": "2025-02-12T15:27:07Z",
    "pdf_url": "https://arxiv.org/pdf/2502.08691v1"
  },
  {
    "arxiv_id": "2502.08353v1",
    "entry_id": "http://arxiv.org/abs/2502.08353v1",
    "title": "Trustworthy GNNs with LLMs: A Systematic Review and Taxonomy",
    "summary": "With the extensive application of Graph Neural Networks (GNNs) across various domains, their trustworthiness has emerged as a focal point of research. Some existing studies have shown that the integration of large language models (LLMs) can improve the semantic understanding and generation capabilities of GNNs, which in turn improves the trustworthiness of GNNs from various aspects. Our review introduces a taxonomy that offers researchers a clear framework for comprehending the principles and applications of different methods and helps clarify the connections and differences among various approaches. Then we systematically survey representative approaches along the four categories of our taxonomy. Through our taxonomy, researchers can understand the applicable scenarios, potential advantages, and limitations of each approach for the the trusted integration of GNNs with LLMs. Finally, we present some promising directions of work and future trends for the integration of LLMs and GNNs to improve model trustworthiness.",
    "authors": [
      "Ruizhan Xue",
      "Huimin Deng",
      "Fang He",
      "Maojun Wang",
      "Zeyu Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-12T12:28:39Z",
    "pdf_url": "https://arxiv.org/pdf/2502.08353v1"
  },
  {
    "arxiv_id": "2502.08346v3",
    "entry_id": "http://arxiv.org/abs/2502.08346v3",
    "title": "Graph Foundation Models for Recommendation: A Comprehensive Survey",
    "summary": "Recommender systems (RS) serve as a fundamental tool for navigating the vast expanse of online information, with deep learning advancements playing an increasingly important role in improving ranking accuracy. Among these, graph neural networks (GNNs) excel at extracting higher-order structural information, while large language models (LLMs) are designed to process and comprehend natural language, making both approaches highly effective and widely adopted. Recent research has focused on graph foundation models (GFMs), which integrate the strengths of GNNs and LLMs to model complex RS problems more efficiently by leveraging the graph-based structure of user-item relationships alongside textual understanding. In this survey, we provide a comprehensive overview of GFM-based RS technologies by introducing a clear taxonomy of current approaches, diving into methodological details, and highlighting key challenges and future directions. By synthesizing recent advancements, we aim to offer valuable insights into the evolving landscape of GFM-based recommender systems.",
    "authors": [
      "Bin Wu",
      "Yihang Wang",
      "Yuanhao Zeng",
      "Jiawei Liu",
      "Jiashu Zhao",
      "Cheng Yang",
      "Yawen Li",
      "Long Xia",
      "Dawei Yin",
      "Chuan Shi"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-12T12:13:51Z",
    "pdf_url": "https://arxiv.org/pdf/2502.08346v3"
  },
  {
    "arxiv_id": "2502.08045v3",
    "entry_id": "http://arxiv.org/abs/2502.08045v3",
    "title": "Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs",
    "summary": "A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs.",
    "authors": [
      "Mohsinul Kabir",
      "Ajwad Abrar",
      "Sophia Ananiadou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-02-12T01:04:13Z",
    "pdf_url": "https://arxiv.org/pdf/2502.08045v3"
  },
  {
    "arxiv_id": "2502.07978v1",
    "entry_id": "http://arxiv.org/abs/2502.07978v1",
    "title": "A Survey of In-Context Reinforcement Learning",
    "summary": "Reinforcement learning (RL) agents typically optimize their policies by performing expensive backward passes to update their network parameters. However, some agents can solve new tasks without updating any parameters by simply conditioning on additional context such as their action-observation histories. This paper surveys work on such behavior, known as in-context reinforcement learning.",
    "authors": [
      "Amir Moeini",
      "Jiuqi Wang",
      "Jacob Beck",
      "Ethan Blaser",
      "Shimon Whiteson",
      "Rohan Chandra",
      "Shangtong Zhang"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-02-11T21:52:19Z",
    "pdf_url": "https://arxiv.org/pdf/2502.07978v1"
  },
  {
    "arxiv_id": "2502.10450v2",
    "entry_id": "http://arxiv.org/abs/2502.10450v2",
    "title": "Trustworthy AI: Safety, Bias, and Privacy -- A Survey",
    "summary": "The capabilities of artificial intelligence systems have been advancing to a great extent, but these systems still struggle with failure modes, vulnerabilities, and biases. In this paper, we study the current state of the field, and present promising insights and perspectives regarding concerns that challenge the trustworthiness of AI models. In particular, this paper investigates the issues regarding three thrusts: safety, privacy, and bias, which hurt models' trustworthiness. For safety, we discuss safety alignment in the context of large language models, preventing them from generating toxic or harmful content. For bias, we focus on spurious biases that can mislead a network. Lastly, for privacy, we cover membership inference attacks in deep neural networks. The discussions addressed in this paper reflect our own experiments and observations.",
    "authors": [
      "Xingli Fang",
      "Jianwei Li",
      "Varun Mulchandani",
      "Jung-Eun Kim"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-02-11T20:08:42Z",
    "pdf_url": "https://arxiv.org/pdf/2502.10450v2"
  },
  {
    "arxiv_id": "2502.07855v2",
    "entry_id": "http://arxiv.org/abs/2502.07855v2",
    "title": "Vision-Language Models for Edge Networks: A Comprehensive Survey",
    "summary": "Vision Large Language Models (VLMs) combine visual understanding with natural language processing, enabling tasks like image captioning, visual question answering, and video analysis. While VLMs show impressive capabilities across domains such as autonomous vehicles, smart surveillance, and healthcare, their deployment on resource-constrained edge devices remains challenging due to processing power, memory, and energy limitations. This survey explores recent advancements in optimizing VLMs for edge environments, focusing on model compression techniques, including pruning, quantization, knowledge distillation, and specialized hardware solutions that enhance efficiency. We provide a detailed discussion of efficient training and fine-tuning methods, edge deployment challenges, and privacy considerations. Additionally, we discuss the diverse applications of lightweight VLMs across healthcare, environmental monitoring, and autonomous systems, illustrating their growing impact. By highlighting key design strategies, current challenges, and offering recommendations for future directions, this survey aims to inspire further research into the practical deployment of VLMs, ultimately making advanced AI accessible in resource-limited settings.",
    "authors": [
      "Ahmed Sharshar",
      "Latif U. Khan",
      "Waseem Ullah",
      "Mohsen Guizani"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-11T14:04:43Z",
    "pdf_url": "https://arxiv.org/pdf/2502.07855v2"
  },
  {
    "arxiv_id": "2502.07542v2",
    "entry_id": "http://arxiv.org/abs/2502.07542v2",
    "title": "Exoplanet Transit Candidate Identification in TESS Full-Frame Images via a Transformer-Based Algorithm",
    "summary": "The Transiting Exoplanet Survey Satellite (TESS) is surveying a large fraction of the sky, generating a vast database of photometric time series data that requires thorough analysis to identify exoplanetary transit signals. Automated learning approaches have been successfully applied to identify transit signals. However, most existing methods focus on the classification and validation of candidates, while few efforts have explored new techniques for the search of candidates. To search for new exoplanet transit candidates, we propose an approach to identify exoplanet transit signals without the need for phase folding or assuming periodicity in the transit signals, such as those observed in multi-transit light curves. To achieve this, we implement a new neural network inspired by Transformers to directly process Full Frame Image (FFI) light curves to detect exoplanet transits. Transformers, originally developed for natural language processing, have recently demonstrated significant success in capturing long-range dependencies compared to previous approaches focused on sequential data. This ability allows us to employ multi-head self-attention to identify exoplanet transit signals directly from the complete light curves, combined with background and centroid time series, without requiring prior transit parameters. The network is trained to learn characteristics of the transit signal, like the dip shape, which helps distinguish planetary transits from other variability sources. Our model successfully identified 214 new planetary system candidates, including 122 multi-transit light curves, 88 single-transit and 4 multi-planet systems from TESS sectors 1-26 with a radius > 0.27 $R_{\\mathrm{Jupiter}}$, demonstrating its ability to detect transits regardless of their periodicity.",
    "authors": [
      "Helem Salinas",
      "Rafael Brahm",
      "Greg Olmschenk",
      "Richard K. Barry",
      "Karim Pichara",
      "Stela Ishitani Silva",
      "Vladimir Araujo"
    ],
    "categories": [
      "astro-ph.EP",
      "astro-ph.GA",
      "astro-ph.IM",
      "cs.AI"
    ],
    "published": "2025-02-11T13:29:58Z",
    "pdf_url": "https://arxiv.org/pdf/2502.07542v2"
  },
  {
    "arxiv_id": "2502.07254v2",
    "entry_id": "http://arxiv.org/abs/2502.07254v2",
    "title": "Fairness in Agentic AI: A Unified Framework for Ethical and Equitable Multi-Agent System",
    "summary": "Ensuring fairness in decentralized multi-agent systems presents significant challenges due to emergent biases, systemic inefficiencies, and conflicting agent incentives. This paper provides a comprehensive survey of fairness in multi-agent AI, introducing a novel framework where fairness is treated as a dynamic, emergent property of agent interactions. The framework integrates fairness constraints, bias mitigation strategies, and incentive mechanisms to align autonomous agent behaviors with societal values while balancing efficiency and robustness. Through empirical validation, we demonstrate that incorporating fairness constraints results in more equitable decision-making. This work bridges the gap between AI ethics and system design, offering a foundation for accountable, transparent, and socially responsible multi-agent AI systems.",
    "authors": [
      "Rajesh Ranjan",
      "Shailja Gupta",
      "Surya Narayan Singh"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-02-11T04:42:00Z",
    "pdf_url": "https://arxiv.org/pdf/2502.07254v2"
  },
  {
    "arxiv_id": "2502.07049v2",
    "entry_id": "http://arxiv.org/abs/2502.07049v2",
    "title": "LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights",
    "summary": "Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection, addressing critical challenges in the security domain. Traditional methods, such as static and dynamic analysis, often falter due to inefficiencies, high false positive rates, and the growing complexity of modern software systems. By leveraging their ability to analyze code structures, identify patterns, and generate repair suggestions, LLMs, exemplified by models like GPT, BERT, and CodeBERT, present a novel and scalable approach to mitigating vulnerabilities. This paper provides a detailed survey of LLMs in vulnerability detection. It examines key aspects, including model architectures, application methods, target languages, fine-tuning strategies, datasets, and evaluation metrics. We also analyze the scope of current research problems, highlighting the strengths and weaknesses of existing approaches. Further, we address challenges such as cross-language vulnerability detection, multimodal data integration, and repository-level analysis. Based on these findings, we propose solutions for issues like dataset scalability, model interpretability, and applications in low-resource scenarios. Our contributions are threefold: (1) a systematic review of how LLMs are applied in vulnerability detection; (2) an analysis of shared patterns and differences across studies, with a unified framework for understanding the field; and (3) a summary of key challenges and future research directions. This work provides valuable insights for advancing LLM-based vulnerability detection. We also maintain and regularly update latest selected paper on https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection",
    "authors": [
      "Ze Sheng",
      "Zhicheng Chen",
      "Shuning Gu",
      "Heqing Huang",
      "Guofei Gu",
      "Jeff Huang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-02-10T21:33:38Z",
    "pdf_url": "https://arxiv.org/pdf/2502.07049v2"
  },
  {
    "arxiv_id": "2502.07045v2",
    "entry_id": "http://arxiv.org/abs/2502.07045v2",
    "title": "Scalable and Ethical Insider Threat Detection through Data Synthesis and Analysis by LLMs",
    "summary": "Insider threats wield an outsized influence on organizations, disproportionate to their small numbers. This is due to the internal access insiders have to systems, information, and infrastructure. %One example of this influence is where anonymous respondents submit web-based job search site reviews, an insider threat risk to organizations. Signals for such risks may be found in anonymous submissions to public web-based job search site reviews. This research studies the potential for large language models (LLMs) to analyze and detect insider threat sentiment within job site reviews. Addressing ethical data collection concerns, this research utilizes synthetic data generation using LLMs alongside existing job review datasets. A comparative analysis of sentiment scores generated by LLMs is benchmarked against expert human scoring. Findings reveal that LLMs demonstrate alignment with human evaluations in most cases, thus effectively identifying nuanced indicators of threat sentiment. The performance is lower on human-generated data than synthetic data, suggesting areas for improvement in evaluating real-world data. Text diversity analysis found differences between human-generated and LLM-generated datasets, with synthetic data exhibiting somewhat lower diversity. Overall, the results demonstrate the applicability of LLMs to insider threat detection, and a scalable solution for insider sentiment testing by overcoming ethical and logistical barriers tied to data acquisition.",
    "authors": [
      "Haywood Gelman",
      "John D. Hastings"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-02-10T21:27:06Z",
    "pdf_url": "https://arxiv.org/pdf/2502.07045v2"
  },
  {
    "arxiv_id": "2502.07017v1",
    "entry_id": "http://arxiv.org/abs/2502.07017v1",
    "title": "Finding Words Associated with DIF: Predicting Differential Item Functioning using LLMs and Explainable AI",
    "summary": "We fine-tuned and compared several encoder-based Transformer large language models (LLM) to predict differential item functioning (DIF) from the item text. We then applied explainable artificial intelligence (XAI) methods to these models to identify specific words associated with DIF. The data included 42,180 items designed for English language arts and mathematics summative state assessments among students in grades 3 to 11. Prediction $R^2$ ranged from .04 to .32 among eight focal and reference group pairs. Our findings suggest that many words associated with DIF reflect minor sub-domains included in the test blueprint by design, rather than construct-irrelevant item content that should be removed from assessments. This may explain why qualitative reviews of DIF items often yield confusing or inconclusive results. Our approach can be used to screen words associated with DIF during the item-writing process for immediate revision, or help review traditional DIF analysis results by highlighting key words in the text. Extensions of this research can enhance the fairness of assessment programs, especially those that lack resources to build high-quality items, and among smaller subpopulations where we do not have sufficient sample sizes for traditional DIF analyses.",
    "authors": [
      "Hotaka Maeda",
      "Yikai Lu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-10T20:22:32Z",
    "pdf_url": "https://arxiv.org/pdf/2502.07017v1"
  },
  {
    "arxiv_id": "2502.06963v2",
    "entry_id": "http://arxiv.org/abs/2502.06963v2",
    "title": "Intelligent Offloading in Vehicular Edge Computing: A Comprehensive Review of Deep Reinforcement Learning Approaches and Architectures",
    "summary": "The increasing complexity of Intelligent Transportation Systems (ITS) has led to significant interest in computational offloading to external infrastructures such as edge servers, vehicular nodes, and UAVs. These dynamic and heterogeneous environments pose challenges for traditional offloading strategies, prompting the exploration of Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) as adaptive decision-making frameworks. This survey presents a comprehensive review of recent advances in DRL-based offloading for vehicular edge computing (VEC). We classify and compare existing works based on learning paradigms (e.g., single-agent, multi-agent), system architectures (e.g., centralized, distributed, hierarchical), and optimization objectives (e.g., latency, energy, fairness). Furthermore, we analyze how Markov Decision Process (MDP) formulations are applied and highlight emerging trends in reward design, coordination mechanisms, and scalability. Finally, we identify open challenges and outline future research directions to guide the development of robust and intelligent offloading strategies for next-generation ITS.",
    "authors": [
      "Ashab Uddin",
      "Ahmed Hamdi Sakr",
      "Ning Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.MA"
    ],
    "published": "2025-02-10T19:02:20Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06963v2"
  },
  {
    "arxiv_id": "2502.06633v1",
    "entry_id": "http://arxiv.org/abs/2502.06633v1",
    "title": "Combining Large Language Models with Static Analyzers for Code Review Generation",
    "summary": "Code review is a crucial but often complex, subjective, and time-consuming activity in software development. Over the past decades, significant efforts have been made to automate this process. Early approaches focused on knowledge-based systems (KBS) that apply rule-based mechanisms to detect code issues, providing precise feedback but struggling with complex, context-dependent cases. More recent work has shifted toward fine-tuning pre-trained language models for code review, enabling broader issue coverage but often at the expense of precision. In this paper, we propose a hybrid approach that combines the strengths of KBS and learning-based systems (LBS) to generate high-quality, comprehensive code reviews. Our method integrates knowledge at three distinct stages of the language model pipeline: during data preparation (Data-Augmented Training, DAT), at inference (Retrieval-Augmented Generation, RAG), and after inference (Naive Concatenation of Outputs, NCO). We empirically evaluate our combination strategies against standalone KBS and LBS fine-tuned on a real-world dataset. Our results show that these hybrid strategies enhance the relevance, completeness, and overall quality of review comments, effectively bridging the gap between rule-based tools and deep learning models.",
    "authors": [
      "Imen Jaoua",
      "Oussama Ben Sghaier",
      "Houari Sahraoui"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-02-10T16:29:12Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06633v1"
  },
  {
    "arxiv_id": "2502.06581v4",
    "entry_id": "http://arxiv.org/abs/2502.06581v4",
    "title": "A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems",
    "summary": "The explosive growth of video data has driven the development of distributed video analytics in cloud-edge-terminal collaborative (CETC) systems, enabling efficient video processing, real-time inference, and privacy-preserving analysis. Among multiple advantages, CETC systems can distribute video processing tasks and enable adaptive analytics across cloud, edge, and terminal devices, leading to breakthroughs in video surveillance, autonomous driving, and smart cities. In this survey, we first analyze fundamental architectural components, including hierarchical, distributed, and hybrid frameworks, alongside edge computing platforms and resource management mechanisms. Building upon these foundations, edge-centric approaches emphasize on-device processing, edge-assisted offloading, and edge intelligence, while cloud-centric methods leverage powerful computational capabilities for complex video understanding and model training. Our investigation also covers hybrid video analytics incorporating adaptive task offloading and resource-aware scheduling techniques that optimize performance across the entire system. Beyond conventional approaches, recent advances in large language models and multimodal integration reveal both opportunities and challenges in platform scalability, data protection, and system reliability. Future directions also encompass explainable systems, efficient processing mechanisms, and advanced video analytics, offering valuable insights for researchers and practitioners in this dynamic field.",
    "authors": [
      "Linxiao Gong",
      "Hao Yang",
      "Gaoyun Fang",
      "Bobo Ju",
      "Juncen Guo",
      "Xiaoguang Zhu",
      "Xiping Hu",
      "Yan Wang",
      "Peng Sun",
      "Azzedine Boukerche"
    ],
    "categories": [
      "cs.NI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-02-10T15:48:11Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06581v4"
  },
  {
    "arxiv_id": "2502.06490v2",
    "entry_id": "http://arxiv.org/abs/2502.06490v2",
    "title": "Recent Advances in Discrete Speech Tokens: A Review",
    "summary": "The rapid advancement of speech generation technologies in the era of large language models (LLMs) has established discrete speech tokens as a foundational paradigm for speech representation. These tokens, characterized by their discrete, compact, and concise nature, are not only advantageous for efficient transmission and storage, but also inherently compatible with the language modeling framework, enabling seamless integration of speech into text-dominated LLM architectures. Current research categorizes discrete speech tokens into two principal classes: acoustic tokens and semantic tokens, each of which has evolved into a rich research domain characterized by unique design philosophies and methodological approaches. This survey systematically synthesizes the existing taxonomy and recent innovations in discrete speech tokenization, conducts a critical examination of the strengths and limitations of each paradigm, and presents systematic experimental comparisons across token types. Furthermore, we identify persistent challenges in the field and propose potential research directions, aiming to offer actionable insights to inspire future advancements in the development and application of discrete speech tokens.",
    "authors": [
      "Yiwei Guo",
      "Zhihan Li",
      "Hankun Wang",
      "Bohan Li",
      "Chongtian Shao",
      "Hanglei Zhang",
      "Chenpeng Du",
      "Xie Chen",
      "Shujie Liu",
      "Kai Yu"
    ],
    "categories": [
      "eess.AS",
      "cs.AI",
      "cs.MM",
      "cs.SD",
      "eess.SP"
    ],
    "published": "2025-02-10T14:08:25Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06490v2"
  },
  {
    "arxiv_id": "2502.06470v1",
    "entry_id": "http://arxiv.org/abs/2502.06470v1",
    "title": "A Survey of Theory of Mind in Large Language Models: Evaluations, Representations, and Safety Risks",
    "summary": "Theory of Mind (ToM), the ability to attribute mental states to others and predict their behaviour, is fundamental to social intelligence. In this paper, we survey studies evaluating behavioural and representational ToM in Large Language Models (LLMs), identify important safety risks from advanced LLM ToM capabilities, and suggest several research directions for effective evaluation and mitigation of these risks.",
    "authors": [
      "Hieu Minh \"Jord\" Nguyen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-10T13:50:25Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06470v1"
  },
  {
    "arxiv_id": "2502.06205v2",
    "entry_id": "http://arxiv.org/abs/2502.06205v2",
    "title": "C-3PO: Compact Plug-and-Play Proxy Optimization to Achieve Human-like Retrieval-Augmented Generation",
    "summary": "Retrieval-augmented generation (RAG) systems face a fundamental challenge in aligning independently developed retrievers and large language models (LLMs). Existing approaches typically involve modifying either component or introducing simple intermediate modules, resulting in practical limitations and sub-optimal performance. Inspired by human search behavior -- typically involving a back-and-forth process of proposing search queries and reviewing documents, we propose C-3PO, a proxy-centric framework that facilitates communication between retrievers and LLMs through a lightweight multi-agent system. Our framework implements three specialized agents that collaboratively optimize the entire RAG pipeline without altering the retriever and LLMs. These agents work together to assess the need for retrieval, generate effective queries, and select information suitable for the LLMs. To enable effective multi-agent coordination, we develop a tree-structured rollout approach for reward credit assignment in reinforcement learning. Extensive experiments in both in-domain and out-of-distribution scenarios demonstrate that C-3PO significantly enhances RAG performance while maintaining plug-and-play flexibility and superior generalization capabilities.",
    "authors": [
      "Guoxin Chen",
      "Minpeng Liao",
      "Peiying Yu",
      "Dingmin Wang",
      "Zile Qiao",
      "Chao Yang",
      "Xin Zhao",
      "Kai Fan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-10T07:04:32Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06205v2"
  },
  {
    "arxiv_id": "2502.07819v1",
    "entry_id": "http://arxiv.org/abs/2502.07819v1",
    "title": "Enhancing kidney transplantation through multi-agent kidney exchange programs: A comprehensive review and optimization models",
    "summary": "This paper presents a comprehensive review of the last two decades of research on Kidney Exchange Programs (KEPs), systematically categorizing and classifying key contributions to provide readers with a structured understanding of advancements in the field. The review highlights the evolution of KEP methodologies and lays the foundation for our contribution. We propose three mathematical models aimed at improving both the quantity and quality of kidney transplants. Model 1 maximizes the number of transplants by focusing on compatibility based on blood type and PRA, without additional constraints. Model 2 introduces a minimum Human Leukocyte Antigen (HLA) compatibility threshold to enhance transplant quality, though this leads to fewer matches. Model 3 extends the problem to a Multi-Agent Kidney Exchange Program (MKEP), pooling incompatible donor-recipient pairs across multiple agents, resulting in a higher number of successful transplants while ensuring fairness across agents. Sensitivity analyses demonstrate trade-offs between transplant quantity and quality, with Model 3 striking the optimal balance by leveraging multi-agent collaboration to improve both the number and quality of transplants. These findings underscore the potential benefits of more integrated kidney exchange systems.",
    "authors": [
      "Shayan Sharifi"
    ],
    "categories": [
      "cs.AI",
      "math.OC"
    ],
    "published": "2025-02-10T04:21:42Z",
    "pdf_url": "https://arxiv.org/pdf/2502.07819v1"
  },
  {
    "arxiv_id": "2502.06039v1",
    "entry_id": "http://arxiv.org/abs/2502.06039v1",
    "title": "Benchmarking Prompt Engineering Techniques for Secure Code Generation with GPT Models",
    "summary": "Prompt engineering reduces reasoning mistakes in Large Language Models (LLMs). However, its effectiveness in mitigating vulnerabilities in LLM-generated code remains underexplored. To address this gap, we implemented a benchmark to automatically assess the impact of various prompt engineering strategies on code security. Our benchmark leverages two peer-reviewed prompt datasets and employs static scanners to evaluate code security at scale. We tested multiple prompt engineering techniques on GPT-3.5-turbo, GPT-4o, and GPT-4o-mini. Our results show that for GPT-4o and GPT-4o-mini, a security-focused prompt prefix can reduce the occurrence of security vulnerabilities by up to 56%. Additionally, all tested models demonstrated the ability to detect and repair between 41.9% and 68.7% of vulnerabilities in previously generated code when using iterative prompting techniques. Finally, we introduce a \"prompt agent\" that demonstrates how the most effective techniques can be applied in real-world development workflows.",
    "authors": [
      "Marc Bruni",
      "Fabio Gabrielli",
      "Mohammad Ghafari",
      "Martin Kropp"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-02-09T21:23:07Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06039v1"
  },
  {
    "arxiv_id": "2503.16433v1",
    "entry_id": "http://arxiv.org/abs/2503.16433v1",
    "title": "The Application of MATEC (Multi-AI Agent Team Care) Framework in Sepsis Care",
    "summary": "Under-resourced or rural hospitals have limited access to medical specialists and healthcare professionals, which can negatively impact patient outcomes in sepsis. To address this gap, we developed the MATEC (Multi-AI Agent Team Care) framework, which integrates a team of specialized AI agents for sepsis care. The sepsis AI agent team includes five doctor agents, four health professional agents, and a risk prediction model agent, with an additional 33 doctor agents available for consultations. Ten attending physicians at a teaching hospital evaluated this framework, spending approximately 40 minutes on the web-based MATEC application and participating in the 5-point Likert scale survey (rated from 1-unfavorable to 5-favorable). The physicians found the MATEC framework very useful (Median=4, P=0.01), and very accurate (Median=4, P<0.01). This pilot study demonstrates that a Multi-AI Agent Team Care framework (MATEC) can potentially be useful in assisting medical professionals, particularly in under-resourced hospital settings.",
    "authors": [
      "Andrew Cho",
      "Jason M. Woo",
      "Brian Shi",
      "Aishwaryaa Udeshi",
      "Jonathan S. H. Woo"
    ],
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2025-02-09T12:46:13Z",
    "pdf_url": "https://arxiv.org/pdf/2503.16433v1"
  },
  {
    "arxiv_id": "2502.09636v2",
    "entry_id": "http://arxiv.org/abs/2502.09636v2",
    "title": "Reading between the Lines: Can LLMs Identify Cross-Cultural Communication Gaps?",
    "summary": "In a rapidly globalizing and digital world, content such as book and product reviews created by people from diverse cultures are read and consumed by others from different corners of the world. In this paper, we investigate the extent and patterns of gaps in understandability of book reviews due to the presence of culturally-specific items and elements that might be alien to users from another culture. Our user-study on 57 book reviews from Goodreads reveal that 83\\% of the reviews had at least one culture-specific difficult-to-understand element. We also evaluate the efficacy of GPT-4o in identifying such items, given the cultural background of the reader; the results are mixed, implying a significant scope for improvement. Our datasets are available here: https://github.com/sougata-ub/reading_between_lines",
    "authors": [
      "Sougata Saha",
      "Saurabh Kumar Pandey",
      "Harshit Gupta",
      "Monojit Choudhury"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-09T04:40:35Z",
    "pdf_url": "https://arxiv.org/pdf/2502.09636v2"
  },
  {
    "arxiv_id": "2502.05718v1",
    "entry_id": "http://arxiv.org/abs/2502.05718v1",
    "title": "Using agent-based models and EXplainable Artificial Intelligence (XAI) to simulate social behaviors and policy intervention scenarios: A case study of private well users in Ireland",
    "summary": "Around 50 percent of Irelands rural population relies on unregulated private wells vulnerable to agricultural runoff and untreated wastewater. High national rates of Shiga toxin-producing Escherichia coli (STEC) and other waterborne illnesses have been linked to well water exposure. Periodic well testing is essential for public health, yet the lack of government incentives places the financial burden on households. Understanding environmental, cognitive, and material factors influencing well-testing behavior is critical.\n  This study employs Agent-Based Modeling (ABM) to simulate policy interventions based on national survey data. The ABM framework, designed for private well-testing behavior, integrates a Deep Q-network reinforcement learning model and Explainable AI (XAI) for decision-making insights. Key features were selected using Recursive Feature Elimination (RFE) with 10-fold cross-validation, while SHAP (Shapley Additive Explanations) provided further interpretability for policy recommendations.\n  Fourteen policy scenarios were tested. The most effective, Free Well Testing plus Communication Campaign, increased participation to 435 out of 561 agents, from a baseline of approximately 5 percent, with rapid behavioral adaptation. Free Well Testing plus Regulation also performed well, with 433 out of 561 agents initiating well testing. Free testing alone raised participation to over 75 percent, with some agents testing multiple times annually. Scenarios with free well testing achieved faster learning efficiency, converging in 1000 episodes, while others took 2000 episodes, indicating slower adaptation.\n  This research demonstrates the value of ABM and XAI in public health policy, providing a framework for evaluating behavioral interventions in environmental health.",
    "authors": [
      "Rabia Asghar",
      "Simon Mooney",
      "Eoin O Neill",
      "Paul Hynds"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-02-08T23:21:50Z",
    "pdf_url": "https://arxiv.org/pdf/2502.05718v1"
  },
  {
    "arxiv_id": "2502.05568v1",
    "entry_id": "http://arxiv.org/abs/2502.05568v1",
    "title": "Large Multimodal Models for Low-Resource Languages: A Survey",
    "summary": "In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 106 studies across 75 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. We aim to provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: https://github.com/marianlupascu/LMM4LRL-Survey.",
    "authors": [
      "Marian Lupascu",
      "Ana-Cristina Rogoz",
      "Mihai Sorin Stupariu",
      "Radu Tudor Ionescu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-08T13:29:44Z",
    "pdf_url": "https://arxiv.org/pdf/2502.05568v1"
  },
  {
    "arxiv_id": "2502.06872v1",
    "entry_id": "http://arxiv.org/abs/2502.06872v1",
    "title": "Towards Trustworthy Retrieval Augmented Generation for Large Language Models: A Survey",
    "summary": "Retrieval-Augmented Generation (RAG) is an advanced technique designed to address the challenges of Artificial Intelligence-Generated Content (AIGC). By integrating context retrieval into content generation, RAG provides reliable and up-to-date external knowledge, reduces hallucinations, and ensures relevant context across a wide range of tasks. However, despite RAG's success and potential, recent studies have shown that the RAG paradigm also introduces new risks, including robustness issues, privacy concerns, adversarial attacks, and accountability issues. Addressing these risks is critical for future applications of RAG systems, as they directly impact their trustworthiness. Although various methods have been developed to improve the trustworthiness of RAG methods, there is a lack of a unified perspective and framework for research in this topic. Thus, in this paper, we aim to address this gap by providing a comprehensive roadmap for developing trustworthy RAG systems. We place our discussion around five key perspectives: reliability, privacy, safety, fairness, explainability, and accountability. For each perspective, we present a general framework and taxonomy, offering a structured approach to understanding the current challenges, evaluating existing solutions, and identifying promising future research directions. To encourage broader adoption and innovation, we also highlight the downstream applications where trustworthy RAG systems have a significant impact.",
    "authors": [
      "Bo Ni",
      "Zheyuan Liu",
      "Leyao Wang",
      "Yongjia Lei",
      "Yuying Zhao",
      "Xueqi Cheng",
      "Qingkai Zeng",
      "Luna Dong",
      "Yinglong Xia",
      "Krishnaram Kenthapadi",
      "Ryan Rossi",
      "Franck Dernoncourt",
      "Md Mehrab Tanjim",
      "Nesreen Ahmed",
      "Xiaorui Liu",
      "Wenqi Fan",
      "Erik Blasch",
      "Yu Wang",
      "Meng Jiang",
      "Tyler Derr"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-08T06:50:47Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06872v1"
  },
  {
    "arxiv_id": "2502.06869v1",
    "entry_id": "http://arxiv.org/abs/2502.06869v1",
    "title": "A Survey on Explainable Deep Reinforcement Learning",
    "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in sequential decision-making tasks across diverse domains, yet its reliance on black-box neural architectures hinders interpretability, trust, and deployment in high-stakes applications. Explainable Deep Reinforcement Learning (XRL) addresses these challenges by enhancing transparency through feature-level, state-level, dataset-level, and model-level explanation techniques. This survey provides a comprehensive review of XRL methods, evaluates their qualitative and quantitative assessment frameworks, and explores their role in policy refinement, adversarial robustness, and security. Additionally, we examine the integration of reinforcement learning with Large Language Models (LLMs), particularly through Reinforcement Learning from Human Feedback (RLHF), which optimizes AI alignment with human preferences. We conclude by highlighting open research challenges and future directions to advance the development of interpretable, reliable, and accountable DRL systems.",
    "authors": [
      "Zelei Cheng",
      "Jiahao Yu",
      "Xinyu Xing"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-02-08T05:30:31Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06869v1"
  },
  {
    "arxiv_id": "2502.05398v3",
    "entry_id": "http://arxiv.org/abs/2502.05398v3",
    "title": "Probabilistic Foundations for Metacognition via Hybrid-AI",
    "summary": "Metacognition is the concept of reasoning about an agent's own internal processes, and it has recently received renewed attention with respect to artificial intelligence (AI) and, more specifically, machine learning systems. This paper reviews a hybrid-AI approach known as \"error detecting and correcting rules\" (EDCR) that allows for the learning of rules to correct perceptual (e.g., neural) models. Additionally, we introduce a probabilistic framework that adds rigor to prior empirical studies, and we use this framework to prove results on necessary and sufficient conditions for metacognitive improvement, as well as limits to the approach. A set of future",
    "authors": [
      "Paulo Shakarian",
      "Gerardo I. Simari",
      "Nathaniel D. Bastian"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-02-08T01:10:56Z",
    "pdf_url": "https://arxiv.org/pdf/2502.05398v3"
  },
  {
    "arxiv_id": "2502.05151v2",
    "entry_id": "http://arxiv.org/abs/2502.05151v2",
    "title": "Transforming Science with Large Language Models: A Survey on AI-assisted Scientific Discovery, Experimentation, Content Generation, and Evaluation",
    "summary": "With the advent of large multimodal language models, science is now at a threshold of an AI-based technological transformation. Recently, a plethora of new AI models and tools has been proposed, promising to empower researchers and academics worldwide to conduct their research more effectively and efficiently. This includes all aspects of the research cycle, especially (1) searching for relevant literature; (2) generating research ideas and conducting experimentation; generating (3) text-based and (4) multimodal content (e.g., scientific figures and diagrams); and (5) AI-based automatic peer review. In this survey, we provide an in-depth overview over these exciting recent developments, which promise to fundamentally alter the scientific research process for good. Our survey covers the five aspects outlined above, indicating relevant datasets, methods and results (including evaluation) as well as limitations and scope for future research. Ethical concerns regarding shortcomings of these tools and potential for misuse (fake science, plagiarism, harms to research integrity) take a particularly prominent place in our discussion. We hope that our survey will not only become a reference guide for newcomers to the field but also a catalyst for new AI-based initiatives in the area of \"AI4Science\".",
    "authors": [
      "Steffen Eger",
      "Yong Cao",
      "Jennifer D'Souza",
      "Andreas Geiger",
      "Christian Greisinger",
      "Stephanie Gross",
      "Yufang Hou",
      "Brigitte Krenn",
      "Anne Lauscher",
      "Yizhi Li",
      "Chenghua Lin",
      "Nafise Sadat Moosavi",
      "Wei Zhao",
      "Tristan Miller"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-02-07T18:26:45Z",
    "pdf_url": "https://arxiv.org/pdf/2502.05151v2"
  },
  {
    "arxiv_id": "2502.06851v3",
    "entry_id": "http://arxiv.org/abs/2502.06851v3",
    "title": "Survey on Vision-Language-Action Models",
    "summary": "This paper presents an AI-generated review of Vision-Language-Action (VLA) models, summarizing key methodologies, findings, and future directions. The content is produced using large language models (LLMs) and is intended only for demonstration purposes. This work does not represent original research, but highlights how AI can help automate literature reviews. As AI-generated content becomes more prevalent, ensuring accuracy, reliability, and proper synthesis remains a challenge. Future research will focus on developing a structured framework for AI-assisted literature reviews, exploring techniques to enhance citation accuracy, source credibility, and contextual understanding. By examining the potential and limitations of LLM in academic writing, this study aims to contribute to the broader discussion of integrating AI into research workflows. This work serves as a preliminary step toward establishing systematic approaches for leveraging AI in literature review generation, making academic knowledge synthesis more efficient and scalable.",
    "authors": [
      "Adilzhan Adilkhanov",
      "Amir Yelenov",
      "Assylkhan Seitzhanov",
      "Ayan Mazhitov",
      "Azamat Abdikarimov",
      "Danissa Sandykbayeva",
      "Daryn Kenzhebek",
      "Dinmukhammed Mukashev",
      "Ilyas Umurbekov",
      "Jabrail Chumakov",
      "Kamila Spanova",
      "Karina Burunchina",
      "Madina Yergibay",
      "Margulan Issa",
      "Moldir Zabirova",
      "Nurdaulet Zhuzbay",
      "Nurlan Kabdyshev",
      "Nurlan Zhaniyar",
      "Rasul Yermagambet",
      "Rustam Chibar",
      "Saltanat Seitzhan",
      "Soibkhon Khajikhanov",
      "Tasbolat Taunyazov",
      "Temirlan Galimzhanov",
      "Temirlan Kaiyrbay",
      "Tleukhan Mussin",
      "Togzhan Syrymova",
      "Valeriya Kostyukova",
      "Yerkebulan Massalim",
      "Yermakhan Kassym",
      "Zerde Nurbayeva",
      "Zhanat Kappassov"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-02-07T11:56:46Z",
    "pdf_url": "https://arxiv.org/pdf/2502.06851v3"
  },
  {
    "arxiv_id": "2502.03814v4",
    "entry_id": "http://arxiv.org/abs/2502.03814v4",
    "title": "Large Language Models for Multi-Robot Systems: A Survey",
    "summary": "The rapid advancement of Large Language Models (LLMs) has opened new possibilities in Multi-Robot Systems (MRS), enabling enhanced communication, task planning, and human-robot interaction. Unlike traditional single-robot and multi-agent systems, MRS poses unique challenges, including coordination, scalability, and real-world adaptability. This survey provides the first comprehensive exploration of LLM integration into MRS. It systematically categorizes their applications across high-level task allocation, mid-level motion planning, low-level action generation, and human intervention. We highlight key applications in diverse domains, such as household robotics, construction, formation control, target tracking, and robot games, showcasing the versatility and transformative potential of LLMs in MRS. Furthermore, we examine the challenges that limit adapting LLMs in MRS, including mathematical reasoning limitations, hallucination, latency issues, and the need for robust benchmarking systems. Finally, we outline opportunities for future research, emphasizing advancements in fine-tuning, reasoning techniques, and task-specific models. This survey aims to guide researchers in the intelligence and real-world deployment of MRS powered by LLMs. Based on the fast-evolving nature of research in the field, we keep updating the papers in the open-source GitHub repository.",
    "authors": [
      "Peihan Li",
      "Zijian An",
      "Shams Abrar",
      "Lifeng Zhou"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2025-02-06T06:52:14Z",
    "pdf_url": "https://arxiv.org/pdf/2502.03814v4"
  },
  {
    "arxiv_id": "2502.05224v1",
    "entry_id": "http://arxiv.org/abs/2502.05224v1",
    "title": "A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations",
    "summary": "Large Language Models (LLMs) have achieved significantly advanced capabilities in understanding and generating human language text, which have gained increasing popularity over recent years. Apart from their state-of-the-art natural language processing (NLP) performance, considering their widespread usage in many industries, including medicine, finance, education, etc., security concerns over their usage grow simultaneously. In recent years, the evolution of backdoor attacks has progressed with the advancement of defense mechanisms against them and more well-developed features in the LLMs. In this paper, we adapt the general taxonomy for classifying machine learning attacks on one of the subdivisions - training-time white-box backdoor attacks. Besides systematically classifying attack methods, we also consider the corresponding defense methods against backdoor attacks. By providing an extensive summary of existing works, we hope this survey can serve as a guideline for inspiring future research that further extends the attack scenarios and creates a stronger defense against them for more robust LLMs.",
    "authors": [
      "Yihe Zhou",
      "Tao Ni",
      "Wei-Bin Lee",
      "Qingchuan Zhao"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2025-02-06T04:43:05Z",
    "pdf_url": "https://arxiv.org/pdf/2502.05224v1"
  },
  {
    "arxiv_id": "2502.03671v2",
    "entry_id": "http://arxiv.org/abs/2502.03671v2",
    "title": "Advancing Reasoning in Large Language Models: Promising Methods and Approaches",
    "summary": "Large Language Models (LLMs) have succeeded remarkably in various natural language processing (NLP) tasks, yet their reasoning capabilities remain a fundamental challenge. While LLMs exhibit impressive fluency and factual recall, their ability to perform complex reasoning-spanning logical deduction, mathematical problem-solving, commonsense inference, and multi-step reasoning-often falls short of human expectations. This survey provides a comprehensive review of emerging techniques enhancing reasoning in LLMs. We categorize existing methods into key approaches, including prompting strategies (e.g., Chain-of-Thought reasoning, Self-Consistency, and Tree-of-Thought reasoning), architectural innovations (e.g., retrieval-augmented models, modular reasoning networks, and neuro-symbolic integration), and learning paradigms (e.g., fine-tuning with reasoning-specific datasets, reinforcement learning, and self-supervised reasoning objectives). Additionally, we explore evaluation frameworks used to assess reasoning in LLMs and highlight open challenges, such as hallucinations, robustness, and reasoning generalization across diverse tasks. By synthesizing recent advancements, this survey aims to provide insights into promising directions for future research and practical applications of reasoning-augmented LLMs.",
    "authors": [
      "Avinash Patil",
      "Aryan Jadon"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-05T23:31:39Z",
    "pdf_url": "https://arxiv.org/pdf/2502.03671v2"
  },
  {
    "arxiv_id": "2503.04742v2",
    "entry_id": "http://arxiv.org/abs/2503.04742v2",
    "title": "A Case for Specialisation in Non-Human Entities",
    "summary": "With the rise of large multi-modal AI models, fuelled by recent interest in large language models (LLMs), the notion of artificial general intelligence (AGI) went from being restricted to a fringe community, to dominate mainstream large AI development programs. In contrast, in this paper, we make a case for specialisation, by reviewing the pitfalls of generality and stressing the industrial value of specialised systems.\n  Our contribution is threefold. First, we review the most widely accepted arguments against specialisation, and discuss how their relevance in the context of human labour is actually an argument for specialisation in the case of non human agents, be they algorithms or human organisations. Second, we propose four arguments in favor of specialisation, ranging from machine learning robustness, to computer security, social sciences and cultural evolution. Third, we finally make a case for specification, discuss how the machine learning approach to AI has so far failed to catch up with good practices from safety-engineering and formal verification of software, and discuss how some emerging good practices in machine learning help reduce this gap. In particular, we justify the need for specified governance for hard-to-specify systems.",
    "authors": [
      "El-Mahdi El-Mhamdi",
      "Lê-Nguyên Hoang",
      "Mariame Tighanimine"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-02-05T20:38:18Z",
    "pdf_url": "https://arxiv.org/pdf/2503.04742v2"
  },
  {
    "arxiv_id": "2502.18474v1",
    "entry_id": "http://arxiv.org/abs/2502.18474v1",
    "title": "A Contemporary Survey of Large Language Model Assisted Program Analysis",
    "summary": "The increasing complexity of software systems has driven significant advancements in program analysis, as traditional methods unable to meet the demands of modern software development. To address these limitations, deep learning techniques, particularly Large Language Models (LLMs), have gained attention due to their context-aware capabilities in code comprehension. Recognizing the potential of LLMs, researchers have extensively explored their application in program analysis since their introduction. Despite existing surveys on LLM applications in cybersecurity, comprehensive reviews specifically addressing their role in program analysis remain scarce. In this survey, we systematically review the application of LLMs in program analysis, categorizing the existing work into static analysis, dynamic analysis, and hybrid approaches. Moreover, by examining and synthesizing recent studies, we identify future directions and challenges in the field. This survey aims to demonstrate the potential of LLMs in advancing program analysis practices and offer actionable insights for security researchers seeking to enhance detection frameworks or develop domain-specific models.",
    "authors": [
      "Jiayimei Wang",
      "Tao Ni",
      "Wei-Bin Lee",
      "Qingchuan Zhao"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-02-05T14:27:17Z",
    "pdf_url": "https://arxiv.org/pdf/2502.18474v1"
  },
  {
    "arxiv_id": "2502.04356v1",
    "entry_id": "http://arxiv.org/abs/2502.04356v1",
    "title": "Open Foundation Models in Healthcare: Challenges, Paradoxes, and Opportunities with GenAI Driven Personalized Prescription",
    "summary": "In response to the success of proprietary Large Language Models (LLMs) such as OpenAI's GPT-4, there is a growing interest in developing open, non-proprietary LLMs and AI foundation models (AIFMs) for transparent use in academic, scientific, and non-commercial applications. Despite their inability to match the refined functionalities of their proprietary counterparts, open models hold immense potential to revolutionize healthcare applications. In this paper, we examine the prospects of open-source LLMs and AIFMs for developing healthcare applications and make two key contributions. Firstly, we present a comprehensive survey of the current state-of-the-art open-source healthcare LLMs and AIFMs and introduce a taxonomy of these open AIFMs, categorizing their utility across various healthcare tasks. Secondly, to evaluate the general-purpose applications of open LLMs in healthcare, we present a case study on personalized prescriptions. This task is particularly significant due to its critical role in delivering tailored, patient-specific medications that can greatly improve treatment outcomes. In addition, we compare the performance of open-source models with proprietary models in settings with and without Retrieval-Augmented Generation (RAG). Our findings suggest that, although less refined, open LLMs can achieve performance comparable to proprietary models when paired with grounding techniques such as RAG. Furthermore, to highlight the clinical significance of LLMs-empowered personalized prescriptions, we perform subjective assessment through an expert clinician. We also elaborate on ethical considerations and potential risks associated with the misuse of powerful LLMs and AIFMs, highlighting the need for a cautious and responsible implementation in healthcare.",
    "authors": [
      "Mahdi Alkaeed",
      "Sofiat Abioye",
      "Adnan Qayyum",
      "Yosra Magdi Mekki",
      "Ilhem Berrou",
      "Mohamad Abdallah",
      "Ala Al-Fuqaha",
      "Muhammad Bilal",
      "Junaid Qadir"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2025-02-04T19:16:56Z",
    "pdf_url": "https://arxiv.org/pdf/2502.04356v1"
  },
  {
    "arxiv_id": "2502.02199v1",
    "entry_id": "http://arxiv.org/abs/2502.02199v1",
    "title": "When Dimensionality Hurts: The Role of LLM Embedding Compression for Noisy Regression Tasks",
    "summary": "Large language models (LLMs) have shown remarkable success in language modelling due to scaling laws found in model size and the hidden dimension of the model's text representation. Yet, we demonstrate that compressed representations of text can yield better performance in LLM-based regression tasks. In this paper, we compare the relative performance of embedding compression in three different signal-to-noise contexts: financial return prediction, writing quality assessment and review scoring. Our results show that compressing embeddings, in a minimally supervised manner using an autoencoder's hidden representation, can mitigate overfitting and improve performance on noisy tasks, such as financial return prediction; but that compression reduces performance on tasks that have high causal dependencies between the input and target data. Our results suggest that the success of interpretable compressed representations such as sentiment may be due to a regularising effect.",
    "authors": [
      "Felix Drinkall",
      "Janet B. Pierrehumbert",
      "Stefan Zohren"
    ],
    "categories": [
      "cs.CL",
      "cs.CE",
      "cs.LG",
      "q-fin.CP"
    ],
    "published": "2025-02-04T10:23:11Z",
    "pdf_url": "https://arxiv.org/pdf/2502.02199v1"
  },
  {
    "arxiv_id": "2502.01524v1",
    "entry_id": "http://arxiv.org/abs/2502.01524v1",
    "title": "Efficiently Integrate Large Language Models with Visual Perception: A Survey from the Training Paradigm Perspective",
    "summary": "The integration of vision-language modalities has been a significant focus in multimodal learning, traditionally relying on Vision-Language Pretrained Models. However, with the advent of Large Language Models (LLMs), there has been a notable shift towards incorporating LLMs with vision modalities. Following this, the training paradigms for incorporating vision modalities into LLMs have evolved. Initially, the approach was to integrate the modalities through pretraining the modality integrator, named Single-stage Tuning. It has since branched out into methods focusing on performance enhancement, denoted as Two-stage Tuning, and those prioritizing parameter efficiency, referred to as Direct Adaptation. However, existing surveys primarily address the latest Vision Large Language Models (VLLMs) with Two-stage Tuning, leaving a gap in understanding the evolution of training paradigms and their unique parameter-efficient considerations. This paper categorizes and reviews 34 VLLMs from top conferences, journals, and highly cited Arxiv papers, focusing on parameter efficiency during adaptation from the training paradigm perspective. We first introduce the architecture of LLMs and parameter-efficient learning methods, followed by a discussion on vision encoders and a comprehensive taxonomy of modality integrators. We then review three training paradigms and their efficiency considerations, summarizing benchmarks in the VLLM field. To gain deeper insights into their effectiveness in parameter efficiency, we compare and discuss the experimental results of representative models, among which the experiment of the Direct Adaptation paradigm is replicated. Providing insights into recent developments and practical uses, this survey is a vital guide for researchers and practitioners navigating the efficient integration of vision modalities into LLMs.",
    "authors": [
      "Xiaorui Ma",
      "Haoran Xie",
      "S. Joe Qin"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-02-03T17:01:59Z",
    "pdf_url": "https://arxiv.org/pdf/2502.01524v1"
  },
  {
    "arxiv_id": "2502.01436v2",
    "entry_id": "http://arxiv.org/abs/2502.01436v2",
    "title": "Towards Safer Chatbots: A Framework for Policy Compliance Evaluation of Custom GPTs",
    "summary": "Large Language Models (LLMs) have gained unprecedented prominence, achieving widespread adoption across diverse domains and integrating deeply into society. The capability to fine-tune general-purpose LLMs, such as Generative Pre-trained Transformers (GPT), for specific tasks has facilitated the emergence of numerous Custom GPTs. These tailored models are increasingly made available through dedicated marketplaces, such as OpenAI's GPT Store. However, their black-box nature introduces significant safety and compliance risks. In this work, we present a scalable framework for the automated evaluation of Custom GPTs against OpenAI's usage policies, which define the permissible behaviors of these systems. Our framework integrates three core components: (1) automated discovery and data collection of models from the GPT store, (2) a red-teaming prompt generator tailored to specific policy categories and the characteristics of each target GPT, and (3) an LLM-as-a-judge technique to analyze each prompt-response pair for potential policy violations. We validate our framework with a manually annotated ground truth, and evaluate it through a large-scale study with 782 Custom GPTs across three categories: Romantic, Cybersecurity, and Academic GPTs. Our manual annotation process achieved an F1 score of 0.975 in identifying policy violations, confirming the reliability of the framework's assessments. The results reveal that 58.7% of the analyzed models exhibit indications of non-compliance, exposing weaknesses in the GPT store's review and approval processes. Furthermore, our findings indicate that a model's popularity does not correlate with compliance, and non-compliance issues largely stem from behaviors inherited from base models rather than user-driven customizations. We believe this approach is extendable to other chatbot platforms and policy domains, improving LLM-based systems safety.",
    "authors": [
      "David Rodriguez",
      "William Seymour",
      "Jose M. Del Alamo",
      "Jose Such"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-02-03T15:19:28Z",
    "pdf_url": "https://arxiv.org/pdf/2502.01436v2"
  },
  {
    "arxiv_id": "2502.00792v1",
    "entry_id": "http://arxiv.org/abs/2502.00792v1",
    "title": "RTBAgent: A LLM-based Agent System for Real-Time Bidding",
    "summary": "Real-Time Bidding (RTB) enables advertisers to place competitive bids on impression opportunities instantaneously, striving for cost-effectiveness in a highly competitive landscape. Although RTB has widely benefited from the utilization of technologies such as deep learning and reinforcement learning, the reliability of related methods often encounters challenges due to the discrepancies between online and offline environments and the rapid fluctuations of online bidding. To handle these challenges, RTBAgent is proposed as the first RTB agent system based on large language models (LLMs), which synchronizes real competitive advertising bidding environments and obtains bidding prices through an integrated decision-making process. Specifically, obtaining reasoning ability through LLMs, RTBAgent is further tailored to be more professional for RTB via involved auxiliary modules, i.e., click-through rate estimation model, expert strategy knowledge, and daily reflection. In addition, we propose a two-step decision-making process and multi-memory retrieval mechanism, which enables RTBAgent to review historical decisions and transaction records and subsequently make decisions more adaptive to market changes in real-time bidding. Empirical testing with real advertising datasets demonstrates that RTBAgent significantly enhances profitability. The RTBAgent code will be publicly accessible at: https://github.com/CaiLeng/RTBAgent.",
    "authors": [
      "Leng Cai",
      "Junxuan He",
      "Yikai Li",
      "Junjie Liang",
      "Yuanping Lin",
      "Ziming Quan",
      "Yawen Zeng",
      "Jin Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-02-02T13:10:15Z",
    "pdf_url": "https://arxiv.org/pdf/2502.00792v1"
  },
  {
    "arxiv_id": "2502.00681v1",
    "entry_id": "http://arxiv.org/abs/2502.00681v1",
    "title": "A Survey of Quantized Graph Representation Learning: Connecting Graph Structures with Large Language Models",
    "summary": "Recent years have witnessed rapid advances in graph representation learning, with the continuous embedding approach emerging as the dominant paradigm. However, such methods encounter issues regarding parameter efficiency, interpretability, and robustness. Thus, Quantized Graph Representation (QGR) learning has recently gained increasing interest, which represents the graph structure with discrete codes instead of conventional continuous embeddings. Given its analogous representation form to natural language, QGR also possesses the capability to seamlessly integrate graph structures with large language models (LLMs). As this emerging paradigm is still in its infancy yet holds significant promise, we undertake this thorough survey to promote its rapid future prosperity. We first present the background of the general quantization methods and their merits. Moreover, we provide an in-depth demonstration of current QGR studies from the perspectives of quantized strategies, training objectives, distinctive designs, knowledge graph quantization, and applications. We further explore the strategies for code dependence learning and integration with LLMs. At last, we give discussions and conclude future directions, aiming to provide a comprehensive picture of QGR and inspire future research.",
    "authors": [
      "Qika Lin",
      "Zhen Peng",
      "Kaize Shi",
      "Kai He",
      "Yiming Xu",
      "Erik Cambria",
      "Mengling Feng"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-02T05:57:34Z",
    "pdf_url": "https://arxiv.org/pdf/2502.00681v1"
  },
  {
    "arxiv_id": "2502.00677v1",
    "entry_id": "http://arxiv.org/abs/2502.00677v1",
    "title": "LLM-based event log analysis techniques: A survey",
    "summary": "Event log analysis is an important task that security professionals undertake. Event logs record key information on activities that occur on computing devices, and due to the substantial number of events generated, they consume a large amount of time and resources to analyse. This demanding and repetitive task is also prone to errors. To address these concerns, researchers have developed automated techniques to improve the event log analysis process. Large Language Models (LLMs) have recently demonstrated the ability to successfully perform a wide range of tasks that individuals would usually partake in, to high standards, and at a pace and degree of complexity that outperform humans. Due to this, researchers are rapidly investigating the use of LLMs for event log analysis. This includes fine-tuning, Retrieval-Augmented Generation (RAG) and in-context learning, which affect performance. These works demonstrate good progress, yet there is a need to understand the developing body of knowledge, identify commonalities between works, and identify key challenges and potential solutions to further developments in this domain. This paper aims to survey LLM-based event log analysis techniques, providing readers with an in-depth overview of the domain, gaps identified in previous research, and concluding with potential avenues to explore in future.",
    "authors": [
      "Siraaj Akhtar",
      "Saad Khan",
      "Simon Parkinson"
    ],
    "categories": [
      "cs.AI",
      "cs.CR"
    ],
    "published": "2025-02-02T05:28:17Z",
    "pdf_url": "https://arxiv.org/pdf/2502.00677v1"
  },
  {
    "arxiv_id": "2502.05206v5",
    "entry_id": "http://arxiv.org/abs/2502.05206v5",
    "title": "Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety",
    "summary": "The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-powered Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.",
    "authors": [
      "Xingjun Ma",
      "Yifeng Gao",
      "Yixu Wang",
      "Ruofan Wang",
      "Xin Wang",
      "Ye Sun",
      "Yifan Ding",
      "Hengyuan Xu",
      "Yunhao Chen",
      "Yunhan Zhao",
      "Hanxun Huang",
      "Yige Li",
      "Yutao Wu",
      "Jiaming Zhang",
      "Xiang Zheng",
      "Yang Bai",
      "Zuxuan Wu",
      "Xipeng Qiu",
      "Jingfeng Zhang",
      "Yiming Li",
      "Xudong Han",
      "Haonan Li",
      "Jun Sun",
      "Cong Wang",
      "Jindong Gu",
      "Baoyuan Wu",
      "Siheng Chen",
      "Tianwei Zhang",
      "Yang Liu",
      "Mingming Gong",
      "Tongliang Liu",
      "Shirui Pan",
      "Cihang Xie",
      "Tianyu Pang",
      "Yinpeng Dong",
      "Ruoxi Jia",
      "Yang Zhang",
      "Shiqing Ma",
      "Xiangyu Zhang",
      "Neil Gong",
      "Chaowei Xiao",
      "Sarah Erfani",
      "Tim Baldwin",
      "Bo Li",
      "Masashi Sugiyama",
      "Dacheng Tao",
      "James Bailey",
      "Yu-Gang Jiang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2025-02-02T05:14:22Z",
    "pdf_url": "https://arxiv.org/pdf/2502.05206v5"
  },
  {
    "arxiv_id": "2502.00409v3",
    "entry_id": "http://arxiv.org/abs/2502.00409v3",
    "title": "Doing More with Less: A Survey on Routing Strategies for Resource Optimisation in Large Language Model-Based Systems",
    "summary": "Large Language Model (LLM)-based systems, i.e. interconnected elements that include an LLM as a central component, such as conversational agents, are usually designed with monolithic, static architectures that rely on a single, general-purpose LLM to handle all user queries. However, these systems may be inefficient as different queries may require different levels of reasoning, domain knowledge or pre-processing. While generalist LLMs (e.g. GPT-4o, Claude-Sonnet) perform well across a wide range of tasks, they may incur significant financial, energy and computational costs. These costs may be disproportionate for simpler queries, resulting in unnecessary resource utilisation. A routing mechanism can therefore be employed to route queries to more appropriate components, such as smaller or specialised models, thereby improving efficiency and optimising resource consumption. This survey aims to provide a comprehensive overview of routing strategies in LLM-based systems. Specifically, it reviews when, why, and how routing should be integrated into LLM pipelines to improve efficiency, scalability, and performance. We define the objectives to optimise, such as cost minimisation and performance maximisation, and discuss the timing of routing within the LLM workflow, whether it occurs before or after generation. We also detail the various implementation strategies, including similarity-based, supervised, reinforcement learning-based, and generative methods. Practical considerations such as industrial applications and current limitations are also examined, like standardising routing experiments, accounting for non-financial costs, and designing adaptive strategies. By formalising routing as a performance-cost optimisation problem, this survey provides tools and directions to guide future research and development of adaptive low-cost LLM-based systems.",
    "authors": [
      "Clovis Varangot-Reille",
      "Christophe Bouvard",
      "Antoine Gourru",
      "Mathieu Ciancone",
      "Marion Schaeffer",
      "François Jacquenet"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-02-01T12:08:38Z",
    "pdf_url": "https://arxiv.org/pdf/2502.00409v3"
  },
  {
    "arxiv_id": "2502.00070v2",
    "entry_id": "http://arxiv.org/abs/2502.00070v2",
    "title": "Can AI Solve the Peer Review Crisis? A Large Scale Cross Model Experiment of LLMs' Performance and Biases in Evaluating over 1000 Economics Papers",
    "summary": "This study examines the potential of large language models (LLMs) to augment the academic peer review process by reliably evaluating the quality of economics research without introducing systematic bias. We conduct one of the first large-scale experimental assessments of four LLMs (GPT-4o, Claude 3.5, Gemma 3, and LLaMA 3.3) across two complementary experiments. In the first, we use nonparametric binscatter and linear regression techniques to analyze over 29,000 evaluations of 1,220 anonymized papers drawn from 110 economics journals excluded from the training data of current LLMs, along with a set of AI-generated submissions. The results show that LLMs consistently distinguish between higher- and lower-quality research based solely on textual content, producing quality gradients that closely align with established journal prestige measures. Claude and Gemma perform exceptionally well in capturing these gradients, while GPT excels in detecting AI-generated content. The second experiment comprises 8,910 evaluations designed to assess whether LLMs replicate human like biases in single blind reviews. By systematically varying author gender, institutional affiliation, and academic prominence across 330 papers, we find that GPT, Gemma, and LLaMA assign significantly higher ratings to submissions from top male authors and elite institutions relative to the same papers presented anonymously. These results emphasize the importance of excluding author-identifying information when deploying LLMs in editorial screening. Overall, our findings provide compelling evidence and practical guidance for integrating LLMs into peer review to enhance efficiency, improve accuracy, and promote equity in the publication process of economics research.",
    "authors": [
      "Pat Pataranutaporn",
      "Nattavudh Powdthavee",
      "Chayapatr Achiwaranguprok",
      "Pattie Maes"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "econ.GN"
    ],
    "published": "2025-01-31T04:04:02Z",
    "pdf_url": "https://arxiv.org/pdf/2502.00070v2"
  },
  {
    "arxiv_id": "2501.18794v1",
    "entry_id": "http://arxiv.org/abs/2501.18794v1",
    "title": "Survey and Improvement Strategies for Gene Prioritization with Large Language Models",
    "summary": "Rare diseases are challenging to diagnose due to limited patient data and genetic diversity. Despite advances in variant prioritization, many cases remain undiagnosed. While large language models (LLMs) have performed well in medical exams, their effectiveness in diagnosing rare genetic diseases has not been assessed. To identify causal genes, we benchmarked various LLMs for gene prioritization. Using multi-agent and Human Phenotype Ontology (HPO) classification, we categorized patients based on phenotypes and solvability levels. As gene set size increased, LLM performance deteriorated, so we used a divide-and-conquer strategy to break the task into smaller subsets. At baseline, GPT-4 outperformed other LLMs, achieving near 30% accuracy in ranking causal genes correctly. The multi-agent and HPO approaches helped distinguish confidently solved cases from challenging ones, highlighting the importance of known gene-phenotype associations and phenotype specificity. We found that cases with specific phenotypes or clear associations were more accurately solved. However, we observed biases toward well-studied genes and input order sensitivity, which hindered gene prioritization. Our divide-and-conquer strategy improved accuracy by overcoming these biases. By utilizing HPO classification, novel multi-agent techniques, and our LLM strategy, we improved causal gene identification accuracy compared to our baseline evaluation. This approach streamlines rare disease diagnosis, facilitates reanalysis of unsolved cases, and accelerates gene discovery, supporting the development of targeted diagnostics and therapies.",
    "authors": [
      "Matthew Neeley",
      "Guantong Qi",
      "Guanchu Wang",
      "Ruixiang Tang",
      "Dongxue Mao",
      "Chaozhong Liu",
      "Sasidhar Pasupuleti",
      "Bo Yuan",
      "Fan Xia",
      "Pengfei Liu",
      "Zhandong Liu",
      "Xia Hu"
    ],
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "published": "2025-01-30T23:03:03Z",
    "pdf_url": "https://arxiv.org/pdf/2501.18794v1"
  },
  {
    "arxiv_id": "2501.18081v1",
    "entry_id": "http://arxiv.org/abs/2501.18081v1",
    "title": "Normative Evaluation of Large Language Models with Everyday Moral Dilemmas",
    "summary": "The rapid adoption of large language models (LLMs) has spurred extensive research into their encoded moral norms and decision-making processes. Much of this research relies on prompting LLMs with survey-style questions to assess how well models are aligned with certain demographic groups, moral beliefs, or political ideologies. While informative, the adherence of these approaches to relatively superficial constructs tends to oversimplify the complexity and nuance underlying everyday moral dilemmas. We argue that auditing LLMs along more detailed axes of human interaction is of paramount importance to better assess the degree to which they may impact human beliefs and actions. To this end, we evaluate LLMs on complex, everyday moral dilemmas sourced from the \"Am I the Asshole\" (AITA) community on Reddit, where users seek moral judgments on everyday conflicts from other community members. We prompted seven LLMs to assign blame and provide explanations for over 10,000 AITA moral dilemmas. We then compared the LLMs' judgments and explanations to those of Redditors and to each other, aiming to uncover patterns in their moral reasoning. Our results demonstrate that large language models exhibit distinct patterns of moral judgment, varying substantially from human evaluations on the AITA subreddit. LLMs demonstrate moderate to high self-consistency but low inter-model agreement. Further analysis of model explanations reveals distinct patterns in how models invoke various moral principles. These findings highlight the complexity of implementing consistent moral reasoning in artificial systems and the need for careful evaluation of how different models approach ethical judgment. As LLMs continue to be used in roles requiring ethical decision-making such as therapists and companions, careful evaluation is crucial to mitigate potential biases and limitations.",
    "authors": [
      "Pratik S. Sachdeva",
      "Tom van Nuenen"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-01-30T01:29:46Z",
    "pdf_url": "https://arxiv.org/pdf/2501.18081v1"
  },
  {
    "arxiv_id": "2501.16836v2",
    "entry_id": "http://arxiv.org/abs/2501.16836v2",
    "title": "Misspellings in Natural Language Processing: A survey",
    "summary": "This survey provides an overview of the challenges of misspellings in natural language processing (NLP). While often unintentional, misspellings have become ubiquitous in digital communication, especially with the proliferation of Web 2.0, user-generated content, and informal text mediums such as social media, blogs, and forums. Even if humans can generally interpret misspelled text, NLP models frequently struggle to handle it: this causes a decline in performance in common tasks like text classification and machine translation. In this paper, we reconstruct a history of misspellings as a scientific problem. We then discuss the latest advancements to address the challenge of misspellings in NLP. Main strategies to mitigate the effect of misspellings include data augmentation, double step, character-order agnostic, and tuple-based methods, among others. This survey also examines dedicated data challenges and competitions to spur progress in the field. Critical safety and ethical concerns are also examined, for example, the voluntary use of misspellings to inject malicious messages and hate speech on social networks. Furthermore, the survey explores psycholinguistic perspectives on how humans process misspellings, potentially informing innovative computational techniques for text normalization and representation. Finally, the misspelling-related challenges and opportunities associated with modern large language models are also analyzed, including benchmarks, datasets, and performances of the most prominent language models against misspellings. This survey aims to be an exhaustive resource for researchers seeking to mitigate the impact of misspellings in the rapidly evolving landscape of NLP.",
    "authors": [
      "Gianluca Sperduti",
      "Alejandro Moreo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-01-28T10:26:04Z",
    "pdf_url": "https://arxiv.org/pdf/2501.16836v2"
  },
  {
    "arxiv_id": "2501.16577v1",
    "entry_id": "http://arxiv.org/abs/2501.16577v1",
    "title": "Generative AI Uses and Risks for Knowledge Workers in a Science Organization",
    "summary": "Generative AI could enhance scientific discovery by supporting knowledge workers in science organizations. However, the real-world applications and perceived concerns of generative AI use in these organizations are uncertain. In this paper, we report on a collaborative study with a US national laboratory with employees spanning Science and Operations about their use of generative AI tools. We surveyed 66 employees, interviewed a subset (N=22), and measured early adoption of an internal generative AI interface called Argo lab-wide. We have four findings: (1) Argo usage data shows small but increasing use by Science and Operations employees; Common current and envisioned use cases for generative AI in this context conceptually fall into either a (2) copilot or (3) workflow agent modality; and (4) Concerns include sensitive data security, academic publishing, and job impacts. Based on our findings, we make recommendations for generative AI use in science and other organizations.",
    "authors": [
      "Kelly B. Wagman",
      "Matthew T. Dearing",
      "Marshini Chetty"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-01-27T23:41:13Z",
    "pdf_url": "https://arxiv.org/pdf/2501.16577v1"
  },
  {
    "arxiv_id": "2501.17200v1",
    "entry_id": "http://arxiv.org/abs/2501.17200v1",
    "title": "Improving LLM Leaderboards with Psychometrical Methodology",
    "summary": "The rapid development of large language models (LLMs) has necessitated the creation of benchmarks to evaluate their performance. These benchmarks resemble human tests and surveys, as they consist of sets of questions designed to measure emergent properties in the cognitive behavior of these systems. However, unlike the well-defined traits and abilities studied in social sciences, the properties measured by these benchmarks are often vaguer and less rigorously defined. The most prominent benchmarks are often grouped into leaderboards for convenience, aggregating performance metrics and enabling comparisons between models. Unfortunately, these leaderboards typically rely on simplistic aggregation methods, such as taking the average score across benchmarks. In this paper, we demonstrate the advantages of applying contemporary psychometric methodologies - originally developed for human tests and surveys - to improve the ranking of large language models on leaderboards. Using data from the Hugging Face Leaderboard as an example, we compare the results of the conventional naive ranking approach with a psychometrically informed ranking. The findings highlight the benefits of adopting psychometric techniques for more robust and meaningful evaluation of LLM performance.",
    "authors": [
      "Denis Federiakin"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "stat.AP"
    ],
    "published": "2025-01-27T21:21:46Z",
    "pdf_url": "https://arxiv.org/pdf/2501.17200v1"
  },
  {
    "arxiv_id": "2501.16309v1",
    "entry_id": "http://arxiv.org/abs/2501.16309v1",
    "title": "Evaluating The Performance of Using Large Language Models to Automate Summarization of CT Simulation Orders in Radiation Oncology",
    "summary": "Purpose: This study aims to use a large language model (LLM) to automate the generation of summaries from the CT simulation orders and evaluate its performance.\n  Materials and Methods: A total of 607 CT simulation orders for patients were collected from the Aria database at our institution. A locally hosted Llama 3.1 405B model, accessed via the Application Programming Interface (API) service, was used to extract keywords from the CT simulation orders and generate summaries. The downloaded CT simulation orders were categorized into seven groups based on treatment modalities and disease sites. For each group, a customized instruction prompt was developed collaboratively with therapists to guide the Llama 3.1 405B model in generating summaries. The ground truth for the corresponding summaries was manually derived by carefully reviewing each CT simulation order and subsequently verified by therapists. The accuracy of the LLM-generated summaries was evaluated by therapists using the verified ground truth as a reference.\n  Results: About 98% of the LLM-generated summaries aligned with the manually generated ground truth in terms of accuracy. Our evaluations showed an improved consistency in format and enhanced readability of the LLM-generated summaries compared to the corresponding therapists-generated summaries. This automated approach demonstrated a consistent performance across all groups, regardless of modality or disease site.\n  Conclusions: This study demonstrated the high precision and consistency of the Llama 3.1 405B model in extracting keywords and summarizing CT simulation orders, suggesting that LLMs have great potential to help with this task, reduce the workload of therapists and improve workflow efficiency.",
    "authors": [
      "Meiyun Cao",
      "Shaw Hu",
      "Jason Sharp",
      "Edward Clouser",
      "Jason Holmes",
      "Linda L. Lam",
      "Xiaoning Ding",
      "Diego Santos Toesca",
      "Wendy S. Lindholm",
      "Samir H. Patel",
      "Sujay A. Vora",
      "Peilong Wang",
      "Wei Liu"
    ],
    "categories": [
      "physics.med-ph",
      "cs.AI"
    ],
    "published": "2025-01-27T18:47:58Z",
    "pdf_url": "https://arxiv.org/pdf/2501.16309v1"
  },
  {
    "arxiv_id": "2501.16150v2",
    "entry_id": "http://arxiv.org/abs/2501.16150v2",
    "title": "A Comprehensive Survey of Agents for Computer Use: Foundations, Challenges, and Future Directions",
    "summary": "Agents for computer use (ACUs) are an emerging class of systems capable of executing complex tasks on digital devices - such as desktops, mobile phones, and web platforms - given instructions in natural language. These agents can automate tasks by controlling software via low-level actions like mouse clicks and touchscreen gestures. However, despite rapid progress, ACUs are not yet mature for everyday use.\n  In this survey, we investigate the state-of-the-art, trends, and research gaps in the development of practical ACUs. We provide a comprehensive review of the ACU landscape, introducing a unifying taxonomy spanning three dimensions: (I) the domain perspective, characterizing agent operating contexts; (II) the interaction perspective, describing observation modalities (e.g., screenshots, HTML) and action modalities (e.g., mouse, keyboard, code execution); and (III) the agent perspective, detailing how agents perceive, reason, and learn.\n  We review 87 ACUs and 33 datasets across foundation model-based and classical approaches through this taxonomy. Our analysis identifies six major research gaps: insufficient generalization, inefficient learning, limited planning, low task complexity in benchmarks, non-standardized evaluation, and a disconnect between research and practical conditions.\n  To address these gaps, we advocate for: (a) vision-based observations and low-level control to enhance generalization; (b) adaptive learning beyond static prompting; (c) effective planning and reasoning methods and models; (d) benchmarks that reflect real-world task complexity; (e) standardized evaluation based on task success; (f) aligning agent design with real-world deployment constraints.\n  Together, our taxonomy and analysis establish a foundation for advancing ACU research toward general-purpose agents for robust and scalable computer use.",
    "authors": [
      "Pascal J. Sager",
      "Benjamin Meyer",
      "Peng Yan",
      "Rebekka von Wartburg-Kottler",
      "Layan Etaiwi",
      "Aref Enayati",
      "Gabriel Nobel",
      "Ahmed Abdulkadir",
      "Benjamin F. Grewe",
      "Thilo Stadelmann"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "eess.SY"
    ],
    "published": "2025-01-27T15:44:02Z",
    "pdf_url": "https://arxiv.org/pdf/2501.16150v2"
  },
  {
    "arxiv_id": "2501.15374v1",
    "entry_id": "http://arxiv.org/abs/2501.15374v1",
    "title": "Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models",
    "summary": "The black-box nature of large language models (LLMs) necessitates the development of eXplainable AI (XAI) techniques for transparency and trustworthiness. However, evaluating these techniques remains a challenge. This study presents a general evaluation framework using four key metrics: Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We assess the effectiveness of six explainability techniques from five different XAI categories model simplification (LIME), perturbation-based methods (SHAP), gradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance Propagation (LRP), and attention mechanisms-based explainability methods (Attention Mechanism Visualization, AMV) across five encoder-based language models: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using the IMDB Movie Reviews and Tweet Sentiment Extraction (TSE) datasets. Our findings show that the model simplification-based XAI method (LIME) consistently outperforms across multiple metrics and models, significantly excelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness, and consistency as the complexity of large language models increases. AMV demonstrates the best Robustness, with scores as low as 0.0020. It also excels in Consistency, achieving near-perfect scores of 0.9999 across all models. Regarding Contrastivity, LRP performs the best, particularly on more complex models, with scores up to 0.9371.",
    "authors": [
      "Melkamu Abay Mersha",
      "Mesay Gemeda Yigezu",
      "Jugal Kalita"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-01-26T03:08:34Z",
    "pdf_url": "https://arxiv.org/pdf/2501.15374v1"
  },
  {
    "arxiv_id": "2504.13183v1",
    "entry_id": "http://arxiv.org/abs/2504.13183v1",
    "title": "Factors That Influence the Adoption of AI-enabled Conversational Agents (AICAs) as an Augmenting Therapeutic Tool by Frontline Healthcare Workers: From Technology Acceptance Model 3 (TAM3) Lens -- A Systematic Mapping Review",
    "summary": "Artificial intelligent (AI) conversational agents hold a promising future in the field of mental health, especially in helping marginalized communities that lack access to mental health support services. It is tempting to have a 24/7 mental health companion that can be accessed anywhere using mobile phones to provide therapist-like advice. Yet, caution should be taken, and studies around their feasibility need to be surveyed. Before adopting such a rapidly changing technology, studies on its feasibility should be explored, summarized, and synthesized to gain a solid understanding of the status quo and to enable us to build a framework that can guide us throughout the development and deployment processes. Different perspectives must be considered when investigating the feasibility of AI conversational agents, including the mental healthcare professional perspective. The literature can provide insights into their perspectives in terms of opportunities, concerns, and implications. Mental health professionals, the subject-matter experts in this field, have their points of view that should be understood and considered. This systematic literature review will explore mental health practitioners' attitudes toward AI conversational agents and the factors that affect their adoption and recommendation of the technology to augment their services and treatments. The TAM3 Framework will be the lens through which this systematic literature review will be conducted.",
    "authors": [
      "Rawan AlMakinah"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2025-01-26T02:31:27Z",
    "pdf_url": "https://arxiv.org/pdf/2504.13183v1"
  },
  {
    "arxiv_id": "2501.15351v1",
    "entry_id": "http://arxiv.org/abs/2501.15351v1",
    "title": "Fairness in LLM-Generated Surveys",
    "summary": "Large Language Models (LLMs) excel in text generation and understanding, especially in simulating socio-political and economic patterns, serving as an alternative to traditional surveys. However, their global applicability remains questionable due to unexplored biases across socio-demographic and geographic contexts. This study examines how LLMs perform across diverse populations by analyzing public surveys from Chile and the United States, focusing on predictive accuracy and fairness metrics. The results show performance disparities, with LLM consistently outperforming on U.S. datasets. This bias originates from the U.S.-centric training data, remaining evident after accounting for socio-demographic differences. In the U.S., political identity and race significantly influence prediction accuracy, while in Chile, gender, education, and religious affiliation play more pronounced roles. Our study presents a novel framework for measuring socio-demographic biases in LLMs, offering a path toward ensuring fairer and more equitable model performance across diverse socio-cultural contexts.",
    "authors": [
      "Andrés Abeliuk",
      "Vanessa Gaete",
      "Naim Bro"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-01-25T23:42:20Z",
    "pdf_url": "https://arxiv.org/pdf/2501.15351v1"
  },
  {
    "arxiv_id": "2502.14870v1",
    "entry_id": "http://arxiv.org/abs/2502.14870v1",
    "title": "Why do Experts Disagree on Existential Risk and P(doom)? A Survey of AI Experts",
    "summary": "The development of artificial general intelligence (AGI) is likely to be one of humanity's most consequential technological advancements. Leading AI labs and scientists have called for the global prioritization of AI safety citing existential risks comparable to nuclear war. However, research on catastrophic risks and AI alignment is often met with skepticism, even by experts. Furthermore, online debate over the existential risk of AI has begun to turn tribal (e.g. name-calling such as \"doomer\" or \"accelerationist\"). Until now, no systematic study has explored the patterns of belief and the levels of familiarity with AI safety concepts among experts. I surveyed 111 AI experts on their familiarity with AI safety concepts, key objections to AI safety, and reactions to safety arguments. My findings reveal that AI experts cluster into two viewpoints -- an \"AI as controllable tool\" and an \"AI as uncontrollable agent\" perspective -- diverging in beliefs toward the importance of AI safety. While most experts (78%) agreed or strongly agreed that \"technical AI researchers should be concerned about catastrophic risks\", many were unfamiliar with specific AI safety concepts. For example, only 21% of surveyed experts had heard of \"instrumental convergence,\" a fundamental concept in AI safety predicting that advanced AI systems will tend to pursue common sub-goals (such as self-preservation). The least concerned participants were the least familiar with concepts like this, suggesting that effective communication of AI safety should begin with establishing clear conceptual foundations in the field.",
    "authors": [
      "Severin Field"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-01-25T01:51:29Z",
    "pdf_url": "https://arxiv.org/pdf/2502.14870v1"
  },
  {
    "arxiv_id": "2501.15014v2",
    "entry_id": "http://arxiv.org/abs/2501.15014v2",
    "title": "On Accelerating Edge AI: Optimizing Resource-Constrained Environments",
    "summary": "Resource-constrained edge deployments demand AI solutions that balance high performance with stringent compute, memory, and energy limitations. In this survey, we present a comprehensive overview of the primary strategies for accelerating deep learning models under such constraints. First, we examine model compression techniques-pruning, quantization, tensor decomposition, and knowledge distillation-that streamline large models into smaller, faster, and more efficient variants. Next, we explore Neural Architecture Search (NAS), a class of automated methods that discover architectures inherently optimized for particular tasks and hardware budgets. We then discuss compiler and deployment frameworks, such as TVM, TensorRT, and OpenVINO, which provide hardware-tailored optimizations at inference time. By integrating these three pillars into unified pipelines, practitioners can achieve multi-objective goals, including latency reduction, memory savings, and energy efficiency-all while maintaining competitive accuracy. We also highlight emerging frontiers in hierarchical NAS, neurosymbolic approaches, and advanced distillation tailored to large language models, underscoring open challenges like pre-training pruning for massive networks. Our survey offers practical insights, identifies current research gaps, and outlines promising directions for building scalable, platform-independent frameworks to accelerate deep learning models at the edge.",
    "authors": [
      "Jacob Sander",
      "Achraf Cohen",
      "Venkat R. Dasari",
      "Brent Venable",
      "Brian Jalaian"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.NE"
    ],
    "published": "2025-01-25T01:37:03Z",
    "pdf_url": "https://arxiv.org/pdf/2501.15014v2"
  },
  {
    "arxiv_id": "2501.14305v1",
    "entry_id": "http://arxiv.org/abs/2501.14305v1",
    "title": "A Zero-Shot LLM Framework for Automatic Assignment Grading in Higher Education",
    "summary": "Automated grading has become an essential tool in education technology due to its ability to efficiently assess large volumes of student work, provide consistent and unbiased evaluations, and deliver immediate feedback to enhance learning. However, current systems face significant limitations, including the need for large datasets in few-shot learning methods, a lack of personalized and actionable feedback, and an overemphasis on benchmark performance rather than student experience. To address these challenges, we propose a Zero-Shot Large Language Model (LLM)-Based Automated Assignment Grading (AAG) system. This framework leverages prompt engineering to evaluate both computational and explanatory student responses without requiring additional training or fine-tuning. The AAG system delivers tailored feedback that highlights individual strengths and areas for improvement, thereby enhancing student learning outcomes. Our study demonstrates the system's effectiveness through comprehensive evaluations, including survey responses from higher education students that indicate significant improvements in motivation, understanding, and preparedness compared to traditional grading methods. The results validate the AAG system's potential to transform educational assessment by prioritizing learning experiences and providing scalable, high-quality feedback.",
    "authors": [
      "Calvin Yeung",
      "Jeff Yu",
      "King Chau Cheung",
      "Tat Wing Wong",
      "Chun Man Chan",
      "Kin Chi Wong",
      "Keisuke Fujii"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-01-24T08:01:41Z",
    "pdf_url": "https://arxiv.org/pdf/2501.14305v1"
  },
  {
    "arxiv_id": "2501.14294v3",
    "entry_id": "http://arxiv.org/abs/2501.14294v3",
    "title": "Examining Alignment of Large Language Models through Representative Heuristics: The Case of Political Stereotypes",
    "summary": "Examining the alignment of large language models (LLMs) has become increasingly important, e.g., when LLMs fail to operate as intended. This study examines the alignment of LLMs with human values for the domain of politics. Prior research has shown that LLM-generated outputs can include political leanings and mimic the stances of political parties on various issues. However, the extent and conditions under which LLMs deviate from empirical positions are insufficiently examined. To address this gap, we analyze the factors that contribute to LLMs' deviations from empirical positions on political issues, aiming to quantify these deviations and identify the conditions that cause them.\n  Drawing on findings from cognitive science about representativeness heuristics, i.e., situations where humans lean on representative attributes of a target group in a way that leads to exaggerated beliefs, we scrutinize LLM responses through this heuristics' lens. We conduct experiments to determine how LLMs inflate predictions about political parties, which results in stereotyping. We find that while LLMs can mimic certain political parties' positions, they often exaggerate these positions more than human survey respondents do. Also, LLMs tend to overemphasize representativeness more than humans. This study highlights the susceptibility of LLMs to representativeness heuristics, suggesting a potential vulnerability of LLMs that facilitates political stereotyping. We also test prompt-based mitigation strategies, finding that strategies that can mitigate representative heuristics in humans are also effective in reducing the influence of representativeness on LLM-generated responses.",
    "authors": [
      "Sullam Jeoung",
      "Yubin Ge",
      "Haohan Wang",
      "Jana Diesner"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-01-24T07:24:23Z",
    "pdf_url": "https://arxiv.org/pdf/2501.14294v3"
  },
  {
    "arxiv_id": "2501.14291v2",
    "entry_id": "http://arxiv.org/abs/2501.14291v2",
    "title": "Advances in Temporal Point Processes: Bayesian, Neural, and LLM Approaches",
    "summary": "Temporal point processes (TPPs) are stochastic process models used to characterize event sequences occurring in continuous time. Traditional statistical TPPs have a long-standing history, with numerous models proposed and successfully applied across diverse domains. In recent years, advances in deep learning have spurred the development of neural TPPs, enabling greater flexibility and expressiveness in capturing complex temporal dynamics. The emergence of large language models (LLMs) has further sparked excitement, offering new possibilities for modeling and analyzing event sequences by leveraging their rich contextual understanding. This survey presents a comprehensive review of recent research on TPPs from three perspectives: Bayesian, deep learning, and LLM approaches. We begin with a review of the fundamental concepts of TPPs, followed by an in-depth discussion of model design and parameter estimation techniques in these three frameworks. We also revisit classic application areas of TPPs to highlight their practical relevance. Finally, we outline challenges and promising directions for future research.",
    "authors": [
      "Feng Zhou",
      "Quyu Kong",
      "Jie Qiao",
      "Cheng Wan",
      "Yixuan Zhang",
      "Ruichu Cai"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-01-24T07:13:26Z",
    "pdf_url": "https://arxiv.org/pdf/2501.14291v2"
  },
  {
    "arxiv_id": "2501.17175v1",
    "entry_id": "http://arxiv.org/abs/2501.17175v1",
    "title": "Document-Level Sentiment Analysis of Urdu Text Using Deep Learning Techniques",
    "summary": "Document level Urdu Sentiment Analysis (SA) is a challenging Natural Language Processing (NLP) task as it deals with large documents in a resource-poor language. In large documents, there are ample amounts of words that exhibit different viewpoints. Deep learning (DL) models comprise of complex neural network architectures that have the ability to learn diverse features of the data to classify various sentiments. Besides audio, image and video classification; DL algorithms are now extensively used in text-based classification problems. To explore the powerful DL techniques for Urdu SA, we have applied five different DL architectures namely, Bidirectional Long Short Term Memory (BiLSTM), Convolutional Neural Network (CNN), Convolutional Neural Network with Bidirectional Long Short Term Memory (CNN-BiLSTM), Bidirectional Encoder Representation from Transformer (BERT). In this paper, we have proposed a DL hybrid model that integrates BiLSTM with Single Layer Multi Filter Convolutional Neural Network (BiLSTM-SLMFCNN). The proposed and baseline techniques are applied on Urdu Customer Support data set and IMDB Urdu movie review data set by using pretrained Urdu word embeddings that are suitable for (SA) at the document level. Results of these techniques are evaluated and our proposed model outperforms all other DL techniques for Urdu SA. BiLSTM-SLMFCNN outperformed the baseline DL models and achieved 83{\\%}, 79{\\%}, 83{\\%} and 94{\\%} accuracy on small, medium and large sized IMDB Urdu movie review data set and Urdu Customer Support data set respectively.",
    "authors": [
      "Ammarah Irum",
      "M. Ali Tahir"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-01-23T21:25:37Z",
    "pdf_url": "https://arxiv.org/pdf/2501.17175v1"
  },
  {
    "arxiv_id": "2501.12728v1",
    "entry_id": "http://arxiv.org/abs/2501.12728v1",
    "title": "A Call for Critically Rethinking and Reforming Data Analysis in Empirical Software Engineering",
    "summary": "Context: Empirical Software Engineering (ESE) drives innovation in SE through qualitative and quantitative studies. However, concerns about the correct application of empirical methodologies have existed since the 2006 Dagstuhl seminar on SE. Objective: To analyze three decades of SE research, identify mistakes in statistical methods, and evaluate experts' ability to detect and address these issues. Methods: We conducted a literature survey of ~27,000 empirical studies, using LLMs to classify statistical methodologies as adequate or inadequate. Additionally, we selected 30 primary studies and held a workshop with 33 ESE experts to assess their ability to identify and resolve statistical issues. Results: Significant statistical issues were found in the primary studies, and experts showed limited ability to detect and correct these methodological problems, raising concerns about the broader ESE community's proficiency in this area. Conclusions. Despite our study's eventual limitations, its results shed light on recurring issues from promoting information copy-and-paste from past authors' works and the continuous publication of inadequate approaches that promote dubious results and jeopardize the spread of the correct statistical strategies among researchers. Besides, it justifies further investigation into empirical rigor in software engineering to expose these recurring issues and establish a framework for reassessing our field's foundation of statistical methodology application. Therefore, this work calls for critically rethinking and reforming data analysis in empirical software engineering, paving the way for our work soon.",
    "authors": [
      "Matteo Esposito",
      "Mikel Robredo",
      "Murali Sridharan",
      "Guilherme Horta Travassos",
      "Rafael Peñaloza",
      "Valentina Lenarduzzi"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.DL"
    ],
    "published": "2025-01-22T09:05:01Z",
    "pdf_url": "https://arxiv.org/pdf/2501.12728v1"
  },
  {
    "arxiv_id": "2501.12557v1",
    "entry_id": "http://arxiv.org/abs/2501.12557v1",
    "title": "Understanding the LLM-ification of CHI: Unpacking the Impact of LLMs at CHI through a Systematic Literature Review",
    "summary": "Large language models (LLMs) have been positioned to revolutionize HCI, by reshaping not only the interfaces, design patterns, and sociotechnical systems that we study, but also the research practices we use. To-date, however, there has been little understanding of LLMs' uptake in HCI. We address this gap via a systematic literature review of 153 CHI papers from 2020-24 that engage with LLMs. We taxonomize: (1) domains where LLMs are applied; (2) roles of LLMs in HCI projects; (3) contribution types; and (4) acknowledged limitations and risks. We find LLM work in 10 diverse domains, primarily via empirical and artifact contributions. Authors use LLMs in five distinct roles, including as research tools or simulated users. Still, authors often raise validity and reproducibility concerns, and overwhelmingly study closed models. We outline opportunities to improve HCI research with and on LLMs, and provide guiding questions for researchers to consider the validity and appropriateness of LLM-related work.",
    "authors": [
      "Rock Yuren Pang",
      "Hope Schroeder",
      "Kynnedy Simone Smith",
      "Solon Barocas",
      "Ziang Xiao",
      "Emily Tseng",
      "Danielle Bragg"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2025-01-22T00:31:51Z",
    "pdf_url": "https://arxiv.org/pdf/2501.12557v1"
  },
  {
    "arxiv_id": "2501.13958v3",
    "entry_id": "http://arxiv.org/abs/2501.13958v3",
    "title": "A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphRAG.",
    "authors": [
      "Qinggang Zhang",
      "Shengyuan Chen",
      "Yuanchen Bei",
      "Zheng Yuan",
      "Huachi Zhou",
      "Zijin Hong",
      "Hao Chen",
      "Yilin Xiao",
      "Chuang Zhou",
      "Junnan Dong",
      "Yi Chang",
      "Xiao Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR"
    ],
    "published": "2025-01-21T06:25:21Z",
    "pdf_url": "https://arxiv.org/pdf/2501.13958v3"
  },
  {
    "arxiv_id": "2501.11847v2",
    "entry_id": "http://arxiv.org/abs/2501.11847v2",
    "title": "A Survey on Memory-Efficient Transformer-Based Model Training in AI for Science",
    "summary": "Scientific research faces high costs and inefficiencies with traditional methods, but the rise of deep learning and large language models (LLMs) offers innovative solutions. This survey reviews transformer-based LLM applications across scientific fields such as biology, medicine, chemistry, and meteorology, underscoring their role in advancing research. However, the continuous expansion of model size has led to significant memory demands, hindering further development and application of LLMs for science. This survey systematically reviews and categorizes memory-efficient pre-training techniques for large-scale transformers, including algorithm-level, system-level, and hardware-software co-optimization. Using AlphaFold 2 as an example, we demonstrate how tailored memory optimization methods can reduce storage needs while preserving prediction accuracy. By bridging model efficiency and scientific application needs, we hope to provide insights for scalable and cost-effective LLM training in AI for science.",
    "authors": [
      "Kaiyuan Tian",
      "Linbo Qiao",
      "Baihui Liu",
      "Gongqingjian Jiang",
      "Shanshan Li",
      "Dongsheng Li"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-01-21T03:06:30Z",
    "pdf_url": "https://arxiv.org/pdf/2501.11847v2"
  },
  {
    "arxiv_id": "2501.13955v1",
    "entry_id": "http://arxiv.org/abs/2501.13955v1",
    "title": "Guided Persona-based AI Surveys: Can we replicate personal mobility preferences at scale using LLMs?",
    "summary": "This study explores the potential of Large Language Models (LLMs) to generate artificial surveys, with a focus on personal mobility preferences in Germany. By leveraging LLMs for synthetic data creation, we aim to address the limitations of traditional survey methods, such as high costs, inefficiency and scalability challenges. A novel approach incorporating \"Personas\" - combinations of demographic and behavioural attributes - is introduced and compared to five other synthetic survey methods, which vary in their use of real-world data and methodological complexity. The MiD 2017 dataset, a comprehensive mobility survey in Germany, serves as a benchmark to assess the alignment of synthetic data with real-world patterns. The results demonstrate that LLMs can effectively capture complex dependencies between demographic attributes and preferences while offering flexibility to explore hypothetical scenarios. This approach presents valuable opportunities for transportation planning and social science research, enabling scalable, cost-efficient and privacy-preserving data generation.",
    "authors": [
      "Ioannis Tzachristas",
      "Santhanakrishnan Narayanan",
      "Constantinos Antoniou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-01-20T15:11:03Z",
    "pdf_url": "https://arxiv.org/pdf/2501.13955v1"
  },
  {
    "arxiv_id": "2501.13117v1",
    "entry_id": "http://arxiv.org/abs/2501.13117v1",
    "title": "MyGO Multiplex CoT: A Method for Self-Reflection in Large Language Models via Double Chain of Thought Thinking",
    "summary": "Recent advancements in large language models (LLMs) have demonstrated their impressive abilities in various reasoning and decision-making tasks. However, the quality and coherence of the reasoning process can still benefit from enhanced introspection and self-reflection. In this paper, we introduce Multiplex CoT (Chain of Thought), a method that enables LLMs to simulate a form of self-review while reasoning, by initiating double Chain of Thought (CoT) thinking. Multiplex CoT leverages the power of iterative reasoning, where the model generates an initial chain of thought and subsequently critiques and refines this reasoning with a second round of thought generation. This recursive approach allows for more coherent, logical, and robust answers, improving the overall decision-making process. We demonstrate how this method can be effectively implemented using simple prompt engineering in existing LLM architectures, achieving an effect similar to that of the Learning-Refinement Model (LRM) without the need for additional training. Additionally, we present a practical guide for implementing the method in Google Colab, enabling easy integration into real-world applications.",
    "authors": [
      "Shihao Ji",
      "Zihui Song",
      "Fucheng Zhong",
      "Jisen Jia",
      "Zhaobo Wu",
      "Zheyi Cao",
      "Tianhao Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-01-20T12:54:57Z",
    "pdf_url": "https://arxiv.org/pdf/2501.13117v1"
  },
  {
    "arxiv_id": "2501.11430v5",
    "entry_id": "http://arxiv.org/abs/2501.11430v5",
    "title": "A Survey on Diffusion Models for Anomaly Detection",
    "summary": "Diffusion models (DMs) have emerged as a powerful class of generative AI models, showing remarkable potential in anomaly detection (AD) tasks across various domains, such as cybersecurity, fraud detection, healthcare, and manufacturing. The intersection of these two fields, termed diffusion models for anomaly detection (DMAD), offers promising solutions for identifying deviations in increasingly complex and high-dimensional data. In this survey, we review recent advances in DMAD research. We begin by presenting the fundamental concepts of AD and DMs, followed by a comprehensive analysis of classic DM architectures including DDPMs, DDIMs, and Score SDEs. We further categorize existing DMAD methods into reconstruction-based, density-based, and hybrid approaches, providing detailed examinations of their methodological innovations. We also explore the diverse tasks across different data modalities, encompassing image, time series, video, and multimodal data analysis. Furthermore, we discuss critical challenges and emerging research directions, including computational efficiency, model interpretability, robustness enhancement, edge-cloud collaboration, and integration with large language models. The collection of DMAD research papers and resources is available at https://github.com/fdjingliu/DMAD.",
    "authors": [
      "Jing Liu",
      "Zhenchao Ma",
      "Zepu Wang",
      "Chenxuanyin Zou",
      "Jiayang Ren",
      "Zehua Wang",
      "Liang Song",
      "Bo Hu",
      "Yang Liu",
      "Victor C. M. Leung"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-01-20T12:06:54Z",
    "pdf_url": "https://arxiv.org/pdf/2501.11430v5"
  },
  {
    "arxiv_id": "2501.11264v3",
    "entry_id": "http://arxiv.org/abs/2501.11264v3",
    "title": "Code Readability in the Age of Large Language Models: An Industrial Case Study from Atlassian",
    "summary": "Software engineers spend a significant amount of time reading code during the software development process, especially in the age of large language models (LLMs) that can automatically generate code. However, little is known about the readability of the LLM-generated code and whether it is still important from practitioners' perspectives in this new era. In this paper, we conduct a survey to explore the practitioners' perspectives on code readability in the age of LLMs and investigate the readability of our LLM-based software development agents framework, HULA, by comparing its generated code with human-written code in real-world scenarios. Overall, the findings underscore that (1) readability remains a critical aspect of software development; (2) the readability of our LLM-generated code is comparable to human-written code, fostering the establishment of appropriate trust and driving the broad adoption of our LLM-powered software development platform.",
    "authors": [
      "Wannita Takerngsaksiri",
      "Chakkrit Tantithamthavorn",
      "Micheal Fu",
      "Jirat Pasuksmit",
      "Kun Chen",
      "Ming Wu"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-01-20T04:11:21Z",
    "pdf_url": "https://arxiv.org/pdf/2501.11264v3"
  },
  {
    "arxiv_id": "2501.11223v4",
    "entry_id": "http://arxiv.org/abs/2501.11223v4",
    "title": "Reasoning Language Models: A Blueprint",
    "summary": "Reasoning language models (RLMs), also known as Large Reasoning Models (LRMs), such as OpenAI's o1 and o3, DeepSeek-R1, and Alibaba's QwQ, have redefined AI's problem-solving capabilities by extending LLMs with advanced reasoning mechanisms. Yet, their high costs, proprietary nature, and complex architectures - uniquely combining reinforcement learning (RL), search heuristics, and LLMs - present accessibility and scalability challenges. To address these, we propose a comprehensive blueprint that organizes RLM components into a modular framework, based on a survey and analysis of all RLM works. This blueprint incorporates diverse reasoning structures (chains, trees, graphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search, Beam Search), RL concepts (policy, value models and others), supervision schemes (Outcome-Based and Process-Based Supervision), and other related concepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent tools). We also provide detailed mathematical formulations and algorithmic specifications to simplify RLM implementation. By showing how schemes like LLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases, we demonstrate the blueprint's versatility and unifying potential. To illustrate its utility, we introduce x1, a modular implementation for rapid RLM prototyping and experimentation. Using x1 and a literature review, we provide key insights, such as multi-phase training for policy and value models, and the importance of familiar training distributions. Finally, we discuss scalable RLM cloud deployments and we outline how RLMs can integrate with a broader LLM ecosystem. Our work demystifies RLM construction, democratizes advanced reasoning capabilities, and fosters innovation, aiming to mitigate the gap between \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and experimentation.",
    "authors": [
      "Maciej Besta",
      "Julia Barth",
      "Eric Schreiber",
      "Ales Kubicek",
      "Afonso Catarino",
      "Robert Gerstenberger",
      "Piotr Nyczyk",
      "Patrick Iff",
      "Yueling Li",
      "Sam Houliston",
      "Tomasz Sternal",
      "Marcin Copik",
      "Grzegorz Kwaśniewski",
      "Jürgen Müller",
      "Łukasz Flis",
      "Hannes Eberhard",
      "Zixuan Chen",
      "Hubert Niewiadomski",
      "Torsten Hoefler"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-01-20T02:16:19Z",
    "pdf_url": "https://arxiv.org/pdf/2501.11223v4"
  },
  {
    "arxiv_id": "2501.13947v3",
    "entry_id": "http://arxiv.org/abs/2501.13947v3",
    "title": "A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods",
    "summary": "The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.",
    "authors": [
      "Wenli Yang",
      "Lilian Some",
      "Michael Bain",
      "Byeong Kang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-01-19T23:25:21Z",
    "pdf_url": "https://arxiv.org/pdf/2501.13947v3"
  },
  {
    "arxiv_id": "2501.11114v1",
    "entry_id": "http://arxiv.org/abs/2501.11114v1",
    "title": "Clinical trial cohort selection using Large Language Models on n2c2 Challenges",
    "summary": "Clinical trials are a critical process in the medical field for introducing new treatments and innovations. However, cohort selection for clinical trials is a time-consuming process that often requires manual review of patient text records for specific keywords. Though there have been studies on standardizing the information across the various platforms, Natural Language Processing (NLP) tools remain crucial for spotting eligibility criteria in textual reports. Recently, pre-trained large language models (LLMs) have gained popularity for various NLP tasks due to their ability to acquire a nuanced understanding of text. In this paper, we study the performance of large language models on clinical trial cohort selection and leverage the n2c2 challenges to benchmark their performance. Our results are promising with regard to the incorporation of LLMs for simple cohort selection tasks, but also highlight the difficulties encountered by these models as soon as fine-grained knowledge and reasoning are required.",
    "authors": [
      "Chi-en Amy Tai",
      "Xavier Tannier"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-01-19T17:07:02Z",
    "pdf_url": "https://arxiv.org/pdf/2501.11114v1"
  },
  {
    "arxiv_id": "2501.13946v1",
    "entry_id": "http://arxiv.org/abs/2501.13946v1",
    "title": "Hallucination Mitigation using Agentic AI Natural Language-Based Frameworks",
    "summary": "Hallucinations remain a significant challenge in current Generative AI models, undermining trust in AI systems and their reliability. This study investigates how orchestrating multiple specialized Artificial Intelligent Agents can help mitigate such hallucinations, with a focus on systems leveraging Natural Language Processing (NLP) to facilitate seamless agent interactions. To achieve this, we design a pipeline that introduces over three hundred prompts, purposefully crafted to induce hallucinations, into a front-end agent. The outputs are then systematically reviewed and refined by second- and third-level agents, each employing distinct large language models and tailored strategies to detect unverified claims, incorporate explicit disclaimers, and clarify speculative content. Additionally, we introduce a set of novel Key Performance Indicators (KPIs) specifically designed to evaluate hallucination score levels. A dedicated fourth-level AI agent is employed to evaluate these KPIs, providing detailed assessments and ensuring accurate quantification of shifts in hallucination-related behaviors. A core component of this investigation is the use of the OVON (Open Voice Network) framework, which relies on universal NLP-based interfaces to transfer contextual information among agents. Through structured JSON messages, each agent communicates its assessment of the hallucination likelihood and the reasons underlying questionable content, thereby enabling the subsequent stage to refine the text without losing context. The results demonstrate that employing multiple specialized agents capable of interoperating with each other through NLP-based agentic frameworks can yield promising outcomes in hallucination mitigation, ultimately bolstering trust within the AI community.",
    "authors": [
      "Diego Gosmar",
      "Deborah A. Dahl"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2025-01-19T11:19:25Z",
    "pdf_url": "https://arxiv.org/pdf/2501.13946v1"
  },
  {
    "arxiv_id": "2501.10945v3",
    "entry_id": "http://arxiv.org/abs/2501.10945v3",
    "title": "Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories, Applications, and Beyond",
    "summary": "Many modern deep learning applications require balancing multiple objectives that are often conflicting. Examples include multi-task learning, fairness-aware learning, and the alignment of Large Language Models (LLMs). This leads to multi-objective deep learning, which tries to find optimal trade-offs or Pareto-optimal solutions by adapting mathematical principles from the field of Multi-Objective Optimization (MOO). However, directly applying gradient-based MOO techniques to deep neural networks presents unique challenges, including high computational costs, optimization instability, and the difficulty of effectively incorporating user preferences. This paper provides a comprehensive survey of gradient-based techniques for multi-objective deep learning. We systematically categorize existing algorithms based on their outputs: (i) methods that find a single, well-balanced solution, (ii) methods that generate a finite set of diverse Pareto-optimal solutions, and (iii) methods that learn a continuous Pareto set of solutions. In addition to this taxonomy, the survey covers theoretical analyses, key applications, practical resources, and highlights open challenges and promising directions for future research. A comprehensive list of multi-objective deep learning algorithms is available at https://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning.",
    "authors": [
      "Weiyu Chen",
      "Baijiong Lin",
      "Xiaoyuan Zhang",
      "Xi Lin",
      "Han Zhao",
      "Qingfu Zhang",
      "James T. Kwok"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2025-01-19T04:56:55Z",
    "pdf_url": "https://arxiv.org/pdf/2501.10945v3"
  },
  {
    "arxiv_id": "2501.10604v1",
    "entry_id": "http://arxiv.org/abs/2501.10604v1",
    "title": "When language and vision meet road safety: leveraging multimodal large language models for video-based traffic accident analysis",
    "summary": "The increasing availability of traffic videos functioning on a 24/7/365 time scale has the great potential of increasing the spatio-temporal coverage of traffic accidents, which will help improve traffic safety. However, analyzing footage from hundreds, if not thousands, of traffic cameras in a 24/7/365 working protocol remains an extremely challenging task, as current vision-based approaches primarily focus on extracting raw information, such as vehicle trajectories or individual object detection, but require laborious post-processing to derive actionable insights. We propose SeeUnsafe, a new framework that integrates Multimodal Large Language Model (MLLM) agents to transform video-based traffic accident analysis from a traditional extraction-then-explanation workflow to a more interactive, conversational approach. This shift significantly enhances processing throughput by automating complex tasks like video classification and visual grounding, while improving adaptability by enabling seamless adjustments to diverse traffic scenarios and user-defined queries. Our framework employs a severity-based aggregation strategy to handle videos of various lengths and a novel multimodal prompt to generate structured responses for review and evaluation and enable fine-grained visual grounding. We introduce IMS (Information Matching Score), a new MLLM-based metric for aligning structured responses with ground truth. We conduct extensive experiments on the Toyota Woven Traffic Safety dataset, demonstrating that SeeUnsafe effectively performs accident-aware video classification and visual grounding by leveraging off-the-shelf MLLMs. Source code will be available at \\url{https://github.com/ai4ce/SeeUnsafe}.",
    "authors": [
      "Ruixuan Zhang",
      "Beichen Wang",
      "Juexiao Zhang",
      "Zilin Bian",
      "Chen Feng",
      "Kaan Ozbay"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-01-17T23:35:34Z",
    "pdf_url": "https://arxiv.org/pdf/2501.10604v1"
  },
  {
    "arxiv_id": "2501.10326v2",
    "entry_id": "http://arxiv.org/abs/2501.10326v2",
    "title": "Large language models for automated scholarly paper review: A survey",
    "summary": "Large language models (LLMs) have significantly impacted human society, influencing various domains. Among them, academia is not simply a domain affected by LLMs, but it is also the pivotal force in the development of LLMs. In academic publication, this phenomenon is represented during the incorporation of LLMs into the peer review mechanism for reviewing manuscripts. LLMs hold transformative potential for the full-scale implementation of automated scholarly paper review (ASPR), but they also pose new issues and challenges that need to be addressed. In this survey paper, we aim to provide a holistic view of ASPR in the era of LLMs. We begin with a survey to find out which LLMs are used to conduct ASPR. Then, we review what ASPR-related technological bottlenecks have been solved with the incorporation of LLM technology. After that, we move on to explore new methods, new datasets, new source code, and new online systems that come with LLMs for ASPR. Furthermore, we summarize the performance and issues of LLMs in ASPR, and investigate the attitudes and reactions of publishers and academia to ASPR. Lastly, we discuss the challenges and future directions associated with the development of LLMs for ASPR. This survey serves as an inspirational reference for the researchers and can promote the progress of ASPR for its actual implementation.",
    "authors": [
      "Zhenzhen Zhuang",
      "Jiandong Chen",
      "Hongfeng Xu",
      "Yuwen Jiang",
      "Jialiang Lin"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.DL"
    ],
    "published": "2025-01-17T17:56:58Z",
    "pdf_url": "https://arxiv.org/pdf/2501.10326v2"
  },
  {
    "arxiv_id": "2501.10186v1",
    "entry_id": "http://arxiv.org/abs/2501.10186v1",
    "title": "Generative Artificial Intelligence: Implications for Biomedical and Health Professions Education",
    "summary": "Generative AI has had a profound impact on biomedicine and health, both in professional work and in education. Based on large language models (LLMs), generative AI has been found to perform as well as humans in simulated situations taking medical board exams, answering clinical questions, solving clinical cases, applying clinical reasoning, and summarizing information. Generative AI is also being used widely in education, performing well in academic courses and their assessments. This review summarizes the successes of LLMs and highlights some of their challenges in the context of education, most notably aspects that may undermines the acquisition of knowledge and skills for professional work. It then provides recommendations for best practices overcoming shortcomings for LLM use in education. Although there are challenges for use of generative AI in education, all students and faculty, in biomedicine and health and beyond, must have understanding and be competent in its use.",
    "authors": [
      "William Hersh"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-17T13:32:19Z",
    "pdf_url": "https://arxiv.org/pdf/2501.10186v1"
  },
  {
    "arxiv_id": "2501.10069v4",
    "entry_id": "http://arxiv.org/abs/2501.10069v4",
    "title": "A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling, Search Algorithms, and Relevant Frameworks",
    "summary": "LLM test-time compute (or LLM inference) via search has emerged as a promising research area with rapid developments. However, current frameworks often adopt distinct perspectives on three key aspects: task definition, LLM profiling, and search procedures, making direct comparisons challenging. Moreover, the search algorithms employed often diverge from standard implementations, and their specific characteristics are not thoroughly specified. This survey aims to provide a comprehensive but integrated technical review on existing LIS frameworks. Specifically, we unify task definitions under Markov Decision Process (MDP) and provides modular definitions of LLM profiling and search procedures. The definitions enable precise comparisons of various LLM inference frameworks while highlighting their departures from conventional search algorithms. We also discuss the applicability, performance, and efficiency of these methods. For ongoing paper updates, please refer to our GitHub repository: https://github.com/xinzhel/LLM-Search.",
    "authors": [
      "Xinzhe Li"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-17T09:42:48Z",
    "pdf_url": "https://arxiv.org/pdf/2501.10069v4"
  },
  {
    "arxiv_id": "2501.09967v1",
    "entry_id": "http://arxiv.org/abs/2501.09967v1",
    "title": "Explainable artificial intelligence (XAI): from inherent explainability to large language models",
    "summary": "Artificial Intelligence (AI) has continued to achieve tremendous success in recent times. However, the decision logic of these frameworks is often not transparent, making it difficult for stakeholders to understand, interpret or explain their behavior. This limitation hinders trust in machine learning systems and causes a general reluctance towards their adoption in practical applications, particularly in mission-critical domains like healthcare and autonomous driving. Explainable AI (XAI) techniques facilitate the explainability or interpretability of machine learning models, enabling users to discern the basis of the decision and possibly avert undesirable behavior. This comprehensive survey details the advancements of explainable AI methods, from inherently interpretable models to modern approaches for achieving interpretability of various black box models, including large language models (LLMs). Additionally, we review explainable AI techniques that leverage LLM and vision-language model (VLM) frameworks to automate or improve the explainability of other machine learning models. The use of LLM and VLM as interpretability methods particularly enables high-level, semantically meaningful explanations of model decisions and behavior. Throughout the paper, we highlight the scientific principles, strengths and weaknesses of state-of-the-art methods and outline different areas of improvement. Where appropriate, we also present qualitative and quantitative comparison results of various methods to show how they compare. Finally, we discuss the key challenges of XAI and directions for future research.",
    "authors": [
      "Fuseini Mumuni",
      "Alhassan Mumuni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2025-01-17T06:16:57Z",
    "pdf_url": "https://arxiv.org/pdf/2501.09967v1"
  },
  {
    "arxiv_id": "2501.09686v3",
    "entry_id": "http://arxiv.org/abs/2501.09686v3",
    "title": "Towards Large Reasoning Models: A Survey of Reinforced Reasoning with Large Language Models",
    "summary": "Language has long been conceived as an essential tool for human reasoning. The breakthrough of Large Language Models (LLMs) has sparked significant research interest in leveraging these models to tackle complex reasoning tasks. Researchers have moved beyond simple autoregressive token generation by introducing the concept of \"thought\" -- a sequence of tokens representing intermediate steps in the reasoning process. This innovative paradigm enables LLMs' to mimic complex human reasoning processes, such as tree search and reflective thinking. Recently, an emerging trend of learning to reason has applied reinforcement learning (RL) to train LLMs to master reasoning processes. This approach enables the automatic generation of high-quality reasoning trajectories through trial-and-error search algorithms, significantly expanding LLMs' reasoning capacity by providing substantially more training data. Furthermore, recent studies demonstrate that encouraging LLMs to \"think\" with more tokens during test-time inference can further significantly boost reasoning accuracy. Therefore, the train-time and test-time scaling combined to show a new research frontier -- a path toward Large Reasoning Model. The introduction of OpenAI's o1 series marks a significant milestone in this research direction. In this survey, we present a comprehensive review of recent progress in LLM reasoning. We begin by introducing the foundational background of LLMs and then explore the key technical components driving the development of large reasoning models, with a focus on automated data construction, learning-to-reason techniques, and test-time scaling. We also analyze popular open-source projects at building large reasoning models, and conclude with open challenges and future research directions.",
    "authors": [
      "Fengli Xu",
      "Qianyue Hao",
      "Zefang Zong",
      "Jingwei Wang",
      "Yunke Zhang",
      "Jingyi Wang",
      "Xiaochong Lan",
      "Jiahui Gong",
      "Tianjian Ouyang",
      "Fanjin Meng",
      "Chenyang Shao",
      "Yuwei Yan",
      "Qinglong Yang",
      "Yiwen Song",
      "Sijian Ren",
      "Xinyuan Hu",
      "Yu Li",
      "Jie Feng",
      "Chen Gao",
      "Yong Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-01-16T17:37:58Z",
    "pdf_url": "https://arxiv.org/pdf/2501.09686v3"
  },
  {
    "arxiv_id": "2501.09655v1",
    "entry_id": "http://arxiv.org/abs/2501.09655v1",
    "title": "A Survey of Research in Large Language Models for Electronic Design Automation",
    "summary": "Within the rapidly evolving domain of Electronic Design Automation (EDA), Large Language Models (LLMs) have emerged as transformative technologies, offering unprecedented capabilities for optimizing and automating various aspects of electronic design. This survey provides a comprehensive exploration of LLM applications in EDA, focusing on advancements in model architectures, the implications of varying model sizes, and innovative customization techniques that enable tailored analytical insights. By examining the intersection of LLM capabilities and EDA requirements, the paper highlights the significant impact these models have on extracting nuanced understandings from complex datasets. Furthermore, it addresses the challenges and opportunities in integrating LLMs into EDA workflows, paving the way for future research and application in this dynamic field. Through this detailed analysis, the survey aims to offer valuable insights to professionals in the EDA industry, AI researchers, and anyone interested in the convergence of advanced AI technologies and electronic design.",
    "authors": [
      "Jingyu Pan",
      "Guanglei Zhou",
      "Chen-Chia Chang",
      "Isaac Jacobson",
      "Jiang Hu",
      "Yiran Chen"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-01-16T16:51:59Z",
    "pdf_url": "https://arxiv.org/pdf/2501.09655v1"
  },
  {
    "arxiv_id": "2501.09646v1",
    "entry_id": "http://arxiv.org/abs/2501.09646v1",
    "title": "NS-Gym: Open-Source Simulation Environments and Benchmarks for Non-Stationary Markov Decision Processes",
    "summary": "In many real-world applications, agents must make sequential decisions in environments where conditions are subject to change due to various exogenous factors. These non-stationary environments pose significant challenges to traditional decision-making models, which typically assume stationary dynamics. Non-stationary Markov decision processes (NS-MDPs) offer a framework to model and solve decision problems under such changing conditions. However, the lack of standardized benchmarks and simulation tools has hindered systematic evaluation and advance in this field. We present NS-Gym, the first simulation toolkit designed explicitly for NS-MDPs, integrated within the popular Gymnasium framework. In NS-Gym, we segregate the evolution of the environmental parameters that characterize non-stationarity from the agent's decision-making module, allowing for modular and flexible adaptations to dynamic environments. We review prior work in this domain and present a toolkit encapsulating key problem characteristics and types in NS-MDPs. This toolkit is the first effort to develop a set of standardized interfaces and benchmark problems to enable consistent and reproducible evaluation of algorithms under non-stationary conditions. We also benchmark six algorithmic approaches from prior work on NS-MDPs using NS-Gym. Our vision is that NS-Gym will enable researchers to assess the adaptability and robustness of their decision-making algorithms to non-stationary conditions.",
    "authors": [
      "Nathaniel S. Keplinger",
      "Baiting Luo",
      "Iliyas Bektas",
      "Yunuo Zhang",
      "Kyle Hollins Wray",
      "Aron Laszka",
      "Abhishek Dubey",
      "Ayan Mukhopadhyay"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-16T16:38:33Z",
    "pdf_url": "https://arxiv.org/pdf/2501.09646v1"
  },
  {
    "arxiv_id": "2501.09431v1",
    "entry_id": "http://arxiv.org/abs/2501.09431v1",
    "title": "A Survey on Responsible LLMs: Inherent Risk, Malicious Use, and Mitigation Strategy",
    "summary": "While large language models (LLMs) present significant potential for supporting numerous real-world applications and delivering positive social impacts, they still face significant challenges in terms of the inherent risk of privacy leakage, hallucinated outputs, and value misalignment, and can be maliciously used for generating toxic content and unethical purposes after been jailbroken. Therefore, in this survey, we present a comprehensive review of recent advancements aimed at mitigating these issues, organized across the four phases of LLM development and usage: data collecting and pre-training, fine-tuning and alignment, prompting and reasoning, and post-processing and auditing. We elaborate on the recent advances for enhancing the performance of LLMs in terms of privacy protection, hallucination reduction, value alignment, toxicity elimination, and jailbreak defenses. In contrast to previous surveys that focus on a single dimension of responsible LLMs, this survey presents a unified framework that encompasses these diverse dimensions, providing a comprehensive view of enhancing LLMs to better serve real-world applications.",
    "authors": [
      "Huandong Wang",
      "Wenjie Fu",
      "Yingzhou Tang",
      "Zhilong Chen",
      "Yuxi Huang",
      "Jinghua Piao",
      "Chen Gao",
      "Fengli Xu",
      "Tao Jiang",
      "Yong Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CY"
    ],
    "published": "2025-01-16T09:59:45Z",
    "pdf_url": "https://arxiv.org/pdf/2501.09431v1"
  },
  {
    "arxiv_id": "2501.09136v3",
    "entry_id": "http://arxiv.org/abs/2501.09136v3",
    "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
    "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI) by enabling human like text generation and natural language understanding. However, their reliance on static training data limits their ability to respond to dynamic, real time queries, resulting in outdated or inaccurate outputs. Retrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs by integrating real time data retrieval to provide contextually relevant and up-to-date responses. Despite its promise, traditional RAG systems are constrained by static workflows and lack the adaptability required for multistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these limitations by embedding autonomous AI agents into the RAG pipeline. These agents leverage agentic design patterns reflection, planning, tool use, and multiagent collaboration to dynamically manage retrieval strategies, iteratively refine contextual understanding, and adapt workflows to meet complex task requirements. This integration enables Agentic RAG systems to deliver unparalleled flexibility, scalability, and context awareness across diverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning with its foundational principles and the evolution of RAG paradigms. It presents a detailed taxonomy of Agentic RAG architectures, highlights key applications in industries such as healthcare, finance, and education, and examines practical implementation strategies. Additionally, it addresses challenges in scaling these systems, ensuring ethical decision making, and optimizing performance for real-world applications, while providing detailed insights into frameworks and tools for implementing Agentic RAG.",
    "authors": [
      "Aditi Singh",
      "Abul Ehtesham",
      "Saket Kumar",
      "Tala Talaei Khoei"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2025-01-15T20:40:25Z",
    "pdf_url": "https://arxiv.org/pdf/2501.09136v3"
  },
  {
    "arxiv_id": "2501.08457v1",
    "entry_id": "http://arxiv.org/abs/2501.08457v1",
    "title": "Large Language Models For Text Classification: Case Study And Comprehensive Review",
    "summary": "Unlocking the potential of Large Language Models (LLMs) in data classification represents a promising frontier in natural language processing. In this work, we evaluate the performance of different LLMs in comparison with state-of-the-art deep-learning and machine-learning models, in two different classification scenarios: i) the classification of employees' working locations based on job reviews posted online (multiclass classification), and 2) the classification of news articles as fake or not (binary classification). Our analysis encompasses a diverse range of language models differentiating in size, quantization, and architecture. We explore the impact of alternative prompting techniques and evaluate the models based on the weighted F1-score. Also, we examine the trade-off between performance (F1-score) and time (inference response time) for each language model to provide a more nuanced understanding of each model's practical applicability. Our work reveals significant variations in model responses based on the prompting strategies. We find that LLMs, particularly Llama3 and GPT-4, can outperform traditional methods in complex classification tasks, such as multiclass classification, though at the cost of longer inference times. In contrast, simpler ML models offer better performance-to-time trade-offs in simpler binary classification tasks.",
    "authors": [
      "Arina Kostina",
      "Marios D. Dikaiakos",
      "Dimosthenis Stefanidis",
      "George Pallis"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-01-14T22:02:38Z",
    "pdf_url": "https://arxiv.org/pdf/2501.08457v1"
  },
  {
    "arxiv_id": "2501.08167v2",
    "entry_id": "http://arxiv.org/abs/2501.08167v2",
    "title": "Potential and Perils of Large Language Models as Judges of Unstructured Textual Data",
    "summary": "Rapid advancements in large language models have unlocked remarkable capabilities when it comes to processing and summarizing unstructured text data. This has implications for the analysis of rich, open-ended datasets, such as survey responses, where LLMs hold the promise of efficiently distilling key themes and sentiments. However, as organizations increasingly turn to these powerful AI systems to make sense of textual feedback, a critical question arises, can we trust LLMs to accurately represent the perspectives contained within these text based datasets? While LLMs excel at generating human-like summaries, there is a risk that their outputs may inadvertently diverge from the true substance of the original responses. Discrepancies between the LLM-generated outputs and the actual themes present in the data could lead to flawed decision-making, with far-reaching consequences for organizations. This research investigates the effectiveness of LLM-as-judge models to evaluate the thematic alignment of summaries generated by other LLMs. We utilized an Anthropic Claude model to generate thematic summaries from open-ended survey responses, with Amazon's Titan Express, Nova Pro, and Meta's Llama serving as judges. This LLM-as-judge approach was compared to human evaluations using Cohen's kappa, Spearman's rho, and Krippendorff's alpha, validating a scalable alternative to traditional human centric evaluation methods. Our findings reveal that while LLM-as-judge offer a scalable solution comparable to human raters, humans may still excel at detecting subtle, context-specific nuances. Our research contributes to the growing body of knowledge on AI assisted text analysis. Further, we provide recommendations for future research, emphasizing the need for careful consideration when generalizing LLM-as-judge models across various contexts and use cases.",
    "authors": [
      "Rewina Bedemariam",
      "Natalie Perez",
      "Sreyoshi Bhaduri",
      "Satya Kapoor",
      "Alex Gil",
      "Elizabeth Conjar",
      "Ikkei Itoku",
      "David Theil",
      "Aman Chadha",
      "Naumaan Nayyar"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-01-14T14:49:14Z",
    "pdf_url": "https://arxiv.org/pdf/2501.08167v2"
  },
  {
    "arxiv_id": "2501.07766v2",
    "entry_id": "http://arxiv.org/abs/2501.07766v2",
    "title": "Large Language Models for Knowledge Graph Embedding: A Survey",
    "summary": "Large language models (LLMs) have garnered significant attention for their superior performance in many knowledge-driven applications on the world wide web.These models are designed to train hundreds of millions or more parameters on large amounts of text data, enabling them to understand and generate naturallanguage effectively. As the superior performance of LLMs becomes apparent,they are increasingly being applied to knowledge graph embedding (KGE) related tasks to improve the processing results. Traditional KGE representation learning methods map entities and relations into a low-dimensional vector space, enablingthe triples in the knowledge graph to satisfy a specific scoring function in thevector space. However, based on the powerful language understanding and seman-tic modeling capabilities of LLMs, that have recently been invoked to varying degrees in different types of KGE related scenarios such as multi-modal KGE andopen KGE according to their task characteristics. In this paper, we investigate awide range of approaches for performing LLMs-related tasks in different types of KGE scenarios. To better compare the various approaches, we summarize each KGE scenario in a classification. Finally, we discuss the applications in which the methods are mainly used and suggest several forward-looking directions for the development of this new research area.",
    "authors": [
      "Bingchen Liu",
      "Yuanyuan Fang",
      "Naixing Xu",
      "Shihao Hou",
      "Xin Li",
      "Qian Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-01-14T00:47:24Z",
    "pdf_url": "https://arxiv.org/pdf/2501.07766v2"
  },
  {
    "arxiv_id": "2501.07468v3",
    "entry_id": "http://arxiv.org/abs/2501.07468v3",
    "title": "From Screens to Scenes: A Survey of Embodied AI in Healthcare",
    "summary": "Healthcare systems worldwide face persistent challenges in efficiency, accessibility, and personalization. Powered by modern AI technologies such as multimodal large language models and world models, Embodied AI (EmAI) represents a transformative frontier, offering enhanced autonomy and the ability to interact with the physical world to address these challenges. As an interdisciplinary and rapidly evolving research domain, \"EmAI in healthcare\" spans diverse fields such as algorithms, robotics, and biomedicine. This complexity underscores the importance of timely reviews and analyses to track advancements, address challenges, and foster cross-disciplinary collaboration. In this paper, we provide a comprehensive overview of the \"brain\" of EmAI for healthcare, wherein we introduce foundational AI algorithms for perception, actuation, planning, and memory, and focus on presenting the healthcare applications spanning clinical interventions, daily care & companionship, infrastructure support, and biomedical research. Despite its promise, the development of EmAI for healthcare is hindered by critical challenges such as safety concerns, gaps between simulation platforms and real-world applications, the absence of standardized benchmarks, and uneven progress across interdisciplinary domains. We discuss the technical barriers and explore ethical considerations, offering a forward-looking perspective on the future of EmAI in healthcare. A hierarchical framework of intelligent levels for EmAI systems is also introduced to guide further development. By providing systematic insights, this work aims to inspire innovation and practical applications, paving the way for a new era of intelligent, patient-centered healthcare.",
    "authors": [
      "Yihao Liu",
      "Xu Cao",
      "Tingting Chen",
      "Yankai Jiang",
      "Junjie You",
      "Minghua Wu",
      "Xiaosong Wang",
      "Mengling Feng",
      "Yaochu Jin",
      "Jintai Chen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-13T16:35:52Z",
    "pdf_url": "https://arxiv.org/pdf/2501.07468v3"
  },
  {
    "arxiv_id": "2501.07278v1",
    "entry_id": "http://arxiv.org/abs/2501.07278v1",
    "title": "Lifelong Learning of Large Language Model based Agents: A Roadmap",
    "summary": "Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at \\href{this url}{https://github.com/qianlima-lab/awesome-lifelong-llm-agent}.",
    "authors": [
      "Junhao Zheng",
      "Chengming Shi",
      "Xidi Cai",
      "Qiuke Li",
      "Duzhen Zhang",
      "Chenxing Li",
      "Dong Yu",
      "Qianli Ma"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-13T12:42:04Z",
    "pdf_url": "https://arxiv.org/pdf/2501.07278v1"
  },
  {
    "arxiv_id": "2501.06932v2",
    "entry_id": "http://arxiv.org/abs/2501.06932v2",
    "title": "Harnessing Large Language Models for Disaster Management: A Survey",
    "summary": "Large language models (LLMs) have revolutionized scientific research with their exceptional capabilities and transformed various fields. Among their practical applications, LLMs have been playing a crucial role in mitigating threats to human life, infrastructure, and the environment. Despite growing research in disaster LLMs, there remains a lack of systematic review and in-depth analysis of LLMs for natural disaster management. To address the gap, this paper presents a comprehensive survey of existing LLMs in natural disaster management, along with a taxonomy that categorizes existing works based on disaster phases and application scenarios. By collecting public datasets and identifying key challenges and opportunities, this study aims to guide the professional community in developing advanced LLMs for disaster management to enhance the resilience against natural disasters.",
    "authors": [
      "Zhenyu Lei",
      "Yushun Dong",
      "Weiyu Li",
      "Rong Ding",
      "Qi Wang",
      "Jundong Li"
    ],
    "categories": [
      "cs.CL",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2025-01-12T21:00:50Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06932v2"
  },
  {
    "arxiv_id": "2501.06819v1",
    "entry_id": "http://arxiv.org/abs/2501.06819v1",
    "title": "A Study on Educational Data Analysis and Personalized Feedback Report Generation Based on Tags and ChatGPT",
    "summary": "This study introduces a novel method that employs tag annotation coupled with the ChatGPT language model to analyze student learning behaviors and generate personalized feedback. Central to this approach is the conversion of complex student data into an extensive set of tags, which are then decoded through tailored prompts to deliver constructive feedback that encourages rather than discourages students. This methodology focuses on accurately feeding student data into large language models and crafting prompts that enhance the constructive nature of feedback. The effectiveness of this approach was validated through surveys conducted with over 20 mathematics teachers, who confirmed the reliability of the generated reports. This method can be seamlessly integrated into intelligent adaptive learning systems or provided as a tool to significantly reduce the workload of teachers, providing accurate and timely feedback to students. By transforming raw educational data into interpretable tags, this method supports the provision of efficient and timely personalized learning feedback that offers constructive suggestions tailored to individual learner needs.",
    "authors": [
      "Yizhou Zhou",
      "Mengqiao Zhang",
      "Yuan-Hao Jiang",
      "Xinyu Gao",
      "Naijie Liu",
      "Bo Jiang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-12T14:23:17Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06819v1"
  },
  {
    "arxiv_id": "2501.06682v1",
    "entry_id": "http://arxiv.org/abs/2501.06682v1",
    "title": "Generative AI in Education: From Foundational Insights to the Socratic Playground for Learning",
    "summary": "This paper explores the synergy between human cognition and Large Language Models (LLMs), highlighting how generative AI can drive personalized learning at scale. We discuss parallels between LLMs and human cognition, emphasizing both the promise and new perspectives on integrating AI systems into education. After examining challenges in aligning technology with pedagogy, we review AutoTutor-one of the earliest Intelligent Tutoring Systems (ITS)-and detail its successes, limitations, and unfulfilled aspirations. We then introduce the Socratic Playground, a next-generation ITS that uses advanced transformer-based models to overcome AutoTutor's constraints and provide personalized, adaptive tutoring. To illustrate its evolving capabilities, we present a JSON-based tutoring prompt that systematically guides learner reflection while tracking misconceptions. Throughout, we underscore the importance of placing pedagogy at the forefront, ensuring that technology's power is harnessed to enhance teaching and learning rather than overshadow it.",
    "authors": [
      "Xiangen Hu",
      "Sheng Xu",
      "Richard Tong",
      "Art Graesser"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-12T01:43:39Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06682v1"
  },
  {
    "arxiv_id": "2501.06577v2",
    "entry_id": "http://arxiv.org/abs/2501.06577v2",
    "title": "Survey Transfer Learning: Recycling Data with Silicon Responses",
    "summary": "As researchers increasingly turn to large language models (LLMs) to generate synthetic survey data, less attention has been paid to alternative AI paradigms given environmental costs of LLMs. This paper introduces Survey Transfer Learning (STL), which develops transfer learning paradigms from computer science for survey research to recycle existing survey data and generate empirically grounded silicon responses. Inspired by political behavior theory, STL leverages shared demographic variables with high predictive power in a polarized American context to transfer knowledge across surveys. Using a neural network pre-trained on the Cooperative Election Study (CES) 2020, freezing early layers to preserve learned structure, and fine-tuning top layers on the American National Election Studies (ANES) 2020, STL generates silicon responses CES 2022 and in held-out ANES 2020 data with accuracy rates of up to 93 percent. Results show that STL outperforms LLMs, especially on sensitive measures such as racial resentment. While LLMs silicon samples are costly and opaque, STL generates empirically grounded silicon responses with high individual-level accuracy, potentially helping to mitigate key challenges in social science and the polling industry.",
    "authors": [
      "Ali Amini"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-11T16:01:44Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06577v2"
  },
  {
    "arxiv_id": "2501.06322v1",
    "entry_id": "http://arxiv.org/abs/2501.06322v1",
    "title": "Multi-Agent Collaboration Mechanisms: A Survey of LLMs",
    "summary": "With recent advances in Large Language Models (LLMs), Agentic AI has become phenomenal in real-world applications, moving toward multiple LLM-based agents to perceive, learn, reason, and act collaboratively. These LLM-based Multi-Agent Systems (MASs) enable groups of intelligent agents to coordinate and solve complex tasks collectively at scale, transitioning from isolated models to collaboration-centric approaches. This work provides an extensive survey of the collaborative aspect of MASs and introduces an extensible framework to guide future research. Our framework characterizes collaboration mechanisms based on key dimensions: actors (agents involved), types (e.g., cooperation, competition, or coopetition), structures (e.g., peer-to-peer, centralized, or distributed), strategies (e.g., role-based or model-based), and coordination protocols. Through a review of existing methodologies, our findings serve as a foundation for demystifying and advancing LLM-based MASs toward more intelligent and collaborative solutions for complex, real-world use cases. In addition, various applications of MASs across diverse domains, including 5G/6G networks, Industry 5.0, question answering, and social and cultural settings, are also investigated, demonstrating their wider adoption and broader impacts. Finally, we identify key lessons learned, open challenges, and potential research directions of MASs towards artificial collective intelligence.",
    "authors": [
      "Khanh-Tung Tran",
      "Dung Dao",
      "Minh-Duong Nguyen",
      "Quoc-Viet Pham",
      "Barry O'Sullivan",
      "Hoang D. Nguyen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-10T19:56:50Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06322v1"
  },
  {
    "arxiv_id": "2501.06143v3",
    "entry_id": "http://arxiv.org/abs/2501.06143v3",
    "title": "Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories",
    "summary": "We investigate the multilingual and multimodal performance of a large language model-based artificial intelligence (AI) system, GPT-4o, using a diverse set of physics concept inventories spanning multiple languages and subject categories. The inventories, sourced from the PhysPort website, cover classical physics topics such as mechanics, electromagnetism, optics, and thermodynamics, as well as relativity, quantum mechanics, astronomy, mathematics, and laboratory skills. Unlike previous text-only studies, we uploaded the inventories as images to reflect what a student would see on paper, thereby assessing the system's multimodal functionality. Our results indicate variation in performance across subjects, with laboratory skills standing out as the weakest. We also observe differences across languages, with English and European languages showing the strongest performance. Notably, the relative difficulty of an inventory item is largely independent of the language of the survey. When comparing AI results to existing literature on student performance, we find that the AI system outperforms average post-instruction undergraduate students in all subject categories except laboratory skills. Furthermore, the AI performs worse on items requiring visual interpretation of images than on those that are purely text-based. While our exploratory findings show GPT-4o's potential usefulness in physics education, they highlight the critical need for instructors to foster students' ability to critically evaluate AI outputs, adapt curricula thoughtfully in response to AI advancements, and address equity concerns associated with AI integration.",
    "authors": [
      "Gerd Kortemeyer",
      "Marina Babayeva",
      "Giulia Polverini",
      "Ralf Widenhorn",
      "Bor Gregorcic"
    ],
    "categories": [
      "physics.ed-ph",
      "cs.AI"
    ],
    "published": "2025-01-10T18:08:07Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06143v3"
  },
  {
    "arxiv_id": "2501.05714v4",
    "entry_id": "http://arxiv.org/abs/2501.05714v4",
    "title": "How to Enable Effective Cooperation Between Humans and NLP Models: A Survey of Principles, Formalizations, and Beyond",
    "summary": "With the advancement of large language models (LLMs), intelligent models have evolved from mere tools to autonomous agents with their own goals and strategies for cooperating with humans. This evolution has birthed a novel paradigm in NLP, i.e., human-model cooperation, that has yielded remarkable progress in numerous NLP tasks in recent years. In this paper, we take the first step to present a thorough review of human-model cooperation, exploring its principles, formalizations, and open challenges. In particular, we introduce a new taxonomy that provides a unified perspective to summarize existing approaches. Also, we discuss potential frontier areas and their corresponding challenges. We regard our work as an entry point, paving the way for more breakthrough research in this regard.",
    "authors": [
      "Chen Huang",
      "Yang Deng",
      "Wenqiang Lei",
      "Jiancheng Lv",
      "Tat-Seng Chua",
      "Jimmy Xiangji Huang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-01-10T05:15:14Z",
    "pdf_url": "https://arxiv.org/pdf/2501.05714v4"
  },
  {
    "arxiv_id": "2501.10421v1",
    "entry_id": "http://arxiv.org/abs/2501.10421v1",
    "title": "CodEv: An Automated Grading Framework Leveraging Large Language Models for Consistent and Constructive Feedback",
    "summary": "Grading programming assignments is crucial for guiding students to improve their programming skills and coding styles. This study presents an automated grading framework, CodEv, which leverages Large Language Models (LLMs) to provide consistent and constructive feedback. We incorporate Chain of Thought (CoT) prompting techniques to enhance the reasoning capabilities of LLMs and ensure that the grading is aligned with human evaluation. Our framework also integrates LLM ensembles to improve the accuracy and consistency of scores, along with agreement tests to deliver reliable feedback and code review comments. The results demonstrate that the framework can yield grading results comparable to human evaluators, by using smaller LLMs. Evaluation and consistency tests of the LLMs further validate our approach, confirming the reliability of the generated scores and feedback.",
    "authors": [
      "En-Qi Tseng",
      "Pei-Cing Huang",
      "Chan Hsu",
      "Peng-Yi Wu",
      "Chan-Tung Ku",
      "Yihuang Kang"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-01-10T03:09:46Z",
    "pdf_url": "https://arxiv.org/pdf/2501.10421v1"
  },
  {
    "arxiv_id": "2501.06271v1",
    "entry_id": "http://arxiv.org/abs/2501.06271v1",
    "title": "Large Language Models for Bioinformatics",
    "summary": "With the rapid advancements in large language model (LLM) technology and the emergence of bioinformatics-specific language models (BioLMs), there is a growing need for a comprehensive analysis of the current landscape, computational characteristics, and diverse applications. This survey aims to address this need by providing a thorough review of BioLMs, focusing on their evolution, classification, and distinguishing features, alongside a detailed examination of training methodologies, datasets, and evaluation frameworks. We explore the wide-ranging applications of BioLMs in critical areas such as disease diagnosis, drug discovery, and vaccine development, highlighting their impact and transformative potential in bioinformatics. We identify key challenges and limitations inherent in BioLMs, including data privacy and security concerns, interpretability issues, biases in training data and model outputs, and domain adaptation complexities. Finally, we highlight emerging trends and future directions, offering valuable insights to guide researchers and clinicians toward advancing BioLMs for increasingly sophisticated biological and clinical applications.",
    "authors": [
      "Wei Ruan",
      "Yanjun Lyu",
      "Jing Zhang",
      "Jiazhang Cai",
      "Peng Shu",
      "Yang Ge",
      "Yao Lu",
      "Shang Gao",
      "Yue Wang",
      "Peilong Wang",
      "Lin Zhao",
      "Tao Wang",
      "Yufang Liu",
      "Luyang Fang",
      "Ziyu Liu",
      "Zhengliang Liu",
      "Yiwei Li",
      "Zihao Wu",
      "Junhao Chen",
      "Hanqi Jiang",
      "Yi Pan",
      "Zhenyuan Yang",
      "Jingyuan Chen",
      "Shizhe Liang",
      "Wei Zhang",
      "Terry Ma",
      "Yuan Dou",
      "Jianli Zhang",
      "Xinyu Gong",
      "Qi Gan",
      "Yusong Zou",
      "Zebang Chen",
      "Yuanxin Qian",
      "Shuo Yu",
      "Jin Lu",
      "Kenan Song",
      "Xianqiao Wang",
      "Andrea Sikora",
      "Gang Li",
      "Xiang Li",
      "Quanzheng Li",
      "Yingfeng Wang",
      "Lu Zhang",
      "Yohannes Abate",
      "Lifang He",
      "Wenxuan Zhong",
      "Rongjie Liu",
      "Chao Huang",
      "Wei Liu",
      "Ye Shen",
      "Ping Ma",
      "Hongtu Zhu",
      "Yajun Yan",
      "Dajiang Zhu",
      "Tianming Liu"
    ],
    "categories": [
      "q-bio.QM",
      "cs.AI",
      "cs.CE"
    ],
    "published": "2025-01-10T01:43:05Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06271v1"
  },
  {
    "arxiv_id": "2501.05443v1",
    "entry_id": "http://arxiv.org/abs/2501.05443v1",
    "title": "A survey of textual cyber abuse detection using cutting-edge language models and large language models",
    "summary": "The success of social media platforms has facilitated the emergence of various forms of online abuse within digital communities. This abuse manifests in multiple ways, including hate speech, cyberbullying, emotional abuse, grooming, and sexting. In this paper, we present a comprehensive analysis of the different forms of abuse prevalent in social media, with a particular focus on how emerging technologies, such as Language Models (LMs) and Large Language Models (LLMs), are reshaping both the detection and generation of abusive content within these networks. We delve into the mechanisms through which social media abuse is perpetuated, exploring the psychological and social impact. Additionally, we examine the dual role of advanced language models-highlighting their potential to enhance automated detection systems for abusive behavior while also acknowledging their capacity to generate harmful content. This paper aims to contribute to the ongoing discourse on online safety and ethics, offering insights into the evolving landscape of cyberabuse and the technological innovations that both mitigate and exacerbate it.",
    "authors": [
      "Jose A. Diaz-Garcia",
      "Joao Paulo Carvalho"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-01-09T18:55:50Z",
    "pdf_url": "https://arxiv.org/pdf/2501.05443v1"
  },
  {
    "arxiv_id": "2501.05165v1",
    "entry_id": "http://arxiv.org/abs/2501.05165v1",
    "title": "Bringing Order Amidst Chaos: On the Role of Artificial Intelligence in Secure Software Engineering",
    "summary": "Context. Developing secure and reliable software remains a key challenge in software engineering (SE). The ever-evolving technological landscape offers both opportunities and threats, creating a dynamic space where chaos and order compete. Secure software engineering (SSE) must continuously address vulnerabilities that endanger software systems and carry broader socio-economic risks, such as compromising critical national infrastructure and causing significant financial losses. Researchers and practitioners have explored methodologies like Static Application Security Testing Tools (SASTTs) and artificial intelligence (AI) approaches, including machine learning (ML) and large language models (LLMs), to detect and mitigate these vulnerabilities. Each method has unique strengths and limitations.\n  Aim. This thesis seeks to bring order to the chaos in SSE by addressing domain-specific differences that impact AI accuracy.\n  Methodology. The research employs a mix of empirical strategies, such as evaluating effort-aware metrics, analyzing SASTTs, conducting method-level analysis, and leveraging evidence-based techniques like systematic dataset reviews. These approaches help characterize vulnerability prediction datasets.\n  Results. Key findings include limitations in static analysis tools for identifying vulnerabilities, gaps in SASTT coverage of vulnerability types, weak relationships among vulnerability severity scores, improved defect prediction accuracy using just-in-time modeling, and threats posed by untouched methods.\n  Conclusions. This thesis highlights the complexity of SSE and the importance of contextual knowledge in improving AI-driven vulnerability and defect prediction. The comprehensive analysis advances effective prediction models, benefiting both researchers and practitioners.",
    "authors": [
      "Matteo Esposito"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.ET"
    ],
    "published": "2025-01-09T11:38:58Z",
    "pdf_url": "https://arxiv.org/pdf/2501.05165v1"
  },
  {
    "arxiv_id": "2502.15702v1",
    "entry_id": "http://arxiv.org/abs/2502.15702v1",
    "title": "Large language models streamline automated systematic review: A preliminary study",
    "summary": "Large Language Models (LLMs) have shown promise in natural language processing tasks, with the potential to automate systematic reviews. This study evaluates the performance of three state-of-the-art LLMs in conducting systematic review tasks. We assessed GPT-4, Claude-3, and Mistral 8x7B across four systematic review tasks: study design formulation, search strategy development, literature screening, and data extraction. Sourced from a previously published systematic review, we provided reference standard including standard PICO (Population, Intervention, Comparison, Outcome) design, standard eligibility criteria, and data from 20 reference literature. Three investigators evaluated the quality of study design and eligibility criteria using 5-point Liker Scale in terms of accuracy, integrity, relevance, consistency and overall performance. For other tasks, the output is defined as accurate if it is the same as the reference standard. Search strategy performance was evaluated through accuracy and retrieval efficacy. Screening accuracy was assessed for both abstracts screening and full texts screening. Data extraction accuracy was evaluated across 1,120 data points comprising 3,360 individual fields. Claude-3 demonstrated superior overall performance in PICO design. In search strategy formulation, GPT-4 and Claude-3 achieved comparable accuracy, outperforming Mistral. For abstract screening, GPT-4 achieved the highest accuracy, followed by Mistral and Claude-3. In data extraction, GPT-4 significantly outperformed other models. LLMs demonstrate potential for automating systematic review tasks, with GPT-4 showing superior performance in search strategy formulation, literature screening and data extraction. These capabilities make them promising assistive tools for researchers and warrant further development and validation in this field.",
    "authors": [
      "Xi Chen",
      "Xue Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-01-09T01:59:35Z",
    "pdf_url": "https://arxiv.org/pdf/2502.15702v1"
  },
  {
    "arxiv_id": "2501.06250v4",
    "entry_id": "http://arxiv.org/abs/2501.06250v4",
    "title": "Generative AI for Cel-Animation: A Survey",
    "summary": "Traditional Celluloid (Cel) Animation production pipeline encompasses multiple essential steps, including storyboarding, layout design, keyframe animation, inbetweening, and colorization, which demand substantial manual effort, technical expertise, and significant time investment. These challenges have historically impeded the efficiency and scalability of Cel-Animation production. The rise of generative artificial intelligence (GenAI), encompassing large language models, multimodal models, and diffusion models, offers innovative solutions by automating tasks such as inbetween frame generation, colorization, and storyboard creation. This survey explores how GenAI integration is revolutionizing traditional animation workflows by lowering technical barriers, broadening accessibility for a wider range of creators through tools like AniDoc, ToonCrafter, and AniSora, and enabling artists to focus more on creative expression and artistic innovation. Despite its potential, challenges like visual consistency, stylistic coherence, and ethical considerations persist. Additionally, this paper explores future directions and advancements in AI-assisted animation. For further exploration and resources, please visit our GitHub repository: https://github.com/yunlong10/Awesome-AI4Animation",
    "authors": [
      "Yolo Yunlong Tang",
      "Junjia Guo",
      "Pinxin Liu",
      "Zhiyuan Wang",
      "Hang Hua",
      "Jia-Xing Zhong",
      "Yunzhong Xiao",
      "Chao Huang",
      "Luchuan Song",
      "Susan Liang",
      "Yizhi Song",
      "Liu He",
      "Jing Bi",
      "Mingqian Feng",
      "Xinyang Li",
      "Zeliang Zhang",
      "Chenliang Xu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2025-01-08T20:57:39Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06250v4"
  },
  {
    "arxiv_id": "2502.00015v3",
    "entry_id": "http://arxiv.org/abs/2502.00015v3",
    "title": "Ethical Concerns of Generative AI and Mitigation Strategies: A Systematic Mapping Study",
    "summary": "[Context] Generative AI technologies, particularly Large Language Models (LLMs), have transformed numerous domains by enhancing convenience and efficiency in information retrieval, content generation, and decision-making processes. However, deploying LLMs also presents diverse ethical challenges, and their mitigation strategies remain complex and domain-dependent. [Objective] This paper aims to identify and categorize the key ethical concerns associated with using LLMs, examine existing mitigation strategies, and assess the outstanding challenges in implementing these strategies across various domains. [Method] We conducted a systematic mapping study, reviewing 39 studies that discuss ethical concerns and mitigation strategies related to LLMs. We analyzed these ethical concerns using five ethical dimensions that we extracted based on various existing guidelines, frameworks, and an analysis of the mitigation strategies and implementation challenges. [Results] Our findings reveal that ethical concerns in LLMs are multi-dimensional and context-dependent. While proposed mitigation strategies address some of these concerns, significant challenges still remain. [Conclusion] Our results highlight that ethical issues often hinder the practical implementation of the mitigation strategies, particularly in high-stake areas like healthcare and public governance; existing frameworks often lack adaptability, failing to accommodate evolving societal expectations and diverse contexts.",
    "authors": [
      "Yutan Huang",
      "Chetan Arora",
      "Wen Cheng Houng",
      "Tanjila Kanij",
      "Anuradha Madulgalla",
      "John Grundy"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2025-01-08T13:05:19Z",
    "pdf_url": "https://arxiv.org/pdf/2502.00015v3"
  },
  {
    "arxiv_id": "2501.04437v1",
    "entry_id": "http://arxiv.org/abs/2501.04437v1",
    "title": "Integrating LLMs with ITS: Recent Advances, Potentials, Challenges, and Future Directions",
    "summary": "Intelligent Transportation Systems (ITS) are crucial for the development and operation of smart cities, addressing key challenges in efficiency, productivity, and environmental sustainability. This paper comprehensively reviews the transformative potential of Large Language Models (LLMs) in optimizing ITS. Initially, we provide an extensive overview of ITS, highlighting its components, operational principles, and overall effectiveness. We then delve into the theoretical background of various LLM techniques, such as GPT, T5, CTRL, and BERT, elucidating their relevance to ITS applications. Following this, we examine the wide-ranging applications of LLMs within ITS, including traffic flow prediction, vehicle detection and classification, autonomous driving, traffic sign recognition, and pedestrian detection. Our analysis reveals how these advanced models can significantly enhance traffic management and safety. Finally, we explore the challenges and limitations LLMs face in ITS, such as data availability, computational constraints, and ethical considerations. We also present several future research directions and potential innovations to address these challenges. This paper aims to guide researchers and practitioners through the complexities and opportunities of integrating LLMs in ITS, offering a roadmap to create more efficient, sustainable, and responsive next-generation transportation systems.",
    "authors": [
      "Doaa Mahmud",
      "Hadeel Hajmohamed",
      "Shamma Almentheri",
      "Shamma Alqaydi",
      "Lameya Aldhaheri",
      "Ruhul Amin Khalil",
      "Nasir Saeed"
    ],
    "categories": [
      "eess.SY",
      "cs.AI",
      "cs.ET"
    ],
    "published": "2025-01-08T11:37:35Z",
    "pdf_url": "https://arxiv.org/pdf/2501.04437v1"
  },
  {
    "arxiv_id": "2501.04227v2",
    "entry_id": "http://arxiv.org/abs/2501.04227v2",
    "title": "Agent Laboratory: Using LLM Agents as Research Assistants",
    "summary": "Historically, scientific discovery has been a lengthy and costly process, demanding substantial time and resources from initial conception to final results. To accelerate scientific discovery, reduce research costs, and improve research quality, we introduce Agent Laboratory, an autonomous LLM-based framework capable of completing the entire research process. This framework accepts a human-provided research idea and progresses through three stages--literature review, experimentation, and report writing to produce comprehensive research outputs, including a code repository and a research report, while enabling users to provide feedback and guidance at each stage. We deploy Agent Laboratory with various state-of-the-art LLMs and invite multiple researchers to assess its quality by participating in a survey, providing human feedback to guide the research process, and then evaluate the final paper. We found that: (1) Agent Laboratory driven by o1-preview generates the best research outcomes; (2) The generated machine learning code is able to achieve state-of-the-art performance compared to existing methods; (3) Human involvement, providing feedback at each stage, significantly improves the overall quality of research; (4) Agent Laboratory significantly reduces research expenses, achieving an 84% decrease compared to previous autonomous research methods. We hope Agent Laboratory enables researchers to allocate more effort toward creative ideation rather than low-level coding and writing, ultimately accelerating scientific discovery.",
    "authors": [
      "Samuel Schmidgall",
      "Yusheng Su",
      "Ze Wang",
      "Ximeng Sun",
      "Jialian Wu",
      "Xiaodong Yu",
      "Jiang Liu",
      "Michael Moor",
      "Zicheng Liu",
      "Emad Barsoum"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-01-08T01:58:42Z",
    "pdf_url": "https://arxiv.org/pdf/2501.04227v2"
  },
  {
    "arxiv_id": "2501.03795v1",
    "entry_id": "http://arxiv.org/abs/2501.03795v1",
    "title": "Self-Adaptive ERP: Embedding NLP into Petri-Net creation and Model Matching",
    "summary": "Enterprise Resource Planning (ERP) consultants play a vital role in customizing systems to meet specific business needs by processing large amounts of data and adapting functionalities. However, the process is resource-intensive, time-consuming, and requires continuous adjustments as business demands evolve. This research introduces a Self-Adaptive ERP Framework that automates customization using enterprise process models and system usage analysis. It leverages Artificial Intelligence (AI) & Natural Language Processing (NLP) for Petri nets to transform business processes into adaptable models, addressing both structural and functional matching. The framework, built using Design Science Research (DSR) and a Systematic Literature Review (SLR), reduces reliance on manual adjustments, improving ERP customization efficiency and accuracy while minimizing the need for consultants.",
    "authors": [
      "Ahmed Maged",
      "Gamal Kassem"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2025-01-07T14:01:59Z",
    "pdf_url": "https://arxiv.org/pdf/2501.03795v1"
  },
  {
    "arxiv_id": "2501.03566v1",
    "entry_id": "http://arxiv.org/abs/2501.03566v1",
    "title": "Applying Large Language Models in Knowledge Graph-based Enterprise Modeling: Challenges and Opportunities",
    "summary": "The role of large language models (LLMs) in enterprise modeling has recently started to shift from academic research to that of industrial applications. Thereby, LLMs represent a further building block for the machine-supported generation of enterprise models. In this paper we employ a knowledge graph-based approach for enterprise modeling and investigate the potential benefits of LLMs in this context. In addition, the findings of an expert survey and ChatGPT-4o-based experiments demonstrate that LLM-based model generations exhibit minimal variability, yet remain constrained to specific tasks, with reliability declining for more intricate tasks. The survey results further suggest that the supervision and intervention of human modeling experts are essential to ensure the accuracy and integrity of the generated models.",
    "authors": [
      "Benedikt Reitemeyer",
      "Hans-Georg Fill"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2025-01-07T06:34:17Z",
    "pdf_url": "https://arxiv.org/pdf/2501.03566v1"
  },
  {
    "arxiv_id": "2501.03151v1",
    "entry_id": "http://arxiv.org/abs/2501.03151v1",
    "title": "Large language models for artificial general intelligence (AGI): A survey of foundational principles and approaches",
    "summary": "Generative artificial intelligence (AI) systems based on large-scale pretrained foundation models (PFMs) such as vision-language models, large language models (LLMs), diffusion models and vision-language-action (VLA) models have demonstrated the ability to solve complex and truly non-trivial AI problems in a wide variety of domains and contexts. Multimodal large language models (MLLMs), in particular, learn from vast and diverse data sources, allowing rich and nuanced representations of the world and, thereby, providing extensive capabilities, including the ability to reason, engage in meaningful dialog; collaborate with humans and other agents to jointly solve complex problems; and understand social and emotional aspects of humans. Despite this impressive feat, the cognitive abilities of state-of-the-art LLMs trained on large-scale datasets are still superficial and brittle. Consequently, generic LLMs are severely limited in their generalist capabilities. A number of foundational problems -- embodiment, symbol grounding, causality and memory -- are required to be addressed for LLMs to attain human-level general intelligence. These concepts are more aligned with human cognition and provide LLMs with inherent human-like cognitive properties that support the realization of physically-plausible, semantically meaningful, flexible and more generalizable knowledge and intelligence. In this work, we discuss the aforementioned foundational issues and survey state-of-the art approaches for implementing these concepts in LLMs. Specifically, we discuss how the principles of embodiment, symbol grounding, causality and memory can be leveraged toward the attainment of artificial general intelligence (AGI) in an organic manner.",
    "authors": [
      "Alhassan Mumuni",
      "Fuseini Mumuni"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2025-01-06T17:18:47Z",
    "pdf_url": "https://arxiv.org/pdf/2501.03151v1"
  },
  {
    "arxiv_id": "2502.15700v1",
    "entry_id": "http://arxiv.org/abs/2502.15700v1",
    "title": "Sustainable Digitalization of Business with Multi-Agent RAG and LLM",
    "summary": "Businesses heavily rely on data sourced from various channels like news articles, financial reports, and consumer reviews to drive their operations, enabling informed decision-making and identifying opportunities. However, traditional manual methods for data extraction are often time-consuming and resource-intensive, prompting the adoption of digital transformation initiatives to enhance efficiency. Yet, concerns persist regarding the sustainability of such initiatives and their alignment with the United Nations (UN)'s Sustainable Development Goals (SDGs). This research aims to explore the integration of Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG) as a sustainable solution for Information Extraction (IE) and processing. The research methodology involves reviewing existing solutions for business decision-making, noting that many systems require training new machine learning models, which are resource-intensive and have significant environmental impacts. Instead, we propose a sustainable business solution using pre-existing LLMs that can work with diverse datasets. We link domain-specific datasets to tailor LLMs to company needs and employ a Multi-Agent architecture to divide tasks such as information retrieval, enrichment, and classification among specialized agents. This approach optimizes the extraction process and improves overall efficiency. Through the utilization of these technologies, businesses can optimize resource utilization, improve decision-making processes, and contribute to sustainable development goals, thereby fostering environmental responsibility within the corporate sector.",
    "authors": [
      "Muhammad Arslan",
      "Saba Munawar",
      "Christophe Cruz"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2025-01-06T08:14:23Z",
    "pdf_url": "https://arxiv.org/pdf/2502.15700v1"
  },
  {
    "arxiv_id": "2501.02765v1",
    "entry_id": "http://arxiv.org/abs/2501.02765v1",
    "title": "Visual Large Language Models for Generalized and Specialized Applications",
    "summary": "Visual-language models (VLM) have emerged as a powerful tool for learning a unified embedding space for vision and language. Inspired by large language models, which have demonstrated strong reasoning and multi-task capabilities, visual large language models (VLLMs) are gaining increasing attention for building general-purpose VLMs. Despite the significant progress made in VLLMs, the related literature remains limited, particularly from a comprehensive application perspective, encompassing generalized and specialized applications across vision (image, video, depth), action, and language modalities. In this survey, we focus on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development. By synthesizing these contents, we aim to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs. The paper list repository is available: https://github.com/JackYFL/awesome-VLLMs.",
    "authors": [
      "Yifan Li",
      "Zhixin Lai",
      "Wentao Bao",
      "Zhen Tan",
      "Anh Dao",
      "Kewei Sui",
      "Jiayi Shen",
      "Dong Liu",
      "Huan Liu",
      "Yu Kong"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2025-01-06T05:15:59Z",
    "pdf_url": "https://arxiv.org/pdf/2501.02765v1"
  },
  {
    "arxiv_id": "2501.02725v4",
    "entry_id": "http://arxiv.org/abs/2501.02725v4",
    "title": "Artificial Intelligence in Creative Industries: Advances Prior to 2025",
    "summary": "The rapid advancements in artificial intelligence (AI), particularly in generative AI and large language models (LLMs), have profoundly impacted the creative industries, enabling more innovative content creation, enhancing workflows, and democratizing access to creative tools. This paper explores these technological shifts, with particular focus on how those that have emerged since our previous review in 2022 have expanded creative opportunities and improved efficiency. These technological advancements have enhanced the capabilities of text-to-image, text-to-video, and multimodal generation technologies. In particular, key breakthroughs in LLMs have established new benchmarks in conversational AI, while advancements in image generators have revolutionized content creation. We also discuss the integration of AI into post-production workflows, which has significantly accelerated and improved traditional processes. Once content has been created, it must be delivered to its audiences; the media industry is now facing the demands of increased communication traffic due to creative content. We therefore include a discussion of how AI is beginning to transform the way we represent and compress media content. We highlight the trend toward unified AI frameworks capable of addressing and integrating multiple creative tasks, and we underscore the importance of human insight to drive the creative process and oversight to mitigate AI-generated inaccuracies. Finally, we explore AI's future potential in the creative sector, stressing the need to navigate emerging challenges and to maximize its benefits while addressing the associated risks.",
    "authors": [
      "Nantheera Anantrasirichai",
      "Fan Zhang",
      "David Bull"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2025-01-06T02:46:33Z",
    "pdf_url": "https://arxiv.org/pdf/2501.02725v4"
  },
  {
    "arxiv_id": "2502.00005v1",
    "entry_id": "http://arxiv.org/abs/2502.00005v1",
    "title": "A Study about Distribution and Acceptance of Conversational Agents for Mental Health in Germany: Keep the Human in the Loop?",
    "summary": "Good mental health enables individuals to cope with the normal stresses of life. In Germany, approximately one-quarter of the adult population is affected by mental illnesses. Teletherapy and digital health applications are available to bridge gaps in care and relieve healthcare professionals. The acceptance of these tools is a strongly influencing factor for their effectiveness, which also needs to be evaluated for AI-based conversational agents (CAs) (e. g. ChatGPT, Siri) to assess the risks and potential for integration into therapeutic practice. This study investigates the perspectives of both the general population and healthcare professionals with the following questions: 1. How frequently are CAs used for mental health? 2. How high is the acceptance of CAs in the field of mental health? 3. To what extent is the use of CAs in counselling, diagnosis, and treatment acceptable? To address these questions, two quantitative online surveys were conducted with 444 participants from the general population and 351 healthcare professionals. Statistical analyses show that 27 % of the surveyed population already confide their concerns to CAs. Not only experience with this technology but also experience with telemedicine shows a higher acceptance among both groups for using CAs for mental health. Additionally, participants from the general population were more likely to support CAs as companions controlled by healthcare professionals rather than as additional experts for the professionals. CAs have the potential to support mental health, particularly in counselling. Future research should examine the influence of different communication media and further possibilities of augmented intelligence. With the right balance between technology and human care, integration into patient-professional interaction can be achieved.",
    "authors": [
      "Christina Lukas"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2025-01-05T12:20:18Z",
    "pdf_url": "https://arxiv.org/pdf/2502.00005v1"
  },
  {
    "arxiv_id": "2501.03265v2",
    "entry_id": "http://arxiv.org/abs/2501.03265v2",
    "title": "Cognitive Edge Computing: A Comprehensive Survey on Optimizing Large Models and AI Agents for Pervasive Deployment",
    "summary": "This article surveys Cognitive Edge Computing as a practical and methodical pathway for deploying reasoning-capable Large Language Models (LLMs) and autonomous AI agents on resource-constrained devices at the network edge. We present a unified, cognition-preserving framework spanning: (1) model optimization (quantization, sparsity, low-rank adaptation, distillation) aimed at retaining multi-step reasoning under tight memory/compute budgets; (2) system architecture (on-device inference, elastic offloading, cloud-edge collaboration) that trades off latency, energy, privacy, and capacity; and (3) adaptive intelligence (context compression, dynamic routing, federated personalization) that tailors computation to task difficulty and device constraints. We synthesize advances in efficient Transformer design, multimodal integration, hardware-aware compilation, privacy-preserving learning, and agentic tool use, and map them to edge-specific operating envelopes. We further outline a standardized evaluation protocol covering latency, throughput, energy per token, accuracy, robustness, privacy, and sustainability, with explicit measurement assumptions to enhance comparability. Remaining challenges include modality-aware reasoning benchmarks, transparent and reproducible energy reporting, edge-oriented safety/alignment evaluation, and multi-agent testbeds. We conclude with practitioner guidelines for cross-layer co-design of algorithms, runtime, and hardware to deliver reliable, efficient, and privacy-preserving cognitive capabilities on edge devices.",
    "authors": [
      "Xubin Wang",
      "Qing Li",
      "Weijia Jia"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2025-01-04T06:17:48Z",
    "pdf_url": "https://arxiv.org/pdf/2501.03265v2"
  },
  {
    "arxiv_id": "2501.02189v6",
    "entry_id": "http://arxiv.org/abs/2501.02189v6",
    "title": "A Survey of State of the Art Large Vision Language Models: Alignment, Benchmark, Evaluations and Challenges",
    "summary": "Multimodal Vision Language Models (VLMs) have emerged as a transformative topic at the intersection of computer vision and natural language processing, enabling machines to perceive and reason about the world through both visual and textual modalities. For example, models such as CLIP, Claude, and GPT-4V demonstrate strong reasoning and understanding abilities on visual and textual data and beat classical single modality vision models on zero-shot classification [93]. With their rapid advancements in research and growing popularity in various applications, we provide a comprehensive survey of VLMs. Specifically, we provide a systematic overview of VLMs in the following aspects: [1] model information of the major VLMs developed up to 2025; [2] the transition of VLM architectures and the newest VLM alignment methods; [3] summary and categorization of the popular benchmarks and evaluation metrics of VLMs; [4] the challenges and issues faced by current VLMs such as hallucination, alignment, fairness, and safety. Detailed collections including papers and model repository links are listed in https://github.com/zli12321/Vision-Language-Models-Overview.",
    "authors": [
      "Zongxia Li",
      "Xiyang Wu",
      "Hongyang Du",
      "Fuxiao Liu",
      "Huy Nghiem",
      "Guangyao Shi"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2025-01-04T04:59:33Z",
    "pdf_url": "https://arxiv.org/pdf/2501.02189v6"
  },
  {
    "arxiv_id": "2501.04040v2",
    "entry_id": "http://arxiv.org/abs/2501.04040v2",
    "title": "A Survey on Large Language Models with some Insights on their Capabilities and Limitations",
    "summary": "The rapid advancement of artificial intelligence, particularly with the development of Large Language Models (LLMs) built on the transformer architecture, has redefined the capabilities of natural language processing. These models now exhibit remarkable performance across various language-related tasks, such as text generation, question answering, translation, and summarization, often rivaling human-like comprehension. More intriguingly, LLMs have demonstrated emergent abilities extending beyond their core functions, showing proficiency in tasks like commonsense reasoning, code generation, and arithmetic. This survey paper explores the foundational components, scaling mechanisms, and architectural strategies that drive these capabilities. Emphasizing models like GPT and LLaMA, we analyze the impact of exponential data and computational growth on LLM performance, while also addressing the trade-offs associated with scaling. We also examine LLM applications across sectors, such as healthcare, finance, education, and law, highlighting their adaptability and potential to solve domain-specific challenges. Central to this work are the questions of how LLMs generalize across diverse tasks, exhibit planning, and reasoning abilities, and whether these emergent abilities can be systematically elicited or enhanced. In particular, we provide some insights into the CoT (Chain of Thought) and PoT (Plan of Thought) abilities within LLMs, focusing on how pre-training data influences their emergence. Additionally, we investigate LLM-modulo frameworks that integrate external systems, allowing LLMs to handle complex, dynamic tasks. By analyzing these factors, this paper aims to foster the ongoing discussion on the capabilities and limits of LLMs, promoting their responsible development and application in novel and increasingly complex environments.",
    "authors": [
      "Andrea Matarazzo",
      "Riccardo Torlone"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2025-01-03T21:04:49Z",
    "pdf_url": "https://arxiv.org/pdf/2501.04040v2"
  },
  {
    "arxiv_id": "2501.01945v2",
    "entry_id": "http://arxiv.org/abs/2501.01945v2",
    "title": "Cold-Start Recommendation towards the Era of Large Language Models (LLMs): A Comprehensive Survey and Roadmap",
    "summary": "Cold-start problem is one of the long-standing challenges in recommender systems, focusing on accurately modeling new or interaction-limited users or items to provide better recommendations. Due to the diversification of internet platforms and the exponential growth of users and items, the importance of cold-start recommendation (CSR) is becoming increasingly evident. At the same time, large language models (LLMs) have achieved tremendous success and possess strong capabilities in modeling user and item information, providing new potential for cold-start recommendations. However, the research community on CSR still lacks a comprehensive review and reflection in this field. Based on this, in this paper, we stand in the context of the era of large language models and provide a comprehensive review and discussion on the roadmap, related literature, and future directions of CSR. Specifically, we have conducted an exploration of the development path of how existing CSR utilizes information, from content features, graph relations, and domain information, to the world knowledge possessed by large language models, aiming to provide new insights for both the research and industrial communities on CSR. Related resources of cold-start recommendations are collected and continuously updated for the community in https://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.",
    "authors": [
      "Weizhi Zhang",
      "Yuanchen Bei",
      "Liangwei Yang",
      "Henry Peng Zou",
      "Peilin Zhou",
      "Aiwei Liu",
      "Yinghui Li",
      "Hao Chen",
      "Jianling Wang",
      "Yu Wang",
      "Feiran Huang",
      "Sheng Zhou",
      "Jiajun Bu",
      "Allen Lin",
      "James Caverlee",
      "Fakhri Karray",
      "Irwin King",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2025-01-03T18:51:18Z",
    "pdf_url": "https://arxiv.org/pdf/2501.01945v2"
  },
  {
    "arxiv_id": "2501.06211v1",
    "entry_id": "http://arxiv.org/abs/2501.06211v1",
    "title": "FLAME: Financial Large-Language Model Assessment and Metrics Evaluation",
    "summary": "LLMs have revolutionized NLP and demonstrated potential across diverse domains. More and more financial LLMs have been introduced for finance-specific tasks, yet comprehensively assessing their value is still challenging. In this paper, we introduce FLAME, a comprehensive financial LLMs evaluation system in Chinese, which includes two core evaluation benchmarks: FLAME-Cer and FLAME-Sce. FLAME-Cer covers 14 types of authoritative financial certifications, including CPA, CFA, and FRM, with a total of approximately 16,000 carefully selected questions. All questions have been manually reviewed to ensure accuracy and representativeness. FLAME-Sce consists of 10 primary core financial business scenarios, 21 secondary financial business scenarios, and a comprehensive evaluation set of nearly 100 tertiary financial application tasks. We evaluate 6 representative LLMs, including GPT-4o, GLM-4, ERNIE-4.0, Qwen2.5, XuanYuan3, and the latest Baichuan4-Finance, revealing Baichuan4-Finance excels other LLMs in most tasks. By establishing a comprehensive and professional evaluation system, FLAME facilitates the advancement of financial LLMs in Chinese contexts. Instructions for participating in the evaluation are available on GitHub: https://github.com/FLAME-ruc/FLAME.",
    "authors": [
      "Jiayu Guo",
      "Yu Guo",
      "Martha Li",
      "Songtao Tan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE"
    ],
    "published": "2025-01-03T09:17:23Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06211v1"
  },
  {
    "arxiv_id": "2501.06210v1",
    "entry_id": "http://arxiv.org/abs/2501.06210v1",
    "title": "Applications of natural language processing in aviation safety: A review and qualitative analysis",
    "summary": "This study explores using Natural Language Processing in aviation safety, focusing on machine learning algorithms to enhance safety measures. There are currently May 2024, 34 Scopus results from the keyword search natural language processing and aviation safety. Analyzing these studies allows us to uncover trends in the methodologies, findings and implications of NLP in aviation. Both qualitative and quantitative tools have been used to investigate the current state of literature on NLP for aviation safety. The qualitative analysis summarises the research motivations, objectives, and outcomes, showing how NLP can be utilized to help identify critical safety issues and improve aviation safety. This study also identifies research gaps and suggests areas for future exploration, providing practical recommendations for the aviation industry. We discuss challenges in implementing NLP in aviation safety, such as the need for large, annotated datasets, and the difficulty in interpreting complex models. We propose solutions like active learning for data annotation and explainable AI for model interpretation. Case studies demonstrate the successful application of NLP in improving aviation safety, highlighting its potential to make aviation safer and more efficient.",
    "authors": [
      "Aziida Nanyonga",
      "Keith Joiner",
      "Ugur Turhan",
      "Graham Wild"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-01-03T07:36:10Z",
    "pdf_url": "https://arxiv.org/pdf/2501.06210v1"
  },
  {
    "arxiv_id": "2501.01256v1",
    "entry_id": "http://arxiv.org/abs/2501.01256v1",
    "title": "Digital Guardians: Can GPT-4, Perspective API, and Moderation API reliably detect hate speech in reader comments of German online newspapers?",
    "summary": "In recent years, toxic content and hate speech have become widespread phenomena on the internet. Moderators of online newspapers and forums are now required, partly due to legal regulations, to carefully review and, if necessary, delete reader comments. This is a labor-intensive process. Some providers of large language models already offer solutions for automated hate speech detection or the identification of toxic content. These include GPT-4o from OpenAI, Jigsaw's (Google) Perspective API, and OpenAI's Moderation API. Based on the selected German test dataset HOCON34k, which was specifically created for developing tools to detect hate speech in reader comments of online newspapers, these solutions are compared with each other and against the HOCON34k baseline. The test dataset contains 1,592 annotated text samples. For GPT-4o, three different promptings are used, employing a Zero-Shot, One-Shot, and Few-Shot approach. The results of the experiments demonstrate that GPT-4o outperforms both the Perspective API and the Moderation API, and exceeds the HOCON34k baseline by approximately 5 percentage points, as measured by a combined metric of MCC and F2-score.",
    "authors": [
      "Manuel Weber",
      "Moritz Huber",
      "Maximilian Auch",
      "Alexander Döschl",
      "Max-Emanuel Keller",
      "Peter Mandl"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2025-01-02T13:48:56Z",
    "pdf_url": "https://arxiv.org/pdf/2501.01256v1"
  },
  {
    "arxiv_id": "2501.01124v1",
    "entry_id": "http://arxiv.org/abs/2501.01124v1",
    "title": "Graph2text or Graph2token: A Perspective of Large Language Models for Graph Learning",
    "summary": "Graphs are data structures used to represent irregular networks and are prevalent in numerous real-world applications. Previous methods directly model graph structures and achieve significant success. However, these methods encounter bottlenecks due to the inherent irregularity of graphs. An innovative solution is converting graphs into textual representations, thereby harnessing the powerful capabilities of Large Language Models (LLMs) to process and comprehend graphs. In this paper, we present a comprehensive review of methodologies for applying LLMs to graphs, termed LLM4graph. The core of LLM4graph lies in transforming graphs into texts for LLMs to understand and analyze. Thus, we propose a novel taxonomy of LLM4graph methods in the view of the transformation. Specifically, existing methods can be divided into two paradigms: Graph2text and Graph2token, which transform graphs into texts or tokens as the input of LLMs, respectively. We point out four challenges during the transformation to systematically present existing methods in a problem-oriented perspective. For practical concerns, we provide a guideline for researchers on selecting appropriate models and LLMs for different graphs and hardware constraints. We also identify five future research directions for LLM4graph.",
    "authors": [
      "Shuo Yu",
      "Yingbo Wang",
      "Ruolin Li",
      "Guchun Liu",
      "Yanming Shen",
      "Shaoxiong Ji",
      "Bowen Li",
      "Fengling Han",
      "Xiuzhen Zhang",
      "Feng Xia"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2025-01-02T07:45:34Z",
    "pdf_url": "https://arxiv.org/pdf/2501.01124v1"
  },
  {
    "arxiv_id": "2501.01031v3",
    "entry_id": "http://arxiv.org/abs/2501.01031v3",
    "title": "ValuesRAG: Enhancing Cultural Alignment Through Retrieval-Augmented Contextual Learning",
    "summary": "Ensuring cultural values alignment in Large Language Models (LLMs) remains a critical challenge, as these models often embed Western-centric biases from their training data, leading to misrepresentations and fairness concerns in cross-cultural applications. Existing approaches such as role assignment and few-shot learning struggle to address these limitations effectively due to their reliance on pre-trained knowledge, limited scalability, and inability to capture nuanced cultural values. To address these issues, we propose ValuesRAG, a novel and effective framework that applies Retrieval-Augmented Generation (RAG) with In-Context Learning (ICL) to integrate cultural and demographic knowledge dynamically during text generation. Leveraging the World Values Survey (WVS) dataset, ValuesRAG first generates summaries of values for each individual. We subsequently curate several representative regional datasets to serve as test datasets and retrieve relevant summaries of values based on demographic features, followed by a reranking step to select the top-k relevant summaries. We evaluate ValuesRAG using 6 diverse regional datasets and show that it consistently outperforms baselines: including zero-shot, role-assignment, few-shot, and hybrid methods, both in main experiments and ablation settings. Notably, ValuesRAG achieves the best overall performance over prior methods, demonstrating its effectiveness in fostering culturally aligned and inclusive AI systems. Our findings underscore the potential of dynamic retrieval-based methods to bridge the gap between global LLM capabilities and localized cultural values.",
    "authors": [
      "Wonduk Seo",
      "Zonghao Yuan",
      "Yi Bu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SI"
    ],
    "published": "2025-01-02T03:26:13Z",
    "pdf_url": "https://arxiv.org/pdf/2501.01031v3"
  },
  {
    "arxiv_id": "2501.00953v2",
    "entry_id": "http://arxiv.org/abs/2501.00953v2",
    "title": "Prior Lessons of Incremental Dialogue and Robot Action Management for the Age of Language Models",
    "summary": "Efforts towards endowing robots with the ability to speak have benefited from recent advancements in natural language processing, in particular large language models. However, current language models are not fully incremental, as their processing is inherently monotonic and thus lack the ability to revise their interpretations or output in light of newer observations. This monotonicity has important implications for the development of dialogue systems for human--robot interaction. In this paper, we review the literature on interactive systems that operate incrementally (i.e., at the word level or below it). We motivate the need for incremental systems, survey incremental modeling of important aspects of dialogue like speech recognition and language generation. Primary focus is on the part of the system that makes decisions, known as the dialogue manager. We find that there is very little research on incremental dialogue management, offer some requirements for practical incremental dialogue management, and the implications of incremental dialogue for embodied, robotic platforms in the age of large language models.",
    "authors": [
      "Casey Kennington",
      "Pierre Lison",
      "David Schlangen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2025-01-01T20:58:03Z",
    "pdf_url": "https://arxiv.org/pdf/2501.00953v2"
  }
]