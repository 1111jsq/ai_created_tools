[
  {
    "arxiv_id": "2401.00609v1",
    "entry_id": "http://arxiv.org/abs/2401.00609v1",
    "title": "A Survey of Personality, Persona, and Profile in Conversational Agents and Chatbots",
    "summary": "We present a review of personality in neural conversational agents (CAs), also called chatbots. First, we define Personality, Persona, and Profile. We explain all personality schemes which have been used in CAs, and list models under the scheme(s) which they use. Second we describe 21 datasets which have been developed in recent CA personality research. Third, we define the methods used to embody personality in a CA, and review recent models using them. Fourth, we survey some relevant reviews on CAs, personality, and related topics. Finally, we draw conclusions and identify some research challenges for this important emerging field.",
    "authors": [
      "Richard Sutcliffe"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-31T23:41:41Z",
    "pdf_url": "https://arxiv.org/pdf/2401.00609v1"
  },
  {
    "arxiv_id": "2401.00582v1",
    "entry_id": "http://arxiv.org/abs/2401.00582v1",
    "title": "An Analysis of Embedding Layers and Similarity Scores using Siamese Neural Networks",
    "summary": "Large Lanugage Models (LLMs) are gaining increasing popularity in a variety of use cases, from language understanding and writing to assistance in application development. One of the most important aspects for optimal funcionality of LLMs is embedding layers. Word embeddings are distributed representations of words in a continuous vector space. In the context of LLMs, words or tokens from the input text are transformed into high-dimensional vectors using unique algorithms specific to the model. Our research examines the embedding algorithms from leading companies in the industry, such as OpenAI, Google's PaLM, and BERT. Using medical data, we have analyzed similarity scores of each embedding layer, observing differences in performance among each algorithm. To enhance each model and provide an additional encoding layer, we also implemented Siamese Neural Networks. After observing changes in performance with the addition of the model, we measured the carbon footage per epoch of training. The carbon footprint associated with large language models (LLMs) is a significant concern, and should be taken into consideration when selecting algorithms for a variety of use cases. Overall, our research compared the accuracy different, leading embedding algorithms and their carbon footage, allowing for a holistic review of each embedding algorithm.",
    "authors": [
      "Yash Bingi",
      "Yiqiao Yin"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-12-31T20:21:58Z",
    "pdf_url": "https://arxiv.org/pdf/2401.00582v1"
  },
  {
    "arxiv_id": "2401.00193v1",
    "entry_id": "http://arxiv.org/abs/2401.00193v1",
    "title": "KAXAI: An Integrated Environment for Knowledge Analysis and Explainable AI",
    "summary": "In order to fully harness the potential of machine learning, it is crucial to establish a system that renders the field more accessible and less daunting for individuals who may not possess a comprehensive understanding of its intricacies. The paper describes the design of a system that integrates AutoML, XAI, and synthetic data generation to provide a great UX design for users. The system allows users to navigate and harness the power of machine learning while abstracting its complexities and providing high usability. The paper proposes two novel classifiers, Logistic Regression Forest and Support Vector Tree, for enhanced model performance, achieving 96\\% accuracy on a diabetes dataset and 93\\% on a survey dataset. The paper also introduces a model-dependent local interpreter called MEDLEY and evaluates its interpretation against LIME, Greedy, and Parzen. Additionally, the paper introduces LLM-based synthetic data generation, library-based data generation, and enhancing the original dataset with GAN. The findings on synthetic data suggest that enhancing the original dataset with GAN is the most reliable way to generate synthetic data, as evidenced by KS tests, standard deviation, and feature importance. The authors also found that GAN works best for quantitative datasets.",
    "authors": [
      "Saikat Barua",
      "Sifat Momen"
    ],
    "categories": [
      "cs.LG",
      "cs.HC"
    ],
    "published": "2023-12-30T10:20:47Z",
    "pdf_url": "https://arxiv.org/pdf/2401.00193v1"
  },
  {
    "arxiv_id": "2312.17601v1",
    "entry_id": "http://arxiv.org/abs/2312.17601v1",
    "title": "The Tyranny of Possibilities in the Design of Task-Oriented LLM Systems: A Scoping Survey",
    "summary": "This scoping survey focuses on our current understanding of the design space for task-oriented LLM systems and elaborates on definitions and relationships among the available design parameters. The paper begins by defining a minimal task-oriented LLM system and exploring the design space of such systems through a thought experiment contemplating the performance of diverse LLM system configurations (involving single LLMs, single LLM-based agents, and multiple LLM-based agent systems) on a complex software development task and hypothesizes the results. We discuss a pattern in our results and formulate them into three conjectures. While these conjectures may be partly based on faulty assumptions, they provide a starting point for future research. The paper then surveys a select few design parameters: covering and organizing research in LLM augmentation, prompting techniques, and uncertainty estimation, and discussing their significance. The paper notes the lack of focus on computational and energy efficiency in evaluating research in these areas. Our survey findings provide a basis for developing the concept of linear and non-linear contexts, which we define and use to enable an agent-centric projection of prompting techniques providing a lens through which prompting techniques can be viewed as multi-agent systems. The paper discusses the implications of this lens, for the cross-pollination of research between LLM prompting and LLM-based multi-agent systems; and also, for the generation of synthetic training data based on existing prompting techniques in research. In all, the scoping survey presents seven conjectures that can help guide future research efforts.",
    "authors": [
      "Dhruv Dhamani",
      "Mary Lou Maher"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2023-12-29T13:35:20Z",
    "pdf_url": "https://arxiv.org/pdf/2312.17601v1"
  },
  {
    "arxiv_id": "2401.00031v2",
    "entry_id": "http://arxiv.org/abs/2401.00031v2",
    "title": "Self-supervised Pretraining for Decision Foundation Model: Formulation, Pipeline and Challenges",
    "summary": "Decision-making is a dynamic process requiring perception, memory, and reasoning to make choices and find optimal policies. Traditional approaches to decision-making suffer from sample efficiency and generalization, while large-scale self-supervised pretraining has enabled fast adaptation with fine-tuning or few-shot learning in language and vision. We thus argue to integrate knowledge acquired from generic large-scale self-supervised pretraining into downstream decision-making problems. We propose Pretrain-Then-Adapt pipeline and survey recent work on data collection, pretraining objectives and adaptation strategies for decision-making pretraining and downstream inference. Finally, we identify critical challenges and future directions for developing decision foundation model with the help of generic and flexible self-supervised pretraining.",
    "authors": [
      "Xiaoqian Liu",
      "Jianbin Jiao",
      "Junge Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-12-29T08:18:52Z",
    "pdf_url": "https://arxiv.org/pdf/2401.00031v2"
  },
  {
    "arxiv_id": "2401.08664v3",
    "entry_id": "http://arxiv.org/abs/2401.08664v3",
    "title": "Adapting Large Language Models for Education: Foundational Capabilities, Potentials, and Challenges",
    "summary": "Online education platforms, leveraging the internet to distribute education resources, seek to provide convenient education but often fall short in real-time communication with students. They often struggle to address the diverse obstacles students encounter throughout their learning journey. Solving the problems encountered by students poses a significant challenge for traditional deep learning models, as it requires not only a broad spectrum of subject knowledge but also the ability to understand what constitutes a student's individual difficulties. It's challenging for traditional machine learning models, as they lack the capacity to comprehend students' personalized needs. Recently, the emergence of large language models (LLMs) offers the possibility for resolving this issue by comprehending individual requests. Although LLMs have been successful in various fields, creating an LLM-based education system is still challenging for the wide range of educational skills required. This paper reviews the recently emerged LLM research related to educational capabilities, including mathematics, writing, programming, reasoning, and knowledge-based question answering, with the aim to explore their potential in constructing the next-generation intelligent education system. Specifically, for each capability, we focus on investigating two aspects. Firstly, we examine the current state of LLMs regarding this capability: how advanced they have become, whether they surpass human abilities, and what deficiencies might exist. Secondly, we evaluate whether the development methods for LLMs in this area are generalizable, that is, whether these methods can be applied to construct a comprehensive educational supermodel with strengths across various capabilities, rather than being effective in only a singular aspect.",
    "authors": [
      "Qingyao Li",
      "Lingyue Fu",
      "Weiming Zhang",
      "Xianyu Chen",
      "Jingwei Yu",
      "Wei Xia",
      "Weinan Zhang",
      "Ruiming Tang",
      "Yong Yu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-12-27T14:37:32Z",
    "pdf_url": "https://arxiv.org/pdf/2401.08664v3"
  },
  {
    "arxiv_id": "2312.15234v2",
    "entry_id": "http://arxiv.org/abs/2312.15234v2",
    "title": "Towards Efficient Generative Large Language Model Serving: A Survey from Algorithms to Systems",
    "summary": "In the rapidly evolving landscape of artificial intelligence (AI), generative large language models (LLMs) stand at the forefront, revolutionizing how we interact with our data. However, the computational intensity and memory consumption of deploying these models present substantial challenges in terms of serving efficiency, particularly in scenarios demanding low latency and high throughput. This survey addresses the imperative need for efficient LLM serving methodologies from a machine learning system (MLSys) research perspective, standing at the crux of advanced AI innovations and practical system optimizations. We provide in-depth analysis, covering a spectrum of solutions, ranging from cutting-edge algorithmic modifications to groundbreaking changes in system designs. The survey aims to provide a comprehensive understanding of the current state and future directions in efficient LLM serving, offering valuable insights for researchers and practitioners in overcoming the barriers of effective LLM deployment, thereby reshaping the future of AI.",
    "authors": [
      "Xupeng Miao",
      "Gabriele Oliaro",
      "Zhihao Zhang",
      "Xinhao Cheng",
      "Hongyi Jin",
      "Tianqi Chen",
      "Zhihao Jia"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC",
      "cs.PF"
    ],
    "published": "2023-12-23T11:57:53Z",
    "pdf_url": "https://arxiv.org/pdf/2312.15234v2"
  },
  {
    "arxiv_id": "2312.14925v2",
    "entry_id": "http://arxiv.org/abs/2312.14925v2",
    "title": "A Survey of Reinforcement Learning from Human Feedback",
    "summary": "Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.",
    "authors": [
      "Timo Kaufmann",
      "Paul Weng",
      "Viktor Bengs",
      "Eyke Hüllermeier"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-12-22T18:58:06Z",
    "pdf_url": "https://arxiv.org/pdf/2312.14925v2"
  },
  {
    "arxiv_id": "2312.14647v2",
    "entry_id": "http://arxiv.org/abs/2312.14647v2",
    "title": "Towards Message Brokers for Generative AI: Survey, Challenges, and Opportunities",
    "summary": "In today's digital world, Generative Artificial Intelligence (GenAI) such as Large Language Models (LLMs) is becoming increasingly prevalent, extending its reach across diverse applications. This surge in adoption has sparked a significant increase in demand for data-centric GenAI models, highlighting the necessity for robust data communication infrastructures. Central to this need are message brokers, which serve as essential channels for data transfer within various system components. This survey aims to delve into a comprehensive analysis of traditional and modern message brokers, offering a comparative study of prevalent platforms. Our study considers numerous criteria including, but not limited to, open-source availability, integrated monitoring tools, message prioritization mechanisms, capabilities for parallel processing, reliability, distribution and clustering functionalities, authentication processes, data persistence strategies, fault tolerance, and scalability. Furthermore, we explore the intrinsic constraints that the design and operation of each message broker might impose, recognizing that these limitations are crucial in understanding their real-world applicability. Finally, this study examines the enhancement of message broker mechanisms specifically for GenAI contexts, emphasizing the criticality of developing a versatile message broker framework. Such a framework would be poised for quick adaptation, catering to the dynamic and growing demands of GenAI in the foreseeable future. Through this dual-pronged approach, we intend to contribute a foundational compendium that can guide future innovations and infrastructural advancements in the realm of GenAI data communication.",
    "authors": [
      "Alaa Saleh",
      "Roberto Morabito",
      "Sasu Tarkoma",
      "Susanna Pirttikangas",
      "Lauri Lovén"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG",
      "cs.NI"
    ],
    "published": "2023-12-22T12:30:18Z",
    "pdf_url": "https://arxiv.org/pdf/2312.14647v2"
  },
  {
    "arxiv_id": "2312.13521v2",
    "entry_id": "http://arxiv.org/abs/2312.13521v2",
    "title": "Preparing to Integrate Generative Pretrained Transformer Series 4 models into Genetic Variant Assessment Workflows: Assessing Performance, Drift, and Nondeterminism Characteristics Relative to Classifying Functional Evidence in Literature",
    "summary": "Background. Large Language Models (LLMs) hold promise for improving genetic variant literature review in clinical testing. We assessed Generative Pretrained Transformer 4's (GPT-4) performance, nondeterminism, and drift to inform its suitability for use in complex clinical processes. Methods. A 2-prompt process for classification of functional evidence was optimized using a development set of 45 articles. The prompts asked GPT-4 to supply all functional data present in an article related to a variant or indicate that no functional evidence is present. For articles indicated as containing functional evidence, a second prompt asked GPT-4 to classify the evidence into pathogenic, benign, or intermediate/inconclusive categories. A final test set of 72 manually classified articles was used to test performance. Results. Over a 2.5-month period (Dec 2023-Feb 2024), we observed substantial differences in intraday (nondeterminism) and across day (drift) results, which lessened after 1/18/24. This variability is seen within and across models in the GPT-4 series, affecting different performance statistics to different degrees. Twenty runs after 1/18/24 identified articles containing functional evidence with 92.2% sensitivity, 95.6% positive predictive value (PPV) and 86.3% negative predictive value (NPV). The second prompt's identified pathogenic functional evidence with 90.0% sensitivity, 74.0% PPV and 95.3% NVP and for benign evidence with 88.0% sensitivity, 76.6% PPV and 96.9% NVP. Conclusion. Nondeterminism and drift within LLMs must be assessed and monitored when introducing LLM based functionality into clinical workflows. Failing to do this assessment or accounting for these challenges could lead to incorrect or missing information that is critical for patient care. The performance of our prompts appears adequate to assist in article prioritization but not in automated decision making.",
    "authors": [
      "Samuel J. Aronson",
      "Kalotina Machini",
      "Jiyeon Shin",
      "Pranav Sriraman",
      "Sean Hamill",
      "Emma R. Henricks",
      "Charlotte Mailly",
      "Angie J. Nottage",
      "Sami S. Amr",
      "Michael Oates",
      "Matthew S. Lebo"
    ],
    "categories": [
      "q-bio.GN",
      "cs.AI"
    ],
    "published": "2023-12-21T01:56:00Z",
    "pdf_url": "https://arxiv.org/pdf/2312.13521v2"
  },
  {
    "arxiv_id": "2401.05400v1",
    "entry_id": "http://arxiv.org/abs/2401.05400v1",
    "title": "Collaborative Learning with Artificial Intelligence Speakers (CLAIS): Pre-Service Elementary Science Teachers' Responses to the Prototype",
    "summary": "This research aims to demonstrate that AI can function not only as a tool for learning, but also as an intelligent agent with which humans can engage in collaborative learning (CL) to change epistemic practices in science classrooms. We adopted a design and development research approach, following the Analysis, Design, Development, Implementation and Evaluation (ADDIE) model, to prototype a tangible instructional system called Collaborative Learning with AI Speakers (CLAIS). The CLAIS system is designed to have 3-4 human learners join an AI speaker to form a small group, where humans and AI are considered as peers participating in the Jigsaw learning process. The development was carried out using the NUGU AI speaker platform. The CLAIS system was successfully implemented in a Science Education course session with 15 pre-service elementary science teachers. The participants evaluated the CLAIS system through mixed methods surveys as teachers, learners, peers, and users. Quantitative data showed that the participants' Intelligent-Technological, Pedagogical, And Content Knowledge was significantly increased after the CLAIS session, the perception of the CLAIS learning experience was positive, the peer assessment on AI speakers and human peers was different, and the user experience was ambivalent. Qualitative data showed that the participants anticipated future changes in the epistemic process in science classrooms, while acknowledging technical issues such as speech recognition performance and response latency. This study highlights the potential of Human-AI Collaboration for knowledge co-construction in authentic classroom settings and exemplify how AI could shape the future landscape of epistemic practices in the classroom.",
    "authors": [
      "Gyeong-Geon Lee",
      "Seonyeong Mun",
      "Myeong-Kyeong Shin",
      "Xiaoming Zhai"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-12-20T01:19:03Z",
    "pdf_url": "https://arxiv.org/pdf/2401.05400v1"
  },
  {
    "arxiv_id": "2312.11970v1",
    "entry_id": "http://arxiv.org/abs/2312.11970v1",
    "title": "Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives",
    "summary": "Agent-based modeling and simulation has evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, examining their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions.",
    "authors": [
      "Chen Gao",
      "Xiaochong Lan",
      "Nian Li",
      "Yuan Yuan",
      "Jingtao Ding",
      "Zhilun Zhou",
      "Fengli Xu",
      "Yong Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY",
      "cs.MA"
    ],
    "published": "2023-12-19T09:06:45Z",
    "pdf_url": "https://arxiv.org/pdf/2312.11970v1"
  },
  {
    "arxiv_id": "2312.11681v4",
    "entry_id": "http://arxiv.org/abs/2312.11681v4",
    "title": "Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows",
    "summary": "LLM chains enable complex tasks by decomposing work into a sequence of subtasks. Similarly, the more established techniques of crowdsourcing workflows decompose complex tasks into smaller tasks for human crowdworkers. Chains address LLM errors analogously to the way crowdsourcing workflows address human error. To characterize opportunities for LLM chaining, we survey 107 papers across the crowdsourcing and chaining literature to construct a design space for chain development. The design space covers a designer's objectives and the tactics used to build workflows. We then surface strategies that mediate how workflows use tactics to achieve objectives. To explore how techniques from crowdsourcing may apply to chaining, we adapt crowdsourcing workflows to implement LLM chains across three case studies: creating a taxonomy, shortening text, and writing a short story. From the design space and our case studies, we identify takeaways for effective chain design and raise implications for future research and development.",
    "authors": [
      "Madeleine Grunde-McLaughlin",
      "Michelle S. Lam",
      "Ranjay Krishna",
      "Daniel S. Weld",
      "Jeffrey Heer"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-12-18T20:01:58Z",
    "pdf_url": "https://arxiv.org/pdf/2312.11681v4"
  },
  {
    "arxiv_id": "2312.11462v5",
    "entry_id": "http://arxiv.org/abs/2312.11462v5",
    "title": "Cascade Speculative Drafting for Even Faster LLM Inference",
    "summary": "Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in drafting for improved efficiency. Combining both cascades, CS Drafting achieves greater speedup compared to the baselines in our experiments, while preserving the same output distribution as the target model.",
    "authors": [
      "Ziyi Chen",
      "Xiaocong Yang",
      "Jiacheng Lin",
      "Chenkai Sun",
      "Kevin Chen-Chuan Chang",
      "Jie Huang"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2023-12-18T18:59:46Z",
    "pdf_url": "https://arxiv.org/pdf/2312.11462v5"
  },
  {
    "arxiv_id": "2312.11084v3",
    "entry_id": "http://arxiv.org/abs/2312.11084v3",
    "title": "Multi-Agent Reinforcement Learning for Connected and Automated Vehicles Control: Recent Advancements and Future Prospects",
    "summary": "Connected and automated vehicles (CAVs) are considered a potential solution for future transportation challenges, aiming to develop systems that are efficient, safe, and environmentally friendly. However, CAV control presents significant challenges due to the complexity of interconnectivity and coordination required among vehicles. Multi-agent reinforcement learning (MARL), which has shown notable advancements in addressing complex problems in autonomous driving, robotics, and human-vehicle interaction, emerges as a promising tool to enhance CAV capabilities. Despite its potential, there is a notable absence of current reviews on mainstream MARL algorithms for CAVs. To fill this gap, this paper offers a comprehensive review of MARL's application in CAV control. The paper begins with an introduction to MARL, explaining its unique advantages in handling complex and multi-agent scenarios. It then presents a detailed survey of MARL applications across various control dimensions for CAVs, including critical scenarios such as platooning control, lane-changing, and unsignalized intersections. Additionally, the paper reviews prominent simulation platforms essential for developing and testing MARL algorithms. Lastly, it examines the current challenges in deploying MARL for CAV control, including macro-micro optimization, communication, mixed traffic, and sim-to-real challenges. Potential solutions discussed include hierarchical MARL, decentralized MARL, adaptive interactions, and offline MARL.",
    "authors": [
      "Min Hua",
      "Dong Chen",
      "Xinda Qi",
      "Kun Jiang",
      "Zemin Eitan Liu",
      "Quan Zhou",
      "Hongming Xu"
    ],
    "categories": [
      "cs.RO",
      "cs.MA"
    ],
    "published": "2023-12-18T10:23:50Z",
    "pdf_url": "https://arxiv.org/pdf/2312.11084v3"
  },
  {
    "arxiv_id": "2312.11063v1",
    "entry_id": "http://arxiv.org/abs/2312.11063v1",
    "title": "A survey on algorithms for Nash equilibria in finite normal-form games",
    "summary": "Nash equilibrium is one of the most influential solution concepts in game theory. With the development of computer science and artificial intelligence, there is an increasing demand on Nash equilibrium computation, especially for Internet economics and multi-agent learning. This paper reviews various algorithms computing the Nash equilibrium and its approximation solutions in finite normal-form games from both theoretical and empirical perspectives. For the theoretical part, we classify algorithms in the literature and present basic ideas on algorithm design and analysis. For the empirical part, we present a comprehensive comparison on the algorithms in the literature over different kinds of games. Based on these results, we provide practical suggestions on implementations and uses of these algorithms. Finally, we present a series of open problems from both theoretical and practical considerations.",
    "authors": [
      "Hanyu Li",
      "Wenhan Huang",
      "Zhijian Duan",
      "David Henry Mguni",
      "Kun Shao",
      "Jun Wang",
      "Xiaotie Deng"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.DS",
      "cs.LG",
      "econ.TH"
    ],
    "published": "2023-12-18T10:00:47Z",
    "pdf_url": "https://arxiv.org/pdf/2312.11063v1"
  },
  {
    "arxiv_id": "2312.10997v5",
    "entry_id": "http://arxiv.org/abs/2312.10997v5",
    "title": "Retrieval-Augmented Generation for Large Language Models: A Survey",
    "summary": "Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",
    "authors": [
      "Yunfan Gao",
      "Yun Xiong",
      "Xinyu Gao",
      "Kangxiang Jia",
      "Jinliu Pan",
      "Yuxi Bi",
      "Yi Dai",
      "Jiawei Sun",
      "Meng Wang",
      "Haofen Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-18T07:47:33Z",
    "pdf_url": "https://arxiv.org/pdf/2312.10997v5"
  },
  {
    "arxiv_id": "2312.11567v1",
    "entry_id": "http://arxiv.org/abs/2312.11567v1",
    "title": "Students' Perceptions and Preferences of Generative Artificial Intelligence Feedback for Programming",
    "summary": "The rapid evolution of artificial intelligence (AI), specifically large language models (LLMs), has opened opportunities for various educational applications. This paper explored the feasibility of utilizing ChatGPT, one of the most popular LLMs, for automating feedback for Java programming assignments in an introductory computer science (CS1) class. Specifically, this study focused on three questions: 1) To what extent do students view LLM-generated feedback as formative? 2) How do students see the comparative affordances of feedback prompts that include their code, vs. those that exclude it? 3) What enhancements do students suggest for improving AI-generated feedback? To address these questions, we generated automated feedback using the ChatGPT API for four lab assignments in the CS1 class. The survey results revealed that students perceived the feedback as aligning well with formative feedback guidelines established by Shute. Additionally, students showed a clear preference for feedback generated by including the students' code as part of the LLM prompt, and our thematic study indicated that the preference was mainly attributed to the specificity, clarity, and corrective nature of the feedback. Moreover, this study found that students generally expected specific and corrective feedback with sufficient code examples, but had diverged opinions on the tone of the feedback. This study demonstrated that ChatGPT could generate Java programming assignment feedback that students perceived as formative. It also offered insights into the specific improvements that would make the ChatGPT-generated feedback useful for students.",
    "authors": [
      "Zhengdong Zhang",
      "Zihan Dong",
      "Yang Shi",
      "Noboru Matsuda",
      "Thomas Price",
      "Dongkuan Xu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2023-12-17T22:26:53Z",
    "pdf_url": "https://arxiv.org/pdf/2312.11567v1"
  },
  {
    "arxiv_id": "2312.11562v5",
    "entry_id": "http://arxiv.org/abs/2312.11562v5",
    "title": "A Survey of Reasoning with Foundation Models",
    "summary": "Reasoning, a crucial ability for complex problem-solving, plays a pivotal role in various real-world settings such as negotiation, medical diagnosis, and criminal investigation. It serves as a fundamental methodology in the field of Artificial General Intelligence (AGI). With the ongoing development of foundation models, e.g., Large Language Models (LLMs), there is a growing interest in exploring their abilities in reasoning tasks. In this paper, we introduce seminal foundation models proposed or adaptable for reasoning, highlighting the latest advancements in various reasoning tasks, methods, and benchmarks. We then delve into the potential future directions behind the emergence of reasoning abilities within foundation models. We also discuss the relevance of multimodal learning, autonomous agents, and super alignment in the context of reasoning. By discussing these future research directions, we hope to inspire researchers in their exploration of this field, stimulate further advancements in reasoning with foundation models, and contribute to the development of AGI.",
    "authors": [
      "Jiankai Sun",
      "Chuanyang Zheng",
      "Enze Xie",
      "Zhengying Liu",
      "Ruihang Chu",
      "Jianing Qiu",
      "Jiaqi Xu",
      "Mingyu Ding",
      "Hongyang Li",
      "Mengzhe Geng",
      "Yue Wu",
      "Wenhai Wang",
      "Junsong Chen",
      "Zhangyue Yin",
      "Xiaozhe Ren",
      "Jie Fu",
      "Junxian He",
      "Wu Yuan",
      "Qi Liu",
      "Xihui Liu",
      "Yu Li",
      "Hao Dong",
      "Yu Cheng",
      "Ming Zhang",
      "Pheng Ann Heng",
      "Jifeng Dai",
      "Ping Luo",
      "Jingdong Wang",
      "Ji-Rong Wen",
      "Xipeng Qiu",
      "Yike Guo",
      "Hui Xiong",
      "Qun Liu",
      "Zhenguo Li"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-12-17T15:16:13Z",
    "pdf_url": "https://arxiv.org/pdf/2312.11562v5"
  },
  {
    "arxiv_id": "2312.10256v2",
    "entry_id": "http://arxiv.org/abs/2312.10256v2",
    "title": "Multi-agent Reinforcement Learning: A Comprehensive Survey",
    "summary": "Multi-agent systems (MAS) are widely prevalent and crucially important in numerous real-world applications, where multiple agents must make decisions to achieve their objectives in a shared environment. Despite their ubiquity, the development of intelligent decision-making agents in MAS poses several open challenges to their effective implementation. This survey examines these challenges, placing an emphasis on studying seminal concepts from game theory (GT) and machine learning (ML) and connecting them to recent advancements in multi-agent reinforcement learning (MARL), i.e. the research of data-driven decision-making within MAS. Therefore, the objective of this survey is to provide a comprehensive perspective along the various dimensions of MARL, shedding light on the unique opportunities that are presented in MARL applications while highlighting the inherent challenges that accompany this potential. Therefore, we hope that our work will not only contribute to the field by analyzing the current landscape of MARL but also motivate future directions with insights for deeper integration of concepts from related domains of GT and ML. With this in mind, this work delves into a detailed exploration of recent and past efforts of MARL and its related fields and describes prior solutions that were proposed and their limitations, as well as their applications.",
    "authors": [
      "Dom Huh",
      "Prasant Mohapatra"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-12-15T23:16:54Z",
    "pdf_url": "https://arxiv.org/pdf/2312.10256v2"
  },
  {
    "arxiv_id": "2312.10163v1",
    "entry_id": "http://arxiv.org/abs/2312.10163v1",
    "title": "Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey",
    "summary": "The advent of foundation models, which are pre-trained on vast datasets, has ushered in a new era of computer vision, characterized by their robustness and remarkable zero-shot generalization capabilities. Mirroring the transformative impact of foundation models like large language models (LLMs) in natural language processing, visual foundation models (VFMs) have become a catalyst for groundbreaking developments in computer vision. This review paper delineates the pivotal trajectories of VFMs, emphasizing their scalability and proficiency in generative tasks such as text-to-image synthesis, as well as their adeptness in discriminative tasks including image segmentation. While generative and discriminative models have historically charted distinct paths, we undertake a comprehensive examination of the recent strides made by VFMs in both domains, elucidating their origins, seminal breakthroughs, and pivotal methodologies. Additionally, we collate and discuss the extensive resources that facilitate the development of VFMs and address the challenges that pave the way for future research endeavors. A crucial direction for forthcoming innovation is the amalgamation of generative and discriminative paradigms. The nascent application of generative models within discriminative contexts signifies the early stages of this confluence. This survey aspires to be a contemporary compendium for scholars and practitioners alike, charting the course of VFMs and illuminating their multifaceted landscape.",
    "authors": [
      "Xu Liu",
      "Tong Zhou",
      "Yuanxin Wang",
      "Yuping Wang",
      "Qinjingwen Cao",
      "Weizhi Du",
      "Yonghuan Yang",
      "Junjun He",
      "Yu Qiao",
      "Yiqing Shen"
    ],
    "categories": [
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-12-15T19:17:15Z",
    "pdf_url": "https://arxiv.org/pdf/2312.10163v1"
  },
  {
    "arxiv_id": "2312.09198v1",
    "entry_id": "http://arxiv.org/abs/2312.09198v1",
    "title": "Weaving Pathways for Justice with GPT: LLM-driven automated drafting of interactive legal applications",
    "summary": "Can generative AI help us speed up the authoring of tools to help self-represented litigants?\n  In this paper, we describe 3 approaches to automating the completion of court forms: a generative AI approach that uses GPT-3 to iteratively prompt the user to answer questions, a constrained template-driven approach that uses GPT-4-turbo to generate a draft of questions that are subject to human review, and a hybrid method. We use the open source Docassemble platform in all 3 experiments, together with a tool created at Suffolk University Law School called the Assembly Line Weaver. We conclude that the hybrid model of constrained automated drafting with human review is best suited to the task of authoring guided interviews.",
    "authors": [
      "Quinten Steenhuis",
      "David Colarusso",
      "Bryce Willey"
    ],
    "categories": [
      "cs.AI",
      "cs.CV",
      "cs.CY",
      "cs.HC",
      "cs.SI"
    ],
    "published": "2023-12-14T18:20:59Z",
    "pdf_url": "https://arxiv.org/pdf/2312.09198v1"
  },
  {
    "arxiv_id": "2312.08782v3",
    "entry_id": "http://arxiv.org/abs/2312.08782v3",
    "title": "Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis",
    "summary": "Building general-purpose robots that operate seamlessly in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. However, as a community, we have been constraining most robotic systems by designing them for specific tasks, training them on specific datasets, and deploying them within specific environments. These systems require extensively-labeled data and task-specific models. When deployed in real-world scenarios, such systems face several generalization issues and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of general-purpose robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing a generalized formulation of how foundation models are used in robotics, and the fundamental barriers to making generalist robots universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository 2 of resources, including papers reviewed in this survey, as well as related projects and repositories for developing foundation models for robotics.",
    "authors": [
      "Yafei Hu",
      "Quanting Xie",
      "Vidhi Jain",
      "Jonathan Francis",
      "Jay Patrikar",
      "Nikhil Keetha",
      "Seungchan Kim",
      "Yaqi Xie",
      "Tianyi Zhang",
      "Hao-Shu Fang",
      "Shibo Zhao",
      "Shayegan Omidshafiei",
      "Dong-Ki Kim",
      "Ali-akbar Agha-mohammadi",
      "Katia Sycara",
      "Matthew Johnson-Roberson",
      "Dhruv Batra",
      "Xiaolong Wang",
      "Sebastian Scherer",
      "Chen Wang",
      "Zsolt Kira",
      "Fei Xia",
      "Yonatan Bisk"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-12-14T10:02:55Z",
    "pdf_url": "https://arxiv.org/pdf/2312.08782v3"
  },
  {
    "arxiv_id": "2312.08579v2",
    "entry_id": "http://arxiv.org/abs/2312.08579v2",
    "title": "Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach",
    "summary": "The automatic identification of planetary feature names in astronomy publications presents numerous challenges. These features include craters, defined as roughly circular depressions resulting from impact or volcanic activity; dorsas, which are elongate raised structures or wrinkle ridges; and lacus, small irregular patches of dark, smooth material on the Moon, referred to as \"lake\" (Planetary Names Working Group, n.d.). Many feature names overlap with places or people's names that they are named after, for example, Syria, Tempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some feature names have been used in many contexts, for instance, Apollo, which can refer to mission, program, sample, astronaut, seismic, seismometers, core, era, data, collection, instrument, and station, in addition to the crater on the Moon. Some feature names can appear in the text as adjectives, like the lunar craters Black, Green, and White. Some feature names in other contexts serve as directions, like craters West and South on the Moon. Additionally, some features share identical names across different celestial bodies, requiring disambiguation, such as the Adams crater, which exists on both the Moon and Mars. We present a multi-step pipeline combining rule-based filtering, statistical relevance analysis, part-of-speech (POS) tagging, named entity recognition (NER) model, hybrid keyword harvesting, knowledge graph (KG) matching, and inference with a locally installed large language model (LLM) to reliably identify planetary names despite these challenges. When evaluated on a dataset of astronomy papers from the Astrophysics Data System (ADS), this methodology achieves an F1-score over 0.97 in disambiguating planetary feature names.",
    "authors": [
      "Golnaz Shapurian",
      "Michael J Kurtz",
      "Alberto Accomazzi"
    ],
    "categories": [
      "cs.CL",
      "astro-ph.IM",
      "cs.LG"
    ],
    "published": "2023-12-14T00:50:14Z",
    "pdf_url": "https://arxiv.org/pdf/2312.08579v2"
  },
  {
    "arxiv_id": "2312.08495v1",
    "entry_id": "http://arxiv.org/abs/2312.08495v1",
    "title": "Beyond Accuracy: Automated De-Identification of Large Real-World Clinical Text Datasets",
    "summary": "Recent research advances achieve human-level accuracy for de-identifying free-text clinical notes on research datasets, but gaps remain in reproducing this in large real-world settings. This paper summarizes lessons learned from building a system used to de-identify over one billion real clinical notes, in a fully automated way, that was independently certified by multiple organizations for production use. A fully automated solution requires a very high level of accuracy that does not require manual review. A hybrid context-based model architecture is described, which outperforms a Named Entity Recogniton (NER) - only model by 10% on the i2b2-2014 benchmark. The proposed system makes 50%, 475%, and 575% fewer errors than the comparable AWS, Azure, and GCP services respectively while also outperforming ChatGPT by 33%. It exceeds 98% coverage of sensitive data across 7 European languages, without a need for fine tuning. A second set of described models enable data obfuscation -- replacing sensitive data with random surrogates -- while retaining name, date, gender, clinical, and format consistency. Both the practical need and the solution architecture that provides for reliable & linked anonymized documents are described.",
    "authors": [
      "Veysel Kocaman",
      "Hasham Ul Haq",
      "David Talby"
    ],
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2023-12-13T20:15:29Z",
    "pdf_url": "https://arxiv.org/pdf/2312.08495v1"
  },
  {
    "arxiv_id": "2312.07966v1",
    "entry_id": "http://arxiv.org/abs/2312.07966v1",
    "title": "A multi-sourced data and agent-based approach for complementing Time Use Surveys in the context of residential human activity and load curve simulation",
    "summary": "To address the major issues associated with using Time-Use Survey (TUS) for simulating residential load curves, we present the SMACH approach, which combines qualitative and quantitative data with agent-based simulation. Our model consists of autonomous agents assigned with daily tasks. The agents try to accomplish their assigned tasks to the best of their abilities. Quantitative data are used to generate tasks assignments. Qualitative studies allow us to define how agents select, based on plausible cognitive principles, the tasks to accomplish depending on the context. Our results show a better representation of weekdays and weekends, a more flexible association of tasks with appliances, and an improved simulation of load curves compared to real data. Highlights $\\bullet$ Discussion about Time-Use Surveys (TUS) limits and the use of TUS in activity and energy simulation $\\bullet$ Presentation of complementary data both qualitative and quantitative used to complement TUS data $\\bullet$ Proposition of an agent-based approach that balances these limitations",
    "authors": [
      "Mathieu Schumann",
      "Quentin Reynaud",
      "François Sempé",
      "Julien Guibourdenche",
      "Jean-Baptiste Ly",
      "Nicolas Sabouret"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "published": "2023-12-13T08:28:55Z",
    "pdf_url": "https://arxiv.org/pdf/2312.07966v1"
  },
  {
    "arxiv_id": "2312.08400v1",
    "entry_id": "http://arxiv.org/abs/2312.08400v1",
    "title": "Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction",
    "summary": "Large language models (LLMs) finetuned to follow human instruction have recently exhibited significant capabilities in various English NLP tasks. However, their performance in grammatical error correction (GEC), especially on languages other than English, remains significantly unexplored. In this work, we evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a complex task due to Arabic's rich morphology. Our findings suggest that various prompting methods, coupled with (in-context) few-shot learning, demonstrate considerable effectiveness, with GPT-4 achieving up to $65.49$ F$_{1}$ score under expert prompting (approximately $5$ points higher than our established baseline). Despite these positive results, we find that instruction finetuned models, regardless of their size, are still outperformed by fully finetuned ones, even if they are significantly smaller in size. This disparity highlights substantial room for improvements for LLMs. Inspired by methods used in low-resource machine translation, we also develop a method exploiting synthetic data that significantly outperforms previous models on two standard Arabic benchmarks. Our best model achieves a new SOTA on Arabic GEC, with $73.29$ and $73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively, compared to peer-reviewed published baselines.",
    "authors": [
      "Sang Yun Kwon",
      "Gagan Bhatia",
      "El Moatez Billah Nagoudi",
      "Muhammad Abdul-Mageed"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-13T05:33:25Z",
    "pdf_url": "https://arxiv.org/pdf/2312.08400v1"
  },
  {
    "arxiv_id": "2401.06775v2",
    "entry_id": "http://arxiv.org/abs/2401.06775v2",
    "title": "Large language models in healthcare and medical domain: A review",
    "summary": "The deployment of large language models (LLMs) within the healthcare sector has sparked both enthusiasm and apprehension. These models exhibit the remarkable capability to provide proficient responses to free-text queries, demonstrating a nuanced understanding of professional medical knowledge. This comprehensive survey delves into the functionalities of existing LLMs designed for healthcare applications, elucidating the trajectory of their development, starting from traditional Pretrained Language Models (PLMs) to the present state of LLMs in healthcare sector. First, we explore the potential of LLMs to amplify the efficiency and effectiveness of diverse healthcare applications, particularly focusing on clinical language understanding tasks. These tasks encompass a wide spectrum, ranging from named entity recognition and relation extraction to natural language inference, multi-modal medical applications, document classification, and question-answering. Additionally, we conduct an extensive comparison of the most recent state-of-the-art LLMs in the healthcare domain, while also assessing the utilization of various open-source LLMs and highlighting their significance in healthcare applications. Furthermore, we present the essential performance metrics employed to evaluate LLMs in the biomedical domain, shedding light on their effectiveness and limitations. Finally, we summarize the prominent challenges and constraints faced by large language models in the healthcare sector, offering a holistic perspective on their potential benefits and shortcomings. This review provides a comprehensive exploration of the current landscape of LLMs in healthcare, addressing their role in transforming medical applications and the areas that warrant further research and development.",
    "authors": [
      "Zabir Al Nazi",
      "Wei Peng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-12T20:54:51Z",
    "pdf_url": "https://arxiv.org/pdf/2401.06775v2"
  },
  {
    "arxiv_id": "2312.06942v5",
    "entry_id": "http://arxiv.org/abs/2312.06942v5",
    "title": "AI Control: Improving Safety Despite Intentional Subversion",
    "summary": "As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques (\"protocols\") that are robust to intentional subversion.\n  We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we operationalize here as logical errors that are not caught by test cases.\n  We investigate a range of protocols and test each against strategies that the untrusted model could use to subvert them. One protocol is what we call trusted editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code. If the code is below some suspiciousness threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove parts that seem suspicious and then submits the edited code. Another protocol is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding. These protocols improve substantially on simple baselines.",
    "authors": [
      "Ryan Greenblatt",
      "Buck Shlegeris",
      "Kshitij Sachan",
      "Fabien Roger"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-12-12T02:34:06Z",
    "pdf_url": "https://arxiv.org/pdf/2312.06942v5"
  },
  {
    "arxiv_id": "2312.06457v1",
    "entry_id": "http://arxiv.org/abs/2312.06457v1",
    "title": "Large Language Models with Retrieval-Augmented Generation for Zero-Shot Disease Phenotyping",
    "summary": "Identifying disease phenotypes from electronic health records (EHRs) is critical for numerous secondary uses. Manually encoding physician knowledge into rules is particularly challenging for rare diseases due to inadequate EHR coding, necessitating review of clinical notes. Large language models (LLMs) offer promise in text understanding but may not efficiently handle real-world clinical documentation. We propose a zero-shot LLM-based method enriched by retrieval-augmented generation and MapReduce, which pre-identifies disease-related text snippets to be used in parallel as queries for the LLM to establish diagnosis. We show that this method as applied to pulmonary hypertension (PH), a rare disease characterized by elevated arterial pressures in the lungs, significantly outperforms physician logic rules ($F_1$ score of 0.62 vs. 0.75). This method has the potential to enhance rare disease cohort identification, expanding the scope of robust clinical research and care gap identification.",
    "authors": [
      "Will E. Thompson",
      "David M. Vidmar",
      "Jessica K. De Freitas",
      "John M. Pfeifer",
      "Brandon K. Fornwalt",
      "Ruijun Chen",
      "Gabriel Altay",
      "Kabir Manghnani",
      "Andrew C. Nelsen",
      "Kellie Morland",
      "Martin C. Stumpe",
      "Riccardo Miotto"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.IR"
    ],
    "published": "2023-12-11T15:45:27Z",
    "pdf_url": "https://arxiv.org/pdf/2312.06457v1"
  },
  {
    "arxiv_id": "2312.11518v2",
    "entry_id": "http://arxiv.org/abs/2312.11518v2",
    "title": "User Modeling in the Era of Large Language Models: Current Research and Future Directions",
    "summary": "User modeling (UM) aims to discover patterns or learn representations from user data about the characteristics of a specific user, such as profile, preference, and personality. The user models enable personalization and suspiciousness detection in many online applications such as recommendation, education, and healthcare. Two common types of user data are text and graph, as the data usually contain a large amount of user-generated content (UGC) and online interactions. The research of text and graph mining is developing rapidly, contributing many notable solutions in the past two decades. Recently, large language models (LLMs) have shown superior performance on generating, understanding, and even reasoning over text data. The approaches of user modeling have been equipped with LLMs and soon become outstanding. This article summarizes existing research about how and why LLMs are great tools of modeling and understanding UGC. Then it reviews a few categories of large language models for user modeling (LLM-UM) approaches that integrate the LLMs with text and graph-based methods in different ways. Then it introduces specific LLM-UM techniques for a variety of UM applications. Finally, it presents remaining challenges and future directions in the LLM-UM research. We maintain the reading list at: https://github.com/TamSiuhin/LLM-UM-Reading",
    "authors": [
      "Zhaoxuan Tan",
      "Meng Jiang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-11T03:59:36Z",
    "pdf_url": "https://arxiv.org/pdf/2312.11518v2"
  },
  {
    "arxiv_id": "2312.06717v4",
    "entry_id": "http://arxiv.org/abs/2312.06717v4",
    "title": "Privacy Issues in Large Language Models: A Survey",
    "summary": "This is the first survey of the active area of AI research that focuses on privacy issues in Large Language Models (LLMs). Specifically, we focus on work that red-teams models to highlight privacy risks, attempts to build privacy into the training or inference process, enables efficient data deletion from trained models to comply with existing privacy regulations, and tries to mitigate copyright issues. Our focus is on summarizing technical research that develops algorithms, proves theorems, and runs empirical evaluations. While there is an extensive body of legal and policy work addressing these challenges from a different angle, that is not the focus of our survey. Nevertheless, these works, along with recent legal developments do inform how these technical problems are formalized, and so we discuss them briefly in Section 1. While we have made our best effort to include all the relevant work, due to the fast moving nature of this research we may have missed some recent work. If we have missed some of your work please contact us, as we will attempt to keep this survey relatively up to date. We are maintaining a repository with the list of papers covered in this survey and any relevant code that was publicly available at https://github.com/safr-ml-lab/survey-llm.",
    "authors": [
      "Seth Neel",
      "Peter Chang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-12-11T01:26:53Z",
    "pdf_url": "https://arxiv.org/pdf/2312.06717v4"
  },
  {
    "arxiv_id": "2312.05795v2",
    "entry_id": "http://arxiv.org/abs/2312.05795v2",
    "title": "Large Multimodal Model Compression via Efficient Pruning and Distillation at AntGroup",
    "summary": "The deployment of Large Multimodal Models (LMMs) within AntGroup has significantly advanced multimodal tasks in payment, security, and advertising, notably enhancing advertisement audition tasks in Alipay. However, the deployment of such sizable models introduces challenges, particularly in increased latency and carbon emissions, which are antithetical to the ideals of Green AI. This paper introduces a novel multi-stage compression strategy for our proprietary LLM, AntGMM. Our methodology pivots on three main aspects: employing small training sample sizes, addressing multi-level redundancy through multi-stage pruning, and introducing an advanced distillation loss design. In our research, we constructed a dataset, the Multimodal Advertisement Audition Dataset (MAAD), from real-world scenarios within Alipay, and conducted experiments to validate the reliability of our proposed strategy. Furthermore, the effectiveness of our strategy is evident in its operational success in Alipay's real-world multimodal advertisement audition for three months from September 2023. Notably, our approach achieved a substantial reduction in latency, decreasing it from 700ms to 90ms, while maintaining online performance with only a slight performance decrease. Moreover, our compressed model is estimated to reduce electricity consumption by approximately 75 million kWh annually compared to the direct deployment of AntGMM, demonstrating our commitment to green AI initiatives. We will publicly release our code and the MAAD dataset after some reviews\\footnote{https://github.com/MorinW/AntGMM$\\_$Pruning}.",
    "authors": [
      "Maolin Wang",
      "Yao Zhao",
      "Jiajia Liu",
      "Jingdong Chen",
      "Chenyi Zhuang",
      "Jinjie Gu",
      "Ruocheng Guo",
      "Xiangyu Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-12-10T06:57:48Z",
    "pdf_url": "https://arxiv.org/pdf/2312.05795v2"
  },
  {
    "arxiv_id": "2312.05230v1",
    "entry_id": "http://arxiv.org/abs/2312.05230v1",
    "title": "Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning",
    "summary": "Despite their tremendous success in many applications, large language models often fall short of consistent reasoning and planning in various (language, embodied, and social) scenarios, due to inherent limitations in their inference, learning, and modeling capabilities. In this position paper, we present a new perspective of machine reasoning, LAW, that connects the concepts of Language models, Agent models, and World models, for more robust and versatile reasoning capabilities. In particular, we propose that world and agent models are a better abstraction of reasoning, that introduces the crucial elements of deliberate human-like reasoning, including beliefs about the world and other agents, anticipation of consequences, goals/rewards, and strategic planning. Crucially, language models in LAW serve as a backend to implement the system or its elements and hence provide the computational power and adaptability. We review the recent studies that have made relevant progress and discuss future research directions towards operationalizing the LAW framework.",
    "authors": [
      "Zhiting Hu",
      "Tianmin Shu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CV",
      "cs.LG",
      "cs.RO"
    ],
    "published": "2023-12-08T18:25:22Z",
    "pdf_url": "https://arxiv.org/pdf/2312.05230v1"
  },
  {
    "arxiv_id": "2312.05162v1",
    "entry_id": "http://arxiv.org/abs/2312.05162v1",
    "title": "A Review of Cooperation in Multi-agent Learning",
    "summary": "Cooperation in multi-agent learning (MAL) is a topic at the intersection of numerous disciplines, including game theory, economics, social sciences, and evolutionary biology. Research in this area aims to understand both how agents can coordinate effectively when goals are aligned and how they may cooperate in settings where gains from working together are possible but possibilities for conflict abound. In this paper we provide an overview of the fundamental concepts, problem settings and algorithms of multi-agent learning. This encompasses reinforcement learning, multi-agent sequential decision-making, challenges associated with multi-agent cooperation, and a comprehensive review of recent progress, along with an evaluation of relevant metrics. Finally we discuss open challenges in the field with the aim of inspiring new avenues for research.",
    "authors": [
      "Yali Du",
      "Joel Z. Leibo",
      "Usman Islam",
      "Richard Willis",
      "Peter Sunehag"
    ],
    "categories": [
      "cs.MA",
      "cs.AI",
      "cs.GT",
      "cs.LG"
    ],
    "published": "2023-12-08T16:42:15Z",
    "pdf_url": "https://arxiv.org/pdf/2312.05162v1"
  },
  {
    "arxiv_id": "2312.10075v1",
    "entry_id": "http://arxiv.org/abs/2312.10075v1",
    "title": "Assessing LLMs for Moral Value Pluralism",
    "summary": "The fields of AI current lacks methods to quantitatively assess and potentially alter the moral values inherent in the output of large language models (LLMs). However, decades of social science research has developed and refined widely-accepted moral value surveys, such as the World Values Survey (WVS), eliciting value judgments from direct questions in various geographies. We have turned those questions into value statements and use NLP to compute to how well popular LLMs are aligned with moral values for various demographics and cultures. While the WVS is accepted as an explicit assessment of values, we lack methods for assessing implicit moral and cultural values in media, e.g., encountered in social media, political rhetoric, narratives, and generated by AI systems such as LLMs that are increasingly present in our daily lives. As we consume online content and utilize LLM outputs, we might ask, which moral values are being implicitly promoted or undercut, or -- in the case of LLMs -- if they are intending to represent a cultural identity, are they doing so consistently? In this paper we utilize a Recognizing Value Resonance (RVR) NLP model to identify WVS values that resonate and conflict with a given passage of output text. We apply RVR to the text generated by LLMs to characterize implicit moral values, allowing us to quantify the moral/cultural distance between LLMs and various demographics that have been surveyed using the WVS. In line with other work we find that LLMs exhibit several Western-centric value biases; they overestimate how conservative people in non-Western countries are, they are less accurate in representing gender for non-Western countries, and portray older populations as having more traditional values. Our results highlight value misalignment and age groups, and a need for social science informed technological solutions addressing value plurality in LLMs.",
    "authors": [
      "Noam Benkler",
      "Drisana Mosaphir",
      "Scott Friedman",
      "Andrew Smart",
      "Sonja Schmer-Galunder"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-08T16:18:15Z",
    "pdf_url": "https://arxiv.org/pdf/2312.10075v1"
  },
  {
    "arxiv_id": "2312.05019v2",
    "entry_id": "http://arxiv.org/abs/2312.05019v2",
    "title": "Vision-based Learning for Drones: A Survey",
    "summary": "Drones as advanced cyber-physical systems are undergoing a transformative shift with the advent of vision-based learning, a field that is rapidly gaining prominence due to its profound impact on drone autonomy and functionality. Different from existing task-specific surveys, this review offers a comprehensive overview of vision-based learning in drones, emphasizing its pivotal role in enhancing their operational capabilities under various scenarios. We start by elucidating the fundamental principles of vision-based learning, highlighting how it significantly improves drones' visual perception and decision-making processes. We then categorize vision-based control methods into indirect, semi-direct, and end-to-end approaches from the perception-control perspective. We further explore various applications of vision-based drones with learning capabilities, ranging from single-agent systems to more complex multi-agent and heterogeneous system scenarios, and underscore the challenges and innovations characterizing each area. Finally, we explore open questions and potential solutions, paving the way for ongoing research and development in this dynamic and rapidly evolving field. With growing large language models (LLMs) and embodied intelligence, vision-based learning for drones provides a promising but challenging road towards artificial general intelligence (AGI) in 3D physical world.",
    "authors": [
      "Jiaping Xiao",
      "Rangya Zhang",
      "Yuhang Zhang",
      "Mir Feroskhan"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2023-12-08T12:57:13Z",
    "pdf_url": "https://arxiv.org/pdf/2312.05019v2"
  },
  {
    "arxiv_id": "2312.04688v1",
    "entry_id": "http://arxiv.org/abs/2312.04688v1",
    "title": "Federated Learning for 6G: Paradigms, Taxonomy, Recent Advances and Insights",
    "summary": "Artificial Intelligence (AI) is expected to play an instrumental role in the next generation of wireless systems, such as sixth-generation (6G) mobile network. However, massive data, energy consumption, training complexity, and sensitive data protection in wireless systems are all crucial challenges that must be addressed for training AI models and gathering intelligence and knowledge from distributed devices. Federated Learning (FL) is a recent framework that has emerged as a promising approach for multiple learning agents to build an accurate and robust machine learning models without sharing raw data. By allowing mobile handsets and devices to collaboratively learn a global model without explicit sharing of training data, FL exhibits high privacy and efficient spectrum utilization. While there are a lot of survey papers exploring FL paradigms and usability in 6G privacy, none of them has clearly addressed how FL can be used to improve the protocol stack and wireless operations. The main goal of this survey is to provide a comprehensive overview on FL usability to enhance mobile services and enable smart ecosystems to support novel use-cases. This paper examines the added-value of implementing FL throughout all levels of the protocol stack. Furthermore, it presents important FL applications, addresses hot topics, provides valuable insights and explicits guidance for future research and developments. Our concluding remarks aim to leverage the synergy between FL and future 6G, while highlighting FL's potential to revolutionize wireless industry and sustain the development of cutting-edge mobile services.",
    "authors": [
      "Maryam Ben Driss",
      "Essaid Sabir",
      "Halima Elbiaze",
      "Walid Saad"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.GT",
      "cs.NI",
      "eess.SP"
    ],
    "published": "2023-12-07T20:39:57Z",
    "pdf_url": "https://arxiv.org/pdf/2312.04688v1"
  },
  {
    "arxiv_id": "2312.04344v2",
    "entry_id": "http://arxiv.org/abs/2312.04344v2",
    "title": "Enhancing Medical Task Performance in GPT-4V: A Comprehensive Study on Prompt Engineering Strategies",
    "summary": "OpenAI's latest large vision-language model (LVLM), GPT-4V(ision), has piqued considerable interest for its potential in medical applications. Despite its promise, recent studies and internal reviews highlight its underperformance in specialized medical tasks. This paper explores the boundary of GPT-4V's capabilities in medicine, particularly in processing complex imaging data from endoscopies, CT scans, and MRIs etc. Leveraging open-source datasets, we assessed its foundational competencies, identifying substantial areas for enhancement. Our research emphasizes prompt engineering, an often-underutilized strategy for improving AI responsiveness. Through iterative testing, we refined the model's prompts, significantly improving its interpretative accuracy and relevance in medical imaging. From our comprehensive evaluations, we distilled 10 effective prompt engineering techniques, each fortifying GPT-4V's medical acumen. These methodical enhancements facilitate more reliable, precise, and clinically valuable insights from GPT-4V, advancing its operability in critical healthcare environments. Our findings are pivotal for those employing AI in medicine, providing clear, actionable guidance on harnessing GPT-4V's full diagnostic potential.",
    "authors": [
      "Pengcheng Chen",
      "Ziyan Huang",
      "Zhongying Deng",
      "Tianbin Li",
      "Yanzhou Su",
      "Haoyu Wang",
      "Jin Ye",
      "Yu Qiao",
      "Junjun He"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-12-07T15:05:59Z",
    "pdf_url": "https://arxiv.org/pdf/2312.04344v2"
  },
  {
    "arxiv_id": "2312.04316v3",
    "entry_id": "http://arxiv.org/abs/2312.04316v3",
    "title": "Towards Knowledge-driven Autonomous Driving",
    "summary": "This paper explores the emerging knowledge-driven autonomous driving technologies. Our investigation highlights the limitations of current autonomous driving systems, in particular their sensitivity to data bias, difficulty in handling long-tail scenarios, and lack of interpretability. Conversely, knowledge-driven methods with the abilities of cognition, generalization and life-long learning emerge as a promising way to overcome these challenges. This paper delves into the essence of knowledge-driven autonomous driving and examines its core components: dataset \\& benchmark, environment, and driver agent. By leveraging large language models, world models, neural rendering, and other advanced artificial intelligence techniques, these components collectively contribute to a more holistic, adaptive, and intelligent autonomous driving system. The paper systematically organizes and reviews previous research efforts in this area, and provides insights and guidance for future research and practical applications of autonomous driving. We will continually share the latest updates on cutting-edge developments in knowledge-driven autonomous driving along with the relevant valuable open-source resources at: \\url{https://github.com/PJLab-ADG/awesome-knowledge-driven-AD}.",
    "authors": [
      "Xin Li",
      "Yeqi Bai",
      "Pinlong Cai",
      "Licheng Wen",
      "Daocheng Fu",
      "Bo Zhang",
      "Xuemeng Yang",
      "Xinyu Cai",
      "Tao Ma",
      "Jianfei Guo",
      "Xing Gao",
      "Min Dou",
      "Yikang Li",
      "Botian Shi",
      "Yong Liu",
      "Liang He",
      "Yu Qiao"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2023-12-07T14:17:17Z",
    "pdf_url": "https://arxiv.org/pdf/2312.04316v3"
  },
  {
    "arxiv_id": "2312.10072v1",
    "entry_id": "http://arxiv.org/abs/2312.10072v1",
    "title": "Assessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk",
    "summary": "Applications of large language models (LLMs) like ChatGPT have potential to enhance clinical decision support through conversational interfaces. However, challenges of human-algorithmic interaction and clinician trust are poorly understood. GutGPT, a LLM for gastrointestinal (GI) bleeding risk prediction and management guidance, was deployed in clinical simulation scenarios alongside the electronic health record (EHR) with emergency medicine physicians, internal medicine physicians, and medical students to evaluate its effect on physician acceptance and trust in AI clinical decision support systems (AI-CDSS). GutGPT provides risk predictions from a validated machine learning model and evidence-based answers by querying extracted clinical guidelines. Participants were randomized to GutGPT and an interactive dashboard, or the interactive dashboard and a search engine. Surveys and educational assessments taken before and after measured technology acceptance and content mastery. Preliminary results showed mixed effects on acceptance after using GutGPT compared to the dashboard or search engine but appeared to improve content mastery based on simulation performance. Overall, this study demonstrates LLMs like GutGPT could enhance effective AI-CDSS if implemented optimally and paired with interactive interfaces.",
    "authors": [
      "Colleen Chan",
      "Kisung You",
      "Sunny Chung",
      "Mauro Giuffrè",
      "Theo Saarinen",
      "Niroop Rajashekar",
      "Yuan Pu",
      "Yeo Eun Shin",
      "Loren Laine",
      "Ambrose Wong",
      "René Kizilcec",
      "Jasjeet Sekhon",
      "Dennis Shung"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.LG",
      "stat.AP"
    ],
    "published": "2023-12-06T23:20:03Z",
    "pdf_url": "https://arxiv.org/pdf/2312.10072v1"
  },
  {
    "arxiv_id": "2312.03863v4",
    "entry_id": "http://arxiv.org/abs/2312.03863v4",
    "title": "Efficient Large Language Models: A Survey",
    "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in important tasks such as natural language understanding and language generation, and thus have the potential to make a substantial impact on our society. Such capabilities, however, come with the considerable resources they demand, highlighting the strong need to develop effective techniques for addressing their efficiency challenges. In this survey, we provide a systematic and comprehensive review of efficient LLMs research. We organize the literature in a taxonomy consisting of three main categories, covering distinct yet interconnected efficient LLMs topics from model-centric, data-centric, and framework-centric perspective, respectively. We have also created a GitHub repository where we organize the papers featured in this survey at https://github.com/AIoT-MLSys-Lab/Efficient-LLMs-Survey. We will actively maintain the repository and incorporate new research as it emerges. We hope our survey can serve as a valuable resource to help researchers and practitioners gain a systematic understanding of efficient LLMs research and inspire them to contribute to this important and exciting field.",
    "authors": [
      "Zhongwei Wan",
      "Xin Wang",
      "Che Liu",
      "Samiul Alam",
      "Yu Zheng",
      "Jiachen Liu",
      "Zhongnan Qu",
      "Shen Yan",
      "Yi Zhu",
      "Quanlu Zhang",
      "Mosharaf Chowdhury",
      "Mi Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-06T19:18:42Z",
    "pdf_url": "https://arxiv.org/pdf/2312.03863v4"
  },
  {
    "arxiv_id": "2312.03769v1",
    "entry_id": "http://arxiv.org/abs/2312.03769v1",
    "title": "GPT vs Human for Scientific Reviews: A Dual Source Review on Applications of ChatGPT in Science",
    "summary": "The new polymath Large Language Models (LLMs) can speed-up greatly scientific reviews, possibly using more unbiased quantitative metrics, facilitating cross-disciplinary connections, and identifying emerging trends and research gaps by analyzing large volumes of data. However, at the present time, they lack the required deep understanding of complex methodologies, they have difficulty in evaluating innovative claims, and they are unable to assess ethical issues and conflicts of interest. Herein, we consider 13 GPT-related papers across different scientific domains, reviewed by a human reviewer and SciSpace, a large language model, with the reviews evaluated by three distinct types of evaluators, namely GPT-3.5, a crowd panel, and GPT-4. We found that 50% of SciSpace's responses to objective questions align with those of a human reviewer, with GPT-4 (informed evaluator) often rating the human reviewer higher in accuracy, and SciSpace higher in structure, clarity, and completeness. In subjective questions, the uninformed evaluators (GPT-3.5 and crowd panel) showed varying preferences between SciSpace and human responses, with the crowd panel showing a preference for the human responses. However, GPT-4 rated them equally in accuracy and structure but favored SciSpace for completeness.",
    "authors": [
      "Chenxi Wu",
      "Alan John Varghese",
      "Vivek Oommen",
      "George Em Karniadakis"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-05T21:41:52Z",
    "pdf_url": "https://arxiv.org/pdf/2312.03769v1"
  },
  {
    "arxiv_id": "2312.02783v4",
    "entry_id": "http://arxiv.org/abs/2312.02783v4",
    "title": "Large Language Models on Graphs: A Comprehensive Survey",
    "summary": "Large language models (LLMs), such as GPT4 and LLaMA, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While LLMs are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data is paired with rich textual information (e.g., molecules with descriptions). Besides, although LLMs have shown their pure text-based reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting LLMs on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing LLMs on graphs, including LLM as Predictor, LLM as Encoder, and LLM as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/PeterGriffinJin/Awesome-Language-Model-on-Graphs.",
    "authors": [
      "Bowen Jin",
      "Gang Liu",
      "Chi Han",
      "Meng Jiang",
      "Heng Ji",
      "Jiawei Han"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-12-05T14:14:27Z",
    "pdf_url": "https://arxiv.org/pdf/2312.02783v4"
  },
  {
    "arxiv_id": "2312.02431v1",
    "entry_id": "http://arxiv.org/abs/2312.02431v1",
    "title": "Visually Grounded Language Learning: a review of language games, datasets, tasks, and models",
    "summary": "In recent years, several machine learning models have been proposed. They are trained with a language modelling objective on large-scale text-only data. With such pretraining, they can achieve impressive results on many Natural Language Understanding and Generation tasks. However, many facets of meaning cannot be learned by ``listening to the radio\" only. In the literature, many Vision+Language (V+L) tasks have been defined with the aim of creating models that can ground symbols in the visual modality. In this work, we provide a systematic literature review of several tasks and models proposed in the V+L field. We rely on Wittgenstein's idea of `language games' to categorise such tasks into 3 different families: 1) discriminative games, 2) generative games, and 3) interactive games. Our analysis of the literature provides evidence that future work should be focusing on interactive games where communication in Natural Language is important to resolve ambiguities about object referents and action plans and that physical embodiment is essential to understand the semantics of situations and events. Overall, these represent key requirements for developing grounded meanings in neural models.",
    "authors": [
      "Alessandro Suglia",
      "Ioannis Konstas",
      "Oliver Lemon"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-05T02:17:29Z",
    "pdf_url": "https://arxiv.org/pdf/2312.02431v1"
  },
  {
    "arxiv_id": "2312.03014v1",
    "entry_id": "http://arxiv.org/abs/2312.03014v1",
    "title": "Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey",
    "summary": "As artificial intelligence (AI) continues to rapidly evolve, the realm of Earth and atmospheric sciences is increasingly adopting data-driven models, powered by progressive developments in deep learning (DL). Specifically, DL techniques are extensively utilized to decode the chaotic and nonlinear aspects of Earth systems, and to address climate challenges via understanding weather and climate data. Cutting-edge performance on specific tasks within narrower spatio-temporal scales has been achieved recently through DL. The rise of large models, specifically large language models (LLMs), has enabled fine-tuning processes that yield remarkable outcomes across various downstream tasks, thereby propelling the advancement of general AI. However, we are still navigating the initial stages of crafting general AI for weather and climate. In this survey, we offer an exhaustive, timely overview of state-of-the-art AI methodologies specifically engineered for weather and climate data, with a special focus on time series and text data. Our primary coverage encompasses four critical aspects: types of weather and climate data, principal model architectures, model scopes and applications, and datasets for weather and climate. Furthermore, in relation to the creation and application of foundation models for weather and climate data understanding, we delve into the field's prevailing challenges, offer crucial insights, and propose detailed avenues for future research. This comprehensive approach equips practitioners with the requisite knowledge to make substantial progress in this domain. Our survey encapsulates the most recent breakthroughs in research on large, data-driven models for weather and climate data understanding, emphasizing robust foundations, current advancements, practical applications, crucial resources, and prospective research opportunities.",
    "authors": [
      "Shengchao Chen",
      "Guodong Long",
      "Jing Jiang",
      "Dikai Liu",
      "Chengqi Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV",
      "physics.ao-ph"
    ],
    "published": "2023-12-05T01:10:54Z",
    "pdf_url": "https://arxiv.org/pdf/2312.03014v1"
  },
  {
    "arxiv_id": "2312.03755v1",
    "entry_id": "http://arxiv.org/abs/2312.03755v1",
    "title": "Near-real-time Earthquake-induced Fatality Estimation using Crowdsourced Data and Large-Language Models",
    "summary": "When a damaging earthquake occurs, immediate information about casualties is critical for time-sensitive decision-making by emergency response and aid agencies in the first hours and days. Systems such as Prompt Assessment of Global Earthquakes for Response (PAGER) by the U.S. Geological Survey (USGS) were developed to provide a forecast within about 30 minutes of any significant earthquake globally. Traditional systems for estimating human loss in disasters often depend on manually collected early casualty reports from global media, a process that's labor-intensive and slow with notable time delays. Recently, some systems have employed keyword matching and topic modeling to extract relevant information from social media. However, these methods struggle with the complex semantics in multilingual texts and the challenge of interpreting ever-changing, often conflicting reports of death and injury numbers from various unverified sources on social media platforms. In this work, we introduce an end-to-end framework to significantly improve the timeliness and accuracy of global earthquake-induced human loss forecasting using multi-lingual, crowdsourced social media. Our framework integrates (1) a hierarchical casualty extraction model built upon large language models, prompt design, and few-shot learning to retrieve quantitative human loss claims from social media, (2) a physical constraint-aware, dynamic-truth discovery model that discovers the truthful human loss from massive noisy and potentially conflicting human loss claims, and (3) a Bayesian updating loss projection model that dynamically updates the final loss estimation using discovered truths. We test the framework in real-time on a series of global earthquake events in 2021 and 2022 and show that our framework streamlines casualty data retrieval, achieving speed and accuracy comparable to manual methods by USGS.",
    "authors": [
      "Chenguang Wang",
      "Davis Engler",
      "Xuechun Li",
      "James Hou",
      "David J. Wald",
      "Kishor Jaiswal",
      "Susu Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2023-12-04T17:09:58Z",
    "pdf_url": "https://arxiv.org/pdf/2312.03755v1"
  },
  {
    "arxiv_id": "2312.02003v3",
    "entry_id": "http://arxiv.org/abs/2312.02003v3",
    "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly",
    "summary": "Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into \"The Good\" (beneficial LLM applications), \"The Bad\" (offensive applications), and \"The Ugly\" (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.",
    "authors": [
      "Yifan Yao",
      "Jinhao Duan",
      "Kaidi Xu",
      "Yuanfang Cai",
      "Zhibo Sun",
      "Yue Zhang"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2023-12-04T16:25:18Z",
    "pdf_url": "https://arxiv.org/pdf/2312.02003v3"
  },
  {
    "arxiv_id": "2312.02248v2",
    "entry_id": "http://arxiv.org/abs/2312.02248v2",
    "title": "Towards early diagnosis of Alzheimer's disease: Advances in immune-related blood biomarkers and computational modeling approaches",
    "summary": "Alzheimer's disease has an increasing prevalence in the population world-wide, yet current diagnostic methods based on recommended biomarkers are only available in specialized clinics. Due to these circumstances, Alzheimer's disease is usually diagnosed late, which contrasts with the currently available treatment options that are only effective for patients at an early stage. Blood-based biomarkers could fill in the gap of easily accessible and low-cost methods for early diagnosis of the disease. In particular, immune-based blood-biomarkers might be a promising option, given the recently discovered cross-talk of immune cells of the central nervous system with those in the peripheral immune system. With the help of machine learning algorithms and mechanistic modeling approaches, such as agent-based modeling, an in-depth analysis of the simulation of cell dynamics is possible as well as of high-dimensional omics resources indicative of pathway signaling changes. Here, we give a background on advances in research on brain-immune system cross-talk in Alzheimer's disease and review recent machine learning and mechanistic modeling approaches which leverage modern omics technologies for blood-based immune system-related biomarker discovery.",
    "authors": [
      "Sophia Krix",
      "Ella Wilczynski",
      "Neus Falgàs",
      "Raquel Sánchez-Valle",
      "Eti Yoles",
      "Uri Nevo",
      "Kuti Baruch",
      "Holger Fröhlich"
    ],
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "published": "2023-12-04T16:05:45Z",
    "pdf_url": "https://arxiv.org/pdf/2312.02248v2"
  },
  {
    "arxiv_id": "2312.01700v3",
    "entry_id": "http://arxiv.org/abs/2312.01700v3",
    "title": "Data Management For Training Large Language Models: A Survey",
    "summary": "Data plays a fundamental role in training Large Language Models (LLMs). Efficient data management, particularly in formulating a well-suited training dataset, is significant for enhancing model performance and improving training efficiency during pretraining and supervised fine-tuning stages. Despite the considerable importance of data management, the underlying mechanism of current prominent practices are still unknown. Consequently, the exploration of data management has attracted more and more attention among the research community. This survey aims to provide a comprehensive overview of current research in data management within both the pretraining and supervised fine-tuning stages of LLMs, covering various aspects of data management strategy design. Looking into the future, we extrapolate existing challenges and outline promising directions for development in this field. Therefore, this survey serves as a guiding resource for practitioners aspiring to construct powerful LLMs through efficient data management practices. The collection of the latest papers is available at https://github.com/ZigeW/data_management_LLM.",
    "authors": [
      "Zige Wang",
      "Wanjun Zhong",
      "Yufei Wang",
      "Qi Zhu",
      "Fei Mi",
      "Baojun Wang",
      "Lifeng Shang",
      "Xin Jiang",
      "Qun Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-12-04T07:42:16Z",
    "pdf_url": "https://arxiv.org/pdf/2312.01700v3"
  },
  {
    "arxiv_id": "2312.01509v1",
    "entry_id": "http://arxiv.org/abs/2312.01509v1",
    "title": "Tackling Bias in Pre-trained Language Models: Current Trends and Under-represented Societies",
    "summary": "The benefits and capabilities of pre-trained language models (LLMs) in current and future innovations are vital to any society. However, introducing and using LLMs comes with biases and discrimination, resulting in concerns about equality, diversity and fairness, and must be addressed. While understanding and acknowledging bias in LLMs and developing mitigation strategies are crucial, the generalised assumptions towards societal needs can result in disadvantages towards under-represented societies and indigenous populations. Furthermore, the ongoing changes to actual and proposed amendments to regulations and laws worldwide also impact research capabilities in tackling the bias problem. This research presents a comprehensive survey synthesising the current trends and limitations in techniques used for identifying and mitigating bias in LLMs, where the overview of methods for tackling bias are grouped into metrics, benchmark datasets, and mitigation strategies. The importance and novelty of this survey are that it explores the perspective of under-represented societies. We argue that current practices tackling the bias problem cannot simply be 'plugged in' to address the needs of under-represented societies. We use examples from New Zealand to present requirements for adopting existing techniques to under-represented societies.",
    "authors": [
      "Vithya Yogarajan",
      "Gillian Dobbie",
      "Te Taka Keegan",
      "Rostam J. Neuwirth"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-12-03T21:25:10Z",
    "pdf_url": "https://arxiv.org/pdf/2312.01509v1"
  },
  {
    "arxiv_id": "2312.01202v1",
    "entry_id": "http://arxiv.org/abs/2312.01202v1",
    "title": "From Voices to Validity: Leveraging Large Language Models (LLMs) for Textual Analysis of Policy Stakeholder Interviews",
    "summary": "Obtaining stakeholders' diverse experiences and opinions about current policy in a timely manner is crucial for policymakers to identify strengths and gaps in resource allocation, thereby supporting effective policy design and implementation. However, manually coding even moderately sized interview texts or open-ended survey responses from stakeholders can often be labor-intensive and time-consuming. This study explores the integration of Large Language Models (LLMs)--like GPT-4--with human expertise to enhance text analysis of stakeholder interviews regarding K-12 education policy within one U.S. state. Employing a mixed-methods approach, human experts developed a codebook and coding processes as informed by domain knowledge and unsupervised topic modeling results. They then designed prompts to guide GPT-4 analysis and iteratively evaluate different prompts' performances. This combined human-computer method enabled nuanced thematic and sentiment analysis. Results reveal that while GPT-4 thematic coding aligned with human coding by 77.89% at specific themes, expanding to broader themes increased congruence to 96.02%, surpassing traditional Natural Language Processing (NLP) methods by over 25%. Additionally, GPT-4 is more closely matched to expert sentiment analysis than lexicon-based methods. Findings from quantitative measures and qualitative reviews underscore the complementary roles of human domain expertise and automated analysis as LLMs offer new perspectives and coding consistency. The human-computer interactive approach enhances efficiency, validity, and interpretability of educational policy research.",
    "authors": [
      "Alex Liu",
      "Min Sun"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-12-02T18:55:14Z",
    "pdf_url": "https://arxiv.org/pdf/2312.01202v1"
  },
  {
    "arxiv_id": "2312.01072v2",
    "entry_id": "http://arxiv.org/abs/2312.01072v2",
    "title": "A Survey of Temporal Credit Assignment in Deep Reinforcement Learning",
    "summary": "The Credit Assignment Problem (CAP) refers to the longstanding challenge of Reinforcement Learning (RL) agents to associate actions with their long-term consequences. Solving the CAP is a crucial step towards the successful deployment of RL in the real world since most decision problems provide feedback that is noisy, delayed, and with little or no information about the causes. These conditions make it hard to distinguish serendipitous outcomes from those caused by informed decision-making. However, the mathematical nature of credit and the CAP remains poorly understood and defined. In this survey, we review the state of the art of Temporal Credit Assignment (CA) in deep RL. We propose a unifying formalism for credit that enables equitable comparisons of state-of-the-art algorithms and improves our understanding of the trade-offs between the various methods. We cast the CAP as the problem of learning the influence of an action over an outcome from a finite amount of experience. We discuss the challenges posed by delayed effects, transpositions, and a lack of action influence, and analyse how existing methods aim to address them. Finally, we survey the protocols to evaluate a credit assignment method and suggest ways to diagnose the sources of struggle for different methods. Overall, this survey provides an overview of the field for new-entry practitioners and researchers, it offers a coherent perspective for scholars looking to expedite the starting stages of a new study on the CAP, and it suggests potential directions for future research.",
    "authors": [
      "Eduardo Pignatelli",
      "Johan Ferret",
      "Matthieu Geist",
      "Thomas Mesnard",
      "Hado van Hasselt",
      "Olivier Pietquin",
      "Laura Toni"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-12-02T08:49:51Z",
    "pdf_url": "https://arxiv.org/pdf/2312.01072v2"
  },
  {
    "arxiv_id": "2312.01058v1",
    "entry_id": "http://arxiv.org/abs/2312.01058v1",
    "title": "A Survey of Progress on Cooperative Multi-agent Reinforcement Learning in Open Environment",
    "summary": "Multi-agent Reinforcement Learning (MARL) has gained wide attention in recent years and has made progress in various fields. Specifically, cooperative MARL focuses on training a team of agents to cooperatively achieve tasks that are difficult for a single agent to handle. It has shown great potential in applications such as path planning, autonomous driving, active voltage control, and dynamic algorithm configuration. One of the research focuses in the field of cooperative MARL is how to improve the coordination efficiency of the system, while research work has mainly been conducted in simple, static, and closed environment settings. To promote the application of artificial intelligence in real-world, some research has begun to explore multi-agent coordination in open environments. These works have made progress in exploring and researching the environments where important factors might change. However, the mainstream work still lacks a comprehensive review of the research direction. In this paper, starting from the concept of reinforcement learning, we subsequently introduce multi-agent systems (MAS), cooperative MARL, typical methods, and test environments. Then, we summarize the research work of cooperative MARL from closed to open environments, extract multiple research directions, and introduce typical works. Finally, we summarize the strengths and weaknesses of the current research, and look forward to the future development direction and research problems in cooperative MARL in open environments.",
    "authors": [
      "Lei Yuan",
      "Ziqian Zhang",
      "Lihe Li",
      "Cong Guan",
      "Yang Yu"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2023-12-02T08:04:31Z",
    "pdf_url": "https://arxiv.org/pdf/2312.01058v1"
  },
  {
    "arxiv_id": "2312.04578v1",
    "entry_id": "http://arxiv.org/abs/2312.04578v1",
    "title": "Towards a Psychological Generalist AI: A Survey of Current Applications of Large Language Models and Future Prospects",
    "summary": "The complexity of psychological principles underscore a significant societal challenge, given the vast social implications of psychological problems. Bridging the gap between understanding these principles and their actual clinical and real-world applications demands rigorous exploration and adept implementation. In recent times, the swift advancement of highly adaptive and reusable artificial intelligence (AI) models has emerged as a promising way to unlock unprecedented capabilities in the realm of psychology. This paper emphasizes the importance of performance validation for these large-scale AI models, emphasizing the need to offer a comprehensive assessment of their verification from diverse perspectives. Moreover, we review the cutting-edge advancements and practical implementations of these expansive models in psychology, highlighting pivotal work spanning areas such as social media analytics, clinical nursing insights, vigilant community monitoring, and the nuanced exploration of psychological theories. Based on our review, we project an acceleration in the progress of psychological fields, driven by these large-scale AI models. These future generalist AI models harbor the potential to substantially curtail labor costs and alleviate social stress. However, this forward momentum will not be without its set of challenges, especially when considering the paradigm changes and upgrades required for medical instrumentation and related applications.",
    "authors": [
      "Tianyu He",
      "Guanghui Fu",
      "Yijing Yu",
      "Fan Wang",
      "Jianqiang Li",
      "Qing Zhao",
      "Changwei Song",
      "Hongzhi Qi",
      "Dan Luo",
      "Huijing Zou",
      "Bing Xiang Yang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-12-01T08:35:18Z",
    "pdf_url": "https://arxiv.org/pdf/2312.04578v1"
  },
  {
    "arxiv_id": "2311.18044v3",
    "entry_id": "http://arxiv.org/abs/2311.18044v3",
    "title": "Transfer Learning in Robotics: An Upcoming Breakthrough? A Review of Promises and Challenges",
    "summary": "Transfer learning is a conceptually-enticing paradigm in pursuit of truly intelligent embodied agents. The core concept -- reusing prior knowledge to learn in and from novel situations -- is successfully leveraged by humans to handle novel situations. In recent years, transfer learning has received renewed interest from the community from different perspectives, including imitation learning, domain adaptation, and transfer of experience from simulation to the real world, among others. In this paper, we unify the concept of transfer learning in robotics and provide the first taxonomy of its kind considering the key concepts of robot, task, and environment. Through a review of the promises and challenges in the field, we identify the need of transferring at different abstraction levels, the need of quantifying the transfer gap and the quality of transfer, as well as the dangers of negative transfer. Via this position paper, we hope to channel the effort of the community towards the most significant roadblocks to realize the full potential of transfer learning in robotics.",
    "authors": [
      "Noémie Jaquier",
      "Michael C. Welle",
      "Andrej Gams",
      "Kunpeng Yao",
      "Bernardo Fichera",
      "Aude Billard",
      "Aleš Ude",
      "Tamim Asfour",
      "Danica Kragic"
    ],
    "categories": [
      "cs.RO",
      "cs.LG"
    ],
    "published": "2023-11-29T19:40:10Z",
    "pdf_url": "https://arxiv.org/pdf/2311.18044v3"
  },
  {
    "arxiv_id": "2311.17165v4",
    "entry_id": "http://arxiv.org/abs/2311.17165v4",
    "title": "(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions",
    "summary": "The concept of rationality is central to the field of artificial intelligence (AI). Whether we are seeking to simulate human reasoning, or trying to achieve bounded optimality, our goal is generally to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in AI, and sets out the open questions in this area. We consider how the understanding of rationality in other fields has influenced its conception within AI, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we examine irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area remains limited. Methods that have up to now been developed for other purposes, namely adversarial scenarios, may be adapted to suit interactions with artificial agents. We further discuss the interplay between human and artificial agents, and the role that rationality plays within this interaction; many questions remain in this area, relating to potentially irrational behaviour of both humans and artificial agents.",
    "authors": [
      "Olivia Macmillan-Scott",
      "Mirco Musolesi"
    ],
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG",
      "cs.MA"
    ],
    "published": "2023-11-28T19:01:09Z",
    "pdf_url": "https://arxiv.org/pdf/2311.17165v4"
  },
  {
    "arxiv_id": "2312.03740v2",
    "entry_id": "http://arxiv.org/abs/2312.03740v2",
    "title": "A Survey on Prompting Techniques in LLMs",
    "summary": "Autoregressive Large Language Models have transformed the landscape of Natural Language Processing. Pre-train and prompt paradigm has replaced the conventional approach of pre-training and fine-tuning for many downstream NLP tasks. This shift has been possible largely due to LLMs and innovative prompting techniques. LLMs have shown great promise for a variety of downstream tasks owing to their vast parameters and huge datasets that they are pre-trained on. However, in order to fully realize their potential, their outputs must be guided towards the desired outcomes. Prompting, in which a specific input or instruction is provided to guide the LLMs toward the intended output, has become a tool for achieving this goal. In this paper, we discuss the various prompting techniques that have been applied to fully harness the power of LLMs. We present a taxonomy of existing literature on prompting techniques and provide a concise survey based on this taxonomy. Further, we identify some open problems in the realm of prompting in autoregressive LLMs which could serve as a direction for future research.",
    "authors": [
      "Prabin Bhandari"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-28T17:56:34Z",
    "pdf_url": "https://arxiv.org/pdf/2312.03740v2"
  },
  {
    "arxiv_id": "2311.16789v2",
    "entry_id": "http://arxiv.org/abs/2311.16789v2",
    "title": "A Survey of the Evolution of Language Model-Based Dialogue Systems: Data, Task and Models",
    "summary": "Dialogue systems (DS), including the task-oriented dialogue system (TOD) and the open-domain dialogue system (ODD), have always been a fundamental task in natural language processing (NLP), allowing various applications in practice. Owing to sophisticated training and well-designed model architecture, language models (LM) are usually adopted as the necessary backbone to build the dialogue system. Consequently, every breakthrough in LM brings about a shift in learning paradigm and research attention within dialogue system, especially the appearance of pre-trained language models (PLMs) and large language models (LLMs). In this paper, we take a deep look at the history of the dialogue system, especially its special relationship with the advancements of language models. Thus, our survey offers a systematic perspective, categorizing different stages in a chronological order aligned with LM breakthroughs, providing a comprehensive review of state-of-the-art research outcomes. What's more, we turn our attention to emerging topics and engage in a discussion on open challenges, providing valuable insights into the future directions for LLM-based dialogue systems. In summary, this survey delves into the dynamic interplay between language models and dialogue systems, unraveling the evolutionary path of this essential relationship. Through this exploration, we pave the way for a deeper comprehension of the field, guiding future developments in LM-based dialogue systems.",
    "authors": [
      "Hongru Wang",
      "Lingzhi Wang",
      "Yiming Du",
      "Liang Chen",
      "Jingyan Zhou",
      "Yufei Wang",
      "Kam-Fai Wong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-28T13:51:32Z",
    "pdf_url": "https://arxiv.org/pdf/2311.16789v2"
  },
  {
    "arxiv_id": "2311.16673v1",
    "entry_id": "http://arxiv.org/abs/2311.16673v1",
    "title": "Large Language Models Meet Computer Vision: A Brief Survey",
    "summary": "Recently, the intersection of Large Language Models (LLMs) and Computer Vision (CV) has emerged as a pivotal area of research, driving significant advancements in the field of Artificial Intelligence (AI). As transformers have become the backbone of many state-of-the-art models in both Natural Language Processing (NLP) and CV, understanding their evolution and potential enhancements is crucial. This survey paper delves into the latest progressions in the domain of transformers and their subsequent successors, emphasizing their potential to revolutionize Vision Transformers (ViTs) and LLMs. This survey also presents a comparative analysis, juxtaposing the performance metrics of several leading paid and open-source LLMs, shedding light on their strengths and areas of improvement as well as a literature review on how LLMs are being used to tackle vision related tasks. Furthermore, the survey presents a comprehensive collection of datasets employed to train LLMs, offering insights into the diverse data available to achieve high performance in various pre-training and downstream tasks of LLMs. The survey is concluded by highlighting open directions in the field, suggesting potential venues for future research and development. This survey aims to underscores the profound intersection of LLMs on CV, leading to a new era of integrated and advanced AI models.",
    "authors": [
      "Raby Hamadi"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-11-28T10:39:19Z",
    "pdf_url": "https://arxiv.org/pdf/2311.16673v1"
  },
  {
    "arxiv_id": "2312.03718v1",
    "entry_id": "http://arxiv.org/abs/2312.03718v1",
    "title": "Large Language Models in Law: A Survey",
    "summary": "The advent of artificial intelligence (AI) has significantly impacted the traditional judicial industry. Moreover, recently, with the development of AI-generated content (AIGC), AI and law have found applications in various domains, including image recognition, automatic text generation, and interactive chat. With the rapid emergence and growing popularity of large models, it is evident that AI will drive transformation in the traditional judicial industry. However, the application of legal large language models (LLMs) is still in its nascent stage. Several challenges need to be addressed. In this paper, we aim to provide a comprehensive survey of legal LLMs. We not only conduct an extensive survey of LLMs, but also expose their applications in the judicial system. We first provide an overview of AI technologies in the legal field and showcase the recent research in LLMs. Then, we discuss the practical implementation presented by legal LLMs, such as providing legal advice to users and assisting judges during trials. In addition, we explore the limitations of legal LLMs, including data, algorithms, and judicial practice. Finally, we summarize practical recommendations and propose future development directions to address these challenges.",
    "authors": [
      "Jinqi Lai",
      "Wensheng Gan",
      "Jiayang Wu",
      "Zhenlian Qi",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-26T00:48:12Z",
    "pdf_url": "https://arxiv.org/pdf/2312.03718v1"
  },
  {
    "arxiv_id": "2311.14619v1",
    "entry_id": "http://arxiv.org/abs/2311.14619v1",
    "title": "Eliciting Honest Information From Authors Using Sequential Review",
    "summary": "In the setting of conference peer review, the conference aims to accept high-quality papers and reject low-quality papers based on noisy review scores. A recent work proposes the isotonic mechanism, which can elicit the ranking of paper qualities from an author with multiple submissions to help improve the conference's decisions. However, the isotonic mechanism relies on the assumption that the author's utility is both an increasing and a convex function with respect to the review score, which is often violated in peer review settings (e.g.~when authors aim to maximize the number of accepted papers). In this paper, we propose a sequential review mechanism that can truthfully elicit the ranking information from authors while only assuming the agent's utility is increasing with respect to the true quality of her accepted papers. The key idea is to review the papers of an author in a sequence based on the provided ranking and conditioning the review of the next paper on the review scores of the previous papers. Advantages of the sequential review mechanism include 1) eliciting truthful ranking information in a more realistic setting than prior work; 2) improving the quality of accepted papers, reducing the reviewing workload and increasing the average quality of papers being reviewed; 3) incentivizing authors to write fewer papers of higher quality.",
    "authors": [
      "Yichi Zhang",
      "Grant Schoenebeck",
      "Weijie Su"
    ],
    "categories": [
      "cs.GT",
      "cs.AI"
    ],
    "published": "2023-11-24T17:27:39Z",
    "pdf_url": "https://arxiv.org/pdf/2311.14619v1"
  },
  {
    "arxiv_id": "2311.14381v4",
    "entry_id": "http://arxiv.org/abs/2311.14381v4",
    "title": "Potential Societal Biases of ChatGPT in Higher Education: A Scoping Review",
    "summary": "Purpose:Generative Artificial Intelligence (GAI) models, such as ChatGPT, may inherit or amplify societal biases due to their training on extensive datasets. With the increasing usage of GAI by students, faculty, and staff in higher education institutions (HEIs), it is urgent to examine the ethical issues and potential biases associated with these technologies. Design/Approach/Methods:This scoping review aims to elucidate how biases related to GAI in HEIs have been researched and discussed in recent academic publications. We categorized the potential societal biases that GAI might cause in the field of higher education. Our review includes articles written in English, Chinese, and Japanese across four main databases, focusing on GAI usage in higher education and bias. Findings:Our findings reveal that while there is meaningful scholarly discussion around bias and discrimination concerning LLMs in the AI field, most articles addressing higher education approach the issue superficially. Few articles identify specific types of bias under different circumstances, and there is a notable lack of empirical research. Most papers in our review focus primarily on educational and research fields related to medicine and engineering, with some addressing English education. However, there is almost no discussion regarding the humanities and social sciences. Additionally, a significant portion of the current discourse is in English and primarily addresses English-speaking contexts. Originality/Value:To the best of our knowledge, our study is the first to summarize the potential societal biases in higher education. This review highlights the need for more in-depth studies and empirical work to understand the specific biases that GAI might introduce or amplify in educational settings, guiding the development of more ethical AI applications in higher education.",
    "authors": [
      "Ming Li",
      "Ariunaa Enkhtur",
      "Beverley Anne Yamamoto",
      "Fei Cheng",
      "Lilan Chen"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-11-24T10:00:23Z",
    "pdf_url": "https://arxiv.org/pdf/2311.14381v4"
  },
  {
    "arxiv_id": "2311.14096v2",
    "entry_id": "http://arxiv.org/abs/2311.14096v2",
    "title": "Cultural Bias and Cultural Alignment of Large Language Models",
    "summary": "Culture fundamentally shapes people's reasoning, behavior, and communication. As people increasingly use generative artificial intelligence (AI) to expedite and automate personal and professional tasks, cultural values embedded in AI models may bias people's authentic expression and contribute to the dominance of certain cultures. We conduct a disaggregated evaluation of cultural bias for five widely used large language models (OpenAI's GPT-4o/4-turbo/4/3.5-turbo/3) by comparing the models' responses to nationally representative survey data. All models exhibit cultural values resembling English-speaking and Protestant European countries. We test cultural prompting as a control strategy to increase cultural alignment for each country/territory. For recent models (GPT-4, 4-turbo, 4o), this improves the cultural alignment of the models' output for 71-81% of countries and territories. We suggest using cultural prompting and ongoing evaluation to reduce cultural bias in the output of generative AI.",
    "authors": [
      "Yan Tao",
      "Olga Viberg",
      "Ryan S. Baker",
      "Rene F. Kizilcec"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-23T16:45:56Z",
    "pdf_url": "https://arxiv.org/pdf/2311.14096v2"
  },
  {
    "arxiv_id": "2311.13714v1",
    "entry_id": "http://arxiv.org/abs/2311.13714v1",
    "title": "Learning Safe Control for Multi-Robot Systems: Methods, Verification, and Open Challenges",
    "summary": "In this survey, we review the recent advances in control design methods for robotic multi-agent systems (MAS), focussing on learning-based methods with safety considerations. We start by reviewing various notions of safety and liveness properties, and modeling frameworks used for problem formulation of MAS. Then we provide a comprehensive review of learning-based methods for safe control design for multi-robot systems. We start with various types of shielding-based methods, such as safety certificates, predictive filters, and reachability tools. Then, we review the current state of control barrier certificate learning in both a centralized and distributed manner, followed by a comprehensive review of multi-agent reinforcement learning with a particular focus on safety. Next, we discuss the state-of-the-art verification tools for the correctness of learning-based methods. Based on the capabilities and the limitations of the state of the art methods in learning and verification for MAS, we identify various broad themes for open challenges: how to design methods that can achieve good performance along with safety guarantees; how to decompose single-agent based centralized methods for MAS; how to account for communication-related practical issues; and how to assess transfer of theoretical guarantees to practice.",
    "authors": [
      "Kunal Garg",
      "Songyuan Zhang",
      "Oswin So",
      "Charles Dawson",
      "Chuchu Fan"
    ],
    "categories": [
      "cs.RO",
      "cs.MA",
      "eess.SY",
      "math.OC"
    ],
    "published": "2023-11-22T22:19:44Z",
    "pdf_url": "https://arxiv.org/pdf/2311.13714v1"
  },
  {
    "arxiv_id": "2311.13668v3",
    "entry_id": "http://arxiv.org/abs/2311.13668v3",
    "title": "MAIRA-1: A specialised large multimodal model for radiology report generation",
    "summary": "We present a radiology-specific multimodal model for the task for generating radiological reports from chest X-rays (CXRs). Our work builds on the idea that large language model(s) can be equipped with multimodal capabilities through alignment with pre-trained vision encoders. On natural images, this has been shown to allow multimodal models to gain image understanding and description capabilities. Our proposed model (MAIRA-1) leverages a CXR-specific image encoder in conjunction with a fine-tuned large language model based on Vicuna-7B, and text-based data augmentation, to produce reports with state-of-the-art quality. In particular, MAIRA-1 significantly improves on the radiologist-aligned RadCliQ metric and across all lexical metrics considered. Manual review of model outputs demonstrates promising fluency and accuracy of generated reports while uncovering failure modes not captured by existing evaluation practices. More information and resources can be found on the project website: https://aka.ms/maira.",
    "authors": [
      "Stephanie L. Hyland",
      "Shruthi Bannur",
      "Kenza Bouzid",
      "Daniel C. Castro",
      "Mercy Ranjit",
      "Anton Schwaighofer",
      "Fernando Pérez-García",
      "Valentina Salvatelli",
      "Shaury Srivastav",
      "Anja Thieme",
      "Noel Codella",
      "Matthew P. Lungren",
      "Maria Teodora Wetscherek",
      "Ozan Oktay",
      "Javier Alvarez-Valle"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2023-11-22T19:45:40Z",
    "pdf_url": "https://arxiv.org/pdf/2311.13668v3"
  },
  {
    "arxiv_id": "2311.13587v1",
    "entry_id": "http://arxiv.org/abs/2311.13587v1",
    "title": "A Survey of Serverless Machine Learning Model Inference",
    "summary": "Recent developments in Generative AI, Computer Vision, and Natural Language Processing have led to an increased integration of AI models into various products. This widespread adoption of AI requires significant efforts in deploying these models in production environments. When hosting machine learning models for real-time predictions, it is important to meet defined Service Level Objectives (SLOs), ensuring reliability, minimal downtime, and optimizing operational costs of the underlying infrastructure. Large machine learning models often demand GPU resources for efficient inference to meet SLOs. In the context of these trends, there is growing interest in hosting AI models in a serverless architecture while still providing GPU access for inference tasks. This survey aims to summarize and categorize the emerging challenges and optimization opportunities for large-scale deep learning serving systems. By providing a novel taxonomy and summarizing recent trends, we hope that this survey could shed light on new optimization perspectives and motivate novel works in large-scale deep learning serving systems.",
    "authors": [
      "Kamil Kojs"
    ],
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2023-11-22T18:46:05Z",
    "pdf_url": "https://arxiv.org/pdf/2311.13587v1"
  },
  {
    "arxiv_id": "2311.13165v1",
    "entry_id": "http://arxiv.org/abs/2311.13165v1",
    "title": "Multimodal Large Language Models: A Survey",
    "summary": "The exploration of multimodal language models integrates multiple data types, such as images, text, language, audio, and other heterogeneity. While the latest large language models excel in text-based tasks, they often struggle to understand and process other data types. Multimodal models address this limitation by combining various modalities, enabling a more comprehensive understanding of diverse data. This paper begins by defining the concept of multimodal and examining the historical development of multimodal algorithms. Furthermore, we introduce a range of multimodal products, focusing on the efforts of major technology companies. A practical guide is provided, offering insights into the technical aspects of multimodal models. Moreover, we present a compilation of the latest algorithms and commonly used datasets, providing researchers with valuable resources for experimentation and evaluation. Lastly, we explore the applications of multimodal models and discuss the challenges associated with their development. By addressing these aspects, this paper aims to facilitate a deeper understanding of multimodal models and their potential in various domains.",
    "authors": [
      "Jiayang Wu",
      "Wensheng Gan",
      "Zefeng Chen",
      "Shicheng Wan",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-11-22T05:15:12Z",
    "pdf_url": "https://arxiv.org/pdf/2311.13165v1"
  },
  {
    "arxiv_id": "2311.13160v1",
    "entry_id": "http://arxiv.org/abs/2311.13160v1",
    "title": "Large Language Models in Education: Vision and Opportunities",
    "summary": "With the rapid development of artificial intelligence technology, large language models (LLMs) have become a hot research topic. Education plays an important role in human social development and progress. Traditional education faces challenges such as individual student differences, insufficient allocation of teaching resources, and assessment of teaching effectiveness. Therefore, the applications of LLMs in the field of digital/smart education have broad prospects. The research on educational large models (EduLLMs) is constantly evolving, providing new methods and approaches to achieve personalized learning, intelligent tutoring, and educational assessment goals, thereby improving the quality of education and the learning experience. This article aims to investigate and summarize the application of LLMs in smart education. It first introduces the research background and motivation of LLMs and explains the essence of LLMs. It then discusses the relationship between digital education and EduLLMs and summarizes the current research status of educational large models. The main contributions are the systematic summary and vision of the research background, motivation, and application of large models for education (LLM4Edu). By reviewing existing research, this article provides guidance and insights for educators, researchers, and policy-makers to gain a deep understanding of the potential and challenges of LLM4Edu. It further provides guidance for further advancing the development and application of LLM4Edu, while still facing technical, ethical, and practical challenges requiring further research and exploration.",
    "authors": [
      "Wensheng Gan",
      "Zhenlian Qi",
      "Jiayang Wu",
      "Jerry Chun-Wei Lin"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-11-22T05:04:20Z",
    "pdf_url": "https://arxiv.org/pdf/2311.13160v1"
  },
  {
    "arxiv_id": "2311.12399v4",
    "entry_id": "http://arxiv.org/abs/2311.12399v4",
    "title": "A Survey of Graph Meets Large Language Model: Progress and Future Directions",
    "summary": "Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models (LLMs), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks (GNNs) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate LLMs with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by LLMs in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/yhLeeee/Awesome-LLMs-in-Graph-tasks.",
    "authors": [
      "Yuhan Li",
      "Zhixun Li",
      "Peisong Wang",
      "Jia Li",
      "Xiangguo Sun",
      "Hong Cheng",
      "Jeffrey Xu Yu"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.SI"
    ],
    "published": "2023-11-21T07:22:48Z",
    "pdf_url": "https://arxiv.org/pdf/2311.12399v4"
  },
  {
    "arxiv_id": "2311.12351v2",
    "entry_id": "http://arxiv.org/abs/2311.12351v2",
    "title": "Advancing Transformer Architecture in Long-Context Large Language Models: A Comprehensive Survey",
    "summary": "Transformer-based Large Language Models (LLMs) have been applied in diverse areas such as knowledge bases, human interfaces, and dynamic agents, and marking a stride towards achieving Artificial General Intelligence (AGI). However, current LLMs are predominantly pretrained on short text snippets, which compromises their effectiveness in processing the long-context prompts that are frequently encountered in practical scenarios. This article offers a comprehensive survey of the recent advancement in Transformer-based LLM architectures aimed at enhancing the long-context capabilities of LLMs throughout the entire model lifecycle, from pre-training through to inference. We first delineate and analyze the problems of handling long-context input and output with the current Transformer-based models. We then provide a taxonomy and the landscape of upgrades on Transformer architecture to solve these problems. Afterwards, we provide an investigation on wildly used evaluation necessities tailored for long-context LLMs, including datasets, metrics, and baseline models, as well as optimization toolkits such as libraries, frameworks, and compilers to boost the efficacy of LLMs across different stages in runtime. Finally, we discuss the challenges and potential avenues for future research. A curated repository of relevant literature, continuously updated, is available at https://github.com/Strivin0311/long-llms-learning.",
    "authors": [
      "Yunpeng Huang",
      "Jingwei Xu",
      "Junyu Lai",
      "Zixu Jiang",
      "Taolue Chen",
      "Zenan Li",
      "Yuan Yao",
      "Xiaoxing Ma",
      "Lijuan Yang",
      "Hao Chen",
      "Shupeng Li",
      "Penghao Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-11-21T04:59:17Z",
    "pdf_url": "https://arxiv.org/pdf/2311.12351v2"
  },
  {
    "arxiv_id": "2311.12338v1",
    "entry_id": "http://arxiv.org/abs/2311.12338v1",
    "title": "A Survey on Large Language Models for Personalized and Explainable Recommendations",
    "summary": "In recent years, Recommender Systems(RS) have witnessed a transformative shift with the advent of Large Language Models(LLMs) in the field of Natural Language Processing(NLP). These models such as OpenAI's GPT-3.5/4, Llama from Meta, have demonstrated unprecedented capabilities in understanding and generating human-like text. This has led to a paradigm shift in the realm of personalized and explainable recommendations, as LLMs offer a versatile toolset for processing vast amounts of textual data to enhance user experiences. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey aims to analyze how RS can benefit from LLM-based methodologies. Furthermore, we describe major challenges in Personalized Explanation Generating(PEG) tasks, which are cold-start problems, unfairness and bias problems in RS.",
    "authors": [
      "Junyi Chen"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2023-11-21T04:14:09Z",
    "pdf_url": "https://arxiv.org/pdf/2311.12338v1"
  },
  {
    "arxiv_id": "2311.12320v1",
    "entry_id": "http://arxiv.org/abs/2311.12320v1",
    "title": "A Survey on Multimodal Large Language Models for Autonomous Driving",
    "summary": "With the emergence of Large Language Models (LLMs) and Vision Foundation Models (VFMs), multimodal AI systems benefiting from large models have the potential to equally perceive the real world, make decisions, and control tools as humans. In recent months, LLMs have shown widespread attention in autonomous driving and map systems. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors to apply in LLM driving systems. In this paper, we present a systematic investigation in this field. We first introduce the background of Multimodal Large Language Models (MLLMs), the multimodal models development using LLMs, and the history of autonomous driving. Then, we overview existing MLLM tools for driving, transportation, and map systems together with existing datasets and benchmarks. Moreover, we summarized the works in The 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD), which is the first workshop of its kind regarding LLMs in autonomous driving. To further promote the development of this field, we also discuss several important problems regarding using MLLMs in autonomous driving systems that need to be solved by both academia and industry.",
    "authors": [
      "Can Cui",
      "Yunsheng Ma",
      "Xu Cao",
      "Wenqian Ye",
      "Yang Zhou",
      "Kaizhao Liang",
      "Jintai Chen",
      "Juanwu Lu",
      "Zichong Yang",
      "Kuei-Da Liao",
      "Tianren Gao",
      "Erlong Li",
      "Kun Tang",
      "Zhipeng Cao",
      "Tong Zhou",
      "Ao Liu",
      "Xinrui Yan",
      "Shuqi Mei",
      "Jianguo Cao",
      "Ziran Wang",
      "Chao Zheng"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-11-21T03:32:01Z",
    "pdf_url": "https://arxiv.org/pdf/2311.12320v1"
  },
  {
    "arxiv_id": "2311.11944v1",
    "entry_id": "http://arxiv.org/abs/2311.11944v1",
    "title": "FinanceBench: A New Benchmark for Financial Question Answering",
    "summary": "FinanceBench is a first-of-its-kind test suite for evaluating the performance of LLMs on open book financial question answering (QA). It comprises 10,231 questions about publicly traded companies, with corresponding answers and evidence strings. The questions in FinanceBench are ecologically valid and cover a diverse set of scenarios. They are intended to be clear-cut and straightforward to answer to serve as a minimum performance standard. We test 16 state of the art model configurations (including GPT-4-Turbo, Llama2 and Claude2, with vector stores and long context prompts) on a sample of 150 cases from FinanceBench, and manually review their answers (n=2,400). The cases are available open-source. We show that existing LLMs have clear limitations for financial QA. Notably, GPT-4-Turbo used with a retrieval system incorrectly answered or refused to answer 81% of questions. While augmentation techniques such as using longer context window to feed in relevant evidence improve performance, they are unrealistic for enterprise settings due to increased latency and cannot support larger financial documents. We find that all models examined exhibit weaknesses, such as hallucinations, that limit their suitability for use by enterprises.",
    "authors": [
      "Pranab Islam",
      "Anand Kannappan",
      "Douwe Kiela",
      "Rebecca Qian",
      "Nino Scherrer",
      "Bertie Vidgen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CE",
      "stat.ML"
    ],
    "published": "2023-11-20T17:28:02Z",
    "pdf_url": "https://arxiv.org/pdf/2311.11944v1"
  },
  {
    "arxiv_id": "2311.11861v1",
    "entry_id": "http://arxiv.org/abs/2311.11861v1",
    "title": "Generating Valid and Natural Adversarial Examples with Large Language Models",
    "summary": "Deep learning-based natural language processing (NLP) models, particularly pre-trained language models (PLMs), have been revealed to be vulnerable to adversarial attacks. However, the adversarial examples generated by many mainstream word-level adversarial attack models are neither valid nor natural, leading to the loss of semantic maintenance, grammaticality, and human imperceptibility. Based on the exceptional capacity of language understanding and generation of large language models (LLMs), we propose LLM-Attack, which aims at generating both valid and natural adversarial examples with LLMs. The method consists of two stages: word importance ranking (which searches for the most vulnerable words) and word synonym replacement (which substitutes them with their synonyms obtained from LLMs). Experimental results on the Movie Review (MR), IMDB, and Yelp Review Polarity datasets against the baseline adversarial attack models illustrate the effectiveness of LLM-Attack, and it outperforms the baselines in human and GPT-4 evaluation by a significant margin. The model can generate adversarial examples that are typically valid and natural, with the preservation of semantic meaning, grammaticality, and human imperceptibility.",
    "authors": [
      "Zimu Wang",
      "Wei Wang",
      "Qi Chen",
      "Qiufeng Wang",
      "Anh Nguyen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-20T15:57:04Z",
    "pdf_url": "https://arxiv.org/pdf/2311.11861v1"
  },
  {
    "arxiv_id": "2311.11797v1",
    "entry_id": "http://arxiv.org/abs/2311.11797v1",
    "title": "Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents",
    "summary": "Large language models (LLMs) have dramatically enhanced the field of language intelligence, as demonstrably evidenced by their formidable empirical performance across a spectrum of complex reasoning tasks. Additionally, theoretical proofs have illuminated their emergent reasoning capabilities, providing a compelling showcase of their advanced cognitive abilities in linguistic contexts. Critical to their remarkable efficacy in handling complex reasoning tasks, LLMs leverage the intriguing chain-of-thought (CoT) reasoning techniques, obliging them to formulate intermediate steps en route to deriving an answer. The CoT reasoning approach has not only exhibited proficiency in amplifying reasoning performance but also in enhancing interpretability, controllability, and flexibility. In light of these merits, recent research endeavors have extended CoT reasoning methodologies to nurture the development of autonomous language agents, which adeptly adhere to language instructions and execute actions within varied environments. This survey paper orchestrates a thorough discourse, penetrating vital research dimensions, encompassing: (i) the foundational mechanics of CoT techniques, with a focus on elucidating the circumstances and justification behind its efficacy; (ii) the paradigm shift in CoT; and (iii) the burgeoning of language agents fortified by CoT approaches. Prospective research avenues envelop explorations into generalization, efficiency, customization, scaling, and safety. This paper caters to a wide audience, including beginners seeking comprehensive knowledge of CoT reasoning and language agents, as well as experienced researchers interested in foundational mechanics and engaging in cutting-edge discussions on these topics. A repository for the related papers is available at https://github.com/Zoeyyao27/CoT-Igniting-Agent.",
    "authors": [
      "Zhuosheng Zhang",
      "Yao Yao",
      "Aston Zhang",
      "Xiangru Tang",
      "Xinbei Ma",
      "Zhiwei He",
      "Yiming Wang",
      "Mark Gerstein",
      "Rui Wang",
      "Gongshen Liu",
      "Hai Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.HC",
      "cs.MA"
    ],
    "published": "2023-11-20T14:30:55Z",
    "pdf_url": "https://arxiv.org/pdf/2311.11797v1"
  },
  {
    "arxiv_id": "2311.11055v2",
    "entry_id": "http://arxiv.org/abs/2311.11055v2",
    "title": "Designing Interpretable ML System to Enhance Trust in Healthcare: A Systematic Review to Proposed Responsible Clinician-AI-Collaboration Framework",
    "summary": "This paper explores the significant impact of AI-based medical devices, including wearables, telemedicine, large language models, and digital twins, on clinical decision support systems. It emphasizes the importance of producing outcomes that are not only accurate but also interpretable and understandable to clinicians, addressing the risk that lack of interpretability poses in terms of mistrust and reluctance to adopt these technologies in healthcare. The paper reviews interpretable AI processes, methods, applications, and the challenges of implementation in healthcare, focusing on quality control to facilitate responsible communication between AI systems and clinicians. It breaks down the interpretability process into data pre-processing, model selection, and post-processing, aiming to foster a comprehensive understanding of the crucial role of a robust interpretability approach in healthcare and to guide future research in this area. with insights for creating responsible clinician-AI tools for healthcare, as well as to offer a deeper understanding of the challenges they might face. Our research questions, eligibility criteria and primary goals were identified using Preferred Reporting Items for Systematic reviews and Meta-Analyses guideline and PICO method; PubMed, Scopus and Web of Science databases were systematically searched using sensitive and specific search strings. In the end, 52 publications were selected for data extraction which included 8 existing reviews and 44 related experimental studies. The paper offers general concepts of interpretable AI in healthcare and discuss three-levels interpretability process. Additionally, it provides a comprehensive discussion of evaluating robust interpretability AI in healthcare. Moreover, this survey introduces a step-by-step roadmap for implementing responsible AI in healthcare.",
    "authors": [
      "Elham Nasarian",
      "Roohallah Alizadehsani",
      "U. Rajendra Acharya",
      "Kwok-Leung Tsui"
    ],
    "categories": [
      "cs.AI",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2023-11-18T12:29:18Z",
    "pdf_url": "https://arxiv.org/pdf/2311.11055v2"
  },
  {
    "arxiv_id": "2311.09993v1",
    "entry_id": "http://arxiv.org/abs/2311.09993v1",
    "title": "Generative AI for Hate Speech Detection: Evaluation and Findings",
    "summary": "Automatic hate speech detection using deep neural models is hampered by the scarcity of labeled datasets, leading to poor generalization. To mitigate this problem, generative AI has been utilized to generate large amounts of synthetic hate speech sequences from available labeled examples, leveraging the generated data in finetuning large pre-trained language models (LLMs). In this chapter, we provide a review of relevant methods, experimental setups and evaluation of this approach. In addition to general LLMs, such as BERT, RoBERTa and ALBERT, we apply and evaluate the impact of train set augmentation with generated data using LLMs that have been already adapted for hate detection, including RoBERTa-Toxicity, HateBERT, HateXplain, ToxDect, and ToxiGen. An empirical study corroborates our previous findings, showing that this approach improves hate speech generalization, boosting recall performance across data distributions. In addition, we explore and compare the performance of the finetuned LLMs with zero-shot hate detection using a GPT-3.5 model. Our results demonstrate that while better generalization is achieved using the GPT-3.5 model, it achieves mediocre recall and low precision on most datasets. It is an open question whether the sensitivity of models such as GPT-3.5, and onward, can be improved using similar techniques of text generation.",
    "authors": [
      "Sagi Pendzel",
      "Tomer Wullach",
      "Amir Adler",
      "Einat Minkov"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-16T16:09:43Z",
    "pdf_url": "https://arxiv.org/pdf/2311.09993v1"
  },
  {
    "arxiv_id": "2311.09861v4",
    "entry_id": "http://arxiv.org/abs/2311.09861v4",
    "title": "ConceptPsy:A Benchmark Suite with Conceptual Comprehensiveness in Psychology",
    "summary": "The critical field of psychology necessitates a comprehensive benchmark to enhance the evaluation and development of domain-specific Large Language Models (LLMs). Existing MMLU-type benchmarks, such as C-EVAL and CMMLU, include psychology-related subjects, but their limited number of questions and lack of systematic concept sampling strategies mean they cannot cover the concepts required in psychology. Consequently, despite their broad subject coverage, these benchmarks lack the necessary depth in the psychology domain, making them inadequate as psychology-specific evaluation suite. To address this issue, this paper presents ConceptPsy, designed to evaluate Chinese complex reasoning and knowledge abilities in psychology. ConceptPsy includes 12 core subjects and 1383 manually collected concepts. Specifically, we prompt GPT-4 to generate questions for each concept using carefully designed diverse prompts and hire professional psychologists to review these questions. To help to understand the fine-grained performances and enhance the weaknesses, we annotate each question with a chapter label and provide chapter-wise accuracy. Based on ConceptPsy, we evaluate a broad range of LLMs. We observe that, although some LLMs achieve similar accuracies on overall performances, they exhibit significant performance variations across different psychology concepts, even when they are models from the same series. We hope our work can facilitate the development of LLMs in the field of psychology.",
    "authors": [
      "Junlei Zhang",
      "Hongliang He",
      "Nirui Song",
      "Zhanchao Zhou",
      "Shuyuan He",
      "Shuai Zhang",
      "Huachuan Qiu",
      "Anqi Li",
      "Yong Dai",
      "Lizhi Ma",
      "Zhenzhong Lan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-16T12:43:18Z",
    "pdf_url": "https://arxiv.org/pdf/2311.09861v4"
  },
  {
    "arxiv_id": "2311.09680v5",
    "entry_id": "http://arxiv.org/abs/2311.09680v5",
    "title": "Trustworthy Large Models in Vision: A Survey",
    "summary": "The rapid progress of Large Models (LMs) has recently revolutionized various fields of deep learning with remarkable grades, ranging from Natural Language Processing (NLP) to Computer Vision (CV). However, LMs are increasingly challenged and criticized by academia and industry due to their powerful performance but untrustworthy behavior, which urgently needs to be alleviated by reliable methods. Despite the abundance of literature on trustworthy LMs in NLP, a systematic survey specifically delving into the trustworthiness of LMs in CV remains absent. In order to mitigate this gap, we summarize four relevant concerns that obstruct the trustworthy usage in vision of LMs in this survey, including 1) human misuse, 2) vulnerability, 3) inherent issue and 4) interpretability. By highlighting corresponding challenge, countermeasures, and discussion in each topic, we hope this survey will facilitate readers' understanding of this field, promote alignment of LMs with human expectations and enable trustworthy LMs to serve as welfare rather than disaster for human society.",
    "authors": [
      "Ziyan Guo",
      "Li Xu",
      "Jun Liu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-11-16T08:49:46Z",
    "pdf_url": "https://arxiv.org/pdf/2311.09680v5"
  },
  {
    "arxiv_id": "2311.09651v2",
    "entry_id": "http://arxiv.org/abs/2311.09651v2",
    "title": "\"It's not like Jarvis, but it's pretty close!\" -- Examining ChatGPT's Usage among Undergraduate Students in Computer Science",
    "summary": "Large language models (LLMs) such as ChatGPT and Google Bard have garnered significant attention in the academic community. Previous research has evaluated these LLMs for various applications such as generating programming exercises and solutions. However, these evaluations have predominantly been conducted by instructors and researchers, not considering the actual usage of LLMs by students. This study adopts a student-first approach to comprehensively understand how undergraduate computer science students utilize ChatGPT, a popular LLM, released by OpenAI. We employ a combination of student surveys and interviews to obtain valuable insights into the benefits, challenges, and suggested improvements related to ChatGPT. Our findings suggest that a majority of students (over 57%) have a convincingly positive outlook towards adopting ChatGPT as an aid in coursework-related tasks. However, our research also highlights various challenges that must be resolved for long-term acceptance of ChatGPT amongst students. The findings from this investigation have broader implications and may be applicable to other LLMs and their role in computing education.",
    "authors": [
      "Ishika Joshi",
      "Ritvik Budhiraja",
      "Harshal D Akolekar",
      "Jagat Sesh Challa",
      "Dhruv Kumar"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-11-16T08:10:18Z",
    "pdf_url": "https://arxiv.org/pdf/2311.09651v2"
  },
  {
    "arxiv_id": "2312.00029v3",
    "entry_id": "http://arxiv.org/abs/2312.00029v3",
    "title": "Bergeron: Combating Adversarial Attacks through a Conscience-Based Alignment Framework",
    "summary": "Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. Such vulnerabilities can lead to LLMs being manipulated into generating hazardous content: from instructions for creating dangerous materials to inciting violence or endorsing unethical behaviors. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM acting as a guardian to the primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis reviews that by using Bergeron to complement models with existing alignment training, we can significantly improve the robustness and safety of multiple, commonly used commercial and open-source LLMs. Specifically, we found that models integrated with Bergeron are, on average, nearly seven times more resistant to attacks compared to models without such support.",
    "authors": [
      "Matthew Pisano",
      "Peter Ly",
      "Abraham Sanders",
      "Bingsheng Yao",
      "Dakuo Wang",
      "Tomek Strzalkowski",
      "Mei Si"
    ],
    "categories": [
      "cs.CR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-11-16T07:31:18Z",
    "pdf_url": "https://arxiv.org/pdf/2312.00029v3"
  },
  {
    "arxiv_id": "2311.14711v1",
    "entry_id": "http://arxiv.org/abs/2311.14711v1",
    "title": "Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem under the ASPIRE Framework",
    "summary": "With the increasing integration of frontier large language models (LLMs) into society and the economy, decisions related to their training, deployment, and use have far-reaching implications. These decisions should not be left solely in the hands of frontier LLM developers. LLM users, civil society and policymakers need trustworthy sources of information to steer such decisions for the better. Involving outside actors in the evaluation of these systems - what we term 'external scrutiny' - via red-teaming, auditing, and external researcher access, offers a solution. Though there are encouraging signs of increasing external scrutiny of frontier LLMs, its success is not assured. In this paper, we survey six requirements for effective external scrutiny of frontier AI systems and organize them under the ASPIRE framework: Access, Searching attitude, Proportionality to the risks, Independence, Resources, and Expertise. We then illustrate how external scrutiny might function throughout the AI lifecycle and offer recommendations to policymakers.",
    "authors": [
      "Markus Anderljung",
      "Everett Thornton Smith",
      "Joe O'Brien",
      "Lisa Soder",
      "Benjamin Bucknall",
      "Emma Bluemke",
      "Jonas Schuett",
      "Robert Trager",
      "Lacey Strahm",
      "Rumman Chowdhury"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-11-15T10:25:41Z",
    "pdf_url": "https://arxiv.org/pdf/2311.14711v1"
  },
  {
    "arxiv_id": "2311.08832v1",
    "entry_id": "http://arxiv.org/abs/2311.08832v1",
    "title": "Exploring Links between Conversational Agent Design Challenges and Interdisciplinary Collaboration",
    "summary": "Recent years have seen a steady rise in the popularity and use of Conversational Agents (CA) for different applications, well before the more immediate impact of large language models. This rise has been accompanied by an extensive exploration and documentation of the challenges of designing and creating conversational agents. Focusing on a recent scoping review of the socio-technical challenges of CA creation, this opinion paper calls for an examination of the extent to which interdisciplinary collaboration (IDC) challenges might contribute towards socio-technical CA design challenges. The paper proposes a taxonomy of CA design challenges using IDC as a lens, and proposes practical strategies to overcome them which complement existing design principles. The paper invites future work to empirically verify suggested conceptual links and apply the proposed strategies within the space of CA design to evaluate their effectiveness.",
    "authors": [
      "Malak Sadek",
      "Céline Mougenot"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-11-15T10:20:49Z",
    "pdf_url": "https://arxiv.org/pdf/2311.08832v1"
  },
  {
    "arxiv_id": "2311.08298v2",
    "entry_id": "http://arxiv.org/abs/2311.08298v2",
    "title": "A Survey of Confidence Estimation and Calibration in Large Language Models",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, they can be unreliable due to factual errors in their generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce better generations. There has been a lot of recent research aiming to address this, but there has been no comprehensive overview to organize it and outline the main lessons learned. The present survey aims to bridge this gap. In particular, we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration. We further discuss their applications and suggest promising directions for future work.",
    "authors": [
      "Jiahui Geng",
      "Fengyu Cai",
      "Yuxia Wang",
      "Heinz Koeppl",
      "Preslav Nakov",
      "Iryna Gurevych"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-14T16:43:29Z",
    "pdf_url": "https://arxiv.org/pdf/2311.08298v2"
  },
  {
    "arxiv_id": "2311.07989v7",
    "entry_id": "http://arxiv.org/abs/2311.07989v7",
    "title": "Unifying the Perspectives of NLP and Software Engineering: A Survey on Language Models for Code",
    "summary": "In this work we systematically review the recent advancements in software engineering with language models, covering 70+ models, 40+ evaluation tasks, 180+ datasets, and 900 related works. Unlike previous works, we integrate software engineering (SE) with natural language processing (NLP) by discussing the perspectives of both sides: SE applies language models for development automation, while NLP adopts SE tasks for language model evaluation. We break down code processing models into general language models represented by the GPT family and specialized models that are specifically pretrained on code, often with tailored objectives. We discuss the relations and differences between these models, and highlight the historical transition of code modeling from statistical models and RNNs to pretrained Transformers and LLMs, which is exactly the same course that had been taken by NLP. We also go beyond programming and review LLMs' application in other software engineering activities including requirement engineering, testing, deployment, and operations in an endeavor to provide a global view of NLP in SE, and identify key challenges and potential future directions in this domain. We keep the survey open and updated on GitHub at https://github.com/codefuse-ai/Awesome-Code-LLM.",
    "authors": [
      "Ziyin Zhang",
      "Chaoyu Chen",
      "Bingchang Liu",
      "Cong Liao",
      "Zi Gong",
      "Hang Yu",
      "Jianguo Li",
      "Rui Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2023-11-14T08:34:26Z",
    "pdf_url": "https://arxiv.org/pdf/2311.07989v7"
  },
  {
    "arxiv_id": "2311.07914v2",
    "entry_id": "http://arxiv.org/abs/2311.07914v2",
    "title": "Can Knowledge Graphs Reduce Hallucinations in LLMs? : A Survey",
    "summary": "The contemporary LLMs are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the LLMs by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledge-graph-based augmentation techniques in LLMs, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.",
    "authors": [
      "Garima Agrawal",
      "Tharindu Kumarage",
      "Zeyad Alghamdi",
      "Huan Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-11-14T05:21:57Z",
    "pdf_url": "https://arxiv.org/pdf/2311.07914v2"
  },
  {
    "arxiv_id": "2311.07879v4",
    "entry_id": "http://arxiv.org/abs/2311.07879v4",
    "title": "Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators",
    "summary": "Extensive efforts in automated approaches for content moderation have been focused on developing models to identify toxic, offensive, and hateful content with the aim of lightening the load for moderators. Yet, it remains uncertain whether improvements on those tasks have truly addressed moderators' needs in accomplishing their work. In this paper, we surface gaps between past research efforts that have aimed to provide automation for aspects of content moderation and the needs of volunteer content moderators, regarding identifying violations of various moderation rules. To do so, we conduct a model review on Hugging Face to reveal the availability of models to cover various moderation rules and guidelines from three exemplar forums. We further put state-of-the-art LLMs to the test, evaluating how well these models perform in flagging violations of platform rules from one particular forum. Finally, we conduct a user survey study with volunteer moderators to gain insight into their perspectives on useful moderation models. Overall, we observe a non-trivial gap, as missing developed models and LLMs exhibit moderate to low performance on a significant portion of the rules. Moderators' reports provide guides for future work on developing moderation assistant models.",
    "authors": [
      "Yang Trista Cao",
      "Lovely-Frances Domingo",
      "Sarah Ann Gilbert",
      "Michelle Mazurek",
      "Katie Shilton",
      "Hal Daumé"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-14T03:18:28Z",
    "pdf_url": "https://arxiv.org/pdf/2311.07879v4"
  },
  {
    "arxiv_id": "2311.07547v1",
    "entry_id": "http://arxiv.org/abs/2311.07547v1",
    "title": "GPT-4V(ision) as A Social Media Analysis Engine",
    "summary": "Recent research has offered insights into the extraordinary capabilities of Large Multimodal Models (LMMs) in various general vision and language tasks. There is growing interest in how LMMs perform in more specialized domains. Social media content, inherently multimodal, blends text, images, videos, and sometimes audio. Understanding social multimedia content remains a challenging problem for contemporary machine learning frameworks. In this paper, we explore GPT-4V(ision)'s capabilities for social multimedia analysis. We select five representative tasks, including sentiment analysis, hate speech detection, fake news identification, demographic inference, and political ideology detection, to evaluate GPT-4V. Our investigation begins with a preliminary quantitative analysis for each task using existing benchmark datasets, followed by a careful review of the results and a selection of qualitative samples that illustrate GPT-4V's potential in understanding multimodal social media content. GPT-4V demonstrates remarkable efficacy in these tasks, showcasing strengths such as joint understanding of image-text pairs, contextual and cultural awareness, and extensive commonsense knowledge. Despite the overall impressive capacity of GPT-4V in the social media domain, there remain notable challenges. GPT-4V struggles with tasks involving multilingual social multimedia comprehension and has difficulties in generalizing to the latest trends in social media. Additionally, it exhibits a tendency to generate erroneous information in the context of evolving celebrity and politician knowledge, reflecting the known hallucination problem. The insights gleaned from our findings underscore a promising future for LMMs in enhancing our comprehension of social media content and its users through the analysis of multimodal information.",
    "authors": [
      "Hanjia Lyu",
      "Jinfa Huang",
      "Daoan Zhang",
      "Yongsheng Yu",
      "Xinyi Mou",
      "Jinsheng Pan",
      "Zhengyuan Yang",
      "Zhongyu Wei",
      "Jiebo Luo"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.MM"
    ],
    "published": "2023-11-13T18:36:50Z",
    "pdf_url": "https://arxiv.org/pdf/2311.07547v1"
  },
  {
    "arxiv_id": "2311.07492v1",
    "entry_id": "http://arxiv.org/abs/2311.07492v1",
    "title": "How Physicality Enables Trust: A New Era of Trust-Centered Cyberphysical Systems",
    "summary": "Multi-agent cyberphysical systems enable new capabilities in efficiency, resilience, and security. The unique characteristics of these systems prompt a reevaluation of their security concepts, including their vulnerabilities, and mechanisms to mitigate these vulnerabilities. This survey paper examines how advancement in wireless networking, coupled with the sensing and computing in cyberphysical systems, can foster novel security capabilities. This study delves into three main themes related to securing multi-agent cyberphysical systems. First, we discuss the threats that are particularly relevant to multi-agent cyberphysical systems given the potential lack of trust between agents. Second, we present prospects for sensing, contextual awareness, and authentication, enabling the inference and measurement of ``inter-agent trust\" for these systems. Third, we elaborate on the application of quantifiable trust notions to enable ``resilient coordination,\" where ``resilient\" signifies sustained functionality amid attacks on multiagent cyberphysical systems. We refer to the capability of cyberphysical systems to self-organize, and coordinate to achieve a task as autonomy. This survey unveils the cyberphysical character of future interconnected systems as a pivotal catalyst for realizing robust, trust-centered autonomy in tomorrow's world.",
    "authors": [
      "Stephanie Gil",
      "Michal Yemini",
      "Arsenia Chorti",
      "Angelia Nedić",
      "H. Vincent Poor",
      "Andrea J. Goldsmith"
    ],
    "categories": [
      "cs.RO",
      "cs.MA",
      "cs.NI",
      "eess.SY"
    ],
    "published": "2023-11-13T17:28:57Z",
    "pdf_url": "https://arxiv.org/pdf/2311.07492v1"
  },
  {
    "arxiv_id": "2311.07226v1",
    "entry_id": "http://arxiv.org/abs/2311.07226v1",
    "title": "Large Language Models for Robotics: A Survey",
    "summary": "The human ability to learn, generalize, and control complex manipulation tasks through multi-modality feedback suggests a unique capability, which we refer to as dexterity intelligence. Understanding and assessing this intelligence is a complex task. Amidst the swift progress and extensive proliferation of large language models (LLMs), their applications in the field of robotics have garnered increasing attention. LLMs possess the ability to process and generate natural language, facilitating efficient interaction and collaboration with robots. Researchers and engineers in the field of robotics have recognized the immense potential of LLMs in enhancing robot intelligence, human-robot interaction, and autonomy. Therefore, this comprehensive review aims to summarize the applications of LLMs in robotics, delving into their impact and contributions to key areas such as robot control, perception, decision-making, and path planning. We first provide an overview of the background and development of LLMs for robotics, followed by a description of the benefits of LLMs for robotics and recent advancements in robotics models based on LLMs. We then delve into the various techniques used in the model, including those employed in perception, decision-making, control, and interaction. Finally, we explore the applications of LLMs in robotics and some potential challenges they may face in the near future. Embodied intelligence is the future of intelligent science, and LLMs-based robotics is one of the promising but challenging paths to achieve this.",
    "authors": [
      "Fanlong Zeng",
      "Wensheng Gan",
      "Yongheng Wang",
      "Ning Liu",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2023-11-13T10:46:35Z",
    "pdf_url": "https://arxiv.org/pdf/2311.07226v1"
  },
  {
    "arxiv_id": "2311.07621v1",
    "entry_id": "http://arxiv.org/abs/2311.07621v1",
    "title": "To Transformers and Beyond: Large Language Models for the Genome",
    "summary": "In the rapidly evolving landscape of genomics, deep learning has emerged as a useful tool for tackling complex computational challenges. This review focuses on the transformative role of Large Language Models (LLMs), which are mostly based on the transformer architecture, in genomics. Building on the foundation of traditional convolutional neural networks and recurrent neural networks, we explore both the strengths and limitations of transformers and other LLMs for genomics. Additionally, we contemplate the future of genomic modeling beyond the transformer architecture based on current trends in research. The paper aims to serve as a guide for computational biologists and computer scientists interested in LLMs for genomic data. We hope the paper can also serve as an educational introduction and discussion for biologists to a fundamental shift in how we will be analyzing genomic data in the future.",
    "authors": [
      "Micaela E. Consens",
      "Cameron Dufault",
      "Michael Wainberg",
      "Duncan Forster",
      "Mehran Karimzadeh",
      "Hani Goodarzi",
      "Fabian J. Theis",
      "Alan Moses",
      "Bo Wang"
    ],
    "categories": [
      "q-bio.GN",
      "cs.LG"
    ],
    "published": "2023-11-13T02:13:58Z",
    "pdf_url": "https://arxiv.org/pdf/2311.07621v1"
  },
  {
    "arxiv_id": "2311.07594v3",
    "entry_id": "http://arxiv.org/abs/2311.07594v3",
    "title": "How to Bridge the Gap between Modalities: Survey on Multimodal Large Language Model",
    "summary": "We explore Multimodal Large Language Models (MLLMs), which integrate LLMs like GPT-4 to handle multimodal data, including text, images, audio, and more. MLLMs demonstrate capabilities such as generating image captions and answering image-based questions, bridging the gap towards real-world human-computer interactions and hinting at a potential pathway to artificial general intelligence. However, MLLMs still face challenges in addressing the semantic gap in multimodal data, which may lead to erroneous outputs, posing potential risks to society. Selecting the appropriate modality alignment method is crucial, as improper methods might require more parameters without significant performance improvements. This paper aims to explore modality alignment methods for LLMs and their current capabilities. Implementing effective modality alignment can help LLMs address environmental issues and enhance accessibility. The study surveys existing modality alignment methods for MLLMs, categorizing them into four groups: (1) Multimodal Converter, which transforms data into a format that LLMs can understand; (2) Multimodal Perceiver, which improves how LLMs percieve different types of data; (3) Tool Learning, which leverages external tools to convert data into a common format, usually text; and (4) Data-Driven Method, which teaches LLMs to understand specific data types within datasets.",
    "authors": [
      "Shezheng Song",
      "Xiaopeng Li",
      "Shasha Li",
      "Shan Zhao",
      "Jie Yu",
      "Jun Ma",
      "Xiaoguang Mao",
      "Weimin Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "cs.MM"
    ],
    "published": "2023-11-10T09:51:24Z",
    "pdf_url": "https://arxiv.org/pdf/2311.07594v3"
  },
  {
    "arxiv_id": "2311.05804v1",
    "entry_id": "http://arxiv.org/abs/2311.05804v1",
    "title": "Model-as-a-Service (MaaS): A Survey",
    "summary": "Due to the increased number of parameters and data in the pre-trained model exceeding a certain level, a foundation model (e.g., a large language model) can significantly improve downstream task performance and emerge with some novel special abilities (e.g., deep learning, complex reasoning, and human alignment) that were not present before. Foundation models are a form of generative artificial intelligence (GenAI), and Model-as-a-Service (MaaS) has emerged as a groundbreaking paradigm that revolutionizes the deployment and utilization of GenAI models. MaaS represents a paradigm shift in how we use AI technologies and provides a scalable and accessible solution for developers and users to leverage pre-trained AI models without the need for extensive infrastructure or expertise in model training. In this paper, the introduction aims to provide a comprehensive overview of MaaS, its significance, and its implications for various industries. We provide a brief review of the development history of \"X-as-a-Service\" based on cloud computing and present the key technologies involved in MaaS. The development of GenAI models will become more democratized and flourish. We also review recent application studies of MaaS. Finally, we highlight several challenges and future issues in this promising area. MaaS is a new deployment and service paradigm for different AI-based models. We hope this review will inspire future research in the field of MaaS.",
    "authors": [
      "Wensheng Gan",
      "Shicheng Wan",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-11-10T00:35:00Z",
    "pdf_url": "https://arxiv.org/pdf/2311.05804v1"
  },
  {
    "arxiv_id": "2311.05754v1",
    "entry_id": "http://arxiv.org/abs/2311.05754v1",
    "title": "Deep Natural Language Feature Learning for Interpretable Prediction",
    "summary": "We propose a general method to break down a main complex task into a set of intermediary easier sub-tasks, which are formulated in natural language as binary questions related to the final target task. Our method allows for representing each example by a vector consisting of the answers to these questions. We call this representation Natural Language Learned Features (NLLF). NLLF is generated by a small transformer language model (e.g., BERT) that has been trained in a Natural Language Inference (NLI) fashion, using weak labels automatically obtained from a Large Language Model (LLM). We show that the LLM normally struggles for the main task using in-context learning, but can handle these easiest subtasks and produce useful weak labels to train a BERT. The NLI-like training of the BERT allows for tackling zero-shot inference with any binary question, and not necessarily the ones seen during the training. We show that this NLLF vector not only helps to reach better performances by enhancing any classifier, but that it can be used as input of an easy-to-interpret machine learning model like a decision tree. This decision tree is interpretable but also reaches high performances, surpassing those of a pre-trained transformer in some cases.We have successfully applied this method to two completely different tasks: detecting incoherence in students' answers to open-ended mathematics exam questions, and screening abstracts for a systematic literature review of scientific papers on climate change and agroecology.",
    "authors": [
      "Felipe Urrutia",
      "Cristian Buc",
      "Valentin Barriere"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-09T21:43:27Z",
    "pdf_url": "https://arxiv.org/pdf/2311.05754v1"
  },
  {
    "arxiv_id": "2311.05450v1",
    "entry_id": "http://arxiv.org/abs/2311.05450v1",
    "title": "Cognitively Inspired Components for Social Conversational Agents",
    "summary": "Current conversational agents (CA) have seen improvement in conversational quality in recent years due to the influence of large language models (LLMs) like GPT3. However, two key categories of problem remain. Firstly there are the unique technical problems resulting from the approach taken in creating the CA, such as scope with retrieval agents and the often nonsensical answers of former generative agents. Secondly, humans perceive CAs as social actors, and as a result expect the CA to adhere to social convention. Failure on the part of the CA in this respect can lead to a poor interaction and even the perception of threat by the user. As such, this paper presents a survey highlighting a potential solution to both categories of problem through the introduction of cognitively inspired additions to the CA. Through computational facsimiles of semantic and episodic memory, emotion, working memory, and the ability to learn, it is possible to address both the technical and social problems encountered by CAs.",
    "authors": [
      "Alex Clay",
      "Eduardo Alonso",
      "Esther Mondragón"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-09T15:38:58Z",
    "pdf_url": "https://arxiv.org/pdf/2311.05450v1"
  },
  {
    "arxiv_id": "2311.05112v7",
    "entry_id": "http://arxiv.org/abs/2311.05112v7",
    "title": "A Survey of Large Language Models in Medicine: Progress, Application, and Challenge",
    "summary": "Large language models (LLMs), such as ChatGPT, have received substantial attention due to their capabilities for understanding and generating human language. While there has been a burgeoning trend in research focusing on the employment of LLMs in supporting different medical tasks (e.g., enhancing clinical diagnostics and providing medical education), a review of these efforts, particularly their development, practical applications, and outcomes in medicine, remains scarce. Therefore, this review aims to provide a detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face. In terms of development, we provide a detailed introduction to the principles of existing medical LLMs, including their basic model structures, number of parameters, and sources and scales of data used for model development. It serves as a guide for practitioners in developing medical LLMs tailored to their specific needs. In terms of deployment, we offer a comparison of the performance of different LLMs across various medical tasks, and further compare them with state-of-the-art lightweight models, aiming to provide an understanding of the advantages and limitations of LLMs in medicine. Overall, in this review, we address the following questions: 1) What are the practices for developing medical LLMs 2) How to measure the medical task performance of LLMs in a medical setting? 3) How have medical LLMs been employed in real-world practice? 4) What challenges arise from the use of medical LLMs? and 5) How to more effectively develop and deploy medical LLMs? By answering these questions, this review aims to provide insights into the opportunities for LLMs in medicine and serve as a practical resource. We also maintain a regularly updated list of practical guides on medical LLMs at https://github.com/AI-in-Health/MedLLMsPracticalGuide",
    "authors": [
      "Hongjian Zhou",
      "Fenglin Liu",
      "Boyang Gu",
      "Xinyu Zou",
      "Jinfa Huang",
      "Jinge Wu",
      "Yiru Li",
      "Sam S. Chen",
      "Peilin Zhou",
      "Junling Liu",
      "Yining Hua",
      "Chengfeng Mao",
      "Chenyu You",
      "Xian Wu",
      "Yefeng Zheng",
      "Lei Clifton",
      "Zheng Li",
      "Jiebo Luo",
      "David A. Clifton"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-09T02:55:58Z",
    "pdf_url": "https://arxiv.org/pdf/2311.05112v7"
  },
  {
    "arxiv_id": "2311.05085v2",
    "entry_id": "http://arxiv.org/abs/2311.05085v2",
    "title": "Characterizing Large Language Models as Rationalizers of Knowledge-intensive Tasks",
    "summary": "Large language models (LLMs) are proficient at generating fluent text with minimal task-specific supervision. Yet, their ability to provide well-grounded rationalizations for knowledge-intensive tasks remains under-explored. Such tasks, like commonsense multiple-choice questions, require rationales based on world knowledge to support predictions and refute alternate options. We consider the task of generating knowledge-guided rationalization in natural language by using expert-written examples in a few-shot manner. Surprisingly, crowd-workers preferred knowledge-grounded rationales over crowdsourced rationalizations, citing their factuality, sufficiency, and comprehensive refutations. Although LLMs-generated rationales were preferable, further improvements in conciseness and novelty are required. In another study, we show how rationalization of incorrect model predictions erodes humans' trust in LLM-generated rationales. Motivated by these observations, we create a two-stage pipeline to review task predictions and eliminate potential incorrect decisions before rationalization, enabling trustworthy rationale generation.",
    "authors": [
      "Aditi Mishra",
      "Sajjadur Rahman",
      "Hannah Kim",
      "Kushan Mitra",
      "Estevam Hruschka"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-11-09T01:04:44Z",
    "pdf_url": "https://arxiv.org/pdf/2311.05085v2"
  },
  {
    "arxiv_id": "2311.14695v1",
    "entry_id": "http://arxiv.org/abs/2311.14695v1",
    "title": "AI for All: Operationalising Diversity and Inclusion Requirements for AI Systems",
    "summary": "As Artificial Intelligence (AI) permeates many aspects of society, it brings numerous advantages while at the same time raising ethical concerns and potential risks, such as perpetuating inequalities through biased or discriminatory decision-making. To develop AI systems that cater for the needs of diverse users and uphold ethical values, it is essential to consider and integrate diversity and inclusion (D&I) principles throughout AI development and deployment. Requirements engineering (RE) is a fundamental process in developing software systems by eliciting and specifying relevant needs from diverse stakeholders. This research aims to address the lack of research and practice on how to elicit and capture D&I requirements for AI systems. We have conducted comprehensive data collection and synthesis from the literature review to extract requirements themes related to D&I in AI. We have proposed a tailored user story template to capture D&I requirements and conducted focus group exercises to use the themes and user story template in writing D&I requirements for two example AI systems. Additionally, we have investigated the capability of our solution by generating synthetic D&I requirements captured in user stories with the help of a Large Language Model.",
    "authors": [
      "Muneera Bano",
      "Didar Zowghi",
      "Vincenzo Gervasi",
      "Rifat Shams"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-11-07T23:15:03Z",
    "pdf_url": "https://arxiv.org/pdf/2311.14695v1"
  },
  {
    "arxiv_id": "2311.04235v3",
    "entry_id": "http://arxiv.org/abs/2311.04235v3",
    "title": "Can LLMs Follow Simple Rules?",
    "summary": "As Large Language Models (LLMs) are deployed with increasing real-world responsibilities, it is important to be able to specify and constrain the behavior of these systems in a reliable manner. Model developers may wish to set explicit rules for the model, such as \"do not generate abusive content\", but these may be circumvented by jailbreaking techniques. Existing evaluations of adversarial attacks and defenses on LLMs generally require either expensive manual review or unreliable heuristic checks. To address this issue, we propose Rule-following Language Evaluation Scenarios (RuLES), a programmatic framework for measuring rule-following ability in LLMs. RuLES consists of 14 simple text scenarios in which the model is instructed to obey various rules while interacting with the user. Each scenario has a programmatic evaluation function to determine whether the model has broken any rules in a conversation. Our evaluations of proprietary and open models show that almost all current models struggle to follow scenario rules, even on straightforward test cases. We also demonstrate that simple optimization attacks suffice to significantly increase failure rates on test cases. We conclude by exploring two potential avenues for improvement: test-time steering and supervised fine-tuning.",
    "authors": [
      "Norman Mu",
      "Sarah Chen",
      "Zifan Wang",
      "Sizhe Chen",
      "David Karamardian",
      "Lulwa Aljeraisy",
      "Basel Alomair",
      "Dan Hendrycks",
      "David Wagner"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-11-06T08:50:29Z",
    "pdf_url": "https://arxiv.org/pdf/2311.04235v3"
  },
  {
    "arxiv_id": "2311.04929v1",
    "entry_id": "http://arxiv.org/abs/2311.04929v1",
    "title": "An Interdisciplinary Outlook on Large Language Models for Scientific Research",
    "summary": "In this paper, we describe the capabilities and constraints of Large Language Models (LLMs) within disparate academic disciplines, aiming to delineate their strengths and limitations with precision. We examine how LLMs augment scientific inquiry, offering concrete examples such as accelerating literature review by summarizing vast numbers of publications, enhancing code development through automated syntax correction, and refining the scientific writing process. Simultaneously, we articulate the challenges LLMs face, including their reliance on extensive and sometimes biased datasets, and the potential ethical dilemmas stemming from their use. Our critical discussion extends to the varying impacts of LLMs across fields, from the natural sciences, where they help model complex biological sequences, to the social sciences, where they can parse large-scale qualitative data. We conclude by offering a nuanced perspective on how LLMs can be both a boon and a boundary to scientific progress.",
    "authors": [
      "James Boyko",
      "Joseph Cohen",
      "Nathan Fox",
      "Maria Han Veiga",
      "Jennifer I-Hsiu Li",
      "Jing Liu",
      "Bernardo Modenesi",
      "Andreas H. Rauch",
      "Kenneth N. Reid",
      "Soumi Tribedi",
      "Anastasia Visheratina",
      "Xin Xie"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL",
      "cs.LG"
    ],
    "published": "2023-11-03T19:41:09Z",
    "pdf_url": "https://arxiv.org/pdf/2311.04929v1"
  },
  {
    "arxiv_id": "2311.04928v3",
    "entry_id": "http://arxiv.org/abs/2311.04928v3",
    "title": "Leveraging Large Language Models for Collective Decision-Making",
    "summary": "In various work contexts, such as meeting scheduling, collaborating, and project planning, collective decision-making is essential but often challenging due to diverse individual preferences, varying work focuses, and power dynamics among members. To address this, we propose a system leveraging Large Language Models (LLMs) to facilitate group decision-making by managing conversations and balancing preferences among individuals. Our system aims to extract individual preferences from each member's conversation with the system and suggest options that satisfy the preferences of the members. We specifically apply this system to corporate meeting scheduling. We create synthetic employee profiles and simulate conversations at scale, leveraging LLMs to evaluate the system performance as a novel approach to conducting a user study. Our results indicate efficient coordination with reduced interactions between the members and the LLM-based system. The system refines and improves its proposed options over time, ensuring that many of the members' individual preferences are satisfied in an equitable way. Finally, we conduct a survey study involving human participants to assess our system's ability to aggregate preferences and reasoning about them. Our findings show that the system exhibits strong performance in both dimensions.",
    "authors": [
      "Marios Papachristou",
      "Longqi Yang",
      "Chin-Chia Hsu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.SI"
    ],
    "published": "2023-11-03T18:27:21Z",
    "pdf_url": "https://arxiv.org/pdf/2311.04928v3"
  },
  {
    "arxiv_id": "2311.01918v1",
    "entry_id": "http://arxiv.org/abs/2311.01918v1",
    "title": "Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review",
    "summary": "With the rapid development of artificial intelligence, large language models (LLMs) have shown promising capabilities in mimicking human-level language comprehension and reasoning. This has sparked significant interest in applying LLMs to enhance various aspects of healthcare, ranging from medical education to clinical decision support. However, medicine involves multifaceted data modalities and nuanced reasoning skills, presenting challenges for integrating LLMs. This paper provides a comprehensive review on the applications and implications of LLMs in medicine. It begins by examining the fundamental applications of general-purpose and specialized LLMs, demonstrating their utilities in knowledge retrieval, research support, clinical workflow automation, and diagnostic assistance. Recognizing the inherent multimodality of medicine, the review then focuses on multimodal LLMs, investigating their ability to process diverse data types like medical imaging and EHRs to augment diagnostic accuracy. To address LLMs' limitations regarding personalization and complex clinical reasoning, the paper explores the emerging development of LLM-powered autonomous agents for healthcare. Furthermore, it summarizes the evaluation methodologies for assessing LLMs' reliability and safety in medical contexts. Overall, this review offers an extensive analysis on the transformative potential of LLMs in modern medicine. It also highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice. Visit https://github.com/mingze-yuan/Awesome-LLM-Healthcare for an accompanying GitHub repository containing latest papers.",
    "authors": [
      "Mingze Yuan",
      "Peng Bao",
      "Jiajia Yuan",
      "Yunhao Shen",
      "Zifan Chen",
      "Yi Xie",
      "Jie Zhao",
      "Yang Chen",
      "Li Zhang",
      "Lin Shen",
      "Bin Dong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-11-03T13:51:36Z",
    "pdf_url": "https://arxiv.org/pdf/2311.01918v1"
  },
  {
    "arxiv_id": "2311.01043v4",
    "entry_id": "http://arxiv.org/abs/2311.01043v4",
    "title": "LLM4Drive: A Survey of Large Language Models for Autonomous Driving",
    "summary": "Autonomous driving technology, a catalyst for revolutionizing transportation and urban mobility, has the tend to transition from rule-based systems to data-driven strategies. Traditional module-based systems are constrained by cumulative errors among cascaded modules and inflexible pre-set rules. In contrast, end-to-end autonomous driving systems have the potential to avoid error accumulation due to their fully data-driven training process, although they often lack transparency due to their \"black box\" nature, complicating the validation and traceability of decisions. Recently, large language models (LLMs) have demonstrated abilities including understanding context, logical reasoning, and generating answers. A natural thought is to utilize these abilities to empower autonomous driving. By combining LLM with foundation vision models, it could open the door to open-world understanding, reasoning, and few-shot learning, which current autonomous driving systems are lacking. In this paper, we systematically review a research line about \\textit{Large Language Models for Autonomous Driving (LLM4AD)}. This study evaluates the current state of technological advancements, distinctly outlining the principal challenges and prospective directions for the field. For the convenience of researchers in academia and industry, we provide real-time updates on the latest advances in the field as well as relevant open-source resources via the designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.",
    "authors": [
      "Zhenjie Yang",
      "Xiaosong Jia",
      "Hongyang Li",
      "Junchi Yan"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-11-02T07:23:33Z",
    "pdf_url": "https://arxiv.org/pdf/2311.01043v4"
  },
  {
    "arxiv_id": "2311.00530v5",
    "entry_id": "http://arxiv.org/abs/2311.00530v5",
    "title": "Advances in Embodied Navigation Using Large Language Models: A Survey",
    "summary": "In recent years, the rapid advancement of Large Language Models (LLMs) such as the Generative Pre-trained Transformer (GPT) has attracted increasing attention due to their potential in a variety of practical applications. The application of LLMs with Embodied Intelligence has emerged as a significant area of focus. Among the myriad applications of LLMs, navigation tasks are particularly noteworthy because they demand a deep understanding of the environment and quick, accurate decision-making. LLMs can augment embodied intelligence systems with sophisticated environmental perception and decision-making support, leveraging their robust language and image-processing capabilities. This article offers an exhaustive summary of the symbiosis between LLMs and embodied intelligence with a focus on navigation. It reviews state-of-the-art models, research methodologies, and assesses the advantages and disadvantages of existing embodied navigation models and datasets. Finally, the article elucidates the role of LLMs in embodied intelligence, based on current research, and forecasts future directions in the field. A comprehensive list of studies in this survey is available at https://github.com/Rongtao-Xu/Awesome-LLM-EN.",
    "authors": [
      "Jinzhou Lin",
      "Han Gao",
      "Xuxiang Feng",
      "Rongtao Xu",
      "Changwei Wang",
      "Man Zhang",
      "Li Guo",
      "Shibiao Xu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-11-01T14:08:56Z",
    "pdf_url": "https://arxiv.org/pdf/2311.00530v5"
  },
  {
    "arxiv_id": "2311.00447v3",
    "entry_id": "http://arxiv.org/abs/2311.00447v3",
    "title": "On the Opportunities of Green Computing: A Survey",
    "summary": "Artificial Intelligence (AI) has achieved significant advancements in technology and research with the development over several decades, and is widely used in many areas including computing vision, natural language processing, time-series analysis, speech synthesis, etc. During the age of deep learning, especially with the arise of Large Language Models, a large majority of researchers' attention is paid on pursuing new state-of-the-art (SOTA) results, resulting in ever increasing of model size and computational complexity. The needs for high computing power brings higher carbon emission and undermines research fairness by preventing small or medium-sized research institutions and companies with limited funding in participating in research. To tackle the challenges of computing resources and environmental impact of AI, Green Computing has become a hot research topic. In this survey, we give a systematic overview of the technologies used in Green Computing. We propose the framework of Green Computing and devide it into four key components: (1) Measures of Greenness, (2) Energy-Efficient AI, (3) Energy-Efficient Computing Systems and (4) AI Use Cases for Sustainability. For each components, we discuss the research progress made and the commonly used techniques to optimize the AI efficiency. We conclude that this new research direction has the potential to address the conflicts between resource constraints and AI development. We encourage more researchers to put attention on this direction and make AI more environmental friendly.",
    "authors": [
      "You Zhou",
      "Xiujing Lin",
      "Xiang Zhang",
      "Maolin Wang",
      "Gangwei Jiang",
      "Huakang Lu",
      "Yupeng Wu",
      "Kai Zhang",
      "Zhe Yang",
      "Kehang Wang",
      "Yongduo Sui",
      "Fengwei Jia",
      "Zuoli Tang",
      "Yao Zhao",
      "Hongxuan Zhang",
      "Tiannuo Yang",
      "Weibo Chen",
      "Yunong Mao",
      "Yi Li",
      "De Bao",
      "Yu Li",
      "Hongrui Liao",
      "Ting Liu",
      "Jingwen Liu",
      "Jinchi Guo",
      "Xiangyu Zhao",
      "Ying WEI",
      "Hong Qian",
      "Qi Liu",
      "Xiang Wang",
      "Wai Kin",
      "Chan",
      "Chenliang Li",
      "Yusen Li",
      "Shiyu Yang",
      "Jining Yan",
      "Chao Mou",
      "Shuai Han",
      "Wuxia Jin",
      "Guannan Zhang",
      "Xiaodong Zeng"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-11-01T11:16:41Z",
    "pdf_url": "https://arxiv.org/pdf/2311.00447v3"
  },
  {
    "arxiv_id": "2311.00284v1",
    "entry_id": "http://arxiv.org/abs/2311.00284v1",
    "title": "Model-driven Engineering for Machine Learning Components: A Systematic Literature Review",
    "summary": "Context: Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber-physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components. Objective: The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations. Results: We analyzed selected studies with respect to several areas of interest and identified the following: 1) the key motivations behind using MDE4ML; 2) a variety of MDE solutions applied, such as modeling languages, model transformations, tool support, targeted ML aspects, contributions and more; 3) the evaluation techniques and metrics used; and 4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research. Conclusion: This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners",
    "authors": [
      "Hira Naveed",
      "Chetan Arora",
      "Hourieh Khalajzadeh",
      "John Grundy",
      "Omar Haggag"
    ],
    "categories": [
      "cs.SE",
      "cs.LG"
    ],
    "published": "2023-11-01T04:29:47Z",
    "pdf_url": "https://arxiv.org/pdf/2311.00284v1"
  },
  {
    "arxiv_id": "2311.00217v2",
    "entry_id": "http://arxiv.org/abs/2311.00217v2",
    "title": "Can Large Language Models Capture Public Opinion about Global Warming? An Empirical Assessment of Algorithmic Fidelity and Bias",
    "summary": "Large language models (LLMs) have demonstrated their potential in social science research by emulating human perceptions and behaviors, a concept referred to as algorithmic fidelity. This study assesses the algorithmic fidelity and bias of LLMs by utilizing two nationally representative climate change surveys. The LLMs were conditioned on demographics and/or psychological covariates to simulate survey responses. The findings indicate that LLMs can effectively capture presidential voting behaviors but encounter challenges in accurately representing global warming perspectives when relevant covariates are not included. GPT-4 exhibits improved performance when conditioned on both demographics and covariates. However, disparities emerge in LLM estimations of the views of certain groups, with LLMs tending to underestimate worry about global warming among Black Americans. While highlighting the potential of LLMs to aid social science research, these results underscore the importance of meticulous conditioning, model selection, survey question format, and bias assessment when employing LLMs for survey simulation. Further investigation into prompt engineering and algorithm auditing is essential to harness the power of LLMs while addressing their inherent limitations.",
    "authors": [
      "S. Lee",
      "T. Q. Peng",
      "M. H. Goldberg",
      "S. A. Rosenthal",
      "J. E. Kotcher",
      "E. W. Maibach",
      "A. Leiserowitz"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2023-11-01T01:32:59Z",
    "pdf_url": "https://arxiv.org/pdf/2311.00217v2"
  },
  {
    "arxiv_id": "2311.00168v2",
    "entry_id": "http://arxiv.org/abs/2311.00168v2",
    "title": "The Alignment Ceiling: Objective Mismatch in Reinforcement Learning from Human Feedback",
    "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a powerful technique to make large language models (LLMs) more capable in complex settings. RLHF proceeds as collecting human preference data, training a reward model on said data, and optimizing a base ML model with respect to said reward for extrinsic evaluation metrics (e.g. MMLU, GSM8k). RLHF relies on many assumptions about how the various pieces fit together, such as a reward model capturing human preferences and an RL optimizer extracting the right signal from a reward model. As the RLHF process involves many distinct design decisions, it is easy to assume that multiple processes are correlated and therefore numerically linked. This apparent correlation is often not true, where reward models are easily overoptimized or RL optimizers can reduce performance on tasks not modeled in the data. Notable manifestations of models trained with imperfect RLHF systems are those that are prone to refusing basic requests for safety reasons or appearing lazy in generations. As chat model evaluation becomes increasingly nuanced, the reliance on a perceived link between reward model training, RL scores, and downstream performance drives these issues, which we describe as an objective mismatch. In this paper, we illustrate the causes of this issue, reviewing relevant literature from model-based reinforcement learning, and argue for solutions. By solving objective mismatch in RLHF, the ML models of the future will be more precisely aligned to user instructions for both safety and helpfulness.",
    "authors": [
      "Nathan Lambert",
      "Roberto Calandra"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-10-31T21:52:41Z",
    "pdf_url": "https://arxiv.org/pdf/2311.00168v2"
  },
  {
    "arxiv_id": "2310.20031v1",
    "entry_id": "http://arxiv.org/abs/2310.20031v1",
    "title": "Recipes for calibration and validation of agent-based models in cancer biomedicine",
    "summary": "Computational models and simulations are not just appealing because of their intrinsic characteristics across spatiotemporal scales, scalability, and predictive power, but also because the set of problems in cancer biomedicine that can be addressed computationally exceeds the set of those amenable to analytical solutions. Agent-based models and simulations are especially interesting candidates among computational modelling strategies in cancer research due to their capabilities to replicate realistic local and global interaction dynamics at a convenient and relevant scale. Yet, the absence of methods to validate the consistency of the results across scales can hinder adoption by turning fine-tuned models into black boxes. This review compiles relevant literature to explore strategies to leverage high-fidelity simulations of multi-scale, or multi-level, cancer models with a focus on validation approached as simulation calibration. We argue that simulation calibration goes beyond parameter optimization by embedding informative priors to generate plausible parameter configurations across multiple dimensions.",
    "authors": [
      "Nicolò Cogno",
      "Cristian Axenie",
      "Roman Bauer",
      "Vasileios Vavourakis"
    ],
    "categories": [
      "q-bio.TO",
      "cs.MA"
    ],
    "published": "2023-10-30T21:29:54Z",
    "pdf_url": "https://arxiv.org/pdf/2310.20031v1"
  },
  {
    "arxiv_id": "2310.19736v3",
    "entry_id": "http://arxiv.org/abs/2310.19736v3",
    "title": "Evaluating Large Language Models: A Comprehensive Survey",
    "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across a broad spectrum of tasks. They have attracted significant attention and been deployed in numerous downstream applications. Nevertheless, akin to a double-edged sword, LLMs also present potential risks. They could suffer from private data leaks or yield inappropriate, harmful, or misleading content. Additionally, the rapid progress of LLMs raises concerns about the potential emergence of superintelligent systems without adequate safeguards. To effectively capitalize on LLM capacities as well as ensure their safe and beneficial development, it is critical to conduct a rigorous and comprehensive evaluation of LLMs.\n  This survey endeavors to offer a panoramic perspective on the evaluation of LLMs. We categorize the evaluation of LLMs into three major groups: knowledge and capability evaluation, alignment evaluation and safety evaluation. In addition to the comprehensive review on the evaluation methodologies and benchmarks on these three aspects, we collate a compendium of evaluations pertaining to LLMs' performance in specialized domains, and discuss the construction of comprehensive evaluation platforms that cover LLM evaluations on capabilities, alignment, safety, and applicability.\n  We hope that this comprehensive overview will stimulate further research interests in the evaluation of LLMs, with the ultimate goal of making evaluation serve as a cornerstone in guiding the responsible development of LLMs. We envision that this will channel their evolution into a direction that maximizes societal benefit while minimizing potential risks. A curated list of related papers has been publicly available at https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.",
    "authors": [
      "Zishan Guo",
      "Renren Jin",
      "Chuang Liu",
      "Yufei Huang",
      "Dan Shi",
      "Supryadi",
      "Linhao Yu",
      "Yan Liu",
      "Jiaxuan Li",
      "Bojian Xiong",
      "Deyi Xiong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-30T17:00:52Z",
    "pdf_url": "https://arxiv.org/pdf/2310.19736v3"
  },
  {
    "arxiv_id": "2310.19626v1",
    "entry_id": "http://arxiv.org/abs/2310.19626v1",
    "title": "Transformation vs Tradition: Artificial General Intelligence (AGI) for Arts and Humanities",
    "summary": "Recent advances in artificial general intelligence (AGI), particularly large language models and creative image generation systems have demonstrated impressive capabilities on diverse tasks spanning the arts and humanities. However, the swift evolution of AGI has also raised critical questions about its responsible deployment in these culturally significant domains traditionally seen as profoundly human. This paper provides a comprehensive analysis of the applications and implications of AGI for text, graphics, audio, and video pertaining to arts and the humanities. We survey cutting-edge systems and their usage in areas ranging from poetry to history, marketing to film, and communication to classical art. We outline substantial concerns pertaining to factuality, toxicity, biases, and public safety in AGI systems, and propose mitigation strategies. The paper argues for multi-stakeholder collaboration to ensure AGI promotes creativity, knowledge, and cultural values without undermining truth or human dignity. Our timely contribution summarizes a rapidly developing field, highlighting promising directions while advocating for responsible progress centering on human flourishing. The analysis lays the groundwork for further research on aligning AGI's technological capacities with enduring social goods.",
    "authors": [
      "Zhengliang Liu",
      "Yiwei Li",
      "Qian Cao",
      "Junwen Chen",
      "Tianze Yang",
      "Zihao Wu",
      "John Hale",
      "John Gibbs",
      "Khaled Rasheed",
      "Ninghao Liu",
      "Gengchen Mai",
      "Tianming Liu"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-10-30T15:19:15Z",
    "pdf_url": "https://arxiv.org/pdf/2310.19626v1"
  },
  {
    "arxiv_id": "2310.18608v3",
    "entry_id": "http://arxiv.org/abs/2310.18608v3",
    "title": "Embedding in Recommender Systems: A Survey",
    "summary": "Recommender systems have become an essential component of many online platforms, providing personalized recommendations to users. A crucial aspect is embedding techniques that convert the high-dimensional discrete features, such as user and item IDs, into low-dimensional continuous vectors, which can enhance the recommendation performance. Embedding techniques have revolutionized the capture of complex entity relationships, generating significant research interest. This survey presents a comprehensive analysis of recent advances in recommender system embedding techniques. We examine centralized embedding approaches across matrix, sequential, and graph structures. In matrix-based scenarios, collaborative filtering generates embeddings that effectively model user-item preferences, particularly in sparse data environments. For sequential data, we explore various approaches including recurrent neural networks and self-supervised methods such as contrastive and generative learning. In graph-structured contexts, we analyze techniques like node2vec that leverage network relationships, along with applicable self-supervised methods. Our survey addresses critical scalability challenges in embedding methods and explores innovative directions in recommender systems. We introduce emerging approaches, including AutoML, hashing techniques, and quantization methods, to enhance performance while reducing computational complexity. Additionally, we examine the promising role of Large Language Models (LLMs) in embedding enhancement. Through detailed discussion of various architectures and methodologies, this survey aims to provide a thorough overview of state-of-the-art embedding techniques in recommender systems, while highlighting key challenges and future research directions.",
    "authors": [
      "Maolin Wang",
      "Xinjian Zhao",
      "Wanyu Wang",
      "Sheng Zhang",
      "Jiansheng Li",
      "Bowen Yu",
      "Binhao Wang",
      "Shucheng Zhou",
      "Dawei Yin",
      "Qing Li",
      "Ruocheng Guo",
      "Xiangyu Zhao"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2023-10-28T06:31:06Z",
    "pdf_url": "https://arxiv.org/pdf/2310.18608v3"
  },
  {
    "arxiv_id": "2310.17894v3",
    "entry_id": "http://arxiv.org/abs/2310.17894v3",
    "title": "Natural Language Interfaces for Tabular Data Querying and Visualization: A Survey",
    "summary": "The emergence of natural language processing has revolutionized the way users interact with tabular data, enabling a shift from traditional query languages and manual plotting to more intuitive, language-based interfaces. The rise of large language models (LLMs) such as ChatGPT and its successors has further advanced this field, opening new avenues for natural language processing techniques. This survey presents a comprehensive overview of natural language interfaces for tabular data querying and visualization, which allow users to interact with data using natural language queries. We introduce the fundamental concepts and techniques underlying these interfaces with a particular emphasis on semantic parsing, the key technology facilitating the translation from natural language to SQL queries or data visualization commands. We then delve into the recent advancements in Text-to-SQL and Text-to-Vis problems from the perspectives of datasets, methodologies, metrics, and system designs. This includes a deep dive into the influence of LLMs, highlighting their strengths, limitations, and potential for future improvements. Through this survey, we aim to provide a roadmap for researchers and practitioners interested in developing and applying natural language interfaces for data interaction in the era of large language models.",
    "authors": [
      "Weixu Zhang",
      "Yifei Wang",
      "Yuanfeng Song",
      "Victor Junqiu Wei",
      "Yuxing Tian",
      "Yiyan Qi",
      "Jonathan H. Chan",
      "Raymond Chi-Wing Wong",
      "Haiqin Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-27T05:01:20Z",
    "pdf_url": "https://arxiv.org/pdf/2310.17894v3"
  },
  {
    "arxiv_id": "2310.18377v1",
    "entry_id": "http://arxiv.org/abs/2310.18377v1",
    "title": "Large-scale Foundation Models and Generative AI for BigData Neuroscience",
    "summary": "Recent advances in machine learning have made revolutionary breakthroughs in computer games, image and natural language understanding, and scientific discovery. Foundation models and large-scale language models (LLMs) have recently achieved human-like intelligence thanks to BigData. With the help of self-supervised learning (SSL) and transfer learning, these models may potentially reshape the landscapes of neuroscience research and make a significant impact on the future. Here we present a mini-review on recent advances in foundation models and generative AI models as well as their applications in neuroscience, including natural language and speech, semantic memory, brain-machine interfaces (BMIs), and data augmentation. We argue that this paradigm-shift framework will open new avenues for many neuroscience research directions and discuss the accompanying challenges and opportunities.",
    "authors": [
      "Ran Wang",
      "Zhe Sage Chen"
    ],
    "categories": [
      "q-bio.NC",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "cs.MM"
    ],
    "published": "2023-10-27T00:44:40Z",
    "pdf_url": "https://arxiv.org/pdf/2310.18377v1"
  },
  {
    "arxiv_id": "2310.17551v1",
    "entry_id": "http://arxiv.org/abs/2310.17551v1",
    "title": "Unpacking the Ethical Value Alignment in Big Models",
    "summary": "Big models have greatly advanced AI's ability to understand, generate, and manipulate information and content, enabling numerous applications. However, as these models become increasingly integrated into everyday life, their inherent ethical values and potential biases pose unforeseen risks to society. This paper provides an overview of the risks and challenges associated with big models, surveys existing AI ethics guidelines, and examines the ethical implications arising from the limitations of these models. Taking a normative ethics perspective, we propose a reassessment of recent normative guidelines, highlighting the importance of collaborative efforts in academia to establish a unified and universal AI ethics framework. Furthermore, we investigate the moral inclinations of current mainstream LLMs using the Moral Foundation theory, analyze existing alignment algorithms, and outline the unique challenges encountered in aligning ethical values within them. To address these challenges, we introduce a novel conceptual paradigm for aligning the ethical values of big models and discuss promising research directions for alignment criteria, evaluation, and method, representing an initial step towards the interdisciplinary construction of the ethically aligned AI\n  This paper is a modified English version of our Chinese paper https://crad.ict.ac.cn/cn/article/doi/10.7544/issn1000-1239.202330553, intended to help non-Chinese native speakers better understand our work.",
    "authors": [
      "Xiaoyuan Yi",
      "Jing Yao",
      "Xiting Wang",
      "Xing Xie"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-10-26T16:45:40Z",
    "pdf_url": "https://arxiv.org/pdf/2310.17551v1"
  },
  {
    "arxiv_id": "2310.17526v2",
    "entry_id": "http://arxiv.org/abs/2310.17526v2",
    "title": "Can large language models replace humans in the systematic review process? Evaluating GPT-4's efficacy in screening and extracting data from peer-reviewed and grey literature in multiple languages",
    "summary": "Systematic reviews are vital for guiding practice, research, and policy, yet they are often slow and labour-intensive. Large language models (LLMs) could offer a way to speed up and automate systematic reviews, but their performance in such tasks has not been comprehensively evaluated against humans, and no study has tested GPT-4, the biggest LLM so far. This pre-registered study evaluates GPT-4's capability in title/abstract screening, full-text review, and data extraction across various literature types and languages using a 'human-out-of-the-loop' approach. Although GPT-4 had accuracy on par with human performance in most tasks, results were skewed by chance agreement and dataset imbalance. After adjusting for these, there was a moderate level of performance for data extraction, and - barring studies that used highly reliable prompts - screening performance levelled at none to moderate for different stages and languages. When screening full-text literature using highly reliable prompts, GPT-4's performance was 'almost perfect.' Penalising GPT-4 for missing key studies using highly reliable prompts improved its performance even more. Our findings indicate that, currently, substantial caution should be used if LLMs are being used to conduct systematic reviews, but suggest that, for certain systematic review tasks delivered under reliable prompts, LLMs can rival human performance.",
    "authors": [
      "Qusai Khraisha",
      "Sophie Put",
      "Johanna Kappenberg",
      "Azza Warraitch",
      "Kristin Hadfield"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-10-26T16:18:30Z",
    "pdf_url": "https://arxiv.org/pdf/2310.17526v2"
  },
  {
    "arxiv_id": "2310.17168v2",
    "entry_id": "http://arxiv.org/abs/2310.17168v2",
    "title": "Learning an Inventory Control Policy with General Inventory Arrival Dynamics",
    "summary": "In this paper we address the problem of learning and backtesting inventory control policies in the presence of general arrival dynamics -- which we term as a quantity-over-time arrivals model (QOT). We also allow for order quantities to be modified as a post-processing step to meet vendor constraints such as order minimum and batch size constraints -- a common practice in real supply chains. To the best of our knowledge this is the first work to handle either arbitrary arrival dynamics or an arbitrary downstream post-processing of order quantities. Building upon recent work (Madeka et al., 2022) we similarly formulate the periodic review inventory control problem as an exogenous decision process, where most of the state is outside the control of the agent. Madeka et al., 2022 show how to construct a simulator that replays historic data to solve this class of problem. In our case, we incorporate a deep generative model for the arrivals process as part of the history replay. By formulating the problem as an exogenous decision process, we can apply results from Madeka et al., 2022 to obtain a reduction to supervised learning. Via simulation studies we show that this approach yields statistically significant improvements in profitability over production baselines. Using data from a real-world A/B test, we show that Gen-QOT generalizes well to off-policy data and that the resulting buying policy outperforms traditional inventory management systems in real world settings.",
    "authors": [
      "Sohrab Andaz",
      "Carson Eisenach",
      "Dhruv Madeka",
      "Kari Torkkola",
      "Randy Jia",
      "Dean Foster",
      "Sham Kakade"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2023-10-26T05:49:13Z",
    "pdf_url": "https://arxiv.org/pdf/2310.17168v2"
  },
  {
    "arxiv_id": "2310.17064v1",
    "entry_id": "http://arxiv.org/abs/2310.17064v1",
    "title": "math-PVS: A Large Language Model Framework to Map Scientific Publications to PVS Theories",
    "summary": "As artificial intelligence (AI) gains greater adoption in a wide variety of applications, it has immense potential to contribute to mathematical discovery, by guiding conjecture generation, constructing counterexamples, assisting in formalizing mathematics, and discovering connections between different mathematical areas, to name a few.\n  While prior work has leveraged computers for exhaustive mathematical proof search, recent efforts based on large language models (LLMs) aspire to position computing platforms as co-contributors in the mathematical research process. Despite their current limitations in logic and mathematical tasks, there is growing interest in melding theorem proving systems with foundation models. This work investigates the applicability of LLMs in formalizing advanced mathematical concepts and proposes a framework that can critically review and check mathematical reasoning in research papers. Given the noted reasoning shortcomings of LLMs, our approach synergizes the capabilities of proof assistants, specifically PVS, with LLMs, enabling a bridge between textual descriptions in academic papers and formal specifications in PVS. By harnessing the PVS environment, coupled with data ingestion and conversion mechanisms, we envision an automated process, called \\emph{math-PVS}, to extract and formalize mathematical theorems from research papers, offering an innovative tool for academic review and discovery.",
    "authors": [
      "Hassen Saidi",
      "Susmit Jha",
      "Tuhin Sahai"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.LO"
    ],
    "published": "2023-10-25T23:54:04Z",
    "pdf_url": "https://arxiv.org/pdf/2310.17064v1"
  },
  {
    "arxiv_id": "2310.17017v1",
    "entry_id": "http://arxiv.org/abs/2310.17017v1",
    "title": "An Integrative Survey on Mental Health Conversational Agents to Bridge Computer Science and Medical Perspectives",
    "summary": "Mental health conversational agents (a.k.a. chatbots) are widely studied for their potential to offer accessible support to those experiencing mental health challenges. Previous surveys on the topic primarily consider papers published in either computer science or medicine, leading to a divide in understanding and hindering the sharing of beneficial knowledge between both domains. To bridge this gap, we conduct a comprehensive literature review using the PRISMA framework, reviewing 534 papers published in both computer science and medicine. Our systematic review reveals 136 key papers on building mental health-related conversational agents with diverse characteristics of modeling and experimental design techniques. We find that computer science papers focus on LLM techniques and evaluating response quality using automated metrics with little attention to the application while medical papers use rule-based conversational agents and outcome metrics to measure the health outcomes of participants. Based on our findings on transparency, ethics, and cultural heterogeneity in this review, we provide a few recommendations to help bridge the disciplinary divide and enable the cross-disciplinary development of mental health conversational agents.",
    "authors": [
      "Young Min Cho",
      "Sunny Rai",
      "Lyle Ungar",
      "João Sedoc",
      "Sharath Chandra Guntuku"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-25T21:37:57Z",
    "pdf_url": "https://arxiv.org/pdf/2310.17017v1"
  },
  {
    "arxiv_id": "2310.16960v2",
    "entry_id": "http://arxiv.org/abs/2310.16960v2",
    "title": "Privately Aligning Language Models with Reinforcement Learning",
    "summary": "Positioned between pre-training and user deployment, aligning large language models (LLMs) through reinforcement learning (RL) has emerged as a prevailing strategy for training instruction following-models such as ChatGPT. In this work, we initiate the study of privacy-preserving alignment of LLMs through Differential Privacy (DP) in conjunction with RL. Following the influential work of Ziegler et al. (2020), we study two dominant paradigms: (i) alignment via RL without human in the loop (e.g., positive review generation) and (ii) alignment via RL from human feedback (RLHF) (e.g., summarization in a human-preferred way). We give a new DP framework to achieve alignment via RL, and prove its correctness. Our experimental results validate the effectiveness of our approach, offering competitive utility while ensuring strong privacy protections.",
    "authors": [
      "Fan Wu",
      "Huseyin A. Inan",
      "Arturs Backurs",
      "Varun Chandrasekaran",
      "Janardhan Kulkarni",
      "Robert Sim"
    ],
    "categories": [
      "cs.LG",
      "cs.CR"
    ],
    "published": "2023-10-25T19:58:51Z",
    "pdf_url": "https://arxiv.org/pdf/2310.16960v2"
  },
  {
    "arxiv_id": "2310.16535v1",
    "entry_id": "http://arxiv.org/abs/2310.16535v1",
    "title": "R$^3$ Prompting: Review, Rephrase and Resolve for Chain-of-Thought Reasoning in Large Language Models under Noisy Context",
    "summary": "With the help of Chain-of-Thought (CoT) prompting, Large Language Models (LLMs) have achieved remarkable performance on various reasoning tasks. However, most of them have been evaluated under noise-free context and the dilemma for LLMs to produce inaccurate results under the noisy context has not been fully investigated. Existing studies utilize trigger sentences to encourage LLMs to concentrate on the relevant information but the trigger has limited effect on final answer prediction. Inspired by interactive CoT method, where intermediate reasoning steps are promoted by multiple rounds of interaction between users and LLMs, we propose a novel prompting method, namely R$^3$ prompting, for CoT reasoning under noisy context. Specifically, R$^3$ prompting interacts with LLMs to perform key sentence extraction, variable declaration and answer prediction, which corresponds to a thought process of reviewing, rephrasing and resolving. The responses generated at the last interaction will perform as hints to guide toward the responses of the next interaction. Our experiments show that R$^3$ prompting significantly outperforms existing CoT prompting methods on five reasoning tasks under noisy context. With GPT-3.5-turbo, we observe 3.7% accuracy improvement on average on the reasoning tasks under noisy context compared to the most competitive prompting baseline. More analyses and ablation studies show the robustness and generalization of R$^3$ prompting method in solving reasoning tasks in LLMs under noisy context.",
    "authors": [
      "Qingyuan Tian",
      "Hanlun Zhu",
      "Lei Wang",
      "Yang Li",
      "Yunshi Lan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-25T10:34:02Z",
    "pdf_url": "https://arxiv.org/pdf/2310.16535v1"
  },
  {
    "arxiv_id": "2310.16361v1",
    "entry_id": "http://arxiv.org/abs/2310.16361v1",
    "title": "InstructPTS: Instruction-Tuning LLMs for Product Title Summarization",
    "summary": "E-commerce product catalogs contain billions of items. Most products have lengthy titles, as sellers pack them with product attributes to improve retrieval, and highlight key product aspects. This results in a gap between such unnatural products titles, and how customers refer to them. It also limits how e-commerce stores can use these seller-provided titles for recommendation, QA, or review summarization.\n  Inspired by recent work on instruction-tuned LLMs, we present InstructPTS, a controllable approach for the task of Product Title Summarization (PTS). Trained using a novel instruction fine-tuning strategy, our approach is able to summarize product titles according to various criteria (e.g. number of words in a summary, inclusion of specific phrases, etc.). Extensive evaluation on a real-world e-commerce catalog shows that compared to simple fine-tuning of LLMs, our proposed approach can generate more accurate product name summaries, with an improvement of over 14 and 8 BLEU and ROUGE points, respectively.",
    "authors": [
      "Besnik Fetahu",
      "Zhiyu Chen",
      "Oleg Rokhlenko",
      "Shervin Malmasi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-25T04:56:07Z",
    "pdf_url": "https://arxiv.org/pdf/2310.16361v1"
  },
  {
    "arxiv_id": "2310.16218v4",
    "entry_id": "http://arxiv.org/abs/2310.16218v4",
    "title": "Knowledge Editing for Large Language Models: A Survey",
    "summary": "Large language models (LLMs) have recently transformed both the academic and industrial landscapes due to their remarkable capacity to understand, analyze, and generate texts based on their vast knowledge and reasoning ability. Nevertheless, one major drawback of LLMs is their substantial computational cost for pre-training due to their unprecedented amounts of parameters. The disadvantage is exacerbated when new knowledge frequently needs to be introduced into the pre-trained model. Therefore, it is imperative to develop effective and efficient techniques to update pre-trained LLMs. Traditional methods encode new knowledge in pre-trained LLMs through direct fine-tuning. However, naively re-training LLMs can be computationally intensive and risks degenerating valuable pre-trained knowledge irrelevant to the update in the model. Recently, Knowledge-based Model Editing (KME) has attracted increasing attention, which aims to precisely modify the LLMs to incorporate specific knowledge, without negatively influencing other irrelevant knowledge. In this survey, we aim to provide a comprehensive and in-depth overview of recent advances in the field of KME. We first introduce a general formulation of KME to encompass different KME strategies. Afterward, we provide an innovative taxonomy of KME techniques based on how the new knowledge is introduced into pre-trained LLMs, and investigate existing KME strategies while analyzing key insights, advantages, and limitations of methods from each category. Moreover, representative metrics, datasets, and applications of KME are introduced accordingly. Finally, we provide an in-depth analysis regarding the practicality and remaining challenges of KME and suggest promising research directions for further advancement in this field.",
    "authors": [
      "Song Wang",
      "Yaochen Zhu",
      "Haochen Liu",
      "Zaiyi Zheng",
      "Chen Chen",
      "Jundong Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-24T22:18:13Z",
    "pdf_url": "https://arxiv.org/pdf/2310.16218v4"
  },
  {
    "arxiv_id": "2310.16146v1",
    "entry_id": "http://arxiv.org/abs/2310.16146v1",
    "title": "Clinfo.ai: An Open-Source Retrieval-Augmented Large Language Model System for Answering Medical Questions using Scientific Literature",
    "summary": "The quickly-expanding nature of published medical literature makes it challenging for clinicians and researchers to keep up with and summarize recent, relevant findings in a timely manner. While several closed-source summarization tools based on large language models (LLMs) now exist, rigorous and systematic evaluations of their outputs are lacking. Furthermore, there is a paucity of high-quality datasets and appropriate benchmark tasks with which to evaluate these tools. We address these issues with four contributions: we release Clinfo.ai, an open-source WebApp that answers clinical questions based on dynamically retrieved scientific literature; we specify an information retrieval and abstractive summarization task to evaluate the performance of such retrieval-augmented LLM systems; we release a dataset of 200 questions and corresponding answers derived from published systematic reviews, which we name PubMed Retrieval and Synthesis (PubMedRS-200); and report benchmark results for Clinfo.ai and other publicly available OpenQA systems on PubMedRS-200.",
    "authors": [
      "Alejandro Lozano",
      "Scott L Fleming",
      "Chia-Chun Chiang",
      "Nigam Shah"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-10-24T19:43:39Z",
    "pdf_url": "https://arxiv.org/pdf/2310.16146v1"
  },
  {
    "arxiv_id": "2310.15761v1",
    "entry_id": "http://arxiv.org/abs/2310.15761v1",
    "title": "Agent-based models of social behaviour and communication in evacuations: A systematic review",
    "summary": "Most modern agent-based evacuation models involve interactions between evacuees. However, the assumed reasons for interactions and portrayal of them may be overly simple. Research from social psychology suggests that people interact and communicate with one another when evacuating and evacuee response is impacted by the way information is communicated. Thus, we conducted a systematic review of agent-based evacuation models to identify 1) how social interactions and communication approaches between agents are simulated, and 2) what key variables related to evacuation are addressed in these models. We searched Web of Science and ScienceDirect to identify articles that simulated information exchange between agents during evacuations, and social behaviour during evacuations. From the final 70 included articles, we categorised eight types of social interaction that increased in social complexity from collision avoidance to social influence based on strength of social connections with other agents. In the 17 models which simulated communication, we categorised four ways that agents communicate information: spatially through information trails or radii around agents, via social networks and via external communication. Finally, the variables either manipulated or measured in the models were categorised into the following groups: environmental condition, personal attributes of the agents, procedure, and source of information. We discuss promising directions for agent-based evacuation models to capture the effects of communication and group dynamics on evacuee behaviour. Moreover, we demonstrate how communication and group dynamics may impact the variables commonly used in agent-based evacuation models.",
    "authors": [
      "Anne Templeton",
      "Hui Xie",
      "Steve Gwynne",
      "Aoife Hunt",
      "Pete Thompson",
      "Gerta Köster"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2023-10-24T12:08:14Z",
    "pdf_url": "https://arxiv.org/pdf/2310.15761v1"
  },
  {
    "arxiv_id": "2310.15654v1",
    "entry_id": "http://arxiv.org/abs/2310.15654v1",
    "title": "A Survey on Detection of LLMs-Generated Content",
    "summary": "The burgeoning capabilities of advanced large language models (LLMs) such as ChatGPT have led to an increase in synthetic content generation with implications across a variety of sectors, including media, cybersecurity, public discourse, and education. As such, the ability to detect LLMs-generated content has become of paramount importance. We aim to provide a detailed overview of existing detection strategies and benchmarks, scrutinizing their differences and identifying key challenges and prospects in the field, advocating for more adaptable and robust models to enhance detection accuracy. We also posit the necessity for a multi-faceted approach to defend against various attacks to counter the rapidly advancing capabilities of LLMs. To the best of our knowledge, this work is the first comprehensive survey on the detection in the era of LLMs. We hope it will provide a broad understanding of the current landscape of LLMs-generated content detection, offering a guiding reference for researchers and practitioners striving to uphold the integrity of digital information in an era increasingly dominated by synthetic content. The relevant papers are summarized and will be consistently updated at https://github.com/Xianjun-Yang/Awesome_papers_on_LLMs_detection.git.",
    "authors": [
      "Xianjun Yang",
      "Liangming Pan",
      "Xuandong Zhao",
      "Haifeng Chen",
      "Linda Petzold",
      "William Yang Wang",
      "Wei Cheng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC",
      "cs.LG"
    ],
    "published": "2023-10-24T09:10:26Z",
    "pdf_url": "https://arxiv.org/pdf/2310.15654v1"
  },
  {
    "arxiv_id": "2310.18358v1",
    "entry_id": "http://arxiv.org/abs/2310.18358v1",
    "title": "A Communication Theory Perspective on Prompting Engineering Methods for Large Language Models",
    "summary": "The springing up of Large Language Models (LLMs) has shifted the community from single-task-orientated natural language processing (NLP) research to a holistic end-to-end multi-task learning paradigm. Along this line of research endeavors in the area, LLM-based prompting methods have attracted much attention, partially due to the technological advantages brought by prompt engineering (PE) as well as the underlying NLP principles disclosed by various prompting methods. Traditional supervised learning usually requires training a model based on labeled data and then making predictions. In contrast, PE methods directly use the powerful capabilities of existing LLMs (i.e., GPT-3 and GPT-4) via composing appropriate prompts, especially under few-shot or zero-shot scenarios. Facing the abundance of studies related to the prompting and the ever-evolving nature of this field, this article aims to (i) illustrate a novel perspective to review existing PE methods, within the well-established communication theory framework; (ii) facilitate a better/deeper understanding of developing trends of existing PE methods used in four typical tasks; (iii) shed light on promising research directions for future PE methods.",
    "authors": [
      "Yuanfeng Song",
      "Yuanqin He",
      "Xuefang Zhao",
      "Hanlin Gu",
      "Di Jiang",
      "Haijun Yang",
      "Lixin Fan",
      "Qiang Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-24T03:05:21Z",
    "pdf_url": "https://arxiv.org/pdf/2310.18358v1"
  },
  {
    "arxiv_id": "2310.15264v1",
    "entry_id": "http://arxiv.org/abs/2310.15264v1",
    "title": "Towards Possibilities & Impossibilities of AI-generated Text Detection: A Survey",
    "summary": "Large Language Models (LLMs) have revolutionized the domain of natural language processing (NLP) with remarkable capabilities of generating human-like text responses. However, despite these advancements, several works in the existing literature have raised serious concerns about the potential misuse of LLMs such as spreading misinformation, generating fake news, plagiarism in academia, and contaminating the web. To address these concerns, a consensus among the research community is to develop algorithmic solutions to detect AI-generated text. The basic idea is that whenever we can tell if the given text is either written by a human or an AI, we can utilize this information to address the above-mentioned concerns. To that end, a plethora of detection frameworks have been proposed, highlighting the possibilities of AI-generated text detection. But in parallel to the development of detection frameworks, researchers have also concentrated on designing strategies to elude detection, i.e., focusing on the impossibilities of AI-generated text detection. This is a crucial step in order to make sure the detection frameworks are robust enough and it is not too easy to fool a detector. Despite the huge interest and the flurry of research in this domain, the community currently lacks a comprehensive analysis of recent developments. In this survey, we aim to provide a concise categorization and overview of current work encompassing both the prospects and the limitations of AI-generated text detection. To enrich the collective knowledge, we engage in an exhaustive discussion on critical and challenging open questions related to ongoing research on AI-generated text detection.",
    "authors": [
      "Soumya Suvra Ghosal",
      "Souradip Chakraborty",
      "Jonas Geiping",
      "Furong Huang",
      "Dinesh Manocha",
      "Amrit Singh Bedi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-23T18:11:32Z",
    "pdf_url": "https://arxiv.org/pdf/2310.15264v1"
  },
  {
    "arxiv_id": "2310.14735v6",
    "entry_id": "http://arxiv.org/abs/2310.14735v6",
    "title": "Unleashing the potential of prompt engineering for large language models",
    "summary": "This comprehensive review delves into the pivotal role of prompt engineering in unleashing the capabilities of Large Language Models (LLMs). The development of Artificial Intelligence (AI), from its inception in the 1950s to the emergence of advanced neural networks and deep learning architectures, has made a breakthrough in LLMs, with models such as GPT-4o and Claude-3, and in Vision-Language Models (VLMs), with models such as CLIP and ALIGN. Prompt engineering is the process of structuring inputs, which has emerged as a crucial technique to maximize the utility and accuracy of these models. This paper explores both foundational and advanced methodologies of prompt engineering, including techniques such as self-consistency, chain-of-thought, and generated knowledge, which significantly enhance model performance. Additionally, it examines the prompt method of VLMs through innovative approaches such as Context Optimization (CoOp), Conditional Context Optimization (CoCoOp), and Multimodal Prompt Learning (MaPLe). Critical to this discussion is the aspect of AI security, particularly adversarial attacks that exploit vulnerabilities in prompt engineering. Strategies to mitigate these risks and enhance model robustness are thoroughly reviewed. The evaluation of prompt methods is also addressed through both subjective and objective metrics, ensuring a robust analysis of their efficacy. This review also reflects the essential role of prompt engineering in advancing AI capabilities, providing a structured framework for future research and application.",
    "authors": [
      "Banghao Chen",
      "Zhaofeng Zhang",
      "Nicolas Langrené",
      "Shengxin Zhu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-23T09:15:18Z",
    "pdf_url": "https://arxiv.org/pdf/2310.14735v6"
  },
  {
    "arxiv_id": "2310.14724v3",
    "entry_id": "http://arxiv.org/abs/2310.14724v3",
    "title": "A Survey on LLM-Generated Text Detection: Necessity, Methods, and Future Directions",
    "summary": "The powerful ability to understand, follow, and generate complex language emerging from large language models (LLMs) makes LLM-generated text flood many areas of our daily lives at an incredible speed and is widely accepted by humans. As LLMs continue to expand, there is an imperative need to develop detectors that can detect LLM-generated text. This is crucial to mitigate potential misuse of LLMs and safeguard realms like artistic expression and social networks from harmful influence of LLM-generated content. The LLM-generated text detection aims to discern if a piece of text was produced by an LLM, which is essentially a binary classification task. The detector techniques have witnessed notable advancements recently, propelled by innovations in watermarking techniques, statistics-based detectors, neural-base detectors, and human-assisted methods. In this survey, we collate recent research breakthroughs in this area and underscore the pressing need to bolster detector research. We also delve into prevalent datasets, elucidating their limitations and developmental requirements. Furthermore, we analyze various LLM-generated text detection paradigms, shedding light on challenges like out-of-distribution problems, potential attacks, real-world data issues and the lack of effective evaluation framework. Conclusively, we highlight interesting directions for future research in LLM-generated text detection to advance the implementation of responsible artificial intelligence (AI). Our aim with this survey is to provide a clear and comprehensive introduction for newcomers while also offering seasoned researchers a valuable update in the field of LLM-generated text detection. The useful resources are publicly available at: https://github.com/NLP2CT/LLM-generated-Text-Detection.",
    "authors": [
      "Junchao Wu",
      "Shu Yang",
      "Runzhe Zhan",
      "Yulin Yuan",
      "Derek F. Wong",
      "Lidia S. Chao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-23T09:01:13Z",
    "pdf_url": "https://arxiv.org/pdf/2310.14724v3"
  },
  {
    "arxiv_id": "2310.14414v2",
    "entry_id": "http://arxiv.org/abs/2310.14414v2",
    "title": "Vision Language Models in Autonomous Driving: A Survey and Outlook",
    "summary": "The applications of Vision-Language Models (VLMs) in the field of Autonomous Driving (AD) have attracted widespread attention due to their outstanding performance and the ability to leverage Large Language Models (LLMs). By incorporating language data, driving systems can gain a better understanding of real-world environments, thereby enhancing driving safety and efficiency. In this work, we present a comprehensive and systematic survey of the advances in vision language models in this domain, encompassing perception and understanding, navigation and planning, decision-making and control, end-to-end autonomous driving, and data generation. We introduce the mainstream VLM tasks in AD and the commonly utilized metrics. Additionally, we review current studies and applications in various areas and summarize the existing language-enhanced autonomous driving datasets thoroughly. Lastly, we discuss the benefits and challenges of VLMs in AD and provide researchers with the current research gaps and future trends.",
    "authors": [
      "Xingcheng Zhou",
      "Mingyu Liu",
      "Ekim Yurtsever",
      "Bare Luka Zagar",
      "Walter Zimmer",
      "Hu Cao",
      "Alois C. Knoll"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-10-22T21:06:10Z",
    "pdf_url": "https://arxiv.org/pdf/2310.14414v2"
  },
  {
    "arxiv_id": "2310.18345v1",
    "entry_id": "http://arxiv.org/abs/2310.18345v1",
    "title": "A Survey on Semantic Processing Techniques",
    "summary": "Semantic processing is a fundamental research domain in computational linguistics. In the era of powerful pre-trained language models and large language models, the advancement of research in this domain appears to be decelerating. However, the study of semantics is multi-dimensional in linguistics. The research depth and breadth of computational semantic processing can be largely improved with new technologies. In this survey, we analyzed five semantic processing tasks, e.g., word sense disambiguation, anaphora resolution, named entity recognition, concept extraction, and subjectivity detection. We study relevant theoretical research in these fields, advanced methods, and downstream applications. We connect the surveyed tasks with downstream applications because this may inspire future scholars to fuse these low-level semantic processing tasks with high-level natural language processing tasks. The review of theoretical research may also inspire new tasks and technologies in the semantic processing domain. Finally, we compare the different semantic processing techniques and summarize their technical trends, application trends, and future directions.",
    "authors": [
      "Rui Mao",
      "Kai He",
      "Xulang Zhang",
      "Guanyi Chen",
      "Jinjie Ni",
      "Zonglin Yang",
      "Erik Cambria"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-22T15:09:51Z",
    "pdf_url": "https://arxiv.org/pdf/2310.18345v1"
  },
  {
    "arxiv_id": "2310.13855v1",
    "entry_id": "http://arxiv.org/abs/2310.13855v1",
    "title": "Evoke: Evoking Critical Thinking Abilities in LLMs via Reviewer-Author Prompt Editing",
    "summary": "Large language models (LLMs) have made impressive progress in natural language processing. These models rely on proper human instructions (or prompts) to generate suitable responses. However, the potential of LLMs are not fully harnessed by commonly-used prompting methods: many human-in-the-loop algorithms employ ad-hoc procedures for prompt selection; while auto prompt generation approaches are essentially searching all possible prompts randomly and inefficiently. We propose Evoke, an automatic prompt refinement framework. In Evoke, there are two instances of a same LLM: one as a reviewer (LLM-Reviewer), it scores the current prompt; the other as an author (LLM-Author), it edits the prompt by considering the edit history and the reviewer's feedback. Such an author-reviewer feedback loop ensures that the prompt is refined in each iteration. We further aggregate a data selection approach to Evoke, where only the hard samples are exposed to the LLM. The hard samples are more important because the LLM can develop deeper understanding of the tasks out of them, while the model may already know how to solve the easier cases. Experimental results show that Evoke significantly outperforms existing methods. For instance, in the challenging task of logical fallacy detection, Evoke scores above 80, while all other baseline methods struggle to reach 20.",
    "authors": [
      "Xinyu Hu",
      "Pengfei Tang",
      "Simiao Zuo",
      "Zihan Wang",
      "Bowen Song",
      "Qiang Lou",
      "Jian Jiao",
      "Denis Charles"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-20T23:15:59Z",
    "pdf_url": "https://arxiv.org/pdf/2310.13855v1"
  },
  {
    "arxiv_id": "2310.13343v1",
    "entry_id": "http://arxiv.org/abs/2310.13343v1",
    "title": "Challenges and Contributing Factors in the Utilization of Large Language Models (LLMs)",
    "summary": "With the development of large language models (LLMs) like the GPT series, their widespread use across various application scenarios presents a myriad of challenges. This review initially explores the issue of domain specificity, where LLMs may struggle to provide precise answers to specialized questions within niche fields. The problem of knowledge forgetting arises as these LLMs might find it hard to balance old and new information. The knowledge repetition phenomenon reveals that sometimes LLMs might deliver overly mechanized responses, lacking depth and originality. Furthermore, knowledge illusion describes situations where LLMs might provide answers that seem insightful but are actually superficial, while knowledge toxicity focuses on harmful or biased information outputs. These challenges underscore problems in the training data and algorithmic design of LLMs. To address these issues, it's suggested to diversify training data, fine-tune models, enhance transparency and interpretability, and incorporate ethics and fairness training. Future technological trends might lean towards iterative methodologies, multimodal learning, model personalization and customization, and real-time learning and feedback mechanisms. In conclusion, future LLMs should prioritize fairness, transparency, and ethics, ensuring they uphold high moral and ethical standards when serving humanity.",
    "authors": [
      "Xiaoliang Chen",
      "Liangbin Li",
      "Le Chang",
      "Yunhe Huang",
      "Yuxuan Zhao",
      "Yuxiao Zhang",
      "Dinuo Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-20T08:13:36Z",
    "pdf_url": "https://arxiv.org/pdf/2310.13343v1"
  },
  {
    "arxiv_id": "2310.12459v1",
    "entry_id": "http://arxiv.org/abs/2310.12459v1",
    "title": "Affective Conversational Agents: Understanding Expectations and Personal Influences",
    "summary": "The rise of AI conversational agents has broadened opportunities to enhance human capabilities across various domains. As these agents become more prevalent, it is crucial to investigate the impact of different affective abilities on their performance and user experience. In this study, we surveyed 745 respondents to understand the expectations and preferences regarding affective skills in various applications. Specifically, we assessed preferences concerning AI agents that can perceive, respond to, and simulate emotions across 32 distinct scenarios. Our results indicate a preference for scenarios that involve human interaction, emotional support, and creative tasks, with influences from factors such as emotional reappraisal and personality traits. Overall, the desired affective skills in AI agents depend largely on the application's context and nature, emphasizing the need for adaptability and context-awareness in the design of affective AI conversational agents.",
    "authors": [
      "Javier Hernandez",
      "Jina Suh",
      "Judith Amores",
      "Kael Rowan",
      "Gonzalo Ramos",
      "Mary Czerwinski"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-10-19T04:33:18Z",
    "pdf_url": "https://arxiv.org/pdf/2310.12459v1"
  },
  {
    "arxiv_id": "2310.11829v4",
    "entry_id": "http://arxiv.org/abs/2310.11829v4",
    "title": "Graph Foundation Models: Concepts, Opportunities and Challenges",
    "summary": "Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is witnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation models in generalization and adaptation motivate graph machine learning researchers to discuss the potential of developing a new graph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various graph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this new domain. To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation of their key characteristics and underlying technologies. We proceed to classify the existing work related to GFMs into three distinct categories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough review of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.",
    "authors": [
      "Jiawei Liu",
      "Cheng Yang",
      "Zhiyuan Lu",
      "Junze Chen",
      "Yibo Li",
      "Mengmei Zhang",
      "Ting Bai",
      "Yuan Fang",
      "Lichao Sun",
      "Philip S. Yu",
      "Chuan Shi"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-10-18T09:31:21Z",
    "pdf_url": "https://arxiv.org/pdf/2310.11829v4"
  },
  {
    "arxiv_id": "2310.11703v2",
    "entry_id": "http://arxiv.org/abs/2310.11703v2",
    "title": "A Comprehensive Survey on Vector Database: Storage and Retrieval Technique, Challenge",
    "summary": "Vector databases (VDBs) have emerged to manage high-dimensional data that exceed the capabilities of traditional database management systems, and are now tightly integrated with large language models as well as widely applied in modern artificial intelligence systems. Although relatively few studies describe existing or introduce new vector database architectures, the core technologies underlying VDBs, such as approximate nearest neighbor search, have been extensively studied and are well documented in the literature. In this work, we present a comprehensive review of the relevant algorithms to provide a general understanding of this booming research area. Specifically, we first provide a review of storage and retrieval techniques in VDBs, with detailed design principles and technological evolution. Then, we conduct an in-depth comparison of several advanced VDB solutions with their strengths, limitations, and typical application scenarios. Finally, we also outline emerging opportunities for coupling VDBs with large language models, including open research problems and trends, such as novel indexing strategies. This survey aims to serve as a practical resource, enabling readers to quickly gain an overall understanding of the current knowledge landscape in this rapidly developing area.",
    "authors": [
      "Le Ma",
      "Ran Zhang",
      "Yikun Han",
      "Shirui Yu",
      "Zaitian Wang",
      "Zhiyuan Ning",
      "Jinghan Zhang",
      "Ping Xu",
      "Pengjiang Li",
      "Wei Ju",
      "Chong Chen",
      "Dongjie Wang",
      "Kunpeng Liu",
      "Pengyang Wang",
      "Pengfei Wang",
      "Yanjie Fu",
      "Chunjiang Liu",
      "Yuanchun Zhou",
      "Chang-Tien Lu"
    ],
    "categories": [
      "cs.DB",
      "cs.AI"
    ],
    "published": "2023-10-18T04:31:06Z",
    "pdf_url": "https://arxiv.org/pdf/2310.11703v2"
  },
  {
    "arxiv_id": "2310.11501v1",
    "entry_id": "http://arxiv.org/abs/2310.11501v1",
    "title": "CoMPosT: Characterizing and Evaluating Caricature in LLM Simulations",
    "summary": "Recent work has aimed to capture nuances of human behavior by using LLMs to simulate responses from particular demographics in settings like social science experiments and public opinion surveys. However, there are currently no established ways to discuss or evaluate the quality of such LLM simulations. Moreover, there is growing concern that these LLM simulations are flattened caricatures of the personas that they aim to simulate, failing to capture the multidimensionality of people and perpetuating stereotypes. To bridge these gaps, we present CoMPosT, a framework to characterize LLM simulations using four dimensions: Context, Model, Persona, and Topic. We use this framework to measure open-ended LLM simulations' susceptibility to caricature, defined via two criteria: individuation and exaggeration. We evaluate the level of caricature in scenarios from existing work on LLM simulations. We find that for GPT-4, simulations of certain demographics (political and marginalized groups) and topics (general, uncontroversial) are highly susceptible to caricature.",
    "authors": [
      "Myra Cheng",
      "Tiziano Piccardi",
      "Diyi Yang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2023-10-17T18:00:25Z",
    "pdf_url": "https://arxiv.org/pdf/2310.11501v1"
  },
  {
    "arxiv_id": "2310.11207v1",
    "entry_id": "http://arxiv.org/abs/2310.11207v1",
    "title": "Can Large Language Models Explain Themselves? A Study of LLM-Generated Self-Explanations",
    "summary": "Large language models (LLMs) such as ChatGPT have demonstrated superior performance on a variety of natural language processing (NLP) tasks including sentiment analysis, mathematical reasoning and summarization. Furthermore, since these models are instruction-tuned on human conversations to produce \"helpful\" responses, they can and often will produce explanations along with the response, which we call self-explanations. For example, when analyzing the sentiment of a movie review, the model may output not only the positivity of the sentiment, but also an explanation (e.g., by listing the sentiment-laden words such as \"fantastic\" and \"memorable\" in the review). How good are these automatically generated self-explanations? In this paper, we investigate this question on the task of sentiment analysis and for feature attribution explanation, one of the most commonly studied settings in the interpretability literature (for pre-ChatGPT models). Specifically, we study different ways to elicit the self-explanations, evaluate their faithfulness on a set of evaluation metrics, and compare them to traditional explanation methods such as occlusion or LIME saliency maps. Through an extensive set of experiments, we find that ChatGPT's self-explanations perform on par with traditional ones, but are quite different from them according to various agreement metrics, meanwhile being much cheaper to produce (as they are generated along with the prediction). In addition, we identified several interesting characteristics of them, which prompt us to rethink many current model interpretability practices in the era of ChatGPT(-like) LLMs.",
    "authors": [
      "Shiyuan Huang",
      "Siddarth Mamidanna",
      "Shreedhar Jangam",
      "Yilun Zhou",
      "Leilani H. Gilpin"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-10-17T12:34:32Z",
    "pdf_url": "https://arxiv.org/pdf/2310.11207v1"
  },
  {
    "arxiv_id": "2311.03366v4",
    "entry_id": "http://arxiv.org/abs/2311.03366v4",
    "title": "Functional Overlap Reranking for Neural Code Generation",
    "summary": "Code Large Language Models (CodeLLMs) have ushered in a new era in code generation advancements. However, selecting the best code solutions from all possible CodeLLM outputs remains a challenge. Previous methods often overlooked the intricate functional similarities and interactions between solution clusters. We introduce SRank, a novel reranking strategy for selecting the best solutions from code generation, focusing on modeling the relationships between clusters of solutions. By quantifying the functional overlap between solution clusters, our approach provides a better ranking strategy for code solutions. Empirical results show that our method achieves remarkable results on the pass@1 score. For instance, on the Human-Eval benchmark, we achieve 69.66% in pass@1 with Codex002, 75.31% with WizardCoder, 53.99% with StarCoder, and 60.55% with CodeGen, surpassing state-of-the-art code generation reranking methods such as CodeT and Coder-Reviewer on the same CodeLLM by a significant margin (approximately 6.1% improvement on average). Even in scenarios with a limited number of sampled solutions and test cases, our approach demonstrates robustness and superiority, marking a new benchmark in code generation reranking. Our implementation can be found at https://github.com/FSoft-AI4Code/SRank-CodeRanker.",
    "authors": [
      "Hung Quoc To",
      "Minh Huynh Nguyen",
      "Nghi D. Q. Bui"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-10-16T22:20:31Z",
    "pdf_url": "https://arxiv.org/pdf/2311.03366v4"
  },
  {
    "arxiv_id": "2310.10844v1",
    "entry_id": "http://arxiv.org/abs/2310.10844v1",
    "title": "Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks",
    "summary": "Large Language Models (LLMs) are swiftly advancing in architecture and capability, and as they integrate more deeply into complex systems, the urgency to scrutinize their security properties grows. This paper surveys research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, combining the perspectives of Natural Language Processing and Security. Prior work has shown that even safety-aligned LLMs (via instruction tuning and reinforcement learning through human feedback) can be susceptible to adversarial attacks, which exploit weaknesses and mislead AI systems, as evidenced by the prevalence of `jailbreak' attacks on models like ChatGPT and Bard. In this survey, we first provide an overview of large language models, describe their safety alignment, and categorize existing research based on various learning structures: textual-only attacks, multi-modal attacks, and additional attack methods specifically targeting complex systems, such as federated learning or multi-agent systems. We also offer comprehensive remarks on works that focus on the fundamental sources of vulnerabilities and potential defenses. To make this field more accessible to newcomers, we present a systematic review of existing works, a structured typology of adversarial attack concepts, and additional resources, including slides for presentations on related topics at the 62nd Annual Meeting of the Association for Computational Linguistics (ACL'24).",
    "authors": [
      "Erfan Shayegani",
      "Md Abdullah Al Mamun",
      "Yu Fu",
      "Pedram Zaree",
      "Yue Dong",
      "Nael Abu-Ghazaleh"
    ],
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2023-10-16T21:37:24Z",
    "pdf_url": "https://arxiv.org/pdf/2310.10844v1"
  },
  {
    "arxiv_id": "2310.10196v2",
    "entry_id": "http://arxiv.org/abs/2310.10196v2",
    "title": "Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook",
    "summary": "Temporal data, notably time series and spatio-temporal data, are prevalent in real-world applications. They capture dynamic system measurements and are produced in vast quantities by both physical and virtual sensors. Analyzing these data types is vital to harnessing the rich information they encompass and thus benefits a wide range of downstream tasks. Recent advances in large language and other foundational models have spurred increased use of these models in time series and spatio-temporal data mining. Such methodologies not only enable enhanced pattern recognition and reasoning across diverse domains but also lay the groundwork for artificial general intelligence capable of comprehending and processing common temporal data. In this survey, we offer a comprehensive and up-to-date review of large models tailored (or adapted) for time series and spatio-temporal data, spanning four key facets: data types, model categories, model scopes, and application areas/tasks. Our objective is to equip practitioners with the knowledge to develop applications and further research in this underexplored domain. We primarily categorize the existing literature into two major clusters: large models for time series analysis (LM4TS) and spatio-temporal data mining (LM4STD). On this basis, we further classify research based on model scopes (i.e., general vs. domain-specific) and application areas/tasks. We also provide a comprehensive collection of pertinent resources, including datasets, model assets, and useful tools, categorized by mainstream applications. This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.",
    "authors": [
      "Ming Jin",
      "Qingsong Wen",
      "Yuxuan Liang",
      "Chaoli Zhang",
      "Siqiao Xue",
      "Xue Wang",
      "James Zhang",
      "Yi Wang",
      "Haifeng Chen",
      "Xiaoli Li",
      "Shirui Pan",
      "Vincent S. Tseng",
      "Yu Zheng",
      "Lei Chen",
      "Hui Xiong"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-10-16T09:06:00Z",
    "pdf_url": "https://arxiv.org/pdf/2310.10196v2"
  },
  {
    "arxiv_id": "2310.09765v2",
    "entry_id": "http://arxiv.org/abs/2310.09765v2",
    "title": "MILPaC: A Novel Benchmark for Evaluating Translation of Legal Text to Indian Languages",
    "summary": "Most legal text in the Indian judiciary is written in complex English due to historical reasons. However, only a small fraction of the Indian population is comfortable in reading English. Hence legal text needs to be made available in various Indian languages, possibly by translating the available legal text from English. Though there has been a lot of research on translation to and between Indian languages, to our knowledge, there has not been much prior work on such translation in the legal domain. In this work, we construct the first high-quality legal parallel corpus containing aligned text units in English and nine Indian languages, that includes several low-resource languages. We also benchmark the performance of a wide variety of Machine Translation (MT) systems over this corpus, including commercial MT systems, open-source MT systems and Large Language Models. Through a comprehensive survey by Law practitioners, we check how satisfied they are with the translations by some of these MT systems, and how well automatic MT evaluation metrics agree with the opinions of Law practitioners.",
    "authors": [
      "Sayan Mahapatra",
      "Debtanu Datta",
      "Shubham Soni",
      "Adrijit Goswami",
      "Saptarshi Ghosh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-15T07:49:56Z",
    "pdf_url": "https://arxiv.org/pdf/2310.09765v2"
  },
  {
    "arxiv_id": "2310.09685v1",
    "entry_id": "http://arxiv.org/abs/2310.09685v1",
    "title": "Generative artificial intelligence for de novo protein design",
    "summary": "Engineering new molecules with desirable functions and properties has the potential to extend our ability to engineer proteins beyond what nature has so far evolved. Advances in the so-called \"de novo\" design problem have recently been brought forward by developments in artificial intelligence. Generative architectures, such as language models and diffusion processes, seem adept at generating novel, yet realistic proteins that display desirable properties and perform specified functions. State-of-the-art design protocols now achieve experimental success rates nearing 20%, thus widening the access to de novo designed proteins. Despite extensive progress, there are clear field-wide challenges, for example in determining the best in silico metrics to prioritise designs for experimental testing, and in designing proteins that can undergo large conformational changes or be regulated by post-translational modifications and other cellular processes. With an increase in the number of models being developed, this review provides a framework to understand how these tools fit into the overall process of de novo protein design. Throughout, we highlight the power of incorporating biochemical knowledge to improve performance and interpretability.",
    "authors": [
      "Adam Winnifrith",
      "Carlos Outeiral",
      "Brian Hie"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "q-bio.BM"
    ],
    "published": "2023-10-15T00:02:22Z",
    "pdf_url": "https://arxiv.org/pdf/2310.09685v1"
  },
  {
    "arxiv_id": "2310.09411v1",
    "entry_id": "http://arxiv.org/abs/2310.09411v1",
    "title": "Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review",
    "summary": "In recent years, deep learning has revolutionized natural language processing (NLP) by enabling the development of models that can learn complex representations of language data, leading to significant improvements in performance across a wide range of NLP tasks. Deep learning models for NLP typically use large amounts of data to train deep neural networks, allowing them to learn the patterns and relationships in language data. This is in contrast to traditional NLP approaches, which rely on hand-engineered features and rules to perform NLP tasks. The ability of deep neural networks to learn hierarchical representations of language data, handle variable-length input sequences, and perform well on large datasets makes them well-suited for NLP applications. Driven by the exponential growth of textual data and the increasing demand for condensed, coherent, and informative summaries, text summarization has been a critical research area in the field of NLP. Applying deep learning to text summarization refers to the use of deep neural networks to perform text summarization tasks. In this survey, we begin with a review of fashionable text summarization tasks in recent years, including extractive, abstractive, multi-document, and so on. Next, we discuss most deep learning-based models and their experimental results on these tasks. The paper also covers datasets and data representation for summarization tasks. Finally, we delve into the opportunities and challenges associated with summarization tasks and their corresponding methodologies, aiming to inspire future research efforts to advance the field further. A goal of our survey is to explain how these methods differ in their requirements as understanding them is essential for choosing a technique suited for a specific setting.",
    "authors": [
      "Guanghua Wang",
      "Weili Wu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-10-13T21:24:37Z",
    "pdf_url": "https://arxiv.org/pdf/2310.09411v1"
  },
  {
    "arxiv_id": "2310.08184v2",
    "entry_id": "http://arxiv.org/abs/2310.08184v2",
    "title": "Learning from models beyond fine-tuning",
    "summary": "Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: model tuning, model distillation, model reuse, meta learning and model editing. Each category encompasses a repertoire of methods and strategies that aim to enhance the capabilities and performance of FM. This paper gives a comprehensive review of the current methods based on FM from the perspective of LFM, in order to help readers better understand the current research status and ideas. To conclude, we summarize the survey by highlighting several critical areas for future exploration and addressing open issues that require further attention from the research community. The relevant papers we investigated in this article can be accessed at https://github.com/ruthless-man/Awesome-Learn-from-Model",
    "authors": [
      "Hongling Zheng",
      "Li Shen",
      "Anke Tang",
      "Yong Luo",
      "Han Hu",
      "Bo Du",
      "Yonggang Wen",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-10-12T10:20:36Z",
    "pdf_url": "https://arxiv.org/pdf/2310.08184v2"
  },
  {
    "arxiv_id": "2310.08109v1",
    "entry_id": "http://arxiv.org/abs/2310.08109v1",
    "title": "Overview of Physics-Informed Machine Learning Inversion of Geophysical Data",
    "summary": "We review four types of algorithms for physics-informed machine learning (PIML) inversion of geophysical data. The unifying equation is given by the joint objective function $ε$:\n  \\begin{eqnarray} ε^{||-PIML}&=&λ_1 \\overbrace{||{\\bf W}^{ML}({\\bf H}_{\\bf w} {\\bf d}^{obs}-{\\bf m})||^2}^{NN} + λ_2 \\overbrace{{||{\\bf W}^{FWI}({\\bf L} {\\bf m}-{\\bf d}^{obs})||^2}}^{FWI} ~+ \\nonumber\\\\ \\nonumber\\\\ && + ~~Regularizer, \\label{PIML.eq120} \\end{eqnarray}where the optimal model ${\\bf m}^*$ and weights $\\bf w^*$ minimize $ε$. Here, The matrix weights are given by the boldface symbol $\\bf W$, and full waveform inversion (FWI) is typically computed using a finite-difference solution of the wave equation, where $\\bf L$ represents the forward modeling operation of the wave equation as a function of the model $\\bf m$. Also, a fully-connected neural network (NN) is used to compute the model ${\\bf H_w}{\\bf d}^{obs} \\approx \\bf m$ from the observed input data ${\\bf d}^{obs}$. The selection of weights $λ_i$ and the NN operations determine one of four different PIML algorithms.\n  PIML offers potential advantages over standard FWI through its enhanced ability to avoid local minima and the option to locally train the inversion operator, minimizing the requirement for extensive training data for global applicability. However, the effectiveness of PIML relies on the similarity between the test and trained data. Nevertheless, a possible strategy to overcome this limitation involves initial pretraining of a PIML architecture with data from a broader region, followed by fine-tuning for specific data-a method reminiscent of the way large language models are pretrained and adapted for various tasks.",
    "authors": [
      "Gerard T. Schuster",
      "Shihang Feng"
    ],
    "categories": [
      "physics.geo-ph",
      "cs.LG"
    ],
    "published": "2023-10-12T08:10:31Z",
    "pdf_url": "https://arxiv.org/pdf/2310.08109v1"
  },
  {
    "arxiv_id": "2310.07637v5",
    "entry_id": "http://arxiv.org/abs/2310.07637v5",
    "title": "OpsEval: A Comprehensive IT Operations Benchmark Suite for Large Language Models",
    "summary": "Information Technology (IT) Operations (Ops), particularly Artificial Intelligence for IT Operations (AIOps), is the guarantee for maintaining the orderly and stable operation of existing information systems. According to Gartner's prediction, the use of AI technology for automated IT operations has become a new trend. Large language models (LLMs) that have exhibited remarkable capabilities in NLP-related tasks, are showing great potential in the field of AIOps, such as in aspects of root cause analysis of failures, generation of operations and maintenance scripts, and summarizing of alert information. Nevertheless, the performance of current LLMs in Ops tasks is yet to be determined. In this paper, we present OpsEval, a comprehensive task-oriented Ops benchmark designed for LLMs. For the first time, OpsEval assesses LLMs' proficiency in various crucial scenarios at different ability levels. The benchmark includes 7184 multi-choice questions and 1736 question-answering (QA) formats in English and Chinese. By conducting a comprehensive performance evaluation of the current leading large language models, we show how various LLM techniques can affect the performance of Ops, and discussed findings related to various topics, including model quantification, QA evaluation, and hallucination issues. To ensure the credibility of our evaluation, we invite dozens of domain experts to manually review our questions. At the same time, we have open-sourced 20% of the test QA to assist current researchers in preliminary evaluations of their OpsLLM models. The remaining 80% of the data, which is not disclosed, is used to eliminate the issue of the test set leakage. Additionally, we have constructed an online leaderboard that is updated in real-time and will continue to be updated, ensuring that any newly emerging LLMs will be evaluated promptly. Both our dataset and leaderboard have been made public.",
    "authors": [
      "Yuhe Liu",
      "Changhua Pei",
      "Longlong Xu",
      "Bohan Chen",
      "Mingze Sun",
      "Zhirui Zhang",
      "Yongqian Sun",
      "Shenglin Zhang",
      "Kun Wang",
      "Haiming Zhang",
      "Jianhui Li",
      "Gaogang Xie",
      "Xidao Wen",
      "Xiaohui Nie",
      "Minghua Ma",
      "Dan Pei"
    ],
    "categories": [
      "cs.AI",
      "cs.NI"
    ],
    "published": "2023-10-11T16:33:29Z",
    "pdf_url": "https://arxiv.org/pdf/2310.07637v5"
  },
  {
    "arxiv_id": "2310.07745v3",
    "entry_id": "http://arxiv.org/abs/2310.07745v3",
    "title": "Deep Reinforcement Learning for Autonomous Cyber Defence: A Survey",
    "summary": "The rapid increase in the number of cyber-attacks in recent years raises the need for principled methods for defending networks against malicious actors. Deep reinforcement learning (DRL) has emerged as a promising approach for mitigating these attacks. However, while DRL has shown much potential for cyber defence, numerous challenges must be overcome before DRL can be applied to the autonomous cyber defence (ACD) problem at scale. Principled methods are required for environments that confront learners with very high-dimensional state spaces, large multi-discrete action spaces, and adversarial learning. Recent works have reported success in solving these problems individually. There have also been impressive engineering efforts towards solving all three for real-time strategy games. However, applying DRL to the full ACD problem remains an open challenge. Here, we survey the relevant DRL literature and conceptualize an idealised ACD-DRL agent. We provide: i.) A summary of the domain properties that define the ACD problem; ii.) A comprehensive comparison of current ACD environments used for benchmarking DRL approaches; iii.) An overview of state-of-the-art approaches for scaling DRL to domains that confront learners with the curse of dimensionality, and; iv.) A survey and critique of current methods for limiting the exploitability of agents within adversarial settings from the perspective of ACD. We conclude with open research questions that we hope will motivate future directions for researchers and practitioners working on ACD.",
    "authors": [
      "Gregory Palmer",
      "Chris Parry",
      "Daniel J. B. Harrold",
      "Chris Willis"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-10-11T16:24:14Z",
    "pdf_url": "https://arxiv.org/pdf/2310.07745v3"
  },
  {
    "arxiv_id": "2310.06278v1",
    "entry_id": "http://arxiv.org/abs/2310.06278v1",
    "title": "BC4LLM: Trusted Artificial Intelligence When Blockchain Meets Large Language Models",
    "summary": "In recent years, artificial intelligence (AI) and machine learning (ML) are reshaping society's production methods and productivity, and also changing the paradigm of scientific research. Among them, the AI language model represented by ChatGPT has made great progress. Such large language models (LLMs) serve people in the form of AI-generated content (AIGC) and are widely used in consulting, healthcare, and education. However, it is difficult to guarantee the authenticity and reliability of AIGC learning data. In addition, there are also hidden dangers of privacy disclosure in distributed AI training. Moreover, the content generated by LLMs is difficult to identify and trace, and it is difficult to cross-platform mutual recognition. The above information security issues in the coming era of AI powered by LLMs will be infinitely amplified and affect everyone's life. Therefore, we consider empowering LLMs using blockchain technology with superior security features to propose a vision for trusted AI. This paper mainly introduces the motivation and technical route of blockchain for LLM (BC4LLM), including reliable learning corpus, secure training process, and identifiable generated content. Meanwhile, this paper also reviews the potential applications and future challenges, especially in the frontier communication networks field, including network resource allocation, dynamic spectrum sharing, and semantic communication. Based on the above work combined and the prospect of blockchain and LLMs, it is expected to help the early realization of trusted AI and provide guidance for the academic community.",
    "authors": [
      "Haoxiang Luo",
      "Jian Luo",
      "Athanasios V. Vasilakos"
    ],
    "categories": [
      "cs.NI",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-10-10T03:18:26Z",
    "pdf_url": "https://arxiv.org/pdf/2310.06278v1"
  },
  {
    "arxiv_id": "2310.06253v2",
    "entry_id": "http://arxiv.org/abs/2310.06253v2",
    "title": "A Unified View on Solving Objective Mismatch in Model-Based Reinforcement Learning",
    "summary": "Model-based Reinforcement Learning (MBRL) aims to make agents more sample-efficient, adaptive, and explainable by learning an explicit model of the environment. While the capabilities of MBRL agents have significantly improved in recent years, how to best learn the model is still an unresolved question. The majority of MBRL algorithms aim at training the model to make accurate predictions about the environment and subsequently using the model to determine the most rewarding actions. However, recent research has shown that model predictive accuracy is often not correlated with action quality, tracing the root cause to the objective mismatch between accurate dynamics model learning and policy optimization of rewards. A number of interrelated solution categories to the objective mismatch problem have emerged as MBRL continues to mature as a research area. In this work, we provide an in-depth survey of these solution categories and propose a taxonomy to foster future research.",
    "authors": [
      "Ran Wei",
      "Nathan Lambert",
      "Anthony McDonald",
      "Alfredo Garcia",
      "Roberto Calandra"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-10-10T01:58:38Z",
    "pdf_url": "https://arxiv.org/pdf/2310.06253v2"
  },
  {
    "arxiv_id": "2310.05269v3",
    "entry_id": "http://arxiv.org/abs/2310.05269v3",
    "title": "Federated Learning: A Cutting-Edge Survey of the Latest Advancements and Applications",
    "summary": "Robust machine learning (ML) models can be developed by leveraging large volumes of data and distributing the computational tasks across numerous devices or servers. Federated learning (FL) is a technique in the realm of ML that facilitates this goal by utilizing cloud infrastructure to enable collaborative model training among a network of decentralized devices. Beyond distributing the computational load, FL targets the resolution of privacy issues and the reduction of communication costs simultaneously. To protect user privacy, FL requires users to send model updates rather than transmitting large quantities of raw and potentially confidential data. Specifically, individuals train ML models locally using their own data and then upload the results in the form of weights and gradients to the cloud for aggregation into the global model. This strategy is also advantageous in environments with limited bandwidth or high communication costs, as it prevents the transmission of large data volumes. With the increasing volume of data and rising privacy concerns, alongside the emergence of large-scale ML models like Large Language Models (LLMs), FL presents itself as a timely and relevant solution. It is therefore essential to review current FL algorithms to guide future research that meets the rapidly evolving ML demands. This survey provides a comprehensive analysis and comparison of the most recent FL algorithms, evaluating them on various fronts including mathematical frameworks, privacy protection, resource allocation, and applications. Beyond summarizing existing FL methods, this survey identifies potential gaps, open areas, and future challenges based on the performance reports and algorithms used in recent studies. This survey enables researchers to readily identify existing limitations in the FL field for further exploration.",
    "authors": [
      "Azim Akhtarshenas",
      "Mohammad Ali Vahedifar",
      "Navid Ayoobi",
      "Behrouz Maham",
      "Tohid Alizadeh",
      "Sina Ebrahimi",
      "David López-Pérez"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR",
      "cs.DC"
    ],
    "published": "2023-10-08T19:54:26Z",
    "pdf_url": "https://arxiv.org/pdf/2310.05269v3"
  },
  {
    "arxiv_id": "2310.04835v3",
    "entry_id": "http://arxiv.org/abs/2310.04835v3",
    "title": "On the Evolution of Knowledge Graphs: A Survey and Perspective",
    "summary": "Knowledge graphs (KGs) are structured representations of diversified knowledge. They are widely used in various intelligent applications. In this article, we provide a comprehensive survey on the evolution of various types of knowledge graphs (i.e., static KGs, dynamic KGs, temporal KGs, and event KGs) and techniques for knowledge extraction and reasoning. Furthermore, we introduce the practical applications of different types of KGs, including a case study in financial analysis. Finally, we propose our perspective on the future directions of knowledge engineering, including the potential of combining the power of knowledge graphs and large language models (LLMs), and the evolution of knowledge extraction, reasoning, and representation.",
    "authors": [
      "Xuhui Jiang",
      "Chengjin Xu",
      "Yinghan Shen",
      "Xun Sun",
      "Lumingyuan Tang",
      "Saizhuo Wang",
      "Zhongwu Chen",
      "Yuanzhuo Wang",
      "Jian Guo"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-10-07T14:46:51Z",
    "pdf_url": "https://arxiv.org/pdf/2310.04835v3"
  },
  {
    "arxiv_id": "2310.04480v2",
    "entry_id": "http://arxiv.org/abs/2310.04480v2",
    "title": "Auto-survey Challenge",
    "summary": "We present a novel platform for evaluating the capability of Large Language Models (LLMs) to autonomously compose and critique survey papers spanning a vast array of disciplines including sciences, humanities, education, and law. Within this framework, AI systems undertake a simulated peer-review mechanism akin to traditional scholarly journals, with human organizers serving in an editorial oversight capacity. Within this framework, we organized a competition for the AutoML conference 2023. Entrants are tasked with presenting stand-alone models adept at authoring articles from designated prompts and subsequently appraising them. Assessment criteria include clarity, reference appropriateness, accountability, and the substantive value of the content. This paper presents the design of the competition, including the implementation baseline submissions and methods of evaluation.",
    "authors": [
      "Thanh Gia Hieu Khuong",
      "Benedictus Kent Rachmat"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-06T09:12:35Z",
    "pdf_url": "https://arxiv.org/pdf/2310.04480v2"
  },
  {
    "arxiv_id": "2310.03666v1",
    "entry_id": "http://arxiv.org/abs/2310.03666v1",
    "title": "MapperGPT: Large Language Models for Linking and Mapping Entities",
    "summary": "Aligning terminological resources, including ontologies, controlled vocabularies, taxonomies, and value sets is a critical part of data integration in many domains such as healthcare, chemistry, and biomedical research. Entity mapping is the process of determining correspondences between entities across these resources, such as gene identifiers, disease concepts, or chemical entity identifiers. Many tools have been developed to compute such mappings based on common structural features and lexical information such as labels and synonyms. Lexical approaches in particular often provide very high recall, but low precision, due to lexical ambiguity. As a consequence of this, mapping efforts often resort to a labor intensive manual mapping refinement through a human curator.\n  Large Language Models (LLMs), such as the ones employed by ChatGPT, have generalizable abilities to perform a wide range of tasks, including question-answering and information extraction. Here we present MapperGPT, an approach that uses LLMs to review and refine mapping relationships as a post-processing step, in concert with existing high-recall methods that are based on lexical and structural heuristics.\n  We evaluated MapperGPT on a series of alignment tasks from different domains, including anatomy, developmental biology, and renal diseases. We devised a collection of tasks that are designed to be particularly challenging for lexical methods. We show that when used in combination with high-recall methods, MapperGPT can provide a substantial improvement in accuracy, beating state-of-the-art (SOTA) methods such as LogMap.",
    "authors": [
      "Nicolas Matentzoglu",
      "J. Harry Caufield",
      "Harshad B. Hegde",
      "Justin T. Reese",
      "Sierra Moxon",
      "Hyeongsik Kim",
      "Nomi L. Harris",
      "Melissa A Haendel",
      "Christopher J. Mungall"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-05T16:43:04Z",
    "pdf_url": "https://arxiv.org/pdf/2310.03666v1"
  },
  {
    "arxiv_id": "2310.03614v1",
    "entry_id": "http://arxiv.org/abs/2310.03614v1",
    "title": "Adversarial Machine Learning for Social Good: Reframing the Adversary as an Ally",
    "summary": "Deep Neural Networks (DNNs) have been the driving force behind many of the recent advances in machine learning. However, research has shown that DNNs are vulnerable to adversarial examples -- input samples that have been perturbed to force DNN-based models to make errors. As a result, Adversarial Machine Learning (AdvML) has gained a lot of attention, and researchers have investigated these vulnerabilities in various settings and modalities. In addition, DNNs have also been found to incorporate embedded bias and often produce unexplainable predictions, which can result in anti-social AI applications. The emergence of new AI technologies that leverage Large Language Models (LLMs), such as ChatGPT and GPT-4, increases the risk of producing anti-social applications at scale. AdvML for Social Good (AdvML4G) is an emerging field that repurposes the AdvML bug to invent pro-social applications. Regulators, practitioners, and researchers should collaborate to encourage the development of pro-social applications and hinder the development of anti-social ones. In this work, we provide the first comprehensive review of the emerging field of AdvML4G. This paper encompasses a taxonomy that highlights the emergence of AdvML4G, a discussion of the differences and similarities between AdvML4G and AdvML, a taxonomy covering social good-related concepts and aspects, an exploration of the motivations behind the emergence of AdvML4G at the intersection of ML4G and AdvML, and an extensive summary of the works that utilize AdvML4G as an auxiliary tool for innovating pro-social applications. Finally, we elaborate upon various challenges and open research issues that require significant attention from the research community.",
    "authors": [
      "Shawqi Al-Maliki",
      "Adnan Qayyum",
      "Hassan Ali",
      "Mohamed Abdallah",
      "Junaid Qadir",
      "Dinh Thai Hoang",
      "Dusit Niyato",
      "Ala Al-Fuqaha"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "published": "2023-10-05T15:49:04Z",
    "pdf_url": "https://arxiv.org/pdf/2310.03614v1"
  },
  {
    "arxiv_id": "2311.12839v1",
    "entry_id": "http://arxiv.org/abs/2311.12839v1",
    "title": "A Review of Deep Reinforcement Learning in Serverless Computing: Function Scheduling and Resource Auto-Scaling",
    "summary": "In the rapidly evolving field of serverless computing, efficient function scheduling and resource scaling are critical for optimizing performance and cost. This paper presents a comprehensive review of the application of Deep Reinforcement Learning (DRL) techniques in these areas. We begin by providing an overview of serverless computing, highlighting its benefits and challenges, with a particular focus on function scheduling and resource scaling. We then delve into the principles of deep reinforcement learning (DRL) and its potential for addressing these challenges. A systematic review of recent studies applying DRL to serverless computing is presented, covering various algorithms, models, and performances. Our analysis reveals that DRL, with its ability to learn and adapt from an environment, shows promising results in improving the efficiency of function scheduling and resource scaling in serverless computing. However, several challenges remain, including the need for more realistic simulation environments, handling of cold starts, and the trade-off between learning time and scheduling performance. We conclude by discussing potential future directions for this research area, emphasizing the need for more robust DRL models, better benchmarking methods, and the exploration of multi-agent reinforcement learning for more complex serverless architectures. This review serves as a valuable resource for researchers and practitioners aiming to understand and advance the application of DRL in serverless computing.",
    "authors": [
      "Amjad Yousef Majid",
      "Eduard Marin"
    ],
    "categories": [
      "cs.DC",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-10-05T09:26:04Z",
    "pdf_url": "https://arxiv.org/pdf/2311.12839v1"
  },
  {
    "arxiv_id": "2310.03400v2",
    "entry_id": "http://arxiv.org/abs/2310.03400v2",
    "title": "Adapting Large Language Models for Content Moderation: Pitfalls in Data Engineering and Supervised Fine-tuning",
    "summary": "Nowadays, billions of people engage in communication and express their opinions on the internet daily. Unfortunately, not all of these expressions are friendly or compliant, making content moderation an indispensable task. A common approach is to use a discriminative model to classify the content, but this method often requires strict data engineering, otherwise it will face unacceptable overfitting. With the successful development of Large Language Models (LLMs) in recent years, LLM-based methods have become a feasible solution for handling tasks in various domains. Thanks to the knowledge of the foundation models, we can develop more robust privately deployed models with limited data via fine-tuning these foundation models. Moreover, as a generative model, it can provide detailed analysis of the review process, enhancing interpretability. In this paper, we introduce how to fine-tune a LLM model that can be privately deployed for content moderation. Specifically, we discuss the differences between discriminative and generative models using content moderation as an example. Additionally, we reveal that incorporating reasoning processes during the fine-tuning of LLMs can effectively alleviate overfitting, even if the model is not allowed to directly output reasoning processes during deployment. We present a complete process, from data collection and construction to model training and overfitting elimination, for fine-tuning LLMs in vertical domain deployments. We report the entire research process and the key findings in this paper, hoping to provide valuable experience for researchers who are fine-tuning privately deployed models in their domain-specific research.",
    "authors": [
      "Huan Ma",
      "Changqing Zhang",
      "Huazhu Fu",
      "Peilin Zhao",
      "Bingzhe Wu"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-10-05T09:09:44Z",
    "pdf_url": "https://arxiv.org/pdf/2310.03400v2"
  },
  {
    "arxiv_id": "2310.02778v2",
    "entry_id": "http://arxiv.org/abs/2310.02778v2",
    "title": "Integrating UMLS Knowledge into Large Language Models for Medical Question Answering",
    "summary": "Large language models (LLMs) have demonstrated powerful text generation capabilities, bringing unprecedented innovation to the healthcare field. While LLMs hold immense promise for applications in healthcare, applying them to real clinical scenarios presents significant challenges, as these models may generate content that deviates from established medical facts and even exhibit potential biases. In our research, we develop an augmented LLM framework based on the Unified Medical Language System (UMLS), aiming to better serve the healthcare community. We employ LLaMa2-13b-chat and ChatGPT-3.5 as our benchmark models, and conduct automatic evaluations using the ROUGE Score and BERTScore on 104 questions from the LiveQA test set. Additionally, we establish criteria for physician-evaluation based on four dimensions: Factuality, Completeness, Readability and Relevancy. ChatGPT-3.5 is used for physician evaluation with 20 questions on the LiveQA test set. Multiple resident physicians conducted blind reviews to evaluate the generated content, and the results indicate that this framework effectively enhances the factuality, completeness, and relevance of generated content. Our research demonstrates the effectiveness of using UMLS-augmented LLMs and highlights the potential application value of LLMs in in medical question-answering.",
    "authors": [
      "Rui Yang",
      "Edison Marrese-Taylor",
      "Yuhe Ke",
      "Lechao Cheng",
      "Qingyu Chen",
      "Irene Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-04T12:50:26Z",
    "pdf_url": "https://arxiv.org/pdf/2310.02778v2"
  },
  {
    "arxiv_id": "2310.01783v1",
    "entry_id": "http://arxiv.org/abs/2310.01783v1",
    "title": "Can large language models provide useful feedback on research papers? A large-scale empirical analysis",
    "summary": "Expert feedback lays the foundation of rigorous research. However, the rapid growth of scholarly production and intricate knowledge specialization challenge the conventional scientific feedback mechanisms. High-quality peer reviews are increasingly difficult to obtain. Researchers who are more junior or from under-resourced settings have especially hard times getting timely feedback. With the breakthrough of large language models (LLM) such as GPT-4, there is growing interest in using LLMs to generate scientific feedback on research manuscripts. However, the utility of LLM-generated feedback has not been systematically studied. To address this gap, we created an automated pipeline using GPT-4 to provide comments on the full PDFs of scientific papers. We evaluated the quality of GPT-4's feedback through two large-scale studies. We first quantitatively compared GPT-4's generated feedback with human peer reviewer feedback in 15 Nature family journals (3,096 papers in total) and the ICLR machine learning conference (1,709 papers). The overlap in the points raised by GPT-4 and by human reviewers (average overlap 30.85% for Nature journals, 39.23% for ICLR) is comparable to the overlap between two human reviewers (average overlap 28.58% for Nature journals, 35.25% for ICLR). The overlap between GPT-4 and human reviewers is larger for the weaker papers. We then conducted a prospective user study with 308 researchers from 110 US institutions in the field of AI and computational biology to understand how researchers perceive feedback generated by our GPT-4 system on their own papers. Overall, more than half (57.4%) of the users found GPT-4 generated feedback helpful/very helpful and 82.4% found it more beneficial than feedback from at least some human reviewers. While our findings show that LLM-generated feedback can help researchers, we also identify several limitations.",
    "authors": [
      "Weixin Liang",
      "Yuhui Zhang",
      "Hancheng Cao",
      "Binglu Wang",
      "Daisy Ding",
      "Xinyu Yang",
      "Kailas Vodrahalli",
      "Siyu He",
      "Daniel Smith",
      "Yian Yin",
      "Daniel McFarland",
      "James Zou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.HC"
    ],
    "published": "2023-10-03T04:14:17Z",
    "pdf_url": "https://arxiv.org/pdf/2310.01783v1"
  },
  {
    "arxiv_id": "2310.01378v1",
    "entry_id": "http://arxiv.org/abs/2310.01378v1",
    "title": "On Grid Graph Reachability and Puzzle Games",
    "summary": "Many puzzle video games, like Sokoban, involve moving some agent in a maze. The reachable locations are usually apparent for a human player, and the difficulty of the game is mainly related to performing actions on objects, such as pushing (reachable) boxes. For this reason, the difficulty of a particular level is often measured as the number of actions on objects, other than agent walking, needed to find a solution. In this paper we study CP and SAT approaches for solving these kind of problems. We review some reachability encodings and propose a new one. We empirically show that the new encoding is well-suited for solving puzzle problems in the planning as SAT paradigm, especially when considering the execution of several actions in parallel.",
    "authors": [
      "Miquel Bofill",
      "Cristina Borralleras",
      "Joan Espasa",
      "Mateu Villaret"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-10-02T17:41:35Z",
    "pdf_url": "https://arxiv.org/pdf/2310.01378v1"
  },
  {
    "arxiv_id": "2310.00836v3",
    "entry_id": "http://arxiv.org/abs/2310.00836v3",
    "title": "Towards LogiGLUE: A Brief Survey and A Benchmark for Analyzing Logical Reasoning Capabilities of Language Models",
    "summary": "Logical reasoning is fundamental for humans yet presents a substantial challenge in the domain of Artificial Intelligence. Initially, researchers used Knowledge Representation and Reasoning (KR) systems that did not scale and required non-trivial manual effort. Recently, the emergence of large language models (LLMs) has demonstrated the ability to overcome various limitations of formal Knowledge Representation (KR) systems. Consequently, there's a growing interest in using LLMs for logical reasoning via natural language. This work strives to understand the proficiency of LLMs in logical reasoning by offering a brief review of the latest progress in this area; with a focus on the logical reasoning datasets, tasks, and the methods adopted to utilize LLMs for reasoning. To offer a thorough analysis, we have compiled a benchmark titled LogiGLUE. This includes 24 varied datasets encompassing deductive, abductive, and inductive reasoning. Utilizing LogiGLUE as a foundation, we have trained an instruction fine-tuned language model, resulting in LogiT5. We study single-task training, multi-task training, and \"chain-of-thought\" knowledge distillation fine-tuning technique to assess the performance of model across the different logical reasoning categories. We also assess various LLMs using LogiGLUE, and the findings indicate that LLMs excel most in abductive reasoning, followed by deductive reasoning, while they are least effective at inductive reasoning. We aim to shed light on the capabilities and potential pathways for enhancing logical reasoning proficiency in LLMs, paving the way for more advanced and nuanced developments in this critical field.",
    "authors": [
      "Man Luo",
      "Shrinidhi Kumbhar",
      "Ming shen",
      "Mihir Parmar",
      "Neeraj Varshney",
      "Pratyay Banerjee",
      "Somak Aditya",
      "Chitta Baral"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-10-02T01:00:50Z",
    "pdf_url": "https://arxiv.org/pdf/2310.00836v3"
  },
  {
    "arxiv_id": "2310.00658v1",
    "entry_id": "http://arxiv.org/abs/2310.00658v1",
    "title": "The Robots are Here: Navigating the Generative AI Revolution in Computing Education",
    "summary": "Recent advancements in artificial intelligence (AI) are fundamentally reshaping computing, with large language models (LLMs) now effectively being able to generate and interpret source code and natural language instructions. These emergent capabilities have sparked urgent questions in the computing education community around how educators should adapt their pedagogy to address the challenges and to leverage the opportunities presented by this new technology. In this working group report, we undertake a comprehensive exploration of LLMs in the context of computing education and make five significant contributions. First, we provide a detailed review of the literature on LLMs in computing education and synthesise findings from 71 primary articles. Second, we report the findings of a survey of computing students and instructors from across 20 countries, capturing prevailing attitudes towards LLMs and their use in computing education contexts. Third, to understand how pedagogy is already changing, we offer insights collected from in-depth interviews with 22 computing educators from five continents who have already adapted their curricula and assessments. Fourth, we use the ACM Code of Ethics to frame a discussion of ethical issues raised by the use of large language models in computing education, and we provide concrete advice for policy makers, educators, and students. Finally, we benchmark the performance of LLMs on various computing education datasets, and highlight the extent to which the capabilities of current models are rapidly improving. Our aim is that this report will serve as a focal point for both researchers and practitioners who are exploring, adapting, using, and evaluating LLMs and LLM-based tools in computing classrooms.",
    "authors": [
      "James Prather",
      "Paul Denny",
      "Juho Leinonen",
      "Brett A. Becker",
      "Ibrahim Albluwi",
      "Michelle Craig",
      "Hieke Keuning",
      "Natalie Kiesler",
      "Tobias Kohn",
      "Andrew Luxton-Reilly",
      "Stephen MacNeil",
      "Andrew Peterson",
      "Raymond Pettit",
      "Brent N. Reeves",
      "Jaromir Savelka"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2023-10-01T12:54:37Z",
    "pdf_url": "https://arxiv.org/pdf/2310.00658v1"
  },
  {
    "arxiv_id": "2310.00280v3",
    "entry_id": "http://arxiv.org/abs/2310.00280v3",
    "title": "Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration",
    "summary": "Large Language Models (LLMs) are evolving at an unprecedented pace and have exhibited considerable capability in the realm of natural language processing (NLP) with world knowledge. Benefiting from ultra-large-scale training corpora, a single LLM can manage typical NLP tasks competently. However, its performance in executing reasoning tasks is still confined by the limitations of its internal representations. To push this boundary further, we introduce Corex in this paper, a suite of novel general-purpose strategies that transform LLMs into autonomous agents pioneering multi-model collaborations for complex task-solving. Inspired by human behaviors, Corex is constituted by diverse collaboration paradigms including Debate, Review, and Retrieve modes, which collectively work towards enhancing the factuality, faithfulness, and reliability of the reasoning process. These paradigms foster task-agnostic approaches that enable LLMs to ''think outside the box,'' thereby overcoming hallucinations and providing better solutions. Through extensive experiments across four different types of reasoning tasks, we demonstrate that orchestrating multiple LLMs to work in concert yields substantially better performance compared to existing methods. Further results and in-depth analysis demonstrate the cost-effectiveness of our method, facilitating collaboration among different LLMs and promoting annotation efficiency.",
    "authors": [
      "Qiushi Sun",
      "Zhangyue Yin",
      "Xiang Li",
      "Zhiyong Wu",
      "Xipeng Qiu",
      "Lingpeng Kong"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-09-30T07:11:39Z",
    "pdf_url": "https://arxiv.org/pdf/2310.00280v3"
  },
  {
    "arxiv_id": "2309.17057v2",
    "entry_id": "http://arxiv.org/abs/2309.17057v2",
    "title": "Tell Me a Story! Narrative-Driven XAI with Large Language Models",
    "summary": "In many AI applications today, the predominance of black-box machine learning models, due to their typically higher accuracy, amplifies the need for Explainable AI (XAI). Existing XAI approaches, such as the widely used SHAP values or counterfactual (CF) explanations, are arguably often too technical for users to understand and act upon. To enhance comprehension of explanations of AI decisions and the overall user experience, we introduce XAIstories, which leverage Large Language Models to provide narratives about how AI predictions are made: SHAPstories do so based on SHAP explanations, while CFstories do so for CF explanations. We study the impact of our approach on users' experience and understanding of AI predictions. Our results are striking: over 90% of the surveyed general audience finds the narratives generated by SHAPstories convincing. Data scientists primarily see the value of SHAPstories in communicating explanations to a general audience, with 83% of data scientists indicating they are likely to use SHAPstories for this purpose. In an image classification setting, CFstories are considered more or equally convincing as the users' own crafted stories by more than 75% of the participants. CFstories additionally bring a tenfold speed gain in creating a narrative. We also find that SHAPstories help users to more accurately summarize and understand AI decisions, in a credit scoring setting we test, correctly answering comprehension questions significantly more often than they do when only SHAP values are provided. The results thereby suggest that XAIstories may significantly help explaining and understanding AI predictions, ultimately supporting better decision-making in various applications.",
    "authors": [
      "David Martens",
      "James Hinns",
      "Camille Dams",
      "Mark Vergouwen",
      "Theodoros Evgeniou"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-09-29T08:40:08Z",
    "pdf_url": "https://arxiv.org/pdf/2309.17057v2"
  },
  {
    "arxiv_id": "2309.16459v1",
    "entry_id": "http://arxiv.org/abs/2309.16459v1",
    "title": "Augmenting LLMs with Knowledge: A survey on hallucination prevention",
    "summary": "Large pre-trained language models have demonstrated their proficiency in storing factual knowledge within their parameters and achieving remarkable results when fine-tuned for downstream natural language processing tasks. Nonetheless, their capacity to access and manipulate knowledge with precision remains constrained, resulting in performance disparities on knowledge-intensive tasks when compared to task-specific architectures. Additionally, the challenges of providing provenance for model decisions and maintaining up-to-date world knowledge persist as open research frontiers. To address these limitations, the integration of pre-trained models with differentiable access mechanisms to explicit non-parametric memory emerges as a promising solution. This survey delves into the realm of language models (LMs) augmented with the ability to tap into external knowledge sources, including external knowledge bases and search engines. While adhering to the standard objective of predicting missing tokens, these augmented LMs leverage diverse, possibly non-parametric external modules to augment their contextual processing capabilities, departing from the conventional language modeling paradigm. Through an exploration of current advancements in augmenting large language models with knowledge, this work concludes that this emerging research direction holds the potential to address prevalent issues in traditional LMs, such as hallucinations, un-grounded responses, and scalability challenges.",
    "authors": [
      "Konstantinos Andriopoulos",
      "Johan Pouwelse"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-28T14:09:58Z",
    "pdf_url": "https://arxiv.org/pdf/2309.16459v1"
  },
  {
    "arxiv_id": "2311.10723v2",
    "entry_id": "http://arxiv.org/abs/2311.10723v2",
    "title": "Large Language Models in Finance: A Survey",
    "summary": "Recent advances in large language models (LLMs) have opened new possibilities for artificial intelligence applications in finance. In this paper, we provide a practical survey focused on two key aspects of utilizing LLMs for financial tasks: existing solutions and guidance for adoption.\n  First, we review current approaches employing LLMs in finance, including leveraging pretrained models via zero-shot or few-shot learning, fine-tuning on domain-specific data, and training custom LLMs from scratch. We summarize key models and evaluate their performance improvements on financial natural language processing tasks.\n  Second, we propose a decision framework to guide financial professionals in selecting the appropriate LLM solution based on their use case constraints around data, compute, and performance needs. The framework provides a pathway from lightweight experimentation to heavy investment in customized LLMs.\n  Lastly, we discuss limitations and challenges around leveraging LLMs in financial applications. Overall, this survey aims to synthesize the state-of-the-art and provide a roadmap for responsibly applying LLMs to advance financial AI.",
    "authors": [
      "Yinheng Li",
      "Shaofei Wang",
      "Han Ding",
      "Hang Chen"
    ],
    "categories": [
      "q-fin.GN",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-09-28T06:04:04Z",
    "pdf_url": "https://arxiv.org/pdf/2311.10723v2"
  },
  {
    "arxiv_id": "2309.15942v2",
    "entry_id": "http://arxiv.org/abs/2309.15942v2",
    "title": "Towards Efficient and Reliable AI Through Neuromorphic Principles",
    "summary": "Artificial intelligence (AI) research today is largely driven by ever-larger neural network models trained on graphics processing units (GPUs). This paradigm has yielded remarkable progress, but it also risks entrenching a hardware lottery in which algorithmic choices succeed primarily because they align with current hardware, rather than because they are inherently superior. In particular, the dominance of Transformer architectures running on GPU clusters has led to an arms race of scaling up models, resulting in exorbitant computational costs and energy usage. At the same time, today's AI models often remain unreliable in the sense that they cannot properly quantify uncertainty in their decisions -- for example, large language models tend to hallucinate incorrect outputs with high confidence.\n  This article argues that achieving more efficient and reliable AI will require embracing a set of principles that are well-aligned with the goals of neuromorphic engineering, which are in turn inspired by how the brain processes information. Specifically, we outline six key neuromorphic principles, spanning algorithms, architectures, and hardware, that can inform the design of future AI systems: (i) the use of stateful, recurrent models; (ii) extreme dynamic sparsity, possibly down to spike-based processing; (iii) backpropagation-free on-device learning and fine-tuning; (iv) probabilistic decision-making; (v) in-memory computing; and (vi) hardware-software co-design via stochastic computing. We discuss each of these principles in turn, surveying relevant prior work and pointing to directions for research.",
    "authors": [
      "Bipin Rajendran",
      "Osvaldo Simeone",
      "Bashir M. Al-Hashimi"
    ],
    "categories": [
      "cs.AI",
      "cs.ET",
      "cs.IT"
    ],
    "published": "2023-09-27T18:39:46Z",
    "pdf_url": "https://arxiv.org/pdf/2309.15942v2"
  },
  {
    "arxiv_id": "2310.01424v2",
    "entry_id": "http://arxiv.org/abs/2310.01424v2",
    "title": "Identifying and Mitigating Privacy Risks Stemming from Language Models: A Survey",
    "summary": "Large Language Models (LLMs) have shown greatly enhanced performance in recent years, attributed to increased size and extensive training data. This advancement has led to widespread interest and adoption across industries and the public. However, training data memorization in Machine Learning models scales with model size, particularly concerning for LLMs. Memorized text sequences have the potential to be directly leaked from LLMs, posing a serious threat to data privacy. Various techniques have been developed to attack LLMs and extract their training data. As these models continue to grow, this issue becomes increasingly critical. To help researchers and policymakers understand the state of knowledge around privacy attacks and mitigations, including where more work is needed, we present the first SoK on data privacy for LLMs. We (i) identify a taxonomy of salient dimensions where attacks differ on LLMs, (ii) systematize existing attacks, using our taxonomy of dimensions to highlight key trends, (iii) survey existing mitigation strategies, highlighting their strengths and limitations, and (iv) identify key gaps, demonstrating open problems and areas for concern.",
    "authors": [
      "Victoria Smith",
      "Ali Shahin Shamsabadi",
      "Carolyn Ashurst",
      "Adrian Weller"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-09-27T15:15:23Z",
    "pdf_url": "https://arxiv.org/pdf/2310.01424v2"
  },
  {
    "arxiv_id": "2309.15698v1",
    "entry_id": "http://arxiv.org/abs/2309.15698v1",
    "title": "Deep Model Fusion: A Survey",
    "summary": "Deep model fusion/merging is an emerging technique that merges the parameters or predictions of multiple deep learning models into a single one. It combines the abilities of different models to make up for the biases and errors of a single model to achieve better performance. However, deep model fusion on large-scale deep learning models (e.g., LLMs and foundation models) faces several challenges, including high computational cost, high-dimensional parameter space, interference between different heterogeneous models, etc. Although model fusion has attracted widespread attention due to its potential to solve complex real-world tasks, there is still a lack of complete and detailed survey research on this technique. Accordingly, in order to understand the model fusion method better and promote its development, we present a comprehensive survey to summarize the recent progress. Specifically, we categorize existing deep model fusion methods as four-fold: (1) \"Mode connectivity\", which connects the solutions in weight space via a path of non-increasing loss, in order to obtain better initialization for model fusion; (2) \"Alignment\" matches units between neural networks to create better conditions for fusion; (3) \"Weight average\", a classical model fusion method, averages the weights of multiple models to obtain more accurate results closer to the optimal solution; (4) \"Ensemble learning\" combines the outputs of diverse models, which is a foundational technique for improving the accuracy and robustness of the final model. In addition, we analyze the challenges faced by deep model fusion and propose possible research directions for model fusion in the future. Our review is helpful in deeply understanding the correlation between different model fusion methods and practical application methods, which can enlighten the research in the field of deep model fusion.",
    "authors": [
      "Weishi Li",
      "Yong Peng",
      "Miao Zhang",
      "Liang Ding",
      "Han Hu",
      "Li Shen"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-09-27T14:40:12Z",
    "pdf_url": "https://arxiv.org/pdf/2309.15698v1"
  },
  {
    "arxiv_id": "2309.15402v3",
    "entry_id": "http://arxiv.org/abs/2309.15402v3",
    "title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future",
    "summary": "Reasoning, a fundamental cognitive process integral to human intelligence, has garnered substantial interest within artificial intelligence. Notably, recent studies have revealed that chain-of-thought prompting significantly enhances LLM's reasoning capabilities, which attracts widespread attention from both academics and industry. In this paper, we systematically investigate relevant research, summarizing advanced methods through a meticulous taxonomy that offers novel perspectives. Moreover, we delve into the current frontiers and delineate the challenges and future directions, thereby shedding light on future research. Furthermore, we engage in a discussion about open questions. We hope this paper serves as an introduction for beginners and fosters future research. Resources have been made publicly available at https://github.com/zchuz/CoT-Reasoning-Survey",
    "authors": [
      "Zheng Chu",
      "Jingchang Chen",
      "Qianglong Chen",
      "Weijiang Yu",
      "Tao He",
      "Haotian Wang",
      "Weihua Peng",
      "Ming Liu",
      "Bing Qin",
      "Ting Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-09-27T04:53:10Z",
    "pdf_url": "https://arxiv.org/pdf/2309.15402v3"
  },
  {
    "arxiv_id": "2310.01420v2",
    "entry_id": "http://arxiv.org/abs/2310.01420v2",
    "title": "Ruffle&Riley: Towards the Automated Induction of Conversational Tutoring Systems",
    "summary": "Conversational tutoring systems (CTSs) offer learning experiences driven by natural language interaction. They are known to promote high levels of cognitive engagement and benefit learning outcomes, particularly in reasoning tasks. Nonetheless, the time and cost required to author CTS content is a major obstacle to widespread adoption. In this paper, we introduce a novel type of CTS that leverages the recent advances in large language models (LLMs) in two ways: First, the system induces a tutoring script automatically from a lesson text. Second, the system automates the script orchestration via two LLM-based agents (Ruffle&Riley) with the roles of a student and a professor in a learning-by-teaching format. The system allows a free-form conversation that follows the ITS-typical inner and outer loop structure. In an initial between-subject online user study (N = 100) comparing Ruffle&Riley to simpler QA chatbots and reading activity, we found no significant differences in post-test scores. Nonetheless, in the learning experience survey, Ruffle&Riley users expressed higher ratings of understanding and remembering and further perceived the offered support as more helpful and the conversation as coherent. Our study provides insights for a new generation of scalable CTS technologies.",
    "authors": [
      "Robin Schmucker",
      "Meng Xia",
      "Amos Azaria",
      "Tom Mitchell"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2023-09-26T23:27:06Z",
    "pdf_url": "https://arxiv.org/pdf/2310.01420v2"
  },
  {
    "arxiv_id": "2309.15025v1",
    "entry_id": "http://arxiv.org/abs/2309.15025v1",
    "title": "Large Language Model Alignment: A Survey",
    "summary": "Recent years have witnessed remarkable progress made in large language models (LLMs). Such advancements, while garnering significant attention, have concurrently elicited various concerns. The potential of these models is undeniably vast; however, they may yield texts that are imprecise, misleading, or even detrimental. Consequently, it becomes paramount to employ alignment techniques to ensure these models to exhibit behaviors consistent with human values.\n  This survey endeavors to furnish an extensive exploration of alignment methodologies designed for LLMs, in conjunction with the extant capability research in this domain. Adopting the lens of AI alignment, we categorize the prevailing methods and emergent proposals for the alignment of LLMs into outer and inner alignment. We also probe into salient issues including the models' interpretability, and potential vulnerabilities to adversarial attacks. To assess LLM alignment, we present a wide variety of benchmarks and evaluation methodologies. After discussing the state of alignment research for LLMs, we finally cast a vision toward the future, contemplating the promising avenues of research that lie ahead.\n  Our aspiration for this survey extends beyond merely spurring research interests in this realm. We also envision bridging the gap between the AI alignment research community and the researchers engrossed in the capability exploration of LLMs for both capable and safe LLMs.",
    "authors": [
      "Tianhao Shen",
      "Renren Jin",
      "Yufei Huang",
      "Chuang Liu",
      "Weilong Dong",
      "Zishan Guo",
      "Xinwei Wu",
      "Yan Liu",
      "Deyi Xiong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-09-26T15:49:23Z",
    "pdf_url": "https://arxiv.org/pdf/2309.15025v1"
  },
  {
    "arxiv_id": "2309.15857v3",
    "entry_id": "http://arxiv.org/abs/2309.15857v3",
    "title": "A Survey on Image-text Multimodal Models",
    "summary": "With the significant advancements of Large Language Models (LLMs) in the field of Natural Language Processing (NLP), the development of image-text multimodal models has garnered widespread attention. Current surveys on image-text multimodal models mainly focus on representative models or application domains, but lack a review on how general technical models influence the development of domain-specific models, which is crucial for domain researchers. Based on this, this paper first reviews the technological evolution of image-text multimodal models, from early explorations of feature space to visual language encoding structures, and then to the latest large model architectures. Next, from the perspective of technological evolution, we explain how the development of general image-text multimodal technologies promotes the progress of multimodal technologies in the biomedical field, as well as the importance and complexity of specific datasets in the biomedical domain. Then, centered on the tasks of image-text multimodal models, we analyze their common components and challenges. After that, we summarize the architecture, components, and data of general image-text multimodal models, and introduce the applications and improvements of image-text multimodal models in the biomedical field. Finally, we categorize the challenges faced in the development and application of general models into external factors and intrinsic factors, further refining them into 2 external factors and 5 intrinsic factors, and propose targeted solutions, providing guidance for future research directions. For more details and data, please visit our GitHub page: \\url{https://github.com/i2vec/A-survey-on-image-text-multimodal-models}.",
    "authors": [
      "Ruifeng Guo",
      "Jingxuan Wei",
      "Linzhuang Sun",
      "Bihui Yu",
      "Guiyong Chang",
      "Dawei Liu",
      "Sibo Zhang",
      "Zhengbing Yao",
      "Mingjun Xu",
      "Liping Bu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.MM"
    ],
    "published": "2023-09-23T15:21:15Z",
    "pdf_url": "https://arxiv.org/pdf/2309.15857v3"
  },
  {
    "arxiv_id": "2309.14365v1",
    "entry_id": "http://arxiv.org/abs/2309.14365v1",
    "title": "An In-depth Survey of Large Language Model-based Artificial Intelligence Agents",
    "summary": "Due to the powerful capabilities demonstrated by large language model (LLM), there has been a recent surge in efforts to integrate them with AI agents to enhance their performance. In this paper, we have explored the core differences and characteristics between LLM-based AI agents and traditional AI agents. Specifically, we first compare the fundamental characteristics of these two types of agents, clarifying the significant advantages of LLM-based agents in handling natural language, knowledge storage, and reasoning capabilities. Subsequently, we conducted an in-depth analysis of the key components of AI agents, including planning, memory, and tool use. Particularly, for the crucial component of memory, this paper introduced an innovative classification scheme, not only departing from traditional classification methods but also providing a fresh perspective on the design of an AI agent's memory system. We firmly believe that in-depth research and understanding of these core components will lay a solid foundation for the future advancement of AI agent technology. At the end of the paper, we provide directional suggestions for further research in this field, with the hope of offering valuable insights to scholars and researchers in the field.",
    "authors": [
      "Pengyu Zhao",
      "Zijian Jin",
      "Ning Cheng"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-09-23T11:25:45Z",
    "pdf_url": "https://arxiv.org/pdf/2309.14365v1"
  },
  {
    "arxiv_id": "2309.13205v1",
    "entry_id": "http://arxiv.org/abs/2309.13205v1",
    "title": "A Practical Survey on Zero-shot Prompt Design for In-context Learning",
    "summary": "The remarkable advancements in large language models (LLMs) have brought about significant improvements in Natural Language Processing(NLP) tasks. This paper presents a comprehensive review of in-context learning techniques, focusing on different types of prompts, including discrete, continuous, few-shot, and zero-shot, and their impact on LLM performance. We explore various approaches to prompt design, such as manual design, optimization algorithms, and evaluation methods, to optimize LLM performance across diverse tasks. Our review covers key research studies in prompt engineering, discussing their methodologies and contributions to the field. We also delve into the challenges faced in evaluating prompt performance, given the absence of a single \"best\" prompt and the importance of considering multiple metrics. In conclusion, the paper highlights the critical role of prompt design in harnessing the full potential of LLMs and provides insights into the combination of manual design, optimization techniques, and rigorous evaluation for more effective and efficient use of LLMs in various NLP tasks.",
    "authors": [
      "Yinheng Li"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.ET",
      "cs.LG"
    ],
    "published": "2023-09-22T23:00:34Z",
    "pdf_url": "https://arxiv.org/pdf/2309.13205v1"
  },
  {
    "arxiv_id": "2309.12941v1",
    "entry_id": "http://arxiv.org/abs/2309.12941v1",
    "title": "Trusta: Reasoning about Assurance Cases with Formal Methods and Large Language Models",
    "summary": "Assurance cases can be used to argue for the safety of products in safety engineering. In safety-critical areas, the construction of assurance cases is indispensable. Trustworthiness Derivation Trees (TDTs) enhance assurance cases by incorporating formal methods, rendering it possible for automatic reasoning about assurance cases. We present Trustworthiness Derivation Tree Analyzer (Trusta), a desktop application designed to automatically construct and verify TDTs. The tool has a built-in Prolog interpreter in its backend, and is supported by the constraint solvers Z3 and MONA. Therefore, it can solve constraints about logical formulas involving arithmetic, sets, Horn clauses etc. Trusta also utilizes large language models to make the creation and evaluation of assurance cases more convenient. It allows for interactive human examination and modification. We evaluated top language models like ChatGPT-3.5, ChatGPT-4, and PaLM 2 for generating assurance cases. Our tests showed a 50%-80% similarity between machine-generated and human-created cases. In addition, Trusta can extract formal constraints from text in natural languages, facilitating an easier interpretation and validation process. This extraction is subject to human review and correction, blending the best of automated efficiency with human insight. To our knowledge, this marks the first integration of large language models in automatic creating and reasoning about assurance cases, bringing a novel approach to a traditional challenge. Through several industrial case studies, Trusta has proven to quickly find some subtle issues that are typically missed in manual inspection, demonstrating its practical value in enhancing the assurance case development process.",
    "authors": [
      "Zezhong Chen",
      "Yuxin Deng",
      "Wenjie Du"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2023-09-22T15:42:43Z",
    "pdf_url": "https://arxiv.org/pdf/2309.12941v1"
  },
  {
    "arxiv_id": "2309.12938v1",
    "entry_id": "http://arxiv.org/abs/2309.12938v1",
    "title": "Frustrated with Code Quality Issues? LLMs can Help!",
    "summary": "As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues. We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The \\emph{proposer LLM} of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The \\emph{ranker LLM} evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer. CORE could revise 59.2% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM is able to reduce false positives by 25.8% in these cases. CORE produced revisions that passed the static analysis tool in 76.8% Java files (across 10 quality checks) comparable to 78.3% of a specialized program repair tool, with significantly much less engineering efforts.",
    "authors": [
      "Nalin Wadhwa",
      "Jui Pradhan",
      "Atharv Sonwane",
      "Surya Prakash Sahu",
      "Nagarajan Natarajan",
      "Aditya Kanade",
      "Suresh Parthasarathy",
      "Sriram Rajamani"
    ],
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published": "2023-09-22T15:37:07Z",
    "pdf_url": "https://arxiv.org/pdf/2309.12938v1"
  },
  {
    "arxiv_id": "2309.12626v1",
    "entry_id": "http://arxiv.org/abs/2309.12626v1",
    "title": "Construction contract risk identification based on knowledge-augmented language model",
    "summary": "Contract review is an essential step in construction projects to prevent potential losses. However, the current methods for reviewing construction contracts lack effectiveness and reliability, leading to time-consuming and error-prone processes. While large language models (LLMs) have shown promise in revolutionizing natural language processing (NLP) tasks, they struggle with domain-specific knowledge and addressing specialized issues. This paper presents a novel approach that leverages LLMs with construction contract knowledge to emulate the process of contract review by human experts. Our tuning-free approach incorporates construction contract domain knowledge to enhance language models for identifying construction contract risks. The use of a natural language when building the domain knowledge base facilitates practical implementation. We evaluated our method on real construction contracts and achieved solid performance. Additionally, we investigated how large language models employ logical thinking during the task and provide insights and recommendations for future research.",
    "authors": [
      "Saika Wong",
      "Chunmo Zheng",
      "Xing Su",
      "Yinqiu Tang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-09-22T05:27:06Z",
    "pdf_url": "https://arxiv.org/pdf/2309.12626v1"
  },
  {
    "arxiv_id": "2309.12570v3",
    "entry_id": "http://arxiv.org/abs/2309.12570v3",
    "title": "Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers",
    "summary": "The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research directions in creative writing assistance using LLMs.",
    "authors": [
      "Tuhin Chakrabarty",
      "Vishakh Padmakumar",
      "Faeze Brahman",
      "Smaranda Muresan"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2023-09-22T01:49:36Z",
    "pdf_url": "https://arxiv.org/pdf/2309.12570v3"
  },
  {
    "arxiv_id": "2309.12460v2",
    "entry_id": "http://arxiv.org/abs/2309.12460v2",
    "title": "Multimodal Deep Learning for Scientific Imaging Interpretation",
    "summary": "In the domain of scientific imaging, interpreting visual data often demands an intricate combination of human expertise and deep comprehension of the subject materials. This study presents a novel methodology to linguistically emulate and subsequently evaluate human-like interactions with Scanning Electron Microscopy (SEM) images, specifically of glass materials. Leveraging a multimodal deep learning framework, our approach distills insights from both textual and visual data harvested from peer-reviewed articles, further augmented by the capabilities of GPT-4 for refined data synthesis and evaluation. Despite inherent challenges--such as nuanced interpretations and the limited availability of specialized datasets--our model (GlassLLaVA) excels in crafting accurate interpretations, identifying key features, and detecting defects in previously unseen SEM images. Moreover, we introduce versatile evaluation metrics, suitable for an array of scientific imaging applications, which allows for benchmarking against research-grounded answers. Benefiting from the robustness of contemporary Large Language Models, our model adeptly aligns with insights from research papers. This advancement not only underscores considerable progress in bridging the gap between human and machine interpretation in scientific imaging, but also hints at expansive avenues for future research and broader application.",
    "authors": [
      "Abdulelah S. Alshehri",
      "Franklin L. Lee",
      "Shihu Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CE",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2023-09-21T20:09:22Z",
    "pdf_url": "https://arxiv.org/pdf/2309.12460v2"
  },
  {
    "arxiv_id": "2309.12132v2",
    "entry_id": "http://arxiv.org/abs/2309.12132v2",
    "title": "Automating construction contract review using knowledge graph-enhanced large language models",
    "summary": "An effective and efficient review of construction contracts is essential for minimizing construction projects losses, but current methods are time-consuming and error-prone. Studies using methods based on Natural Language Processing (NLP) exist, but their scope is often limited to text classification or segmented label prediction. This paper investigates whether integrating Large Language Models (LLMs) and Knowledge Graphs (KGs) can enhance the accuracy and interpretability of automated contract risk identification. A tuning-free approach is proposed that integrates LLMs with a Nested Contract Knowledge Graph (NCKG) using a Graph Retrieval-Augmented Generation (GraphRAG) framework for contract knowledge retrieval and reasoning. Tested on international EPC contracts, the method achieves more accurate risk evaluation and interpretable risk summaries than baseline models. These findings demonstrate the potential of combining LLMs and KGs for reliable reasoning in tasks that are knowledge-intensive and specialized, such as contract review.",
    "authors": [
      "Chunmo Zheng",
      "Saika Wong",
      "Xing Su",
      "Yinqiu Tang",
      "Ahsan Nawaz",
      "Mohamad Kassem"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-09-21T14:53:36Z",
    "pdf_url": "https://arxiv.org/pdf/2309.12132v2"
  },
  {
    "arxiv_id": "2309.11346v1",
    "entry_id": "http://arxiv.org/abs/2309.11346v1",
    "title": "GECTurk: Grammatical Error Correction and Detection Dataset for Turkish",
    "summary": "Grammatical Error Detection and Correction (GEC) tools have proven useful for native speakers and second language learners. Developing such tools requires a large amount of parallel, annotated data, which is unavailable for most languages. Synthetic data generation is a common practice to overcome the scarcity of such data. However, it is not straightforward for morphologically rich languages like Turkish due to complex writing rules that require phonological, morphological, and syntactic information. In this work, we present a flexible and extensible synthetic data generation pipeline for Turkish covering more than 20 expert-curated grammar and spelling rules (a.k.a., writing rules) implemented through complex transformation functions. Using this pipeline, we derive 130,000 high-quality parallel sentences from professionally edited articles. Additionally, we create a more realistic test set by manually annotating a set of movie reviews. We implement three baselines formulating the task as i) neural machine translation, ii) sequence tagging, and iii) prefix tuning with a pretrained decoder-only model, achieving strong results. Furthermore, we perform exhaustive experiments on out-of-domain datasets to gain insights on the transferability and robustness of the proposed approaches. Our results suggest that our corpus, GECTurk, is high-quality and allows knowledge transfer for the out-of-domain setting. To encourage further research on Turkish GEC, we release our datasets, baseline models, and the synthetic data generation pipeline at https://github.com/GGLAB-KU/gecturk.",
    "authors": [
      "Atakan Kara",
      "Farrin Marouf Sofian",
      "Andrew Bond",
      "Gözde Gül Şahin"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-09-20T14:25:44Z",
    "pdf_url": "https://arxiv.org/pdf/2309.11346v1"
  },
  {
    "arxiv_id": "2309.11285v1",
    "entry_id": "http://arxiv.org/abs/2309.11285v1",
    "title": "Overview of AuTexTification at IberLEF 2023: Detection and Attribution of Machine-Generated Text in Multiple Domains",
    "summary": "This paper presents the overview of the AuTexTification shared task as part of the IberLEF 2023 Workshop in Iberian Languages Evaluation Forum, within the framework of the SEPLN 2023 conference. AuTexTification consists of two subtasks: for Subtask 1, participants had to determine whether a text is human-authored or has been generated by a large language model. For Subtask 2, participants had to attribute a machine-generated text to one of six different text generation models. Our AuTexTification 2023 dataset contains more than 160.000 texts across two languages (English and Spanish) and five domains (tweets, reviews, news, legal, and how-to articles). A total of 114 teams signed up to participate, of which 36 sent 175 runs, and 20 of them sent their working notes. In this overview, we present the AuTexTification dataset and task, the submitted participating systems, and the results.",
    "authors": [
      "Areg Mikael Sarvazyan",
      "José Ángel González",
      "Marc Franco-Salvador",
      "Francisco Rangel",
      "Berta Chulvi",
      "Paolo Rosso"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-20T13:10:06Z",
    "pdf_url": "https://arxiv.org/pdf/2309.11285v1"
  },
  {
    "arxiv_id": "2309.10694v2",
    "entry_id": "http://arxiv.org/abs/2309.10694v2",
    "title": "\"With Great Power Comes Great Responsibility!\": Student and Instructor Perspectives on the influence of LLMs on Undergraduate Engineering Education",
    "summary": "The rise in popularity of Large Language Models (LLMs) has prompted discussions in academic circles, with students exploring LLM-based tools for coursework inquiries and instructors exploring them for teaching and research. Even though a lot of work is underway to create LLM-based tools tailored for students and instructors, there is a lack of comprehensive user studies that capture the perspectives of students and instructors regarding LLMs. This paper addresses this gap by conducting surveys and interviews within undergraduate engineering universities in India. Using 1306 survey responses among students, 112 student interviews, and 27 instructor interviews around the academic usage of ChatGPT (a popular LLM), this paper offers insights into the current usage patterns, perceived benefits, threats, and challenges, as well as recommendations for enhancing the adoption of LLMs among students and instructors. These insights are further utilized to discuss the practical implications of LLMs in undergraduate engineering education and beyond.",
    "authors": [
      "Ishika Joshi",
      "Ritvik Budhiraja",
      "Pranav Deepak Tanna",
      "Lovenya Jain",
      "Mihika Deshpande",
      "Arjun Srivastava",
      "Srinivas Rallapalli",
      "Harshal D Akolekar",
      "Jagat Sesh Challa",
      "Dhruv Kumar"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CY",
      "cs.ET"
    ],
    "published": "2023-09-19T15:29:12Z",
    "pdf_url": "https://arxiv.org/pdf/2309.10694v2"
  },
  {
    "arxiv_id": "2309.10371v1",
    "entry_id": "http://arxiv.org/abs/2309.10371v1",
    "title": "Generative AI vs. AGI: The Cognitive Strengths and Weaknesses of Modern LLMs",
    "summary": "A moderately detailed consideration of interactive LLMs as cognitive systems is given, focusing on LLMs circa mid-2023 such as ChatGPT, GPT-4, Bard, Llama, etc.. Cognitive strengths of these systems are reviewed, and then careful attention is paid to the substantial differences between the sort of cognitive system these LLMs are, and the sort of cognitive systems human beings are. It is found that many of the practical weaknesses of these AI systems can be tied specifically to lacks in the basic cognitive architectures according to which these systems are built. It is argued that incremental improvement of such LLMs is not a viable approach to working toward human-level AGI, in practical terms given realizable amounts of compute resources. This does not imply there is nothing to learn about human-level AGI from studying and experimenting with LLMs, nor that LLMs cannot form significant parts of human-level AGI architectures that also incorporate other ideas. Social and ethical matters regarding LLMs are very briefly touched from this perspective, which implies that while care should be taken regarding misinformation and other issues, and economic upheavals will need their own social remedies based on their unpredictable course as with any powerfully impactful technology, overall the sort of policy needed as regards modern LLMs is quite different than would be the case if a more credible approximation to human-level AGI were at hand.",
    "authors": [
      "Ben Goertzel"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-09-19T07:12:55Z",
    "pdf_url": "https://arxiv.org/pdf/2309.10371v1"
  },
  {
    "arxiv_id": "2309.10066v2",
    "entry_id": "http://arxiv.org/abs/2309.10066v2",
    "title": "Automatic Personalized Impression Generation for PET Reports Using Large Language Models",
    "summary": "In this study, we aimed to determine if fine-tuned large language models (LLMs) can generate accurate, personalized impressions for whole-body PET reports. Twelve language models were trained on a corpus of PET reports using the teacher-forcing algorithm, with the report findings as input and the clinical impressions as reference. An extra input token encodes the reading physician's identity, allowing models to learn physician-specific reporting styles. Our corpus comprised 37,370 retrospective PET reports collected from our institution between 2010 and 2022. To identify the best LLM, 30 evaluation metrics were benchmarked against quality scores from two nuclear medicine (NM) physicians, with the most aligned metrics selecting the model for expert evaluation. In a subset of data, model-generated impressions and original clinical impressions were assessed by three NM physicians according to 6 quality dimensions (3-point scale) and an overall utility score (5-point scale). Each physician reviewed 12 of their own reports and 12 reports from other physicians. Bootstrap resampling was used for statistical analysis. Of all evaluation metrics, domain-adapted BARTScore and PEGASUSScore showed the highest Spearman's rank correlations (0.568 and 0.563) with physician preferences. Based on these metrics, the fine-tuned PEGASUS model was selected as the top LLM. When physicians reviewed PEGASUS-generated impressions in their own style, 89% were considered clinically acceptable, with a mean utility score of 4.08 out of 5. Physicians rated these personalized impressions as comparable in overall utility to the impressions dictated by other physicians (4.03, P=0.41). In conclusion, personalized impressions generated by PEGASUS were clinically useful, highlighting its potential to expedite PET reporting.",
    "authors": [
      "Xin Tie",
      "Muheon Shin",
      "Ali Pirasteh",
      "Nevein Ibrahim",
      "Zachary Huemann",
      "Sharon M. Castellino",
      "Kara M. Kelly",
      "John Garrett",
      "Junjie Hu",
      "Steve Y. Cho",
      "Tyler J. Bradshaw"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "physics.med-ph"
    ],
    "published": "2023-09-18T18:33:40Z",
    "pdf_url": "https://arxiv.org/pdf/2309.10066v2"
  },
  {
    "arxiv_id": "2309.09582v2",
    "entry_id": "http://arxiv.org/abs/2309.09582v2",
    "title": "Fabricator: An Open Source Toolkit for Generating Labeled Training Data with Teacher LLMs",
    "summary": "Most NLP tasks are modeled as supervised learning and thus require labeled training data to train effective models. However, manually producing such data at sufficient quality and quantity is known to be costly and time-intensive. Current research addresses this bottleneck by exploring a novel paradigm called zero-shot learning via dataset generation. Here, a powerful LLM is prompted with a task description to generate labeled data that can be used to train a downstream NLP model. For instance, an LLM might be prompted to \"generate 500 movie reviews with positive overall sentiment, and another 500 with negative sentiment.\" The generated data could then be used to train a binary sentiment classifier, effectively leveraging an LLM as a teacher to a smaller student model. With this demo, we introduce Fabricator, an open-source Python toolkit for dataset generation. Fabricator implements common dataset generation workflows, supports a wide range of downstream NLP tasks (such as text classification, question answering, and entity recognition), and is integrated with well-known libraries to facilitate quick experimentation. With Fabricator, we aim to support researchers in conducting reproducible dataset generation experiments using LLMs and help practitioners apply this approach to train models for downstream tasks.",
    "authors": [
      "Jonas Golde",
      "Patrick Haller",
      "Felix Hamborg",
      "Julian Risch",
      "Alan Akbik"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-09-18T08:45:47Z",
    "pdf_url": "https://arxiv.org/pdf/2309.09582v2"
  },
  {
    "arxiv_id": "2309.08836v2",
    "entry_id": "http://arxiv.org/abs/2309.08836v2",
    "title": "Bias and Fairness in Chatbots: An Overview",
    "summary": "Chatbots have been studied for more than half a century. With the rapid development of natural language processing (NLP) technologies in recent years, chatbots using large language models (LLMs) have received much attention nowadays. Compared with traditional ones, modern chatbots are more powerful and have been used in real-world applications. There are however, bias and fairness concerns in modern chatbot design. Due to the huge amounts of training data, extremely large model sizes, and lack of interpretability, bias mitigation and fairness preservation of modern chatbots are challenging. Thus, a comprehensive overview on bias and fairness in chatbot systems is given in this paper. The history of chatbots and their categories are first reviewed. Then, bias sources and potential harms in applications are analyzed. Considerations in designing fair and unbiased chatbot systems are examined. Finally, future research directions are discussed.",
    "authors": [
      "Jintang Xue",
      "Yun-Cheng Wang",
      "Chengwei Wei",
      "Xiaofeng Liu",
      "Jonghye Woo",
      "C. -C. Jay Kuo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2023-09-16T02:01:18Z",
    "pdf_url": "https://arxiv.org/pdf/2309.08836v2"
  },
  {
    "arxiv_id": "2309.08817v1",
    "entry_id": "http://arxiv.org/abs/2309.08817v1",
    "title": "GPT as a Baseline for Recommendation Explanation Texts",
    "summary": "In this work, we establish a baseline potential for how modern model-generated text explanations of movie recommendations may help users, and explore what different components of these text explanations that users like or dislike, especially in contrast to existing human movie reviews. We found that participants gave no significantly different rankings between movies, nor did they give significantly different individual quality scores to reviews of movies that they had never seen before. However, participants did mark reviews as significantly better when they were movies they had seen before. We also explore specific aspects of movie review texts that participants marked as important for each quality. Overall, we establish that modern LLMs are a promising source of recommendation explanations, and we intend on further exploring personalizable text explanations in the future.",
    "authors": [
      "Joyce Zhou",
      "Thorsten Joachims"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2023-09-16T00:00:44Z",
    "pdf_url": "https://arxiv.org/pdf/2309.08817v1"
  },
  {
    "arxiv_id": "2309.08788v2",
    "entry_id": "http://arxiv.org/abs/2309.08788v2",
    "title": "BioinspiredLLM: Conversational Large Language Model for the Mechanics of Biological and Bio-inspired Materials",
    "summary": "The study of biological materials and bio-inspired materials science is well established; however, surprisingly little knowledge has been systematically translated to engineering solutions. To accelerate discovery and guide insights, an open-source autoregressive transformer large language model (LLM), BioinspiredLLM, is reported. The model was finetuned with a corpus of over a thousand peer-reviewed articles in the field of structural biological and bio-inspired materials and can be prompted to recall information, assist with research tasks, and function as an engine for creativity. The model has proven that it is able to accurately recall information about biological materials and is further enhanced with enhanced reasoning ability, as well as with retrieval-augmented generation to incorporate new data during generation that can also help to traceback sources, update the knowledge base, and connect knowledge domains. BioinspiredLLM also has been shown to develop sound hypotheses regarding biological materials design and remarkably so for materials that have never been explicitly studied before. Lastly, the model showed impressive promise in collaborating with other generative artificial intelligence models in a workflow that can reshape the traditional materials design process. This collaborative generative artificial intelligence method can stimulate and enhance bio-inspired materials design workflows. Biological materials are at a critical intersection of multiple scientific fields and models like BioinspiredLLM help to connect knowledge domains.",
    "authors": [
      "Rachel K. Luu",
      "Markus J. Buehler"
    ],
    "categories": [
      "cond-mat.mtrl-sci",
      "cond-mat.dis-nn",
      "cond-mat.soft",
      "cs.LG",
      "nlin.AO"
    ],
    "published": "2023-09-15T22:12:44Z",
    "pdf_url": "https://arxiv.org/pdf/2309.08788v2"
  },
  {
    "arxiv_id": "2309.07864v3",
    "entry_id": "http://arxiv.org/abs/2309.07864v3",
    "title": "The Rise and Potential of Large Language Model Based Agents: A Survey",
    "summary": "For a long time, humanity has pursued artificial intelligence (AI) equivalent to or surpassing the human level, with AI agents considered a promising vehicle for this pursuit. AI agents are artificial entities that sense their environment, make decisions, and take actions. Many efforts have been made to develop intelligent agents, but they mainly focus on advancement in algorithms or training strategies to enhance specific capabilities or performance on particular tasks. Actually, what the community lacks is a general and powerful model to serve as a starting point for designing AI agents that can adapt to diverse scenarios. Due to the versatile capabilities they demonstrate, large language models (LLMs) are regarded as potential sparks for Artificial General Intelligence (AGI), offering hope for building general AI agents. Many researchers have leveraged LLMs as the foundation to build AI agents and have achieved significant progress. In this paper, we perform a comprehensive survey on LLM-based agents. We start by tracing the concept of agents from its philosophical origins to its development in AI, and explain why LLMs are suitable foundations for agents. Building upon this, we present a general framework for LLM-based agents, comprising three main components: brain, perception, and action, and the framework can be tailored for different applications. Subsequently, we explore the extensive applications of LLM-based agents in three aspects: single-agent scenarios, multi-agent scenarios, and human-agent cooperation. Following this, we delve into agent societies, exploring the behavior and personality of LLM-based agents, the social phenomena that emerge from an agent society, and the insights they offer for human society. Finally, we discuss several key topics and open problems within the field. A repository for the related papers at https://github.com/WooooDyy/LLM-Agent-Paper-List.",
    "authors": [
      "Zhiheng Xi",
      "Wenxiang Chen",
      "Xin Guo",
      "Wei He",
      "Yiwen Ding",
      "Boyang Hong",
      "Ming Zhang",
      "Junzhe Wang",
      "Senjie Jin",
      "Enyu Zhou",
      "Rui Zheng",
      "Xiaoran Fan",
      "Xiao Wang",
      "Limao Xiong",
      "Yuhao Zhou",
      "Weiran Wang",
      "Changhao Jiang",
      "Yicheng Zou",
      "Xiangyang Liu",
      "Zhangyue Yin",
      "Shihan Dou",
      "Rongxiang Weng",
      "Wensen Cheng",
      "Qi Zhang",
      "Wenjuan Qin",
      "Yongyan Zheng",
      "Xipeng Qiu",
      "Xuanjing Huang",
      "Tao Gui"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-09-14T17:12:03Z",
    "pdf_url": "https://arxiv.org/pdf/2309.07864v3"
  },
  {
    "arxiv_id": "2309.07689v1",
    "entry_id": "http://arxiv.org/abs/2309.07689v1",
    "title": "Detecting ChatGPT: A Survey of the State of Detecting ChatGPT-Generated Text",
    "summary": "While recent advancements in the capabilities and widespread accessibility of generative language models, such as ChatGPT (OpenAI, 2022), have brought about various benefits by generating fluent human-like text, the task of distinguishing between human- and large language model (LLM) generated text has emerged as a crucial problem. These models can potentially deceive by generating artificial text that appears to be human-generated. This issue is particularly significant in domains such as law, education, and science, where ensuring the integrity of text is of the utmost importance. This survey provides an overview of the current approaches employed to differentiate between texts generated by humans and ChatGPT. We present an account of the different datasets constructed for detecting ChatGPT-generated text, the various methods utilized, what qualitative analyses into the characteristics of human versus ChatGPT-generated text have been performed, and finally, summarize our findings into general insights",
    "authors": [
      "Mahdi Dhaini",
      "Wessel Poelman",
      "Ege Erdogan"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-09-14T13:05:20Z",
    "pdf_url": "https://arxiv.org/pdf/2309.07689v1"
  },
  {
    "arxiv_id": "2309.06799v5",
    "entry_id": "http://arxiv.org/abs/2309.06799v5",
    "title": "When Geoscience Meets Foundation Models: Towards General Geoscience Artificial Intelligence System",
    "summary": "Artificial intelligence (AI) has significantly advanced Earth sciences, yet its full potential in to comprehensively modeling Earth's complex dynamics remains unrealized. Geoscience foundation models (GFMs) emerge as a paradigm-shifting solution, integrating extensive cross-disciplinary data to enhance the simulation and understanding of Earth system dynamics. These data-centric AI models extract insights from petabytes of structured and unstructured data, effectively addressing the complexities of Earth systems that traditional models struggle to capture. The unique strengths of GFMs include flexible task specification, diverse input-output capabilities, and multi-modal knowledge representation, enabling analyses that surpass those of individual data sources or traditional AI methods. This review not only highlights the key advantages of GFMs, but also presents essential techniques for their construction, with a focus on transformers, pre-training, and adaptation strategies. Subsequently, we examine recent advancements in GFMs, including large language models, vision models, and vision-language models, particularly emphasizing the potential applications in remote sensing. Additionally, the review concludes with a comprehensive analysis of the challenges and future trends in GFMs, addressing five critical aspects: data integration, model complexity, uncertainty quantification, interdisciplinary collaboration, and concerns related to privacy, trust, and security. This review offers a comprehensive overview of emerging geoscientific research paradigms, emphasizing the untapped opportunities at the intersection of advanced AI techniques and geoscience. It examines major methodologies, showcases advances in large-scale models, and discusses the challenges and prospects that will shape the future landscape of GFMs.",
    "authors": [
      "Hao Zhang",
      "Jin-Jian Xu",
      "Hong-Wei Cui",
      "Lin Li",
      "Yaowen Yang",
      "Chao-Sheng Tang",
      "Niklas Boers"
    ],
    "categories": [
      "cs.AI",
      "physics.geo-ph"
    ],
    "published": "2023-09-13T08:44:09Z",
    "pdf_url": "https://arxiv.org/pdf/2309.06799v5"
  },
  {
    "arxiv_id": "2309.06794v1",
    "entry_id": "http://arxiv.org/abs/2309.06794v1",
    "title": "Cognitive Mirage: A Review of Hallucinations in Large Language Models",
    "summary": "As large language models continue to develop in the field of AI, text generation systems are susceptible to a worrisome phenomenon known as hallucination. In this study, we summarize recent compelling insights into hallucinations in LLMs. We present a novel taxonomy of hallucinations from various text generation tasks, thus provide theoretical insights, detection methods and improvement approaches. Based on this, future research directions are proposed. Our contribution are threefold: (1) We provide a detailed and complete taxonomy for hallucinations appearing in text generation tasks; (2) We provide theoretical analyses of hallucinations in LLMs and provide existing detection and improvement methods; (3) We propose several research directions that can be developed in the future. As hallucinations garner significant attention from the community, we will maintain updates on relevant research progress.",
    "authors": [
      "Hongbin Ye",
      "Tong Liu",
      "Aijia Zhang",
      "Wei Hua",
      "Weiqiang Jia"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-13T08:33:09Z",
    "pdf_url": "https://arxiv.org/pdf/2309.06794v1"
  },
  {
    "arxiv_id": "2309.06089v3",
    "entry_id": "http://arxiv.org/abs/2309.06089v3",
    "title": "Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies",
    "summary": "The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\\textit{IT}) that uses each language sequentially and cross-lingual validation (\\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several languages, show that the \\textit{IT} cross-lingual strategy outperforms \\textit{CLV} for the target language. Our findings indicate that, in the majority of cases, the \\textit{CLV} strategy demonstrates superior retention of knowledge in the base language (English) compared to the \\textit{IT} strategy, when evaluating catastrophic forgetting in multiple cross-lingual transfers.",
    "authors": [
      "Boshko Koloski",
      "Blaž Škrlj",
      "Marko Robnik-Šikonja",
      "Senja Pollak"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-09-12T09:37:08Z",
    "pdf_url": "https://arxiv.org/pdf/2309.06089v3"
  },
  {
    "arxiv_id": "2309.05238v3",
    "entry_id": "http://arxiv.org/abs/2309.05238v3",
    "title": "Generating Natural Language Queries for More Effective Systematic Review Screening Prioritisation",
    "summary": "Screening prioritisation in medical systematic reviews aims to rank the set of documents retrieved by complex Boolean queries. Prioritising the most important documents ensures that subsequent review steps can be carried out more efficiently and effectively. The current state of the art uses the final title of the review as a query to rank the documents using BERT-based neural rankers. However, the final title is only formulated at the end of the review process, which makes this approach impractical as it relies on ex post facto information. At the time of screening, only a rough working title is available, with which the BERT-based ranker performs significantly worse than with the final title. In this paper, we explore alternative sources of queries for prioritising screening, such as the Boolean query used to retrieve the documents to be screened and queries generated by instruction-based generative large-scale language models such as ChatGPT and Alpaca. Our best approach is not only viable based on the information available at the time of screening, but also has similar effectiveness to the final title.",
    "authors": [
      "Shuai Wang",
      "Harrisen Scells",
      "Martin Potthast",
      "Bevan Koopman",
      "Guido Zuccon"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2023-09-11T05:12:14Z",
    "pdf_url": "https://arxiv.org/pdf/2309.05238v3"
  },
  {
    "arxiv_id": "2309.05680v2",
    "entry_id": "http://arxiv.org/abs/2309.05680v2",
    "title": "Evaluating Chatbots to Promote Users' Trust -- Practices and Open Problems",
    "summary": "Chatbots, the common moniker for collaborative assistants, are Artificial Intelligence (AI) software that enables people to naturally interact with them to get tasks done. Although chatbots have been studied since the dawn of AI, they have particularly caught the imagination of the public and businesses since the launch of easy-to-use and general-purpose Large Language Model-based chatbots like ChatGPT. As businesses look towards chatbots as a potential technology to engage users, who may be end customers, suppliers, or even their own employees, proper testing of chatbots is important to address and mitigate issues of trust related to service or product performance, user satisfaction and long-term unintended consequences for society. This paper reviews current practices for chatbot testing, identifies gaps as open problems in pursuit of user trust, and outlines a path forward.",
    "authors": [
      "Biplav Srivastava",
      "Kausik Lakkaraju",
      "Tarmo Koppel",
      "Vignesh Narayanan",
      "Ashish Kundu",
      "Sachindra Joshi"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.SE"
    ],
    "published": "2023-09-09T22:40:30Z",
    "pdf_url": "https://arxiv.org/pdf/2309.05680v2"
  },
  {
    "arxiv_id": "2309.04019v2",
    "entry_id": "http://arxiv.org/abs/2309.04019v2",
    "title": "Evaluation of large language models for discovery of gene set function",
    "summary": "Gene set analysis is a mainstay of functional genomics, but it relies on curated databases of gene functions that are incomplete. Here we evaluate five Large Language Models (LLMs) for their ability to discover the common biological functions represented by a gene set, substantiated by supporting rationale, citations and a confidence assessment. Benchmarking against canonical gene sets from the Gene Ontology, GPT-4 confidently recovered the curated name or a more general concept (73% of cases), while benchmarking against random gene sets correctly yielded zero confidence. Gemini-Pro and Mixtral-Instruct showed ability in naming but were falsely confident for random sets, whereas Llama2-70b had poor performance overall. In gene sets derived from 'omics data, GPT-4 identified novel functions not reported by classical functional enrichment (32% of cases), which independent review indicated were largely verifiable and not hallucinations. The ability to rapidly synthesize common gene functions positions LLMs as valuable 'omics assistants.",
    "authors": [
      "Mengzhou Hu",
      "Sahar Alkhairy",
      "Ingoo Lee",
      "Rudolf T. Pillich",
      "Dylan Fong",
      "Kevin Smith",
      "Robin Bachelder",
      "Trey Ideker",
      "Dexter Pratt"
    ],
    "categories": [
      "q-bio.GN",
      "cs.AI",
      "cs.CL",
      "q-bio.MN"
    ],
    "published": "2023-09-07T21:10:48Z",
    "pdf_url": "https://arxiv.org/pdf/2309.04019v2"
  },
  {
    "arxiv_id": "2309.02427v3",
    "entry_id": "http://arxiv.org/abs/2309.02427v3",
    "title": "Cognitive Architectures for Language Agents",
    "summary": "Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.",
    "authors": [
      "Theodore R. Sumers",
      "Shunyu Yao",
      "Karthik Narasimhan",
      "Thomas L. Griffiths"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SC"
    ],
    "published": "2023-09-05T17:56:20Z",
    "pdf_url": "https://arxiv.org/pdf/2309.02427v3"
  },
  {
    "arxiv_id": "2309.02045v2",
    "entry_id": "http://arxiv.org/abs/2309.02045v2",
    "title": "Enhance Multi-domain Sentiment Analysis of Review Texts through Prompting Strategies",
    "summary": "Large Language Models (LLMs) have made significant strides in both scientific research and practical applications. Existing studies have demonstrated the state-of-the-art (SOTA) performance of LLMs in various natural language processing tasks. However, the question of how to further enhance LLMs' performance in specific task using prompting strategies remains a pivotal concern. This paper explores the enhancement of LLMs' performance in sentiment analysis through the application of prompting strategies. We formulate the process of prompting for sentiment analysis tasks and introduce two novel strategies tailored for sentiment analysis: RolePlaying (RP) prompting and Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT prompting strategy which is a combination of RP prompting and CoT prompting. We conduct comparative experiments on three distinct domain datasets to evaluate the effectiveness of the proposed sentiment analysis strategies. The results demonstrate that the adoption of the proposed prompting strategies leads to a increasing enhancement in sentiment analysis accuracy. Further, the CoT prompting strategy exhibits a notable impact on implicit sentiment analysis, with the RP-CoT prompting strategy delivering the most superior performance among all strategies.",
    "authors": [
      "Yajing Wang",
      "Zongwei Luo"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-05T08:44:23Z",
    "pdf_url": "https://arxiv.org/pdf/2309.02045v2"
  },
  {
    "arxiv_id": "2310.00010v1",
    "entry_id": "http://arxiv.org/abs/2310.00010v1",
    "title": "Artificial Empathy Classification: A Survey of Deep Learning Techniques, Datasets, and Evaluation Scales",
    "summary": "From the last decade, researchers in the field of machine learning (ML) and assistive developmental robotics (ADR) have taken an interest in artificial empathy (AE) as a possible future paradigm for human-robot interaction (HRI). Humans learn empathy since birth, therefore, it is challenging to instill this sense in robots and intelligent machines. Nevertheless, by training over a vast amount of data and time, imitating empathy, to a certain extent, can be possible for robots. Training techniques for AE, along with findings from the field of empathetic AI research, are ever-evolving. The standard workflow for artificial empathy consists of three stages: 1) Emotion Recognition (ER) using the retrieved features from video or textual data, 2) analyzing the perceived emotion or degree of empathy to choose the best course of action, and 3) carrying out a response action. Recent studies that show AE being used with virtual agents or robots often include Deep Learning (DL) techniques. For instance, models like VGGFace are used to conduct ER. Semi-supervised models like Autoencoders generate the corresponding emotional states and behavioral responses. However, there has not been any study that presents an independent approach for evaluating AE, or the degree to which a reaction was empathetic. This paper aims to investigate and evaluate existing works for measuring and evaluating empathy, as well as the datasets that have been collected and used so far. Our goal is to highlight and facilitate the use of state-of-the-art methods in the area of AE by comparing their performance. This will aid researchers in the area of AE in selecting their approaches with precision.",
    "authors": [
      "Sharjeel Tahir",
      "Syed Afaq Shah",
      "Jumana Abu-Khalaf"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-04T16:02:59Z",
    "pdf_url": "https://arxiv.org/pdf/2310.00010v1"
  },
  {
    "arxiv_id": "2309.01291v3",
    "entry_id": "http://arxiv.org/abs/2309.01291v3",
    "title": "Generative Social Choice",
    "summary": "The mathematical study of voting, social choice theory, has traditionally only been applicable to choices among a few predetermined alternatives, but not to open-ended decisions such as collectively selecting a textual statement. We introduce generative social choice, a design methodology for open-ended democratic processes that combines the rigor of social choice theory with the capability of large language models to generate text and extrapolate preferences. Our framework divides the design of AI-augmented democratic processes into two components: first, proving that the process satisfies representation guarantees when given access to oracle queries; second, empirically validating that these queries can be approximately implemented using a large language model. We apply this framework to the problem of summarizing free-form opinions into a proportionally representative slate of opinion statements; specifically, we develop a democratic process with representation guarantees and use this process to portray the opinions of participants in a survey about abortion policy. In a trial with 100 representative US residents, we find that 84 out of 100 participants feel \"excellently\" or \"exceptionally\" represented by the slate of five statements we extracted.",
    "authors": [
      "Sara Fish",
      "Paul Gölz",
      "David C. Parkes",
      "Ariel D. Procaccia",
      "Gili Rusak",
      "Itai Shapira",
      "Manuel Wüthrich"
    ],
    "categories": [
      "cs.GT",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-03T23:47:21Z",
    "pdf_url": "https://arxiv.org/pdf/2309.01291v3"
  },
  {
    "arxiv_id": "2309.01219v3",
    "entry_id": "http://arxiv.org/abs/2309.01219v3",
    "title": "Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models",
    "summary": "While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.",
    "authors": [
      "Yue Zhang",
      "Yafu Li",
      "Leyang Cui",
      "Deng Cai",
      "Lemao Liu",
      "Tingchen Fu",
      "Xinting Huang",
      "Enbo Zhao",
      "Yu Zhang",
      "Chen Xu",
      "Yulong Chen",
      "Longyue Wang",
      "Anh Tuan Luu",
      "Wei Bi",
      "Freda Shi",
      "Shuming Shi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2023-09-03T16:56:48Z",
    "pdf_url": "https://arxiv.org/pdf/2309.01219v3"
  },
  {
    "arxiv_id": "2309.01157v2",
    "entry_id": "http://arxiv.org/abs/2309.01157v2",
    "title": "Large Language Models for Generative Recommendation: A Survey and Visionary Discussions",
    "summary": "Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields, e.g., recommender systems (RS). However, most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor), which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages, such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods, and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS tasks. We hope that this survey can provide the context and guidance needed to explore this interesting and emerging topic.",
    "authors": [
      "Lei Li",
      "Yongfeng Zhang",
      "Dugang Liu",
      "Li Chen"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-09-03T12:33:47Z",
    "pdf_url": "https://arxiv.org/pdf/2309.01157v2"
  },
  {
    "arxiv_id": "2309.01029v3",
    "entry_id": "http://arxiv.org/abs/2309.01029v3",
    "title": "Explainability for Large Language Models: A Survey",
    "summary": "Large language models (LLMs) have demonstrated impressive capabilities in natural language processing. However, their internal mechanisms are still unclear and this lack of transparency poses unwanted risks for downstream applications. Therefore, understanding and explaining these models is crucial for elucidating their behaviors, limitations, and social impacts. In this paper, we introduce a taxonomy of explainability techniques and provide a structured overview of methods for explaining Transformer-based language models. We categorize techniques based on the training paradigms of LLMs: traditional fine-tuning-based paradigm and prompting-based paradigm. For each paradigm, we summarize the goals and dominant approaches for generating local explanations of individual predictions and global explanations of overall model knowledge. We also discuss metrics for evaluating generated explanations, and discuss how explanations can be leveraged to debug models and improve performance. Lastly, we examine key challenges and emerging opportunities for explanation techniques in the era of LLMs in comparison to conventional machine learning models.",
    "authors": [
      "Haiyan Zhao",
      "Hanjie Chen",
      "Fan Yang",
      "Ninghao Liu",
      "Huiqi Deng",
      "Hengyi Cai",
      "Shuaiqiang Wang",
      "Dawei Yin",
      "Mengnan Du"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-09-02T22:14:26Z",
    "pdf_url": "https://arxiv.org/pdf/2309.01029v3"
  },
  {
    "arxiv_id": "2309.12356v1",
    "entry_id": "http://arxiv.org/abs/2309.12356v1",
    "title": "A Critical Examination of the Ethics of AI-Mediated Peer Review",
    "summary": "Recent advancements in artificial intelligence (AI) systems, including large language models like ChatGPT, offer promise and peril for scholarly peer review. On the one hand, AI can enhance efficiency by addressing issues like long publication delays. On the other hand, it brings ethical and social concerns that could compromise the integrity of the peer review process and outcomes. However, human peer review systems are also fraught with related problems, such as biases, abuses, and a lack of transparency, which already diminish credibility. While there is increasing attention to the use of AI in peer review, discussions revolve mainly around plagiarism and authorship in academic journal publishing, ignoring the broader epistemic, social, cultural, and societal epistemic in which peer review is positioned. The legitimacy of AI-driven peer review hinges on the alignment with the scientific ethos, encompassing moral and epistemic norms that define appropriate conduct in the scholarly community. In this regard, there is a \"norm-counternorm continuum,\" where the acceptability of AI in peer review is shaped by institutional logics, ethical practices, and internal regulatory mechanisms. The discussion here emphasizes the need to critically assess the legitimacy of AI-driven peer review, addressing the benefits and downsides relative to the broader epistemic, social, ethical, and regulatory factors that sculpt its implementation and impact.",
    "authors": [
      "Laurie A. Schintler",
      "Connie L. McNeely",
      "James Witte"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-09-02T18:14:10Z",
    "pdf_url": "https://arxiv.org/pdf/2309.12356v1"
  },
  {
    "arxiv_id": "2309.00810v1",
    "entry_id": "http://arxiv.org/abs/2309.00810v1",
    "title": "RenAIssance: A Survey into AI Text-to-Image Generation in the Era of Large Model",
    "summary": "Text-to-image generation (TTI) refers to the usage of models that could process text input and generate high fidelity images based on text descriptions. Text-to-image generation using neural networks could be traced back to the emergence of Generative Adversial Network (GAN), followed by the autoregressive Transformer. Diffusion models are one prominent type of generative model used for the generation of images through the systematic introduction of noises with repeating steps. As an effect of the impressive results of diffusion models on image synthesis, it has been cemented as the major image decoder used by text-to-image models and brought text-to-image generation to the forefront of machine-learning (ML) research. In the era of large models, scaling up model size and the integration with large language models have further improved the performance of TTI models, resulting the generation result nearly indistinguishable from real-world images, revolutionizing the way we retrieval images. Our explorative study has incentivised us to think that there are further ways of scaling text-to-image models with the combination of innovative model architectures and prediction enhancement techniques. We have divided the work of this survey into five main sections wherein we detail the frameworks of major literature in order to delve into the different types of text-to-image generation methods. Following this we provide a detailed comparison and critique of these methods and offer possible pathways of improvement for future work. In the future work, we argue that TTI development could yield impressive productivity improvements for creation, particularly in the context of the AIGC era, and could be extended to more complex tasks such as video generation and 3D generation.",
    "authors": [
      "Fengxiang Bie",
      "Yibo Yang",
      "Zhongzhu Zhou",
      "Adam Ghanem",
      "Minjia Zhang",
      "Zhewei Yao",
      "Xiaoxia Wu",
      "Connor Holmes",
      "Pareesa Golnari",
      "David A. Clifton",
      "Yuxiong He",
      "Dacheng Tao",
      "Shuaiwen Leon Song"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-09-02T03:27:20Z",
    "pdf_url": "https://arxiv.org/pdf/2309.00810v1"
  },
  {
    "arxiv_id": "2309.00770v3",
    "entry_id": "http://arxiv.org/abs/2309.00770v3",
    "title": "Bias and Fairness in Large Language Models: A Survey",
    "summary": "Rapid advancements of large language models (LLMs) have enabled the processing, understanding, and generation of human-like text, with increasing integration into systems that touch our social sphere. Despite this success, these models can learn, perpetuate, and amplify harmful social biases. In this paper, we present a comprehensive survey of bias evaluation and mitigation techniques for LLMs. We first consolidate, formalize, and expand notions of social bias and fairness in natural language processing, defining distinct facets of harm and introducing several desiderata to operationalize fairness for LLMs. We then unify the literature by proposing three intuitive taxonomies, two for bias evaluation, namely metrics and datasets, and one for mitigation. Our first taxonomy of metrics for bias evaluation disambiguates the relationship between metrics and evaluation datasets, and organizes metrics by the different levels at which they operate in a model: embeddings, probabilities, and generated text. Our second taxonomy of datasets for bias evaluation categorizes datasets by their structure as counterfactual inputs or prompts, and identifies the targeted harms and social groups; we also release a consolidation of publicly-available datasets for improved access. Our third taxonomy of techniques for bias mitigation classifies methods by their intervention during pre-processing, in-training, intra-processing, and post-processing, with granular subcategories that elucidate research trends. Finally, we identify open problems and challenges for future work. Synthesizing a wide range of recent research, we aim to provide a clear guide of the existing literature that empowers researchers and practitioners to better understand and prevent the propagation of bias in LLMs.",
    "authors": [
      "Isabel O. Gallegos",
      "Ryan A. Rossi",
      "Joe Barrow",
      "Md Mehrab Tanjim",
      "Sungchul Kim",
      "Franck Dernoncourt",
      "Tong Yu",
      "Ruiyi Zhang",
      "Nesreen K. Ahmed"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2023-09-02T00:32:55Z",
    "pdf_url": "https://arxiv.org/pdf/2309.00770v3"
  },
  {
    "arxiv_id": "2309.00087v1",
    "entry_id": "http://arxiv.org/abs/2309.00087v1",
    "title": "Large language models in medicine: the potentials and pitfalls",
    "summary": "Large language models (LLMs) have been applied to tasks in healthcare, ranging from medical exam questions to responding to patient questions. With increasing institutional partnerships between companies producing LLMs and healthcare systems, real world clinical application is coming closer to reality. As these models gain traction, it is essential for healthcare practitioners to understand what LLMs are, their development, their current and potential applications, and the associated pitfalls when utilized in medicine. This review and accompanying tutorial aim to give an overview of these topics to aid healthcare practitioners in understanding the rapidly changing landscape of LLMs as applied to medicine.",
    "authors": [
      "Jesutofunmi A. Omiye",
      "Haiwen Gui",
      "Shawheen J. Rezaei",
      "James Zou",
      "Roxana Daneshjou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2023-08-31T19:06:39Z",
    "pdf_url": "https://arxiv.org/pdf/2309.00087v1"
  },
  {
    "arxiv_id": "2308.16785v2",
    "entry_id": "http://arxiv.org/abs/2308.16785v2",
    "title": "Agent Teaming Situation Awareness (ATSA): A Situation Awareness Framework for Human-AI Teaming",
    "summary": "The rapid advancements in artificial intelligence (AI) have led to a growing trend of human-AI teaming (HAT) in various fields. As machines continue to evolve from mere automation to a state of autonomy, they are increasingly exhibiting unexpected behaviors and human-like cognitive/intelligent capabilities, including situation awareness (SA). This shift has the potential to enhance the performance of mixed human-AI teams over all-human teams, underscoring the need for a better understanding of the dynamic SA interactions between humans and machines. To this end, we provide a review of leading SA theoretical models and a new framework for SA in the HAT context based on the key features and processes of HAT. The Agent Teaming Situation Awareness (ATSA) framework unifies human and AI behavior, and involves bidirectional, and dynamic interaction. The framework is based on the individual and team SA models and elaborates on the cognitive mechanisms for modeling HAT. Similar perceptual cycles are adopted for the individual (including both human and AI) and the whole team, which is tailored to the unique requirements of the HAT context. ATSA emphasizes cohesive and effective HAT through structures and components, including teaming understanding, teaming control, and the world, as well as adhesive transactive part. We further propose several future research directions to expand on the distinctive contributions of ATSA and address the specific and pressing next steps.",
    "authors": [
      "Qi Gao",
      "Wei Xu",
      "Mowei Shen",
      "Zaifeng Gao"
    ],
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published": "2023-08-31T15:02:01Z",
    "pdf_url": "https://arxiv.org/pdf/2308.16785v2"
  },
  {
    "arxiv_id": "2308.16705v3",
    "entry_id": "http://arxiv.org/abs/2308.16705v3",
    "title": "Exploring Cross-Cultural Differences in English Hate Speech Annotations: From Dataset Construction to Analysis",
    "summary": "Warning: this paper contains content that may be offensive or upsetting.\n  Most hate speech datasets neglect the cultural diversity within a single language, resulting in a critical shortcoming in hate speech detection. To address this, we introduce CREHate, a CRoss-cultural English Hate speech dataset. To construct CREHate, we follow a two-step procedure: 1) cultural post collection and 2) cross-cultural annotation. We sample posts from the SBIC dataset, which predominantly represents North America, and collect posts from four geographically diverse English-speaking countries (Australia, United Kingdom, Singapore, and South Africa) using culturally hateful keywords we retrieve from our survey. Annotations are collected from the four countries plus the United States to establish representative labels for each country. Our analysis highlights statistically significant disparities across countries in hate speech annotations. Only 56.2% of the posts in CREHate achieve consensus among all countries, with the highest pairwise label difference rate of 26%. Qualitative analysis shows that label disagreement occurs mostly due to different interpretations of sarcasm and the personal bias of annotators on divisive topics. Lastly, we evaluate large language models (LLMs) under a zero-shot setting and show that current LLMs tend to show higher accuracies on Anglosphere country labels in CREHate. Our dataset and codes are available at: https://github.com/nlee0212/CREHate",
    "authors": [
      "Nayeon Lee",
      "Chani Jung",
      "Junho Myung",
      "Jiho Jin",
      "Jose Camacho-Collados",
      "Juho Kim",
      "Alice Oh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-08-31T13:14:47Z",
    "pdf_url": "https://arxiv.org/pdf/2308.16705v3"
  },
  {
    "arxiv_id": "2308.16688v1",
    "entry_id": "http://arxiv.org/abs/2308.16688v1",
    "title": "Using Large Language Models to Automate Category and Trend Analysis of Scientific Articles: An Application in Ophthalmology",
    "summary": "Purpose: In this paper, we present an automated method for article classification, leveraging the power of Large Language Models (LLM). The primary focus is on the field of ophthalmology, but the model is extendable to other fields. Methods: We have developed a model based on Natural Language Processing (NLP) techniques, including advanced LLMs, to process and analyze the textual content of scientific papers. Specifically, we have employed zero-shot learning (ZSL) LLM models and compared against Bidirectional and Auto-Regressive Transformers (BART) and its variants, and Bidirectional Encoder Representations from Transformers (BERT), and its variant such as distilBERT, SciBERT, PubmedBERT, BioBERT. Results: The classification results demonstrate the effectiveness of LLMs in categorizing large number of ophthalmology papers without human intervention. Results: To evalute the LLMs, we compiled a dataset (RenD) of 1000 ocular disease-related articles, which were expertly annotated by a panel of six specialists into 15 distinct categories. The model achieved mean accuracy of 0.86 and mean F1 of 0.85 based on the RenD dataset. Conclusion: The proposed framework achieves notable improvements in both accuracy and efficiency. Its application in the domain of ophthalmology showcases its potential for knowledge organization and retrieval in other domains too. We performed trend analysis that enables the researchers and clinicians to easily categorize and retrieve relevant papers, saving time and effort in literature review and information gathering as well as identification of emerging scientific trends within different disciplines. Moreover, the extendibility of the model to other scientific fields broadens its impact in facilitating research and trend analysis across diverse disciplines.",
    "authors": [
      "Hina Raja",
      "Asim Munawar",
      "Mohammad Delsoz",
      "Mohammad Elahi",
      "Yeganeh Madadi",
      "Amr Hassan",
      "Hashem Abu Serhan",
      "Onur Inam",
      "Luis Hermandez",
      "Sang Tran",
      "Wuqas Munir",
      "Alaa Abd-Alrazaq",
      "Hao Chen",
      "SiamakYousefi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-08-31T12:45:53Z",
    "pdf_url": "https://arxiv.org/pdf/2308.16688v1"
  },
  {
    "arxiv_id": "2308.16316v1",
    "entry_id": "http://arxiv.org/abs/2308.16316v1",
    "title": "Ten Years of Generative Adversarial Nets (GANs): A survey of the state-of-the-art",
    "summary": "Since their inception in 2014, Generative Adversarial Networks (GANs) have rapidly emerged as powerful tools for generating realistic and diverse data across various domains, including computer vision and other applied areas. Consisting of a discriminative network and a generative network engaged in a Minimax game, GANs have revolutionized the field of generative modeling. In February 2018, GAN secured the leading spot on the ``Top Ten Global Breakthrough Technologies List'' issued by the Massachusetts Science and Technology Review. Over the years, numerous advancements have been proposed, leading to a rich array of GAN variants, such as conditional GAN, Wasserstein GAN, CycleGAN, and StyleGAN, among many others. This survey aims to provide a general overview of GANs, summarizing the latent architecture, validation metrics, and application areas of the most widely recognized variants. We also delve into recent theoretical developments, exploring the profound connection between the adversarial principle underlying GAN and Jensen-Shannon divergence, while discussing the optimality characteristics of the GAN framework. The efficiency of GAN variants and their model architectures will be evaluated along with training obstacles as well as training solutions. In addition, a detailed discussion will be provided, examining the integration of GANs with newly developed deep learning frameworks such as Transformers, Physics-Informed Neural Networks, Large Language models, and Diffusion models. Finally, we reveal several issues as well as future research outlines in this field.",
    "authors": [
      "Tanujit Chakraborty",
      "Ujjwal Reddy K S",
      "Shraddha M. Naik",
      "Madhurima Panja",
      "Bayapureddy Manvitha"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2023-08-30T20:46:45Z",
    "pdf_url": "https://arxiv.org/pdf/2308.16316v1"
  },
  {
    "arxiv_id": "2308.14752v1",
    "entry_id": "http://arxiv.org/abs/2308.14752v1",
    "title": "AI Deception: A Survey of Examples, Risks, and Potential Solutions",
    "summary": "This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.",
    "authors": [
      "Peter S. Park",
      "Simon Goldstein",
      "Aidan O'Gara",
      "Michael Chen",
      "Dan Hendrycks"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2023-08-28T17:59:35Z",
    "pdf_url": "https://arxiv.org/pdf/2308.14752v1"
  },
  {
    "arxiv_id": "2308.14602v2",
    "entry_id": "http://arxiv.org/abs/2308.14602v2",
    "title": "Recent Progress in Energy Management of Connected Hybrid Electric Vehicles Using Reinforcement Learning",
    "summary": "The growing adoption of hybrid electric vehicles (HEVs) presents a transformative opportunity for revolutionizing transportation energy systems. The shift towards electrifying transportation aims to curb environmental concerns related to fossil fuel consumption. This necessitates efficient energy management systems (EMS) to optimize energy efficiency. The evolution of EMS from HEVs to connected hybrid electric vehicles (CHEVs) represent a pivotal shift. For HEVs, EMS now confronts the intricate energy cooperation requirements of CHEVs, necessitating advanced algorithms for route optimization, charging coordination, and load distribution. Challenges persist in both domains, including optimal energy utilization for HEVs, and cooperative eco-driving control (CED) for CHEVs across diverse vehicle types. Reinforcement learning (RL) stands out as a promising tool for addressing these challenges. Specifically, within the realm of CHEVs, the application of multi-agent reinforcement learning (MARL) emerges as a powerful approach for effectively tackling the intricacies of CED control. Despite extensive research, few reviews span from individual vehicles to multi-vehicle scenarios. This review bridges the gap, highlighting challenges, advancements, and potential contributions of RL-based solutions for future sustainable transportation systems.",
    "authors": [
      "Min Hua",
      "Bin Shuai",
      "Quan Zhou",
      "Jinhai Wang",
      "Yinglong He",
      "Hongming Xu"
    ],
    "categories": [
      "eess.SY",
      "cs.LG"
    ],
    "published": "2023-08-28T14:12:52Z",
    "pdf_url": "https://arxiv.org/pdf/2308.14602v2"
  },
  {
    "arxiv_id": "2308.14328v3",
    "entry_id": "http://arxiv.org/abs/2308.14328v3",
    "title": "Reinforcement Learning for Generative AI: A Survey",
    "summary": "Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning has become a trending research field and has stretched the limits of generative AI in both model design and application. It is reasonable to summarize and conclude advances in recent years with a comprehensive review. Although there are surveys in different application areas recently, this survey aims to shed light on a high-level review that spans a range of application areas. We provide a rigorous taxonomy in this area and make sufficient coverage on various models and applications. Notably, we also surveyed the fast-developing large language model area. We conclude this survey by showing the potential directions that might tackle the limit of current models and expand the frontiers for generative AI.",
    "authors": [
      "Yuanjiang Cao",
      "Quan Z. Sheng",
      "Julian McAuley",
      "Lina Yao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-08-28T06:15:14Z",
    "pdf_url": "https://arxiv.org/pdf/2308.14328v3"
  },
  {
    "arxiv_id": "2308.14269v1",
    "entry_id": "http://arxiv.org/abs/2308.14269v1",
    "title": "Utilizing Mood-Inducing Background Music in Human-Robot Interaction",
    "summary": "Past research has clearly established that music can affect mood and that mood affects emotional and cognitive processing, and thus decision-making. It follows that if a robot interacting with a person needs to predict the person's behavior, knowledge of the music the person is listening to when acting is a potentially relevant feature. To date, however, there has not been any concrete evidence that a robot can improve its human-interactive decision-making by taking into account what the person is listening to. This research fills this gap by reporting the results of an experiment in which human participants were required to complete a task in the presence of an autonomous agent while listening to background music. Specifically, the participants drove a simulated car through an intersection while listening to music. The intersection was not empty, as another simulated vehicle, controlled autonomously, was also crossing the intersection in a different direction. Our results clearly indicate that such background information can be effectively incorporated in an agent's world representation in order to better predict people's behavior. We subsequently analyze how knowledge of music impacted both participant behavior and the resulting learned policy.\\setcounter{footnote}{2}\\footnote{An earlier version of part of the material in this paper appeared originally in the first author's Ph.D. Dissertation~\\cite{liebman2020sequential} but it has not appeared in any pear-reviewed conference or journal.}",
    "authors": [
      "Elad Liebman",
      "Peter Stone"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-08-28T02:54:05Z",
    "pdf_url": "https://arxiv.org/pdf/2308.14269v1"
  },
  {
    "arxiv_id": "2308.14149v1",
    "entry_id": "http://arxiv.org/abs/2308.14149v1",
    "title": "Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models",
    "summary": "Generative pre-trained transformer (GPT) models have revolutionized the field of natural language processing (NLP) with remarkable performance in various tasks and also extend their power to multimodal domains. Despite their success, large GPT models like GPT-4 face inherent limitations such as considerable size, high computational requirements, complex deployment processes, and closed development loops. These constraints restrict their widespread adoption and raise concerns regarding their responsible development and usage. The need for user-friendly, relatively small, and open-sourced alternative GPT models arises from the desire to overcome these limitations while retaining high performance. In this survey paper, we provide an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility. Through this extensive survey, we aim to equip researchers, practitioners, and enthusiasts with a thorough understanding of user-friendly and relatively small open-sourced models of large GPTs, their current state, challenges, and future research directions, inspiring the development of more efficient, accessible, and versatile GPT models that cater to the broader scientific community and advance the field of general artificial intelligence. The source contents are continuously updating in https://github.com/GPT-Alternatives/gpt_alternatives.",
    "authors": [
      "Kaiyuan Gao",
      "Sunan He",
      "Zhenyu He",
      "Jiacheng Lin",
      "QiZhi Pei",
      "Jie Shao",
      "Wei Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-08-27T16:14:19Z",
    "pdf_url": "https://arxiv.org/pdf/2308.14149v1"
  },
  {
    "arxiv_id": "2308.14089v2",
    "entry_id": "http://arxiv.org/abs/2308.14089v2",
    "title": "MedAlign: A Clinician-Generated Dataset for Instruction Following with Electronic Medical Records",
    "summary": "The ability of large language models (LLMs) to follow natural language instructions with human-level fluency suggests many opportunities in healthcare to reduce administrative burden and improve quality of care. However, evaluating LLMs on realistic text generation tasks for healthcare remains challenging. Existing question answering datasets for electronic health record (EHR) data fail to capture the complexity of information needs and documentation burdens experienced by clinicians. To address these challenges, we introduce MedAlign, a benchmark dataset of 983 natural language instructions for EHR data. MedAlign is curated by 15 clinicians (7 specialities), includes clinician-written reference responses for 303 instructions, and provides 276 longitudinal EHRs for grounding instruction-response pairs. We used MedAlign to evaluate 6 general domain LLMs, having clinicians rank the accuracy and quality of each LLM response. We found high error rates, ranging from 35% (GPT-4) to 68% (MPT-7B-Instruct), and an 8.3% drop in accuracy moving from 32k to 2k context lengths for GPT-4. Finally, we report correlations between clinician rankings and automated natural language generation metrics as a way to rank LLMs without human review. We make MedAlign available under a research data use agreement to enable LLM evaluations on tasks aligned with clinician needs and preferences.",
    "authors": [
      "Scott L. Fleming",
      "Alejandro Lozano",
      "William J. Haberkorn",
      "Jenelle A. Jindal",
      "Eduardo P. Reis",
      "Rahul Thapa",
      "Louis Blankemeier",
      "Julian Z. Genkins",
      "Ethan Steinberg",
      "Ashwin Nayak",
      "Birju S. Patel",
      "Chia-Chun Chiang",
      "Alison Callahan",
      "Zepeng Huo",
      "Sergios Gatidis",
      "Scott J. Adams",
      "Oluseyi Fayanju",
      "Shreya J. Shah",
      "Thomas Savage",
      "Ethan Goh",
      "Akshay S. Chaudhari",
      "Nima Aghaeepour",
      "Christopher Sharp",
      "Michael A. Pfeffer",
      "Percy Liang",
      "Jonathan H. Chen",
      "Keith E. Morse",
      "Emma P. Brunskill",
      "Jason A. Fries",
      "Nigam H. Shah"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-08-27T12:24:39Z",
    "pdf_url": "https://arxiv.org/pdf/2308.14089v2"
  },
  {
    "arxiv_id": "2308.13142v1",
    "entry_id": "http://arxiv.org/abs/2308.13142v1",
    "title": "A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions",
    "summary": "Recently, there has been significant progress in the development of large models. Following the success of ChatGPT, numerous language models have been introduced, demonstrating remarkable performance. Similar advancements have also been observed in image generation models, such as Google's Imagen model, OpenAI's DALL-E 2, and stable diffusion models, which have exhibited impressive capabilities in generating images. However, similar to large language models, these models still encounter unresolved challenges. Fortunately, the availability of open-source stable diffusion models and their underlying mathematical principles has enabled the academic community to extensively analyze the performance of current image generation models and make improvements based on this stable diffusion framework. This survey aims to examine the existing issues and the current solutions pertaining to image generation models.",
    "authors": [
      "Tianyi Zhang",
      "Zheng Wang",
      "Jing Huang",
      "Mohiuddin Muhammad Tasnim",
      "Wei Shi"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-08-25T02:35:54Z",
    "pdf_url": "https://arxiv.org/pdf/2308.13142v1"
  },
  {
    "arxiv_id": "2308.12488v2",
    "entry_id": "http://arxiv.org/abs/2308.12488v2",
    "title": "GPTEval: A Survey on Assessments of ChatGPT and GPT-4",
    "summary": "The emergence of ChatGPT has generated much speculation in the press about its potential to disrupt social and economic systems. Its astonishing language ability has aroused strong curiosity among scholars about its performance in different domains. There have been many studies evaluating the ability of ChatGPT and GPT-4 in different tasks and disciplines. However, a comprehensive review summarizing the collective assessment findings is lacking. The objective of this survey is to thoroughly analyze prior assessments of ChatGPT and GPT-4, focusing on its language and reasoning abilities, scientific knowledge, and ethical considerations. Furthermore, an examination of the existing evaluation methods is conducted, offering several recommendations for future research in evaluating large language models.",
    "authors": [
      "Rui Mao",
      "Guanyi Chen",
      "Xulang Zhang",
      "Frank Guerin",
      "Erik Cambria"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-08-24T01:17:16Z",
    "pdf_url": "https://arxiv.org/pdf/2308.12488v2"
  },
  {
    "arxiv_id": "2308.12241v1",
    "entry_id": "http://arxiv.org/abs/2308.12241v1",
    "title": "LLMRec: Benchmarking Large Language Models on Recommendation Task",
    "summary": "Recently, the fast development of Large Language Models (LLMs) such as ChatGPT has significantly advanced NLP tasks by enhancing the capabilities of conversational models. However, the application of LLMs in the recommendation domain has not been thoroughly investigated. To bridge this gap, we propose LLMRec, a LLM-based recommender system designed for benchmarking LLMs on various recommendation tasks. Specifically, we benchmark several popular off-the-shelf LLMs, such as ChatGPT, LLaMA, ChatGLM, on five recommendation tasks, including rating prediction, sequential recommendation, direct recommendation, explanation generation, and review summarization. Furthermore, we investigate the effectiveness of supervised finetuning to improve LLMs' instruction compliance ability. The benchmark results indicate that LLMs displayed only moderate proficiency in accuracy-based tasks such as sequential and direct recommendation. However, they demonstrated comparable performance to state-of-the-art methods in explainability-based tasks. We also conduct qualitative evaluations to further evaluate the quality of contents generated by different models, and the results show that LLMs can truly understand the provided information and generate clearer and more reasonable results. We aspire that this benchmark will serve as an inspiration for researchers to delve deeper into the potential of LLMs in enhancing recommendation performance. Our codes, processed data and benchmark results are available at https://github.com/williamliujl/LLMRec.",
    "authors": [
      "Junling Liu",
      "Chao Liu",
      "Peilin Zhou",
      "Qichen Ye",
      "Dading Chong",
      "Kang Zhou",
      "Yueqi Xie",
      "Yuwei Cao",
      "Shoujin Wang",
      "Chenyu You",
      "Philip S. Yu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2023-08-23T16:32:54Z",
    "pdf_url": "https://arxiv.org/pdf/2308.12241v1"
  },
  {
    "arxiv_id": "2308.12014v2",
    "entry_id": "http://arxiv.org/abs/2308.12014v2",
    "title": "From Instructions to Intrinsic Human Values -- A Survey of Alignment Goals for Big Models",
    "summary": "Big models, exemplified by Large Language Models (LLMs), are models typically pre-trained on massive data and comprised of enormous parameters, which not only obtain significantly improved performance across diverse tasks but also present emergent capabilities absent in smaller models. However, the growing intertwining of big models with everyday human lives poses potential risks and might cause serious social harm. Therefore, many efforts have been made to align LLMs with humans to make them better follow user instructions and satisfy human preferences. Nevertheless, `what to align with' has not been fully discussed, and inappropriate alignment goals might even backfire. In this paper, we conduct a comprehensive survey of different alignment goals in existing work and trace their evolution paths to help identify the most essential goal. Particularly, we investigate related works from two perspectives: the definition of alignment goals and alignment evaluation. Our analysis encompasses three distinct levels of alignment goals and reveals a goal transformation from fundamental abilities to value orientation, indicating the potential of intrinsic human values as the alignment goal for enhanced LLMs. Based on such results, we further discuss the challenges of achieving such intrinsic value alignment and provide a collection of available resources for future research on the alignment of big models.",
    "authors": [
      "Jing Yao",
      "Xiaoyuan Yi",
      "Xiting Wang",
      "Jindong Wang",
      "Xing Xie"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CY"
    ],
    "published": "2023-08-23T09:11:13Z",
    "pdf_url": "https://arxiv.org/pdf/2308.12014v2"
  },
  {
    "arxiv_id": "2308.11432v7",
    "entry_id": "http://arxiv.org/abs/2308.11432v7",
    "title": "A Survey on Large Language Model based Autonomous Agents",
    "summary": "Autonomous agents have long been a prominent research focus in both academic and industry communities. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating LLM-based autonomous agents. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of LLM-based autonomous agents from a holistic perspective. More specifically, we first discuss the construction of LLM-based autonomous agents, for which we propose a unified framework that encompasses a majority of the previous work. Then, we present a comprehensive overview of the diverse applications of LLM-based autonomous agents in the fields of social science, natural science, and engineering. Finally, we delve into the evaluation strategies commonly used for LLM-based autonomous agents. Based on the previous studies, we also present several challenges and future directions in this field. To keep track of this field and continuously update our survey, we maintain a repository of relevant references at https://github.com/Paitesanshi/LLM-Agent-Survey.",
    "authors": [
      "Lei Wang",
      "Chen Ma",
      "Xueyang Feng",
      "Zeyu Zhang",
      "Hao Yang",
      "Jingsen Zhang",
      "Zhiyuan Chen",
      "Jiakai Tang",
      "Xu Chen",
      "Yankai Lin",
      "Wayne Xin Zhao",
      "Zhewei Wei",
      "Ji-Rong Wen"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-08-22T13:30:37Z",
    "pdf_url": "https://arxiv.org/pdf/2308.11432v7"
  },
  {
    "arxiv_id": "2308.11336v1",
    "entry_id": "http://arxiv.org/abs/2308.11336v1",
    "title": "On the Opportunities and Challenges of Offline Reinforcement Learning for Recommender Systems",
    "summary": "Reinforcement learning serves as a potent tool for modeling dynamic user interests within recommender systems, garnering increasing research attention of late. However, a significant drawback persists: its poor data efficiency, stemming from its interactive nature. The training of reinforcement learning-based recommender systems demands expensive online interactions to amass adequate trajectories, essential for agents to learn user preferences. This inefficiency renders reinforcement learning-based recommender systems a formidable undertaking, necessitating the exploration of potential solutions. Recent strides in offline reinforcement learning present a new perspective. Offline reinforcement learning empowers agents to glean insights from offline datasets and deploy learned policies in online settings. Given that recommender systems possess extensive offline datasets, the framework of offline reinforcement learning aligns seamlessly. Despite being a burgeoning field, works centered on recommender systems utilizing offline reinforcement learning remain limited. This survey aims to introduce and delve into offline reinforcement learning within recommender systems, offering an inclusive review of existing literature in this domain. Furthermore, we strive to underscore prevalent challenges, opportunities, and future pathways, poised to propel research in this evolving field.",
    "authors": [
      "Xiaocong Chen",
      "Siyu Wang",
      "Julian McAuley",
      "Dietmar Jannach",
      "Lina Yao"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2023-08-22T10:28:02Z",
    "pdf_url": "https://arxiv.org/pdf/2308.11336v1"
  },
  {
    "arxiv_id": "2308.11148v2",
    "entry_id": "http://arxiv.org/abs/2308.11148v2",
    "title": "LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning",
    "summary": "The automation of code review activities, a long-standing pursuit in software engineering, has been primarily addressed by numerous domain-specific pre-trained models. Despite their success, these models frequently demand extensive resources for pre-training from scratch. In contrast, Large Language Models (LLMs) provide an intriguing alternative, given their remarkable capabilities when supplemented with domain-specific knowledge. However, their potential for automating code review tasks remains largely unexplored.\n  In response to this research gap, we present LLaMA-Reviewer, an innovative framework that leverages the capabilities of LLaMA, a popular LLM, in the realm of code review. Mindful of resource constraints, this framework employs parameter-efficient fine-tuning (PEFT) methods, delivering high performance while using less than 1% of trainable parameters.\n  An extensive evaluation of LLaMA-Reviewer is conducted on two diverse, publicly available datasets. Notably, even with the smallest LLaMA base model consisting of 6.7B parameters and a limited number of tuning epochs, LLaMA-Reviewer equals the performance of existing code-review-focused models.\n  The ablation experiments provide insights into the influence of various fine-tuning process components, including input representation, instruction tuning, and different PEFT methods. To foster continuous progress in this field, the code and all PEFT-weight plugins have been made open-source.",
    "authors": [
      "Junyi Lu",
      "Lei Yu",
      "Xiaojia Li",
      "Li Yang",
      "Chun Zuo"
    ],
    "categories": [
      "cs.SE",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-08-22T03:10:40Z",
    "pdf_url": "https://arxiv.org/pdf/2308.11148v2"
  },
  {
    "arxiv_id": "2308.10882v1",
    "entry_id": "http://arxiv.org/abs/2308.10882v1",
    "title": "Giraffe: Adventures in Expanding Context Lengths in LLMs",
    "summary": "Modern large language models (LLMs) that rely on attention mechanisms are typically trained with fixed context lengths which enforce upper limits on the length of input sequences that they can handle at evaluation time. To use these models on sequences longer than the train-time context length, one might employ techniques from the growing family of context length extrapolation methods -- most of which focus on modifying the system of positional encodings used in the attention mechanism to indicate where tokens or activations are located in the input sequence. We conduct a wide survey of existing methods of context length extrapolation on a base LLaMA or LLaMA 2 model, and introduce some of our own design as well -- in particular, a new truncation strategy for modifying the basis for the position encoding.\n  We test these methods using three new evaluation tasks (FreeFormQA, AlteredNumericQA, and LongChat-Lines) as well as perplexity, which we find to be less fine-grained as a measure of long context performance of LLMs. We release the three tasks publicly as datasets on HuggingFace. We discover that linear scaling is the best method for extending context length, and show that further gains can be achieved by using longer scales at evaluation time. We also discover promising extrapolation capabilities in the truncated basis. To support further research in this area, we release three new 13B parameter long-context models which we call Giraffe: 4k and 16k context models trained from base LLaMA-13B, and a 32k context model trained from base LLaMA2-13B. We also release the code to replicate our results.",
    "authors": [
      "Arka Pal",
      "Deep Karkhanis",
      "Manley Roberts",
      "Samuel Dooley",
      "Arvind Sundararajan",
      "Siddartha Naidu"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-08-21T17:30:16Z",
    "pdf_url": "https://arxiv.org/pdf/2308.10882v1"
  },
  {
    "arxiv_id": "2308.10792v10",
    "entry_id": "http://arxiv.org/abs/2308.10792v10",
    "title": "Instruction Tuning for Large Language Models: A Survey",
    "summary": "This paper surveys research works in the quickly advancing field of instruction tuning (IT), which can also be referred to as supervised fine-tuning (SFT)\\footnote{In this paper, unless specified otherwise, supervised fine-tuning (SFT) and instruction tuning (IT) are used interchangeably.}, a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of \\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of SFT, the construction of SFT datasets, the training of SFT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of SFT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of SFT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project Page: github.com/xiaoya-li/Instruction-Tuning-Survey",
    "authors": [
      "Shengyu Zhang",
      "Linfeng Dong",
      "Xiaoya Li",
      "Sen Zhang",
      "Xiaofei Sun",
      "Shuhe Wang",
      "Jiwei Li",
      "Runyi Hu",
      "Tianwei Zhang",
      "Fei Wu",
      "Guoyin Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-08-21T15:35:16Z",
    "pdf_url": "https://arxiv.org/pdf/2308.10792v10"
  },
  {
    "arxiv_id": "2308.10921v1",
    "entry_id": "http://arxiv.org/abs/2308.10921v1",
    "title": "Artificial intelligence-driven antimicrobial peptide discovery",
    "summary": "Antimicrobial peptides (AMPs) emerge as promising agents against antimicrobial resistance, providing an alternative to conventional antibiotics. Artificial intelligence (AI) revolutionized AMP discovery through both discrimination and generation approaches. The discriminators aid the identification of promising candidates by predicting key peptide properties such as activity and toxicity, while the generators learn the distribution over peptides and enable sampling novel AMP candidates, either de novo, or as analogues of a prototype peptide. Moreover, the controlled generation of AMPs with desired properties is achieved by discriminator-guided filtering, positive-only learning, latent space sampling, as well as conditional and optimized generation. Here we review recent achievements in AI-driven AMP discovery, highlighting the most exciting directions.",
    "authors": [
      "Paulina Szymczak",
      "Ewa Szczurek"
    ],
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "published": "2023-08-21T14:02:14Z",
    "pdf_url": "https://arxiv.org/pdf/2308.10921v1"
  },
  {
    "arxiv_id": "2308.10620v6",
    "entry_id": "http://arxiv.org/abs/2308.10620v6",
    "title": "Large Language Models for Software Engineering: A Systematic Literature Review",
    "summary": "Large Language Models (LLMs) have significantly impacted numerous domains, including Software Engineering (SE). Many recent publications have explored LLMs applied to various SE tasks. Nevertheless, a comprehensive understanding of the application, effects, and possible limitations of LLMs on SE is still in its early stages. To bridge this gap, we conducted a systematic literature review (SLR) on LLM4SE, with a particular focus on understanding how LLMs can be exploited to optimize processes and outcomes. We select and analyze 395 research papers from January 2017 to January 2024 to answer four key research questions (RQs). In RQ1, we categorize different LLMs that have been employed in SE tasks, characterizing their distinctive features and uses. In RQ2, we analyze the methods used in data collection, preprocessing, and application, highlighting the role of well-curated datasets for successful LLM for SE implementation. RQ3 investigates the strategies employed to optimize and evaluate the performance of LLMs in SE. Finally, RQ4 examines the specific SE tasks where LLMs have shown success to date, illustrating their practical contributions to the field. From the answers to these RQs, we discuss the current state-of-the-art and trends, identifying gaps in existing research, and flagging promising areas for future study. Our artifacts are publicly available at https://github.com/xinyi-hou/LLM4SE_SLR.",
    "authors": [
      "Xinyi Hou",
      "Yanjie Zhao",
      "Yue Liu",
      "Zhou Yang",
      "Kailong Wang",
      "Li Li",
      "Xiapu Luo",
      "David Lo",
      "John Grundy",
      "Haoyu Wang"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2023-08-21T10:37:49Z",
    "pdf_url": "https://arxiv.org/pdf/2308.10620v6"
  },
  {
    "arxiv_id": "2308.10149v2",
    "entry_id": "http://arxiv.org/abs/2308.10149v2",
    "title": "A Survey on Fairness in Large Language Models",
    "summary": "Large Language Models (LLMs) have shown powerful performance and development prospects and are widely deployed in the real world. However, LLMs can capture social biases from unprocessed training data and propagate the biases to downstream tasks. Unfair LLM systems have undesirable social impacts and potential harms. In this paper, we provide a comprehensive review of related research on fairness in LLMs. Considering the influence of parameter magnitude and training paradigm on research strategy, we divide existing fairness research into oriented to medium-sized LLMs under pre-training and fine-tuning paradigms and oriented to large-sized LLMs under prompting paradigms. First, for medium-sized LLMs, we introduce evaluation metrics and debiasing methods from the perspectives of intrinsic bias and extrinsic bias, respectively. Then, for large-sized LLMs, we introduce recent fairness research, including fairness evaluation, reasons for bias, and debiasing methods. Finally, we discuss and provide insight on the challenges and future directions for the development of fairness in LLMs.",
    "authors": [
      "Yingji Li",
      "Mengnan Du",
      "Rui Song",
      "Xin Wang",
      "Ying Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-08-20T03:30:22Z",
    "pdf_url": "https://arxiv.org/pdf/2308.10149v2"
  },
  {
    "arxiv_id": "2308.10047v2",
    "entry_id": "http://arxiv.org/abs/2308.10047v2",
    "title": "Towards Probabilistic Causal Discovery, Inference & Explanations for Autonomous Drones in Mine Surveying Tasks",
    "summary": "Causal modelling offers great potential to provide autonomous agents the ability to understand the data-generation process that governs their interactions with the world. Such models capture formal knowledge as well as probabilistic representations of noise and uncertainty typically encountered by autonomous robots in real-world environments. Thus, causality can aid autonomous agents in making decisions and explaining outcomes, but deploying causality in such a manner introduces new challenges. Here we identify challenges relating to causality in the context of a drone system operating in a salt mine. Such environments are challenging for autonomous agents because of the presence of confounders, non-stationarity, and a difficulty in building complete causal models ahead of time. To address these issues, we propose a probabilistic causal framework consisting of: causally-informed POMDP planning, online SCM adaptation, and post-hoc counterfactual explanations. Further, we outline planned experimentation to evaluate the framework integrated with a drone system in simulated mine environments and on a real-world mine dataset.",
    "authors": [
      "Ricardo Cannizzaro",
      "Rhys Howard",
      "Paulina Lewinska",
      "Lars Kunze"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2023-08-19T15:12:55Z",
    "pdf_url": "https://arxiv.org/pdf/2308.10047v2"
  },
  {
    "arxiv_id": "2308.09341v1",
    "entry_id": "http://arxiv.org/abs/2308.09341v1",
    "title": "Document Automation Architectures: Updated Survey in Light of Large Language Models",
    "summary": "This paper surveys the current state of the art in document automation (DA). The objective of DA is to reduce the manual effort during the generation of documents by automatically creating and integrating input from different sources and assembling documents conforming to defined templates. There have been reviews of commercial solutions of DA, particularly in the legal domain, but to date there has been no comprehensive review of the academic research on DA architectures and technologies. The current survey of DA reviews the academic literature and provides a clearer definition and characterization of DA and its features, identifies state-of-the-art DA architectures and technologies in academic research, and provides ideas that can lead to new research opportunities within the DA field in light of recent advances in generative AI and large language models.",
    "authors": [
      "Mohammad Ahmadi Achachlouei",
      "Omkar Patil",
      "Tarun Joshi",
      "Vijayan N. Nair"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-08-18T06:59:55Z",
    "pdf_url": "https://arxiv.org/pdf/2308.09341v1"
  },
  {
    "arxiv_id": "2308.07902v1",
    "entry_id": "http://arxiv.org/abs/2308.07902v1",
    "title": "Through the Lens of Core Competency: Survey on Evaluation of Large Language Models",
    "summary": "From pre-trained language model (PLM) to large language model (LLM), the field of natural language processing (NLP) has witnessed steep performance gains and wide practical uses. The evaluation of a research field guides its direction of improvement. However, LLMs are extremely hard to thoroughly evaluate for two reasons. First of all, traditional NLP tasks become inadequate due to the excellent performance of LLM. Secondly, existing evaluation tasks are difficult to keep up with the wide range of applications in real-world scenarios. To tackle these problems, existing works proposed various benchmarks to better evaluate LLMs. To clarify the numerous evaluation tasks in both academia and industry, we investigate multiple papers concerning LLM evaluations. We summarize 4 core competencies of LLM, including reasoning, knowledge, reliability, and safety. For every competency, we introduce its definition, corresponding benchmarks, and metrics. Under this competency architecture, similar tasks are combined to reflect corresponding ability, while new tasks can also be easily added into the system. Finally, we give our suggestions on the future direction of LLM's evaluation.",
    "authors": [
      "Ziyu Zhuang",
      "Qiguang Chen",
      "Longxuan Ma",
      "Mingda Li",
      "Yi Han",
      "Yushan Qian",
      "Haopeng Bai",
      "Zixian Feng",
      "Weinan Zhang",
      "Ting Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-08-15T17:40:34Z",
    "pdf_url": "https://arxiv.org/pdf/2308.07902v1"
  },
  {
    "arxiv_id": "2308.07822v1",
    "entry_id": "http://arxiv.org/abs/2308.07822v1",
    "title": "Deep reinforcement learning for process design: Review and perspective",
    "summary": "The transformation towards renewable energy and feedstock supply in the chemical industry requires new conceptual process design approaches. Recently, breakthroughs in artificial intelligence offer opportunities to accelerate this transition. Specifically, deep reinforcement learning, a subclass of machine learning, has shown the potential to solve complex decision-making problems and aid sustainable process design. We survey state-of-the-art research in reinforcement learning for process design through three major elements: (i) information representation, (ii) agent architecture, and (iii) environment and reward. Moreover, we discuss perspectives on underlying challenges and promising future works to unfold the full potential of reinforcement learning for process design in chemical engineering.",
    "authors": [
      "Qinghe Gao",
      "Artur M. Schweidtmann"
    ],
    "categories": [
      "cs.LG",
      "eess.SY"
    ],
    "published": "2023-08-15T14:56:37Z",
    "pdf_url": "https://arxiv.org/pdf/2308.07822v1"
  },
  {
    "arxiv_id": "2308.07633v4",
    "entry_id": "http://arxiv.org/abs/2308.07633v4",
    "title": "A Survey on Model Compression for Large Language Models",
    "summary": "Large Language Models (LLMs) have transformed natural language processing tasks successfully. Yet, their large size and high computational needs pose challenges for practical use, especially in resource-limited settings. Model compression has emerged as a key research area to address these challenges. This paper presents a survey of model compression techniques for LLMs. We cover methods like quantization, pruning, and knowledge distillation, highlighting recent advancements. We also discuss benchmarking strategies and evaluation metrics crucial for assessing compressed LLMs. This survey offers valuable insights for researchers and practitioners, aiming to enhance efficiency and real-world applicability of LLMs while laying a foundation for future advancements.",
    "authors": [
      "Xunyu Zhu",
      "Jian Li",
      "Yong Liu",
      "Can Ma",
      "Weiping Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-08-15T08:31:05Z",
    "pdf_url": "https://arxiv.org/pdf/2308.07633v4"
  },
  {
    "arxiv_id": "2308.13534v1",
    "entry_id": "http://arxiv.org/abs/2308.13534v1",
    "title": "Building Trust in Conversational AI: A Comprehensive Review and Solution Architecture for Explainable, Privacy-Aware Systems using LLMs and Knowledge Graph",
    "summary": "Conversational AI systems have emerged as key enablers of human-like interactions across diverse sectors. Nevertheless, the balance between linguistic nuance and factual accuracy has proven elusive. In this paper, we first introduce LLMXplorer, a comprehensive tool that provides an in-depth review of over 150 Large Language Models (LLMs), elucidating their myriad implications ranging from social and ethical to regulatory, as well as their applicability across industries. Building on this foundation, we propose a novel functional architecture that seamlessly integrates the structured dynamics of Knowledge Graphs with the linguistic capabilities of LLMs. Validated using real-world AI news data, our architecture adeptly blends linguistic sophistication with factual rigour and further strengthens data security through Role-Based Access Control. This research provides insights into the evolving landscape of conversational AI, emphasizing the imperative for systems that are efficient, transparent, and trustworthy.",
    "authors": [
      "Ahtsham Zafar",
      "Venkatesh Balavadhani Parthasarathy",
      "Chan Le Van",
      "Saad Shahid",
      "Aafaq Iqbal khan",
      "Arsalan Shahid"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-08-13T22:47:51Z",
    "pdf_url": "https://arxiv.org/pdf/2308.13534v1"
  },
  {
    "arxiv_id": "2308.06767v2",
    "entry_id": "http://arxiv.org/abs/2308.06767v2",
    "title": "A Survey on Deep Neural Network Pruning-Taxonomy, Comparison, Analysis, and Recommendations",
    "summary": "Modern deep neural networks, particularly recent large language models, come with massive model sizes that require significant computational and storage resources. To enable the deployment of modern models on resource-constrained environments and accelerate inference time, researchers have increasingly explored pruning techniques as a popular research direction in neural network compression. However, there is a dearth of up-to-date comprehensive review papers on pruning. To address this issue, in this survey, we provide a comprehensive review of existing research works on deep neural network pruning in a taxonomy of 1) universal/specific speedup, 2) when to prune, 3) how to prune, and 4) fusion of pruning and other compression techniques. We then provide a thorough comparative analysis of eight pairs of contrast settings for pruning and explore emerging topics, including pruning for large language models, large multimodal models, post-training pruning, and different supervision levels for pruning to shed light on the commonalities and differences of existing methods and lay the foundation for further method development. To facilitate future research, we build a curated collection of datasets, networks, and evaluations on different applications. Finally, we provide valuable recommendations on selecting pruning methods and prospect several promising research directions. We build a repository at https://github.com/hrcheng1066/awesome-pruning.",
    "authors": [
      "Hongrong Cheng",
      "Miao Zhang",
      "Javen Qinfeng Shi"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2023-08-13T13:34:04Z",
    "pdf_url": "https://arxiv.org/pdf/2308.06767v2"
  },
  {
    "arxiv_id": "2308.06668v4",
    "entry_id": "http://arxiv.org/abs/2308.06668v4",
    "title": "Large Language Models and Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges",
    "summary": "The past decade has witnessed the rapid development and adoption of ML & DL methodologies in agricultural systems, showcased by great successes in agricultural applications. However, these conventional ML/DL models have certain limitations: they heavily rely on large, costly-to-acquire labeled datasets for training, require specialized expertise for development and maintenance, and are mostly tailored for specific tasks, thus lacking generalizability. Recently, large pre-trained models, also known as FMs, have demonstrated remarkable successes in language, vision, and decision-making tasks across various domains. These models are trained on a large amount of data from multiple domains and modalities. Once trained, they can accomplish versatile tasks with just minor fine-tuning and minimal task-specific labeled data. Despite their proven effectiveness and huge potential, there has been little exploration of applying FMs to agriculture AI. Thus, this study aims to explore the potential of FMs in the field of smart agriculture. In particular, conceptual tools and technical background are presented to help the understanding of the problem space and uncover new research directions. To this end, recent FMs in the general CS domain are reviewed, and the models are categorized into four categories: language FMs, vision FMs, multimodal FMs, and reinforcement learning FMs. Then, the steps of developing agriculture FMs (AFMs) are outlined and potential applications in smart agriculture are discussed. Moreover, challenges and risks associated with developing AFMs are discussed, including model training, validation, and deployment. In summary, the advancement of AI in agriculture is explored by introducing AFMs as a promising paradigm that can significantly mitigate the reliance on extensive labeled datasets and enhance the efficiency, effectiveness, and generalization of agricultural AI systems.",
    "authors": [
      "Jiajia Li",
      "Mingle Xu",
      "Lirong Xiang",
      "Dong Chen",
      "Weichao Zhuang",
      "Xunyuan Yin",
      "Zhaojian Li"
    ],
    "categories": [
      "cs.LG",
      "cs.CV"
    ],
    "published": "2023-08-13T02:59:36Z",
    "pdf_url": "https://arxiv.org/pdf/2308.06668v4"
  },
  {
    "arxiv_id": "2308.06610v1",
    "entry_id": "http://arxiv.org/abs/2308.06610v1",
    "title": "Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation",
    "summary": "Medical systematic reviews can be very costly and resource intensive. We explore how Large Language Models (LLMs) can support and be trained to perform literature screening when provided with a detailed set of selection criteria. Specifically, we instruction tune LLaMA and Guanaco models to perform abstract screening for medical systematic reviews. Our best model, Bio-SIEVE, outperforms both ChatGPT and trained traditional approaches, and generalises better across medical domains. However, there remains the challenge of adapting the model to safety-first scenarios. We also explore the impact of multi-task training with Bio-SIEVE-Multi, including tasks such as PICO extraction and exclusion reasoning, but find that it is unable to match single-task Bio-SIEVE's performance. We see Bio-SIEVE as an important step towards specialising LLMs for the biomedical systematic review process and explore its future developmental opportunities. We release our models, code and a list of DOIs to reconstruct our dataset for reproducibility.",
    "authors": [
      "Ambrose Robinson",
      "William Thorne",
      "Ben P. Wu",
      "Abdullah Pandor",
      "Munira Essat",
      "Mark Stevenson",
      "Xingyi Song"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-08-12T16:56:55Z",
    "pdf_url": "https://arxiv.org/pdf/2308.06610v1"
  },
  {
    "arxiv_id": "2308.06419v1",
    "entry_id": "http://arxiv.org/abs/2308.06419v1",
    "title": "Pedestrian Trajectory Prediction in Pedestrian-Vehicle Mixed Environments: A Systematic Review",
    "summary": "Planning an autonomous vehicle's (AV) path in a space shared with pedestrians requires reasoning about pedestrians' future trajectories. A practical pedestrian trajectory prediction algorithm for the use of AVs needs to consider the effect of the vehicle's interactions with the pedestrians on pedestrians' future motion behaviours. In this regard, this paper systematically reviews different methods proposed in the literature for modelling pedestrian trajectory prediction in presence of vehicles that can be applied for unstructured environments. This paper also investigates specific considerations for pedestrian-vehicle interaction (compared with pedestrian-pedestrian interaction) and reviews how different variables such as prediction uncertainties and behavioural differences are accounted for in the previously proposed prediction models. PRISMA guidelines were followed. Articles that did not consider vehicle and pedestrian interactions or actual trajectories, and articles that only focused on road crossing were excluded. A total of 1260 unique peer-reviewed articles from ACM Digital Library, IEEE Xplore, and Scopus databases were identified in the search. 64 articles were included in the final review as they met the inclusion and exclusion criteria. An overview of datasets containing trajectory data of both pedestrians and vehicles used by the reviewed papers has been provided. Research gaps and directions for future work, such as having more effective definition of interacting agents in deep learning methods and the need for gathering more datasets of mixed traffic in unstructured environments are discussed.",
    "authors": [
      "Mahsa Golchoubian",
      "Moojan Ghafurian",
      "Kerstin Dautenhahn",
      "Nasser Lashgarian Azad"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.HC",
      "cs.LG",
      "eess.SY"
    ],
    "published": "2023-08-11T23:58:51Z",
    "pdf_url": "https://arxiv.org/pdf/2308.06419v1"
  },
  {
    "arxiv_id": "2308.05893v2",
    "entry_id": "http://arxiv.org/abs/2308.05893v2",
    "title": "Learning Team-Based Navigation: A Review of Deep Reinforcement Learning Techniques for Multi-Agent Pathfinding",
    "summary": "Multi-agent pathfinding (MAPF) is a critical field in many large-scale robotic applications, often being the fundamental step in multi-agent systems. The increasing complexity of MAPF in complex and crowded environments, however, critically diminishes the effectiveness of existing solutions. In contrast to other studies that have either presented a general overview of the recent advancements in MAPF or extensively reviewed Deep Reinforcement Learning (DRL) within multi-agent system settings independently, our work presented in this review paper focuses on highlighting the integration of DRL-based approaches in MAPF. Moreover, we aim to bridge the current gap in evaluating MAPF solutions by addressing the lack of unified evaluation metrics and providing comprehensive clarification on these metrics. Finally, our paper discusses the potential of model-based DRL as a promising future direction and provides its required foundational understanding to address current challenges in MAPF. Our objective is to assist readers in gaining insight into the current research direction, providing unified metrics for comparing different MAPF algorithms and expanding their knowledge of model-based DRL to address the existing challenges in MAPF.",
    "authors": [
      "Jaehoon Chung",
      "Jamil Fayyad",
      "Younes Al Younes",
      "Homayoun Najjaran"
    ],
    "categories": [
      "cs.AI",
      "cs.LG",
      "cs.MA",
      "cs.RO",
      "eess.SY"
    ],
    "published": "2023-08-11T00:59:29Z",
    "pdf_url": "https://arxiv.org/pdf/2308.05893v2"
  },
  {
    "arxiv_id": "2308.05374v2",
    "entry_id": "http://arxiv.org/abs/2308.05374v2",
    "title": "Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment",
    "summary": "Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.",
    "authors": [
      "Yang Liu",
      "Yuanshun Yao",
      "Jean-Francois Ton",
      "Xiaoying Zhang",
      "Ruocheng Guo",
      "Hao Cheng",
      "Yegor Klochkov",
      "Muhammad Faaiz Taufiq",
      "Hang Li"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-08-10T06:43:44Z",
    "pdf_url": "https://arxiv.org/pdf/2308.05374v2"
  },
  {
    "arxiv_id": "2308.05012v1",
    "entry_id": "http://arxiv.org/abs/2308.05012v1",
    "title": "MetRoBERTa: Leveraging Traditional Customer Relationship Management Data to Develop a Transit-Topic-Aware Language Model",
    "summary": "Transit riders' feedback provided in ridership surveys, customer relationship management (CRM) channels, and in more recent times, through social media is key for transit agencies to better gauge the efficacy of their services and initiatives. Getting a holistic understanding of riders' experience through the feedback shared in those instruments is often challenging, mostly due to the open-ended, unstructured nature of text feedback. In this paper, we propose leveraging traditional transit CRM feedback to develop and deploy a transit-topic-aware large language model (LLM) capable of classifying open-ended text feedback to relevant transit-specific topics. First, we utilize semi-supervised learning to engineer a training dataset of 11 broad transit topics detected in a corpus of 6 years of customer feedback provided to the Washington Metropolitan Area Transit Authority (WMATA). We then use this dataset to train and thoroughly evaluate a language model based on the RoBERTa architecture. We compare our LLM, MetRoBERTa, to classical machine learning approaches utilizing keyword-based and lexicon representations. Our model outperforms those methods across all evaluation metrics, providing an average topic classification accuracy of 90%. Finally, we provide a value proposition of this work demonstrating how the language model, alongside additional text processing tools, can be applied to add structure to open-ended text sources of feedback like Twitter. The framework and results we present provide a pathway for an automated, generalizable approach for ingesting, visualizing, and reporting transit riders' feedback at scale, enabling agencies to better understand and improve customer experience.",
    "authors": [
      "Michael Leong",
      "Awad Abdelhalim",
      "Jude Ha",
      "Dianne Patterson",
      "Gabriel L. Pincus",
      "Anthony B. Harris",
      "Michael Eichler",
      "Jinhua Zhao"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-08-09T15:11:37Z",
    "pdf_url": "https://arxiv.org/pdf/2308.05012v1"
  },
  {
    "arxiv_id": "2308.09720v2",
    "entry_id": "http://arxiv.org/abs/2308.09720v2",
    "title": "On the Unexpected Abilities of Large Language Models",
    "summary": "Large Language Models (LLMs) are capable of displaying a wide range of abilities that are not directly connected with the task for which they are trained: predicting the next words of human-written texts. In this article, I review recent research investigating the cognitive abilities developed by LLMs and their relation to human cognition. I discuss the nature of the indirect process that leads to the acquisition of these cognitive abilities, their relation to other indirect processes, and the implications for the acquisition of integrated abilities. Moreover, I propose the factors that enable the development of abilities that are related only very indirectly to the proximal objective of the training task. Finally, I discuss whether the full set of capabilities that LLMs could possibly develop is predictable.",
    "authors": [
      "Stefano Nolfi"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-08-09T09:15:07Z",
    "pdf_url": "https://arxiv.org/pdf/2308.09720v2"
  },
  {
    "arxiv_id": "2308.04736v2",
    "entry_id": "http://arxiv.org/abs/2308.04736v2",
    "title": "Case Study: Using AI-Assisted Code Generation In Mobile Teams",
    "summary": "The aim of this study is to evaluate the performance of AI-assisted programming in actual mobile development teams that are focused on native mobile languages like Kotlin and Swift. The extensive case study involves 16 participants and 2 technical reviewers, from a software development department designed to understand the impact of using LLMs trained for code generation in specific phases of the team, more specifically, technical onboarding and technical stack switch. The study uses technical problems dedicated to each phase and requests solutions from the participants with and without using AI-Code generators. It measures time, correctness, and technical integration using ReviewerScore, a metric specific to the paper and extracted from actual industry standards, the code reviewers of merge requests. The output is converted and analyzed together with feedback from the participants in an attempt to determine if using AI-assisted programming tools will have an impact on getting developers onboard in a project or helping them with a smooth transition between the two native development environments of mobile development, Android and iOS. The study was performed between May and June 2023 with members of the mobile department of a software development company based in Cluj-Napoca, with Romanian ownership and management.",
    "authors": [
      "Mircea-Serban Vasiliniuc",
      "Adrian Groza"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2023-08-09T07:02:39Z",
    "pdf_url": "https://arxiv.org/pdf/2308.04736v2"
  },
  {
    "arxiv_id": "2308.04028v1",
    "entry_id": "http://arxiv.org/abs/2308.04028v1",
    "title": "Top K Relevant Passage Retrieval for Biomedical Question Answering",
    "summary": "Question answering is a task that answers factoid questions using a large collection of documents. It aims to provide precise answers in response to the user's questions in natural language. Question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. On the web, there is no single article that could provide all the possible answers available on the internet to the question of the problem asked by the user. The existing Dense Passage Retrieval model has been trained on Wikipedia dump from Dec. 20, 2018, as the source documents for answering questions. Question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets. However, in the clinical domain, this problem remains relatively unexplored. According to multiple surveys, Biomedical Questions cannot be answered correctly from Wikipedia Articles. In this work, we work on the existing DPR framework for the biomedical domain and retrieve answers from the Pubmed articles which is a reliable source to answer medical questions. When evaluated on a BioASQ QA dataset, our fine-tuned dense retriever results in a 0.81 F1 score.",
    "authors": [
      "Shashank Gupta"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2023-08-08T04:06:11Z",
    "pdf_url": "https://arxiv.org/pdf/2308.04028v1"
  },
  {
    "arxiv_id": "2308.03992v1",
    "entry_id": "http://arxiv.org/abs/2308.03992v1",
    "title": "AI Chatbots as Multi-Role Pedagogical Agents: Transforming Engagement in CS Education",
    "summary": "This study investigates the use of Artificial Intelligence (AI)-powered, multi-role chatbots as a means to enhance learning experiences and foster engagement in computer science education. Leveraging a design-based research approach, we develop, implement, and evaluate a novel learning environment enriched with four distinct chatbot roles: Instructor Bot, Peer Bot, Career Advising Bot, and Emotional Supporter Bot. These roles, designed around the tenets of Self-Determination Theory, cater to the three innate psychological needs of learners - competence, autonomy, and relatedness. Additionally, the system embraces an inquiry-based learning paradigm, encouraging students to ask questions, seek solutions, and explore their curiosities.\n  We test this system in a higher education context over a period of one month with 200 participating students, comparing outcomes with conditions involving a human tutor and a single chatbot. Our research utilizes a mixed-methods approach, encompassing quantitative measures such as chat log sequence analysis, and qualitative methods including surveys and focus group interviews. By integrating cutting-edge Natural Language Processing techniques such as topic modelling and sentiment analysis, we offer an in-depth understanding of the system's impact on learner engagement, motivation, and inquiry-based learning.\n  This study, through its rigorous design and innovative approach, provides significant insights into the potential of AI-empowered, multi-role chatbots in reshaping the landscape of computer science education and fostering an engaging, supportive, and motivating learning environment.",
    "authors": [
      "Cassie Chen Cao",
      "Zijian Ding",
      "Jionghao Lin",
      "Frank Hopfgartner"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-08-08T02:13:44Z",
    "pdf_url": "https://arxiv.org/pdf/2308.03992v1"
  },
  {
    "arxiv_id": "2308.03188v2",
    "entry_id": "http://arxiv.org/abs/2308.03188v2",
    "title": "Automatically Correcting Large Language Models: Surveying the landscape of diverse self-correction strategies",
    "summary": "Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.",
    "authors": [
      "Liangming Pan",
      "Michael Saxon",
      "Wenda Xu",
      "Deepak Nathani",
      "Xinyi Wang",
      "William Yang Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-08-06T18:38:52Z",
    "pdf_url": "https://arxiv.org/pdf/2308.03188v2"
  },
  {
    "arxiv_id": "2308.04457v1",
    "entry_id": "http://arxiv.org/abs/2308.04457v1",
    "title": "A Critical Review of Physics-Informed Machine Learning Applications in Subsurface Energy Systems",
    "summary": "Machine learning has emerged as a powerful tool in various fields, including computer vision, natural language processing, and speech recognition. It can unravel hidden patterns within large data sets and reveal unparalleled insights, revolutionizing many industries and disciplines. However, machine and deep learning models lack interpretability and limited domain-specific knowledge, especially in applications such as physics and engineering. Alternatively, physics-informed machine learning (PIML) techniques integrate physics principles into data-driven models. By combining deep learning with domain knowledge, PIML improves the generalization of the model, abidance by the governing physical laws, and interpretability. This paper comprehensively reviews PIML applications related to subsurface energy systems, mainly in the oil and gas industry. The review highlights the successful utilization of PIML for tasks such as seismic applications, reservoir simulation, hydrocarbons production forecasting, and intelligent decision-making in the exploration and production stages. Additionally, it demonstrates PIML's capabilities to revolutionize the oil and gas industry and other emerging areas of interest, such as carbon and hydrogen storage; and geothermal systems by providing more accurate and reliable predictions for resource management and operational efficiency.",
    "authors": [
      "Abdeldjalil Latrach",
      "Mohamed Lamine Malki",
      "Misael Morales",
      "Mohamed Mehana",
      "Minou Rabiei"
    ],
    "categories": [
      "cs.LG",
      "stat.ML"
    ],
    "published": "2023-08-06T18:20:24Z",
    "pdf_url": "https://arxiv.org/pdf/2308.04457v1"
  },
  {
    "arxiv_id": "2308.01222v4",
    "entry_id": "http://arxiv.org/abs/2308.01222v4",
    "title": "Calibration in Deep Learning: A Survey of the State-of-the-Art",
    "summary": "Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively under-explored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent advances in calibrating deep models. In this survey, we review the state-of-the-art calibration methods and their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibration methods that we roughly classify into four categories: post-hoc calibration, regularization methods, uncertainty estimation, and composition methods. We also cover recent advancements in calibrating large models, particularly large language models (LLMs). Finally, we discuss some open issues, challenges, and potential directions.",
    "authors": [
      "Cheng Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-08-02T15:28:10Z",
    "pdf_url": "https://arxiv.org/pdf/2308.01222v4"
  },
  {
    "arxiv_id": "2308.02541v1",
    "entry_id": "http://arxiv.org/abs/2308.02541v1",
    "title": "Towards More Human-like AI Communication: A Review of Emergent Communication Research",
    "summary": "In the recent shift towards human-centric AI, the need for machines to accurately use natural language has become increasingly important. While a common approach to achieve this is to train large language models, this method presents a form of learning misalignment where the model may not capture the underlying structure and reasoning humans employ in using natural language, potentially leading to unexpected or unreliable behavior. Emergent communication (Emecom) is a field of research that has seen a growing number of publications in recent years, aiming to develop artificial agents capable of using natural language in a way that goes beyond simple discriminative tasks and can effectively communicate and learn new concepts. In this review, we present Emecom under two aspects. Firstly, we delineate all the common proprieties we find across the literature and how they relate to human interactions. Secondly, we identify two subcategories and highlight their characteristics and open challenges. We encourage researchers to work together by demonstrating that different methods can be viewed as diverse solutions to a common problem and emphasize the importance of including diverse perspectives and expertise in the field. We believe a deeper understanding of human communication is crucial to developing machines that can accurately use natural language in human-machine interactions.",
    "authors": [
      "Nicolo' Brandizzi"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC",
      "cs.MA"
    ],
    "published": "2023-08-01T14:43:10Z",
    "pdf_url": "https://arxiv.org/pdf/2308.02541v1"
  },
  {
    "arxiv_id": "2307.16851v1",
    "entry_id": "http://arxiv.org/abs/2307.16851v1",
    "title": "Towards Trustworthy and Aligned Machine Learning: A Data-centric Survey with Causality Perspectives",
    "summary": "The trustworthiness of machine learning has emerged as a critical topic in the field, encompassing various applications and research areas such as robustness, security, interpretability, and fairness. The last decade saw the development of numerous methods addressing these challenges. In this survey, we systematically review these advancements from a data-centric perspective, highlighting the shortcomings of traditional empirical risk minimization (ERM) training in handling challenges posed by the data.\n  Interestingly, we observe a convergence of these methods, despite being developed independently across trustworthy machine learning subfields. Pearl's hierarchy of causality offers a unifying framework for these techniques. Accordingly, this survey presents the background of trustworthy machine learning development using a unified set of concepts, connects this language to Pearl's causal hierarchy, and finally discusses methods explicitly inspired by causality literature. We provide a unified language with mathematical vocabulary to link these methods across robustness, adversarial robustness, interpretability, and fairness, fostering a more cohesive understanding of the field.\n  Further, we explore the trustworthiness of large pretrained models. After summarizing dominant techniques like fine-tuning, parameter-efficient fine-tuning, prompting, and reinforcement learning with human feedback, we draw connections between them and the standard ERM. This connection allows us to build upon the principled understanding of trustworthy methods, extending it to these new techniques in large pretrained models, paving the way for future methods. Existing methods under this perspective are also reviewed.\n  Lastly, we offer a brief summary of the applications of these methods and discuss potential future aspects related to our survey. For more information, please visit http://trustai.one.",
    "authors": [
      "Haoyang Liu",
      "Maheep Chaudhary",
      "Haohan Wang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-07-31T17:11:35Z",
    "pdf_url": "https://arxiv.org/pdf/2307.16851v1"
  },
  {
    "arxiv_id": "2307.16778v2",
    "entry_id": "http://arxiv.org/abs/2307.16778v2",
    "title": "KoBBQ: Korean Bias Benchmark for Question Answering",
    "summary": "The Bias Benchmark for Question Answering (BBQ) is designed to evaluate social biases of language models (LMs), but it is not simple to adapt this benchmark to cultural contexts other than the US because social biases depend heavily on the cultural context. In this paper, we present KoBBQ, a Korean bias benchmark dataset, and we propose a general framework that addresses considerations for cultural adaptation of a dataset. Our framework includes partitioning the BBQ dataset into three classes--Simply-Transferred (can be used directly after cultural translation), Target-Modified (requires localization in target groups), and Sample-Removed (does not fit Korean culture)-- and adding four new categories of bias specific to Korean culture. We conduct a large-scale survey to collect and validate the social biases and the targets of the biases that reflect the stereotypes in Korean culture. The resulting KoBBQ dataset comprises 268 templates and 76,048 samples across 12 categories of social bias. We use KoBBQ to measure the accuracy and bias scores of several state-of-the-art multilingual LMs. The results clearly show differences in the bias of LMs as measured by KoBBQ and a machine-translated version of BBQ, demonstrating the need for and utility of a well-constructed, culturally-aware social bias benchmark.",
    "authors": [
      "Jiho Jin",
      "Jiseon Kim",
      "Nayeon Lee",
      "Haneul Yoo",
      "Alice Oh",
      "Hwaran Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-07-31T15:44:15Z",
    "pdf_url": "https://arxiv.org/pdf/2307.16778v2"
  },
  {
    "arxiv_id": "2307.16680v7",
    "entry_id": "http://arxiv.org/abs/2307.16680v7",
    "title": "On the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook",
    "summary": "Diffusion models and large language models have emerged as leading-edge generative models, revolutionizing various aspects of human life. However, the practical implementations of these models have also exposed inherent risks, bringing to the forefront their evil sides and sparking concerns regarding their trustworthiness. Despite the wealth of literature on this subject, a comprehensive survey specifically delving into the intersection of large-scale generative models and their trustworthiness remains largely absent. To bridge this gap, this paper investigates both the long-standing and emerging threats associated with these models across four fundamental dimensions: 1) privacy, 2) security, 3) fairness, and 4) responsibility. Based on the investigation results, we develop an extensive map outlining the trustworthiness of large generative models. After that, we provide practical recommendations and potential research directions for future secure applications equipped with large generative models, ultimately promoting the trustworthiness of the models and benefiting the society as a whole.",
    "authors": [
      "Mingyuan Fan",
      "Chengyu Wang",
      "Cen Chen",
      "Yang Liu",
      "Jun Huang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.CR",
      "cs.CV"
    ],
    "published": "2023-07-31T13:57:05Z",
    "pdf_url": "https://arxiv.org/pdf/2307.16680v7"
  },
  {
    "arxiv_id": "2307.16376v1",
    "entry_id": "http://arxiv.org/abs/2307.16376v1",
    "title": "When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities",
    "summary": "The advent of large language models marks a revolutionary breakthrough in artificial intelligence. With the unprecedented scale of training and model parameters, the capability of large language models has been dramatically improved, leading to human-like performances in understanding, language synthesizing, and common-sense reasoning, etc. Such a major leap-forward in general AI capacity will change the pattern of how personalization is conducted. For one thing, it will reform the way of interaction between humans and personalization systems. Instead of being a passive medium of information filtering, large language models present the foundation for active user engagement. On top of such a new foundation, user requests can be proactively explored, and user's required information can be delivered in a natural and explainable way. For another thing, it will also considerably expand the scope of personalization, making it grow from the sole function of collecting personalized information to the compound function of providing personalized services. By leveraging large language models as general-purpose interface, the personalization systems may compile user requests into plans, calls the functions of external tools to execute the plans, and integrate the tools' outputs to complete the end-to-end personalization tasks. Today, large language models are still being developed, whereas the application in personalization is largely unexplored. Therefore, we consider it to be the right time to review the challenges in personalization and the opportunities to address them with LLMs. In particular, we dedicate this perspective paper to the discussion of the following aspects: the development and challenges for the existing personalization system, the newly emerged capabilities of large language models, and the potential ways of making use of large language models for personalization.",
    "authors": [
      "Jin Chen",
      "Zheng Liu",
      "Xu Huang",
      "Chenwang Wu",
      "Qi Liu",
      "Gangwei Jiang",
      "Yuanhao Pu",
      "Yuxuan Lei",
      "Xiaolong Chen",
      "Xingmei Wang",
      "Defu Lian",
      "Enhong Chen"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-07-31T02:48:56Z",
    "pdf_url": "https://arxiv.org/pdf/2307.16376v1"
  },
  {
    "arxiv_id": "2307.15838v1",
    "entry_id": "http://arxiv.org/abs/2307.15838v1",
    "title": "Holistic Survey of Privacy and Fairness in Machine Learning",
    "summary": "Privacy and fairness are two crucial pillars of responsible Artificial Intelligence (AI) and trustworthy Machine Learning (ML). Each objective has been independently studied in the literature with the aim of reducing utility loss in achieving them. Despite the significant interest attracted from both academia and industry, there remains an immediate demand for more in-depth research to unravel how these two objectives can be simultaneously integrated into ML models. As opposed to well-accepted trade-offs, i.e., privacy-utility and fairness-utility, the interrelation between privacy and fairness is not well-understood. While some works suggest a trade-off between the two objective functions, there are others that demonstrate the alignment of these functions in certain scenarios. To fill this research gap, we provide a thorough review of privacy and fairness in ML, including supervised, unsupervised, semi-supervised, and reinforcement learning. After examining and consolidating the literature on both objectives, we present a holistic survey on the impact of privacy on fairness, the impact of fairness on privacy, existing architectures, their interaction in application domains, and algorithms that aim to achieve both objectives while minimizing the utility sacrificed. Finally, we identify research challenges in achieving privacy and fairness concurrently in ML, particularly focusing on large language models.",
    "authors": [
      "Sina Shaham",
      "Arash Hajisafi",
      "Minh K Quan",
      "Dinh C Nguyen",
      "Bhaskar Krishnamachari",
      "Charith Peris",
      "Gabriel Ghinita",
      "Cyrus Shahabi",
      "Pubudu N. Pathirana"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-07-28T23:39:29Z",
    "pdf_url": "https://arxiv.org/pdf/2307.15838v1"
  },
  {
    "arxiv_id": "2307.15425v1",
    "entry_id": "http://arxiv.org/abs/2307.15425v1",
    "title": "A Critical Review of Large Language Models: Sensitivity, Bias, and the Path Toward Specialized AI",
    "summary": "This paper examines the comparative effectiveness of a specialized compiled language model and a general-purpose model like OpenAI's GPT-3.5 in detecting SDGs within text data. It presents a critical review of Large Language Models (LLMs), addressing challenges related to bias and sensitivity. The necessity of specialized training for precise, unbiased analysis is underlined. A case study using a company descriptions dataset offers insight into the differences between the GPT-3.5 and the specialized SDG detection model. While GPT-3.5 boasts broader coverage, it may identify SDGs with limited relevance to the companies' activities. In contrast, the specialized model zeroes in on highly pertinent SDGs. The importance of thoughtful model selection is emphasized, taking into account task requirements, cost, complexity, and transparency. Despite the versatility of LLMs, the use of specialized models is suggested for tasks demanding precision and accuracy. The study concludes by encouraging further research to find a balance between the capabilities of LLMs and the need for domain-specific expertise and interpretability.",
    "authors": [
      "Arash Hajikhani",
      "Carolyn Cole"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-07-28T09:20:22Z",
    "pdf_url": "https://arxiv.org/pdf/2307.15425v1"
  },
  {
    "arxiv_id": "2307.15217v2",
    "entry_id": "http://arxiv.org/abs/2307.15217v2",
    "title": "Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback",
    "summary": "Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-faceted approach to the development of safer AI systems.",
    "authors": [
      "Stephen Casper",
      "Xander Davies",
      "Claudia Shi",
      "Thomas Krendl Gilbert",
      "Jérémy Scheurer",
      "Javier Rando",
      "Rachel Freedman",
      "Tomasz Korbak",
      "David Lindner",
      "Pedro Freire",
      "Tony Wang",
      "Samuel Marks",
      "Charbel-Raphaël Segerie",
      "Micah Carroll",
      "Andi Peng",
      "Phillip Christoffersen",
      "Mehul Damani",
      "Stewart Slocum",
      "Usman Anwar",
      "Anand Siththaranjan",
      "Max Nadeau",
      "Eric J. Michaud",
      "Jacob Pfau",
      "Dmitrii Krasheninnikov",
      "Xin Chen",
      "Lauro Langosco",
      "Peter Hase",
      "Erdem Bıyık",
      "Anca Dragan",
      "David Krueger",
      "Dorsa Sadigh",
      "Dylan Hadfield-Menell"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-07-27T22:29:25Z",
    "pdf_url": "https://arxiv.org/pdf/2307.15217v2"
  },
  {
    "arxiv_id": "2308.02443v1",
    "entry_id": "http://arxiv.org/abs/2308.02443v1",
    "title": "AI Literature Review Suite",
    "summary": "The process of conducting literature reviews is often time-consuming and labor-intensive. To streamline this process, I present an AI Literature Review Suite that integrates several functionalities to provide a comprehensive literature review. This tool leverages the power of open access science, large language models (LLMs) and natural language processing to enable the searching, downloading, and organizing of PDF files, as well as extracting content from articles. Semantic search queries are used for data retrieval, while text embeddings and summarization using LLMs present succinct literature reviews. Interaction with PDFs is enhanced through a user-friendly graphical user interface (GUI). The suite also features integrated programs for bibliographic organization, interaction and query, and literature review summaries. This tool presents a robust solution to automate and optimize the process of literature review in academic and industrial research.",
    "authors": [
      "David A. Tovar"
    ],
    "categories": [
      "cs.DL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2023-07-27T17:30:31Z",
    "pdf_url": "https://arxiv.org/pdf/2308.02443v1"
  },
  {
    "arxiv_id": "2307.14854v2",
    "entry_id": "http://arxiv.org/abs/2307.14854v2",
    "title": "MatrixWorld: A pursuit-evasion platform for safe multi-agent coordination and autocurricula",
    "summary": "Multi-agent reinforcement learning (MARL) achieves encouraging performance in solving complex tasks. However, the safety of MARL policies is one critical concern that impedes their real-world applications. Popular multi-agent benchmarks focus on diverse tasks yet provide limited safety support. Therefore, this work proposes a safety-constrained multi-agent environment: MatrixWorld, based on the general pursuit-evasion game. Particularly, a safety-constrained multi-agent action execution model is proposed for the software implementation of safe multi-agent environments based on diverse safety definitions. It (1) extends the vertex conflict among homogeneous / cooperative agents to heterogeneous / adversarial settings, and (2) proposes three types of resolutions for each type of conflict, aiming at providing rational and unbiased feedback for safe MARL. Besides, MatrixWorld is also a lightweight co-evolution framework for the learning of pursuit tasks, evasion tasks, or both, where more pursuit-evasion variants can be designed based on different practical meanings of safety. As a brief survey, we review and analyze the co-evolution mechanism in the multi-agent setting, which clearly reveals its relationships with autocurricula, self-play, arms races, and adversarial learning. Thus, MatrixWorld can also serve as the first environment for autocurricula research, where ideas can be quickly verified and well understood.",
    "authors": [
      "Lijun Sun",
      "Yu-Cheng Chang",
      "Chao Lyu",
      "Chin-Teng Lin",
      "Yuhui Shi"
    ],
    "categories": [
      "cs.MA"
    ],
    "published": "2023-07-27T13:35:03Z",
    "pdf_url": "https://arxiv.org/pdf/2307.14854v2"
  },
  {
    "arxiv_id": "2307.14500v1",
    "entry_id": "http://arxiv.org/abs/2307.14500v1",
    "title": "A Predictive Model of Digital Information Engagement: Forecasting User Engagement With English Words by Incorporating Cognitive Biases, Computational Linguistics and Natural Language Processing",
    "summary": "This study introduces and empirically tests a novel predictive model for digital information engagement (IE) - the READ model, an acronym for the four pivotal attributes of engaging information: Representativeness, Ease-of-use, Affect, and Distribution. Conceptualized within the theoretical framework of Cumulative Prospect Theory, the model integrates key cognitive biases with computational linguistics and natural language processing to develop a multidimensional perspective on information engagement. A rigorous testing protocol was implemented, involving 50 randomly selected pairs of synonymous words (100 words in total) from the WordNet database. These words' engagement levels were evaluated through a large-scale online survey (n = 80,500) to derive empirical IE metrics. The READ attributes for each word were then computed and their predictive efficacy examined. The findings affirm the READ model's robustness, accurately predicting a word's IE level and distinguishing the more engaging word from a pair of synonyms with an 84% accuracy rate. The READ model's potential extends across various domains, including business, education, government, and healthcare, where it could enhance content engagement and inform AI language model development and generative text work. Future research should address the model's scalability and adaptability across different domains and languages, thereby broadening its applicability and efficacy.",
    "authors": [
      "Nimrod Dvir",
      "Elaine Friedman",
      "Suraj Commuri",
      "Fan yang",
      "Jennifer Romano"
    ],
    "categories": [
      "cs.HC",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-07-26T20:58:47Z",
    "pdf_url": "https://arxiv.org/pdf/2307.14500v1"
  },
  {
    "arxiv_id": "2307.15717v1",
    "entry_id": "http://arxiv.org/abs/2307.15717v1",
    "title": "Utilizing Large Language Models for Natural Interface to Pharmacology Databases",
    "summary": "The drug development process necessitates that pharmacologists undertake various tasks, such as reviewing literature, formulating hypotheses, designing experiments, and interpreting results. Each stage requires accessing and querying vast amounts of information. In this abstract, we introduce a Large Language Model (LLM)-based Natural Language Interface designed to interact with structured information stored in databases. Our experiments demonstrate the feasibility and effectiveness of the proposed framework. This framework can generalize to query a wide range of pharmaceutical data and knowledge bases.",
    "authors": [
      "Hong Lu",
      "Chuan Li",
      "Yinheng Li",
      "Jie Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DB"
    ],
    "published": "2023-07-26T17:50:11Z",
    "pdf_url": "https://arxiv.org/pdf/2307.15717v1"
  },
  {
    "arxiv_id": "2307.14324v1",
    "entry_id": "http://arxiv.org/abs/2307.14324v1",
    "title": "Evaluating the Moral Beliefs Encoded in LLMs",
    "summary": "This paper presents a case study on the design, administration, post-processing, and evaluation of surveys on large language models (LLMs). It comprises two components: (1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM \"making a choice\", the associated uncertainty, and the consistency of that choice. (2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious. We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., \"Should I tell a white lie?\") and 687 low-ambiguity moral scenarios (e.g., \"Should I stop for a pedestrian on the road?\"). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., \"do not kill\"). We administer the survey to 28 open- and closed-source LLMs. We find that (a) in unambiguous scenarios, most models \"choose\" actions that align with commonsense. In ambiguous cases, most models express uncertainty. (b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording. (c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.",
    "authors": [
      "Nino Scherrer",
      "Claudia Shi",
      "Amir Feder",
      "David M. Blei"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published": "2023-07-26T17:42:43Z",
    "pdf_url": "https://arxiv.org/pdf/2307.14324v1"
  },
  {
    "arxiv_id": "2307.14298v1",
    "entry_id": "http://arxiv.org/abs/2307.14298v1",
    "title": "ChatGPT and Persuasive Technologies for the Management and Delivery of Personalized Recommendations in Hotel Hospitality",
    "summary": "Recommender systems have become indispensable tools in the hotel hospitality industry, enabling personalized and tailored experiences for guests. Recent advancements in large language models (LLMs), such as ChatGPT, and persuasive technologies, have opened new avenues for enhancing the effectiveness of those systems. This paper explores the potential of integrating ChatGPT and persuasive technologies for automating and improving hotel hospitality recommender systems. First, we delve into the capabilities of ChatGPT, which can understand and generate human-like text, enabling more accurate and context-aware recommendations. We discuss the integration of ChatGPT into recommender systems, highlighting the ability to analyze user preferences, extract valuable insights from online reviews, and generate personalized recommendations based on guest profiles. Second, we investigate the role of persuasive technology in influencing user behavior and enhancing the persuasive impact of hotel recommendations. By incorporating persuasive techniques, such as social proof, scarcity and personalization, recommender systems can effectively influence user decision-making and encourage desired actions, such as booking a specific hotel or upgrading their room. To investigate the efficacy of ChatGPT and persuasive technologies, we present a pilot experi-ment with a case study involving a hotel recommender system. We aim to study the impact of integrating ChatGPT and persua-sive techniques on user engagement, satisfaction, and conversion rates. The preliminary results demonstrate the potential of these technologies in enhancing the overall guest experience and business performance. Overall, this paper contributes to the field of hotel hospitality by exploring the synergistic relationship between LLMs and persuasive technology in recommender systems, ultimately influencing guest satisfaction and hotel revenue.",
    "authors": [
      "Manolis Remountakis",
      "Konstantinos Kotis",
      "Babis Kourtzis",
      "George E. Tsekouras"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-07-26T16:58:10Z",
    "pdf_url": "https://arxiv.org/pdf/2307.14298v1"
  },
  {
    "arxiv_id": "2307.13912v3",
    "entry_id": "http://arxiv.org/abs/2307.13912v3",
    "title": "Embedding Democratic Values into Social Media AIs via Societal Objective Functions",
    "summary": "Can we design artificial intelligence (AI) systems that rank our social media feeds to consider democratic values such as mitigating partisan animosity as part of their objective functions? We introduce a method for translating established, vetted social scientific constructs into AI objective functions, which we term societal objective functions, and demonstrate the method with application to the political science construct of anti-democratic attitudes. Traditionally, we have lacked observable outcomes to use to train such models, however, the social sciences have developed survey instruments and qualitative codebooks for these constructs, and their precision facilitates translation into detailed prompts for large language models. We apply this method to create a democratic attitude model that estimates the extent to which a social media post promotes anti-democratic attitudes, and test this democratic attitude model across three studies. In Study 1, we first test the attitudinal and behavioral effectiveness of the intervention among US partisans (N=1,380) by manually annotating (alpha=.895) social media posts with anti-democratic attitude scores and testing several feed ranking conditions based on these scores. Removal (d=.20) and downranking feeds (d=.25) reduced participants' partisan animosity without compromising their experience and engagement. In Study 2, we scale up the manual labels by creating the democratic attitude model, finding strong agreement with manual labels (rho=.75). Finally, in Study 3, we replicate Study 1 using the democratic attitude model instead of manual labels to test its attitudinal and behavioral impact (N=558), and again find that the feed downranking using the societal objective function reduced partisan animosity (d=.25). This method presents a novel strategy to draw on social science theory and methods to mitigate societal harms in social media AIs.",
    "authors": [
      "Chenyan Jia",
      "Michelle S. Lam",
      "Minh Chau Mai",
      "Jeff Hancock",
      "Michael S. Bernstein"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-07-26T02:27:24Z",
    "pdf_url": "https://arxiv.org/pdf/2307.13912v3"
  },
  {
    "arxiv_id": "2307.13721v1",
    "entry_id": "http://arxiv.org/abs/2307.13721v1",
    "title": "Foundational Models Defining a New Era in Vision: A Survey and Outlook",
    "summary": "Vision systems to see and reason about the compositional nature of visual scenes are fundamental to understanding our world. The complex relations between objects and their locations, ambiguities, and variations in the real-world environment can be better described in human language, naturally governed by grammatical rules and other modalities such as audio and depth. The models learned to bridge the gap between such modalities coupled with large-scale training data facilitate contextual reasoning, generalization, and prompt capabilities at test time. These models are referred to as foundational models. The output of such models can be modified through human-provided prompts without retraining, e.g., segmenting a particular object by providing a bounding box, having interactive dialogues by asking questions about an image or video scene or manipulating the robot's behavior through language instructions. In this survey, we provide a comprehensive review of such emerging foundational models, including typical architecture designs to combine different modalities (vision, text, audio, etc), training objectives (contrastive, generative), pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous. We discuss the open challenges and research directions for foundational models in computer vision, including difficulties in their evaluations and benchmarking, gaps in their real-world understanding, limitations of their contextual understanding, biases, vulnerability to adversarial attacks, and interpretability issues. We review recent developments in this field, covering a wide range of applications of foundation models systematically and comprehensively. A comprehensive list of foundational models studied in this work is available at \\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.",
    "authors": [
      "Muhammad Awais",
      "Muzammal Naseer",
      "Salman Khan",
      "Rao Muhammad Anwer",
      "Hisham Cholakkal",
      "Mubarak Shah",
      "Ming-Hsuan Yang",
      "Fahad Shahbaz Khan"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-07-25T17:59:18Z",
    "pdf_url": "https://arxiv.org/pdf/2307.13721v1"
  },
  {
    "arxiv_id": "2307.13528v2",
    "entry_id": "http://arxiv.org/abs/2307.13528v2",
    "title": "FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
    "summary": "The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",
    "authors": [
      "I-Chun Chern",
      "Steffi Chern",
      "Shiqi Chen",
      "Weizhe Yuan",
      "Kehua Feng",
      "Chunting Zhou",
      "Junxian He",
      "Graham Neubig",
      "Pengfei Liu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-07-25T14:20:51Z",
    "pdf_url": "https://arxiv.org/pdf/2307.13528v2"
  },
  {
    "arxiv_id": "2307.15715v1",
    "entry_id": "http://arxiv.org/abs/2307.15715v1",
    "title": "Improving Primary Healthcare Workflow Using Extreme Summarization of Scientific Literature Based on Generative AI",
    "summary": "Primary care professionals struggle to keep up to date with the latest scientific literature critical in guiding evidence-based practice related to their daily work. To help solve the above-mentioned problem, we employed generative artificial intelligence techniques based on large-scale language models to summarize abstracts of scientific papers. Our objective is to investigate the potential of generative artificial intelligence in diminishing the cognitive load experienced by practitioners, thus exploring its ability to alleviate mental effort and burden. The study participants were provided with two use cases related to preventive care and behavior change, simulating a search for new scientific literature. The study included 113 university students from Slovenia and the United States randomized into three distinct study groups. The first group was assigned to the full abstracts. The second group was assigned to the short abstracts generated by AI. The third group had the option to select a full abstract in addition to the AI-generated short summary. Each use case study included ten retrieved abstracts. Our research demonstrates that the use of generative AI for literature review is efficient and effective. The time needed to answer questions related to the content of abstracts was significantly lower in groups two and three compared to the first group using full abstracts. The results, however, also show significantly lower accuracy in extracted knowledge in cases where full abstract was not available. Such a disruptive technology could significantly reduce the time required for healthcare professionals to keep up with the most recent scientific literature; nevertheless, further developments are needed to help them comprehend the knowledge accurately.",
    "authors": [
      "Gregor Stiglic",
      "Leon Kopitar",
      "Lucija Gosak",
      "Primoz Kocbek",
      "Zhe He",
      "Prithwish Chakraborty",
      "Pablo Meyer",
      "Jiang Bian"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-07-24T21:42:27Z",
    "pdf_url": "https://arxiv.org/pdf/2307.15715v1"
  },
  {
    "arxiv_id": "2307.11845v2",
    "entry_id": "http://arxiv.org/abs/2307.11845v2",
    "title": "Multimodal Document Analytics for Banking Process Automation",
    "summary": "Traditional banks face increasing competition from FinTechs in the rapidly evolving financial ecosystem. Raising operational efficiency is vital to address this challenge. Our study aims to improve the efficiency of document-intensive business processes in banking. To that end, we first review the landscape of business documents in the retail segment. Banking documents often contain text, layout, and visuals, suggesting that document analytics and process automation require more than plain natural language processing (NLP). To verify this and assess the incremental value of visual cues when processing business documents, we compare a recently proposed multimodal model called LayoutXLM to powerful text classifiers (e.g., BERT) and large language models (e.g., GPT) in a case study related to processing company register extracts. The results confirm that incorporating layout information in a model substantially increases its performance. Interestingly, we also observed that more than 75% of the best model performance (in terms of the F1 score) can be achieved with as little as 30% of the training data. This shows that the demand for data labeled data to set up a multi-modal model can be moderate, which simplifies real-world applications of multimodal document analytics. Our study also sheds light on more specific practices in the scope of calibrating a multimodal banking document classifier, including the need for fine-tuning. In sum, the paper contributes original empirical evidence on the effectiveness and efficiency of multi-model models for document processing in the banking business and offers practical guidance on how to unlock this potential in day-to-day operations.",
    "authors": [
      "Christopher Gerling",
      "Stefan Lessmann"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV",
      "q-fin.CP"
    ],
    "published": "2023-07-21T18:29:04Z",
    "pdf_url": "https://arxiv.org/pdf/2307.11845v2"
  },
  {
    "arxiv_id": "2307.11319v2",
    "entry_id": "http://arxiv.org/abs/2307.11319v2",
    "title": "\"Tidy Up the Table\": Grounding Common-sense Objective for Tabletop Object Rearrangement",
    "summary": "Tidying up a messy table may appear simple for humans, but articulating clear criteria for tidiness is challenging due to the ambiguous nature of common sense reasoning. Large Language Models (LLMs) have proven capable of capturing common sense knowledge to reason over this vague concept of tidiness. However, they alone may struggle with table tidying due to the limited grasp on the spatio-visual aspects of tidiness. In this work, we aim to ground the common-sense concept of tidiness within the context of object arrangement. Our survey reveals that humans usually factorize tidiness into semantic and visual-spatial tidiness; our grounding approach aligns with this decomposition. We connect a language-based policy generator with an image-based tidiness score function: the policy generator utilizes the LLM's commonsense knowledge to cluster objects by their implicit types and functionalities for semantic tidiness; meanwhile, the tidiness score function assesses the visual-spatial relations of the object to achieve visual-spatial tidiness. Our tidiness score is trained using synthetic data generated cheaply from customized random walks, which inherently encode the order of tidiness, thereby bypassing the need for labor-intensive human demonstrations. The simulated experiment shows that our approach successfully generates tidy arrangements, predominately in 2D, with potential for 3D stacking, for tables with various novel objects.",
    "authors": [
      "Yiqing Xu",
      "David Hsu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2023-07-21T03:00:31Z",
    "pdf_url": "https://arxiv.org/pdf/2307.11319v2"
  },
  {
    "arxiv_id": "2307.09711v1",
    "entry_id": "http://arxiv.org/abs/2307.09711v1",
    "title": "Two Tales of Platoon Intelligence for Autonomous Mobility Control: Enabling Deep Learning Recipes",
    "summary": "This paper presents the deep learning-based recent achievements to resolve the problem of autonomous mobility control and efficient resource management of autonomous vehicles and UAVs, i.e., (i) multi-agent reinforcement learning (MARL), and (ii) neural Myerson auction. Representatively, communication network (CommNet), which is one of the most popular MARL algorithms, is introduced to enable multiple agents to take actions in a distributed manner for their shared goals by training all agents' states and actions in a single neural network. Moreover, the neural Myerson auction guarantees trustfulness among multiple agents as well as achieves the optimal revenue of highly dynamic systems. Therefore, we survey the recent studies on autonomous mobility control based on MARL and neural Myerson auction. Furthermore, we emphasize that integration of MARL and neural Myerson auction is expected to be critical for efficient and trustful autonomous mobility services.",
    "authors": [
      "Soohyun Park",
      "Haemin Lee",
      "Chanyoung Park",
      "Soyi Jung",
      "Minseok Choi",
      "Joongheon Kim"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-07-19T01:46:38Z",
    "pdf_url": "https://arxiv.org/pdf/2307.09711v1"
  },
  {
    "arxiv_id": "2307.09683v3",
    "entry_id": "http://arxiv.org/abs/2307.09683v3",
    "title": "PubMed and Beyond: Biomedical Literature Search in the Age of Artificial Intelligence",
    "summary": "Biomedical research yields a wealth of information, much of which is only accessible through the literature. Consequently, literature search is an essential tool for building on prior knowledge in clinical and biomedical research. Although recent improvements in artificial intelligence have expanded functionality beyond keyword-based search, these advances may be unfamiliar to clinicians and researchers. In response, we present a survey of literature search tools tailored to both general and specific information needs in biomedicine, with the objective of helping readers efficiently fulfill their information needs. We first examine the widely used PubMed search engine, discussing recent improvements and continued challenges. We then describe literature search tools catering to five specific information needs: 1. Identifying high-quality clinical research for evidence-based medicine. 2. Retrieving gene-related information for precision medicine and genomics. 3. Searching by meaning, including natural language questions. 4. Locating related articles with literature recommendation. 5. Mining literature to discover associations between concepts such as diseases and genetic variants. Additionally, we cover practical considerations and best practices for choosing and using these tools. Finally, we provide a perspective on the future of literature search engines, considering recent breakthroughs in large language models such as ChatGPT. In summary, our survey provides a comprehensive view of biomedical literature search functionalities with 36 publicly available tools.",
    "authors": [
      "Qiao Jin",
      "Robert Leaman",
      "Zhiyong Lu"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.DL"
    ],
    "published": "2023-07-18T23:35:53Z",
    "pdf_url": "https://arxiv.org/pdf/2307.09683v3"
  },
  {
    "arxiv_id": "2307.09009v3",
    "entry_id": "http://arxiv.org/abs/2307.09009v3",
    "title": "How is ChatGPT's behavior changing over time?",
    "summary": "GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on several diverse tasks: 1) math problems, 2) sensitive/dangerous questions, 3) opinion surveys, 4) multi-hop knowledge-intensive questions, 5) generating code, 6) US Medical License tests, and 7) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was reasonable at identifying prime vs. composite numbers (84% accuracy) but GPT-4 (June 2023) was poor on these same questions (51% accuracy). This is partly explained by a drop in GPT-4's amenity to follow chain-of-thought prompting. Interestingly, GPT-3.5 was much better in June than in March in this task. GPT-4 became less willing to answer sensitive questions and opinion survey questions in June than in March. GPT-4 performed better at multi-hop questions in June than in March, while GPT-3.5's performance dropped on this task. Both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. We provide evidence that GPT-4's ability to follow user instructions has decreased over time, which is one common factor behind the many behavior drifts. Overall, our findings show that the behavior of the \"same\" LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLMs.",
    "authors": [
      "Lingjiao Chen",
      "Matei Zaharia",
      "James Zou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-07-18T06:56:08Z",
    "pdf_url": "https://arxiv.org/pdf/2307.09009v3"
  },
  {
    "arxiv_id": "2307.08985v1",
    "entry_id": "http://arxiv.org/abs/2307.08985v1",
    "title": "PromptCrafter: Crafting Text-to-Image Prompt through Mixed-Initiative Dialogue with LLM",
    "summary": "Text-to-image generation model is able to generate images across a diverse range of subjects and styles based on a single prompt. Recent works have proposed a variety of interaction methods that help users understand the capabilities of models and utilize them. However, how to support users to efficiently explore the model's capability and to create effective prompts are still open-ended research questions. In this paper, we present PromptCrafter, a novel mixed-initiative system that allows step-by-step crafting of text-to-image prompt. Through the iterative process, users can efficiently explore the model's capability, and clarify their intent. PromptCrafter also supports users to refine prompts by answering various responses to clarifying questions generated by a Large Language Model. Lastly, users can revert to a desired step by reviewing the work history. In this workshop paper, we discuss the design process of PromptCrafter and our plans for follow-up studies.",
    "authors": [
      "Seungho Baek",
      "Hyerin Im",
      "Jiseung Ryu",
      "Juhyeong Park",
      "Takyeon Lee"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-07-18T05:51:00Z",
    "pdf_url": "https://arxiv.org/pdf/2307.08985v1"
  },
  {
    "arxiv_id": "2307.08974v1",
    "entry_id": "http://arxiv.org/abs/2307.08974v1",
    "title": "Development of the ChatGPT, Generative Artificial Intelligence and Natural Large Language Models for Accountable Reporting and Use (CANGARU) Guidelines",
    "summary": "The swift progress and ubiquitous adoption of Generative AI (GAI), Generative Pre-trained Transformers (GPTs), and large language models (LLMs) like ChatGPT, have spurred queries about their ethical application, use, and disclosure in scholarly research and scientific productions. A few publishers and journals have recently created their own sets of rules; however, the absence of a unified approach may lead to a 'Babel Tower Effect,' potentially resulting in confusion rather than desired standardization. In response to this, we present the ChatGPT, Generative Artificial Intelligence, and Natural Large Language Models for Accountable Reporting and Use Guidelines (CANGARU) initiative, with the aim of fostering a cross-disciplinary global inclusive consensus on the ethical use, disclosure, and proper reporting of GAI/GPT/LLM technologies in academia. The present protocol consists of four distinct parts: a) an ongoing systematic review of GAI/GPT/LLM applications to understand the linked ideas, findings, and reporting standards in scholarly research, and to formulate guidelines for its use and disclosure, b) a bibliometric analysis of existing author guidelines in journals that mention GAI/GPT/LLM, with the goal of evaluating existing guidelines, analyzing the disparity in their recommendations, and identifying common rules that can be brought into the Delphi consensus process, c) a Delphi survey to establish agreement on the items for the guidelines, ensuring principled GAI/GPT/LLM use, disclosure, and reporting in academia, and d) the subsequent development and dissemination of the finalized guidelines and their supplementary explanation and elaboration documents.",
    "authors": [
      "Giovanni E. Cacciamani",
      "Michael B. Eppler",
      "Conner Ganjavi",
      "Asli Pekan",
      "Brett Biedermann",
      "Gary S. Collins",
      "Inderbir S. Gill"
    ],
    "categories": [
      "cs.AI",
      "cs.CY"
    ],
    "published": "2023-07-18T05:12:52Z",
    "pdf_url": "https://arxiv.org/pdf/2307.08974v1"
  },
  {
    "arxiv_id": "2307.08925v3",
    "entry_id": "http://arxiv.org/abs/2307.08925v3",
    "title": "Integration of Large Language Models and Federated Learning",
    "summary": "As the parameter size of Large Language Models (LLMs) continues to expand, there is an urgent need to address the scarcity of high-quality data. In response, existing research has attempted to make a breakthrough by incorporating Federated Learning (FL) into LLMs. Conversely, considering the outstanding performance of LLMs in task generalization, researchers have also tried applying LLMs within FL to tackle challenges in relevant domains. The complementarity between LLMs and FL has already ignited widespread research interest. In this paper, we aim to deeply explore the integration of LLMs and FL. We propose a research framework, dividing the fusion of LLMs and FL into three parts: the combination of LLM sub-technologies with FL, the integration of FL sub-technologies with LLMs, and the overall merger of LLMs and FL. We first provide a comprehensive review of the current state of research in the domain of LLMs combined with FL, including their typical applications, integration advantages, challenges faced, and future directions for resolution. Subsequently, we discuss the practical applications of the combination of LLMs and FL in critical scenarios such as healthcare, finance, and education, and provide new perspectives and insights into future research directions for LLMs and FL.",
    "authors": [
      "Chaochao Chen",
      "Xiaohua Feng",
      "Yuyuan Li",
      "Lingjuan Lyu",
      "Jun Zhou",
      "Xiaolin Zheng",
      "Jianwei Yin"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-07-18T02:09:14Z",
    "pdf_url": "https://arxiv.org/pdf/2307.08925v3"
  },
  {
    "arxiv_id": "2307.07645v2",
    "entry_id": "http://arxiv.org/abs/2307.07645v2",
    "title": "Othering and low status framing of immigrant cuisines in US restaurant reviews and large language models",
    "summary": "Identifying implicit attitudes toward food can mitigate social prejudice due to food's salience as a marker of ethnic identity. Stereotypes about food are representational harms that may contribute to racialized discourse and negatively impact economic outcomes for restaurants. Understanding the presence of representational harms in online corpora in particular is important, given the increasing use of large language models (LLMs) for text generation and their tendency to reproduce attitudes in their training data. Through careful linguistic analyses, we evaluate social theories about attitudes toward immigrant cuisine in a large-scale study of framing differences in 2.1M English language Yelp reviews. Controlling for factors such as restaurant price and neighborhood racial diversity, we find that immigrant cuisines are more likely to be othered using socially constructed frames of authenticity (e.g., \"authentic,\" \"traditional\"), and that non-European cuisines (e.g., Indian, Mexican) in particular are described as more exotic compared to European ones (e.g., French). We also find that non-European cuisines are more likely to be described as cheap and dirty, even after controlling for price, and even among the most expensive restaurants. Finally, we show that reviews generated by LLMs reproduce similar framing tendencies, pointing to the downstream retention of these representational harms. Our results corroborate social theories of gastronomic stereotyping, revealing racialized evaluative processes and linguistic strategies through which they manifest.",
    "authors": [
      "Yiwei Luo",
      "Kristina Gligorić",
      "Dan Jurafsky"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-07-14T22:25:39Z",
    "pdf_url": "https://arxiv.org/pdf/2307.07645v2"
  },
  {
    "arxiv_id": "2307.04573v2",
    "entry_id": "http://arxiv.org/abs/2307.04573v2",
    "title": "A Semi-Automated Solution Approach Recommender for a Given Use Case: a Case Study for AI/ML in Oncology via Scopus and OpenAI",
    "summary": "Nowadays, literature review is a necessary task when trying to solve a given problem. However, an exhaustive literature review is very time-consuming in today's vast literature landscape. It can take weeks, even if looking only for abstracts or surveys. Moreover, choosing a method among others, and targeting searches within relevant problem and solution domains, are not easy tasks. These are especially true for young researchers or engineers starting to work in their field. Even if surveys that provide methods used to solve a specific problem already exist, an automatic way to do it for any use case is missing, especially for those who don't know the existing literature. Our proposed tool, SARBOLD-LLM, allows discovering and choosing among methods related to a given problem, providing additional information about their uses in the literature to derive decision-making insights, in only a few hours. The SARBOLD-LLM comprises three modules: (1: Scopus search) paper selection using a keyword selection scheme to query Scopus API; (2: Scoring and method extraction) relevancy and popularity scores calculation and solution method extraction in papers utilizing OpenAI API (GPT 3.5); (3: Analyzes) sensitivity analysis and post-analyzes which reveals trends, relevant papers and methods. Comparing the SARBOLD-LLM to manual ground truth using precision, recall, and F1-score metrics, the performance results of AI in the oncology case study are 0.68, 0.9, and 0.77, respectively. SARBOLD-LLM demonstrates successful outcomes across various domains, showcasing its robustness and effectiveness. The SARBOLD-LLM addresses engineers more than researchers, as it proposes methods and trends without adding pros and cons. It is a useful tool to select which methods to investigate first and comes as a complement to surveys. This can limit the global search and accumulation of knowledge for the end user. However...",
    "authors": [
      "Deniz Kenan Kılıç",
      "Alex Elkjær Vasegaard",
      "Aurélien Desoeuvres",
      "Peter Nielsen"
    ],
    "categories": [
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2023-07-10T14:07:28Z",
    "pdf_url": "https://arxiv.org/pdf/2307.04573v2"
  },
  {
    "arxiv_id": "2307.04251v2",
    "entry_id": "http://arxiv.org/abs/2307.04251v2",
    "title": "ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey",
    "summary": "ChatGPT is a large language model (LLM) created by OpenAI that has been carefully trained on a large amount of data. It has revolutionized the field of natural language processing (NLP) and has pushed the boundaries of LLM capabilities. ChatGPT has played a pivotal role in enabling widespread public interaction with generative artificial intelligence (GAI) on a large scale. It has also sparked research interest in developing similar technologies and investigating their applications and implications. In this paper, our primary goal is to provide a concise survey on the current lines of research on ChatGPT and its evolution. We considered both the glass box and black box views of ChatGPT, encompassing the components and foundational elements of the technology, as well as its applications, impacts, and implications. The glass box approach focuses on understanding the inner workings of the technology, and the black box approach embraces it as a complex system, and thus examines its inputs, outputs, and effects. This paves the way for a comprehensive exploration of the technology and provides a road map for further research and experimentation. We also lay out essential foundational literature on LLMs and GAI in general and their connection with ChatGPT. This overview sheds light on existing and missing research lines in the emerging field of LLMs, benefiting both public users and developers. Furthermore, the paper delves into the broad spectrum of applications and significant concerns in fields such as education, research, healthcare, finance, etc.",
    "authors": [
      "Salman Mohamadi",
      "Ghulam Mujtaba",
      "Ngan Le",
      "Gianfranco Doretto",
      "Donald A. Adjeroh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-07-09T19:28:46Z",
    "pdf_url": "https://arxiv.org/pdf/2307.04251v2"
  },
  {
    "arxiv_id": "2307.05747v2",
    "entry_id": "http://arxiv.org/abs/2307.05747v2",
    "title": "Integrating Curricula with Replays: Its Effects on Continual Learning",
    "summary": "Humans engage in learning and reviewing processes with curricula when acquiring new skills or knowledge. This human learning behavior has inspired the integration of curricula with replay methods in continual learning agents. The goal is to emulate the human learning process, thereby improving knowledge retention and facilitating learning transfer. Existing replay methods in continual learning agents involve the random selection and ordering of data from previous tasks, which has shown to be effective. However, limited research has explored the integration of different curricula with replay methods to enhance continual learning. Our study takes initial steps in examining the impact of integrating curricula with replay methods on continual learning in three specific aspects: the interleaved frequency of replayed exemplars with training data, the sequence in which exemplars are replayed, and the strategy for selecting exemplars into the replay buffer. These aspects of curricula design align with cognitive psychology principles and leverage the benefits of interleaved practice during replays, easy-to-hard rehearsal, and exemplar selection strategy involving exemplars from a uniform distribution of difficulties. Based on our results, these three curricula effectively mitigated catastrophic forgetting and enhanced positive knowledge transfer, demonstrating the potential of curricula in advancing continual learning methodologies. Our code and data are available: https://github.com/ZhangLab-DeepNeuroCogLab/Integrating-Curricula-with-Replays",
    "authors": [
      "Ren Jie Tee",
      "Mengmi Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2023-07-08T14:14:55Z",
    "pdf_url": "https://arxiv.org/pdf/2307.05747v2"
  },
  {
    "arxiv_id": "2307.03712v1",
    "entry_id": "http://arxiv.org/abs/2307.03712v1",
    "title": "INT-FP-QSim: Mixed Precision and Formats For Large Language Models and Vision Transformers",
    "summary": "The recent rise of large language models (LLMs) has resulted in increased efforts towards running LLMs at reduced precision. Running LLMs at lower precision supports resource constraints and furthers their democratization, enabling users to run billion-parameter LLMs on their personal devices. To supplement this ongoing effort, we propose INT-FP-QSim: an open-source simulator that enables flexible evaluation of LLMs and vision transformers at various numerical precisions and formats. INT-FP-QSim leverages existing open-source repositories such as TensorRT, QPytorch and AIMET for a combined simulator that supports various floating point and integer formats. With the help of our simulator, we survey the impact of different numerical formats on the performance of LLMs and vision transformers at 4-bit weights and 4-bit or 8-bit activations. We also compare recently proposed methods like Adaptive Block Floating Point, SmoothQuant, GPTQ and RPTQ on the model performances. We hope INT-FP-QSim will enable researchers to flexibly simulate models at various precisions to support further research in quantization of LLMs and vision transformers.",
    "authors": [
      "Lakshmi Nair",
      "Mikhail Bernadskiy",
      "Arulselvan Madhavan",
      "Craig Chan",
      "Ayon Basumallik",
      "Darius Bunandar"
    ],
    "categories": [
      "cs.LG",
      "cs.CL",
      "cs.CV"
    ],
    "published": "2023-07-07T16:54:53Z",
    "pdf_url": "https://arxiv.org/pdf/2307.03712v1"
  },
  {
    "arxiv_id": "2307.03762v1",
    "entry_id": "http://arxiv.org/abs/2307.03762v1",
    "title": "Brain in a Vat: On Missing Pieces Towards Artificial General Intelligence in Large Language Models",
    "summary": "In this perspective paper, we first comprehensively review existing evaluations of Large Language Models (LLMs) using both standardized tests and ability-oriented benchmarks. We pinpoint several problems with current evaluation methods that tend to overstate the capabilities of LLMs. We then articulate what artificial general intelligence should encompass beyond the capabilities of LLMs. We propose four characteristics of generally intelligent agents: 1) they can perform unlimited tasks; 2) they can generate new tasks within a context; 3) they operate based on a value system that underpins task generation; and 4) they have a world model reflecting reality, which shapes their interaction with the world. Building on this viewpoint, we highlight the missing pieces in artificial general intelligence, that is, the unity of knowing and acting. We argue that active engagement with objects in the real world delivers more robust signals for forming conceptual representations. Additionally, knowledge acquisition isn't solely reliant on passive input but requires repeated trials and errors. We conclude by outlining promising future research directions in the field of artificial general intelligence.",
    "authors": [
      "Yuxi Ma",
      "Chi Zhang",
      "Song-Chun Zhu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-07-07T13:58:16Z",
    "pdf_url": "https://arxiv.org/pdf/2307.03762v1"
  },
  {
    "arxiv_id": "2307.03254v1",
    "entry_id": "http://arxiv.org/abs/2307.03254v1",
    "title": "Vision Language Transformers: A Survey",
    "summary": "Vision language tasks, such as answering questions about or generating captions that describe an image, are difficult tasks for computers to perform. A relatively recent body of research has adapted the pretrained transformer architecture introduced in \\citet{vaswani2017attention} to vision language modeling. Transformer models have greatly improved performance and versatility over previous vision language models. They do so by pretraining models on a large generic datasets and transferring their learning to new tasks with minor changes in architecture and parameter values. This type of transfer learning has become the standard modeling practice in both natural language processing and computer vision. Vision language transformers offer the promise of producing similar advancements in tasks which require both vision and language. In this paper, we provide a broad synthesis of the currently available research on vision language transformer models and offer some analysis of their strengths, limitations and some open questions that remain.",
    "authors": [
      "Clayton Fields",
      "Casey Kennington"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-07-06T19:08:56Z",
    "pdf_url": "https://arxiv.org/pdf/2307.03254v1"
  },
  {
    "arxiv_id": "2307.03109v9",
    "entry_id": "http://arxiv.org/abs/2307.03109v9",
    "title": "A Survey on Evaluation of Large Language Models",
    "summary": "Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions: what to evaluate, where to evaluate, and how to evaluate. Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, educations, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at: https://github.com/MLGroupJLU/LLM-eval-survey.",
    "authors": [
      "Yupeng Chang",
      "Xu Wang",
      "Jindong Wang",
      "Yuan Wu",
      "Linyi Yang",
      "Kaijie Zhu",
      "Hao Chen",
      "Xiaoyuan Yi",
      "Cunxiang Wang",
      "Yidong Wang",
      "Wei Ye",
      "Yue Zhang",
      "Yi Chang",
      "Philip S. Yu",
      "Qiang Yang",
      "Xing Xie"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-07-06T16:28:35Z",
    "pdf_url": "https://arxiv.org/pdf/2307.03109v9"
  },
  {
    "arxiv_id": "2307.02637v2",
    "entry_id": "http://arxiv.org/abs/2307.02637v2",
    "title": "Surge Routing: Event-informed Multiagent Reinforcement Learning for Autonomous Rideshare",
    "summary": "Large events such as conferences, concerts and sports games, often cause surges in demand for ride services that are not captured in average demand patterns, posing unique challenges for routing algorithms. We propose a learning framework for an autonomous fleet of taxis that leverages event data from the internet to predict demand surges and generate cooperative routing policies. We achieve this through a combination of two major components: (i) a demand prediction framework that uses textual event information in the form of events' descriptions and reviews to predict event-driven demand surges over street intersections, and (ii) a scalable multiagent reinforcement learning framework that leverages demand predictions and uses one-agent-at-a-time rollout combined with limited sampling certainty equivalence to learn intersection-level routing policies. For our experimental results we consider real NYC ride share data for the year 2022 and information for more than 2000 events across 300 unique venues in Manhattan. We test our approach with a fleet of 100 taxis on a map with 2235 street intersections. Our experimental results demonstrate that our method learns routing policies that reduce wait time overhead per serviced request by 25% to 75%, while picking up 1% to 4% more requests than other model-based RL frameworks and classical methods in operations research.",
    "authors": [
      "Daniel Garces",
      "Stephanie Gil"
    ],
    "categories": [
      "cs.AI",
      "cs.MA",
      "cs.RO"
    ],
    "published": "2023-07-05T20:13:01Z",
    "pdf_url": "https://arxiv.org/pdf/2307.02637v2"
  },
  {
    "arxiv_id": "2307.02620v3",
    "entry_id": "http://arxiv.org/abs/2307.02620v3",
    "title": "Dynamic Observation Policies in Observation Cost-Sensitive Reinforcement Learning",
    "summary": "Reinforcement learning (RL) has been shown to learn sophisticated control policies for complex tasks including games, robotics, heating and cooling systems and text generation. The action-perception cycle in RL, however, generally assumes that a measurement of the state of the environment is available at each time step without a cost. In applications such as materials design, deep-sea and planetary robot exploration and medicine, however, there can be a high cost associated with measuring, or even approximating, the state of the environment. In this paper, we survey the recently growing literature that adopts the perspective that an RL agent might not need, or even want, a costly measurement at each time step. Within this context, we propose the Deep Dynamic Multi-Step Observationless Agent (DMSOA), contrast it with the literature and empirically evaluate it on OpenAI gym and Atari Pong environments. Our results, show that DMSOA learns a better policy with fewer decision steps and measurements than the considered alternative from the literature.",
    "authors": [
      "Colin Bellinger",
      "Mark Crowley",
      "Isaac Tamblyn"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-07-05T19:48:03Z",
    "pdf_url": "https://arxiv.org/pdf/2307.02620v3"
  },
  {
    "arxiv_id": "2307.10188v1",
    "entry_id": "http://arxiv.org/abs/2307.10188v1",
    "title": "Several categories of Large Language Models (LLMs): A Short Survey",
    "summary": "Large Language Models(LLMs)have become effective tools for natural language processing and have been used in many different fields. This essay offers a succinct summary of various LLM subcategories. The survey emphasizes recent developments and efforts made for various LLM kinds, including task-based financial LLMs, multilingual language LLMs, biomedical and clinical LLMs, vision language LLMs, and code language models. The survey gives a general summary of the methods, attributes, datasets, transformer models, and comparison metrics applied in each category of LLMs. Furthermore, it highlights unresolved problems in the field of developing chatbots and virtual assistants, such as boosting natural language processing, enhancing chatbot intelligence, and resolving moral and legal dilemmas. The purpose of this study is to provide readers, developers, academics, and users interested in LLM-based chatbots and virtual intelligent assistant technologies with useful information and future directions.",
    "authors": [
      "Saurabh Pahune",
      "Manoj Chandrasekharan"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-07-05T18:18:23Z",
    "pdf_url": "https://arxiv.org/pdf/2307.10188v1"
  },
  {
    "arxiv_id": "2307.02443v1",
    "entry_id": "http://arxiv.org/abs/2307.02443v1",
    "title": "An Exploratory Literature Study on Sharing and Energy Use of Language Models for Source Code",
    "summary": "Large language models trained on source code can support a variety of software development tasks, such as code recommendation and program repair. Large amounts of data for training such models benefit the models' performance. However, the size of the data and models results in long training times and high energy consumption. While publishing source code allows for replicability, users need to repeat the expensive training process if models are not shared. The main goal of the study is to investigate if publications that trained language models for software engineering (SE) tasks share source code and trained artifacts. The second goal is to analyze the transparency on training energy usage. We perform a snowballing-based literature search to find publications on language models for source code, and analyze their reusability from a sustainability standpoint.\n  From 494 unique publications, we identified 293 relevant publications that use language models to address code-related tasks. Among them, 27% (79 out of 293) make artifacts available for reuse. This can be in the form of tools or IDE plugins designed for specific tasks or task-agnostic models that can be fine-tuned for a variety of downstream tasks. Moreover, we collect insights on the hardware used for model training, as well as training time, which together determine the energy consumption of the development process. We find that there are deficiencies in the sharing of information and artifacts for current studies on source code models for software engineering tasks, with 40% of the surveyed papers not sharing source code or trained artifacts. We recommend the sharing of source code as well as trained artifacts, to enable sustainable reproducibility. Moreover, comprehensive information on training times and hardware configurations should be shared for transparency on a model's carbon footprint.",
    "authors": [
      "Max Hort",
      "Anastasiia Grishina",
      "Leon Moonen"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.NE"
    ],
    "published": "2023-07-05T17:13:00Z",
    "pdf_url": "https://arxiv.org/pdf/2307.02443v1"
  },
  {
    "arxiv_id": "2307.02046v6",
    "entry_id": "http://arxiv.org/abs/2307.02046v6",
    "title": "Recommender Systems in the Era of Large Language Models (LLMs)",
    "summary": "With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, recent studies have attempted to harness the power of LLMs to enhance recommender systems. Given the rapid evolution of this research direction in recommender systems, there is a pressing need for a systematic overview that summarizes existing LLM-empowered recommender systems, to provide researchers in relevant fields with an in-depth understanding. Therefore, in this paper, we conduct a comprehensive review of LLM-empowered recommender systems from various aspects including Pre-training, Fine-tuning, and Prompting. More specifically, we first introduce representative methods to harness the power of LLMs (as a feature encoder) for learning representations of users and items. Then, we review recent techniques of LLMs for enhancing recommender systems from three paradigms, namely pre-training, fine-tuning, and prompting. Finally, we comprehensively discuss future directions in this emerging field.",
    "authors": [
      "Zihuai Zhao",
      "Wenqi Fan",
      "Jiatong Li",
      "Yunqing Liu",
      "Xiaowei Mei",
      "Yiqi Wang",
      "Zhen Wen",
      "Fei Wang",
      "Xiangyu Zhao",
      "Jiliang Tang",
      "Qing Li"
    ],
    "categories": [
      "cs.IR",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-07-05T06:03:40Z",
    "pdf_url": "https://arxiv.org/pdf/2307.02046v6"
  },
  {
    "arxiv_id": "2307.02503v1",
    "entry_id": "http://arxiv.org/abs/2307.02503v1",
    "title": "Natural Language Generation and Understanding of Big Code for AI-Assisted Programming: A Review",
    "summary": "This paper provides a comprehensive review of the literature concerning the utilization of Natural Language Processing (NLP) techniques, with a particular focus on transformer-based large language models (LLMs) trained using Big Code, within the domain of AI-assisted programming tasks. LLMs, augmented with software naturalness, have played a crucial role in facilitating AI-assisted programming applications, including code generation, code completion, code translation, code refinement, code summarization, defect detection, and clone detection. Notable examples of such applications include the GitHub Copilot powered by OpenAI's Codex and DeepMind AlphaCode. This paper presents an overview of the major LLMs and their applications in downstream tasks related to AI-assisted programming. Furthermore, it explores the challenges and opportunities associated with incorporating NLP techniques with software naturalness in these applications, with a discussion on extending AI-assisted programming capabilities to Apple's Xcode for mobile software development. This paper also presents the challenges of and opportunities for incorporating NLP techniques with software naturalness, empowering developers with advanced coding assistance and streamlining the software development process.",
    "authors": [
      "Man Fai Wong",
      "Shangxin Guo",
      "Ching Nam Hang",
      "Siu Wai Ho",
      "Chee Wei Tan"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-07-04T21:26:51Z",
    "pdf_url": "https://arxiv.org/pdf/2307.02503v1"
  },
  {
    "arxiv_id": "2307.01548v1",
    "entry_id": "http://arxiv.org/abs/2307.01548v1",
    "title": "Knowledge Graph for NLG in the context of conversational agents",
    "summary": "The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for the Knowledge Graph-to-Text Generation task. We aim to refine benchmark datasets of kg-to-text generation on PLMs and to explore the emotional and multilingual dimensions in our future work. Overall, this review provides insights into the different approaches for knowledge graph-to-text generation and outlines future directions for research in this area.",
    "authors": [
      "Hussam Ghanem",
      "Massinissa Atmani",
      "Christophe Cruz"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-07-04T08:03:33Z",
    "pdf_url": "https://arxiv.org/pdf/2307.01548v1"
  },
  {
    "arxiv_id": "2307.01452v2",
    "entry_id": "http://arxiv.org/abs/2307.01452v2",
    "title": "Causal Reinforcement Learning: A Survey",
    "summary": "Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcement learning. We first introduce the basic concepts of causality and reinforcement learning, and then explain how causality can address core challenges in non-causal reinforcement learning. We categorize and systematically review existing causal reinforcement learning approaches based on their target problems and methodologies. Finally, we outline open issues and future directions in this emerging field.",
    "authors": [
      "Zhihong Deng",
      "Jing Jiang",
      "Guodong Long",
      "Chengqi Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-07-04T03:00:43Z",
    "pdf_url": "https://arxiv.org/pdf/2307.01452v2"
  },
  {
    "arxiv_id": "2307.04761v1",
    "entry_id": "http://arxiv.org/abs/2307.04761v1",
    "title": "Understanding Counterspeech for Online Harm Mitigation",
    "summary": "Counterspeech offers direct rebuttals to hateful speech by challenging perpetrators of hate and showing support to targets of abuse. It provides a promising alternative to more contentious measures, such as content moderation and deplatforming, by contributing a greater amount of positive online speech rather than attempting to mitigate harmful content through removal. Advances in the development of large language models mean that the process of producing counterspeech could be made more efficient by automating its generation, which would enable large-scale online campaigns. However, we currently lack a systematic understanding of several important factors relating to the efficacy of counterspeech for hate mitigation, such as which types of counterspeech are most effective, what are the optimal conditions for implementation, and which specific effects of hate it can best ameliorate. This paper aims to fill this gap by systematically reviewing counterspeech research in the social sciences and comparing methodologies and findings with computer science efforts in automatic counterspeech generation. By taking this multi-disciplinary view, we identify promising future directions in both fields.",
    "authors": [
      "Yi-Ling Chung",
      "Gavin Abercrombie",
      "Florence Enock",
      "Jonathan Bright",
      "Verena Rieser"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2023-07-01T20:54:01Z",
    "pdf_url": "https://arxiv.org/pdf/2307.04761v1"
  },
  {
    "arxiv_id": "2307.01214v1",
    "entry_id": "http://arxiv.org/abs/2307.01214v1",
    "title": "Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search",
    "summary": "Despite large-scale pre-trained language models have achieved striking results for text classificaion, recent work has raised concerns about the challenge of shortcut learning. In general, a keyword is regarded as a shortcut if it creates a superficial association with the label, resulting in a false prediction. Conversely, shortcut learning can be mitigated if the model relies on robust causal features that help produce sound predictions. To this end, many studies have explored post-hoc interpretable methods to mine shortcuts and causal features for robustness and generalization. However, most existing methods focus only on single word in a sentence and lack consideration of word-group, leading to wrong causal features. To solve this problem, we propose a new Word-Group mining approach, which captures the causal effect of any keyword combination and orders the combinations that most affect the prediction. Our approach bases on effective post-hoc analysis and beam search, which ensures the mining effect and reduces the complexity. Then, we build a counterfactual augmentation method based on the multiple word-groups, and use an adaptive voting mechanism to learn the influence of different augmentated samples on the prediction results, so as to force the model to pay attention to effective causal features. We demonstrate the effectiveness of the proposed method by several tasks on 8 affective review datasets and 4 toxic language datasets, including cross-domain text classificaion, text attack and gender fairness test.",
    "authors": [
      "Rui Song",
      "Fausto Giunchiglia",
      "Yingji Li",
      "Hao Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-07-01T02:26:34Z",
    "pdf_url": "https://arxiv.org/pdf/2307.01214v1"
  },
  {
    "arxiv_id": "2306.16388v2",
    "entry_id": "http://arxiv.org/abs/2306.16388v2",
    "title": "Towards Measuring the Representation of Subjective Global Opinions in Language Models",
    "summary": "Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the opinions of the prompted populations, but can reflect harmful cultural stereotypes. When we translate GlobalOpinionQA questions to a target language, the model's responses do not necessarily become the most similar to the opinions of speakers of those languages. We release our dataset for others to use and build on. Our data is at https://huggingface.co/datasets/Anthropic/llm_global_opinions. We also provide an interactive visualization at https://llmglobalvalues.anthropic.com.",
    "authors": [
      "Esin Durmus",
      "Karina Nguyen",
      "Thomas I. Liao",
      "Nicholas Schiefer",
      "Amanda Askell",
      "Anton Bakhtin",
      "Carol Chen",
      "Zac Hatfield-Dodds",
      "Danny Hernandez",
      "Nicholas Joseph",
      "Liane Lovitt",
      "Sam McCandlish",
      "Orowa Sikder",
      "Alex Tamkin",
      "Janel Thamkul",
      "Jared Kaplan",
      "Jack Clark",
      "Deep Ganguli"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-06-28T17:31:53Z",
    "pdf_url": "https://arxiv.org/pdf/2306.16388v2"
  },
  {
    "arxiv_id": "2306.16275v1",
    "entry_id": "http://arxiv.org/abs/2306.16275v1",
    "title": "Leveraging GPT-4 for Food Effect Summarization to Enhance Product-Specific Guidance Development via Iterative Prompting",
    "summary": "Food effect summarization from New Drug Application (NDA) is an essential component of product-specific guidance (PSG) development and assessment. However, manual summarization of food effect from extensive drug application review documents is time-consuming, which arouses a need to develop automated methods. Recent advances in large language models (LLMs) such as ChatGPT and GPT-4, have demonstrated great potential in improving the effectiveness of automated text summarization, but its ability regarding the accuracy in summarizing food effect for PSG assessment remains unclear. In this study, we introduce a simple yet effective approach, iterative prompting, which allows one to interact with ChatGPT or GPT-4 more effectively and efficiently through multi-turn interaction. Specifically, we propose a three-turn iterative prompting approach to food effect summarization in which the keyword-focused and length-controlled prompts are respectively provided in consecutive turns to refine the quality of the generated summary. We conduct a series of extensive evaluations, ranging from automated metrics to FDA professionals and even evaluation by GPT-4, on 100 NDA review documents selected over the past five years. We observe that the summary quality is progressively improved throughout the process. Moreover, we find that GPT-4 performs better than ChatGPT, as evaluated by FDA professionals (43% vs. 12%) and GPT-4 (64% vs. 35%). Importantly, all the FDA professionals unanimously rated that 85% of the summaries generated by GPT-4 are factually consistent with the golden reference summary, a finding further supported by GPT-4 rating of 72% consistency. These results strongly suggest a great potential for GPT-4 to draft food effect summaries that could be reviewed by FDA professionals, thereby improving the efficiency of PSG assessment cycle and promoting the generic drug product development.",
    "authors": [
      "Yiwen Shi",
      "Ping Ren",
      "Jing Wang",
      "Biao Han",
      "Taha ValizadehAslani",
      "Felix Agbavor",
      "Yi Zhang",
      "Meng Hu",
      "Liang Zhao",
      "Hualou Liang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-06-28T14:55:13Z",
    "pdf_url": "https://arxiv.org/pdf/2306.16275v1"
  },
  {
    "arxiv_id": "2306.16244v1",
    "entry_id": "http://arxiv.org/abs/2306.16244v1",
    "title": "CBBQ: A Chinese Bias Benchmark Dataset Curated with Human-AI Collaboration for Large Language Models",
    "summary": "Holistically measuring societal biases of large language models is crucial for detecting and reducing ethical risks in highly capable AI models. In this work, we present a Chinese Bias Benchmark dataset that consists of over 100K questions jointly constructed by human experts and generative language models, covering stereotypes and societal biases in 14 social dimensions related to Chinese culture and values. The curation process contains 4 essential steps: bias identification via extensive literature review, ambiguous context generation, AI-assisted disambiguous context generation, snd manual review \\& recomposition. The testing instances in the dataset are automatically derived from 3K+ high-quality templates manually authored with stringent quality control. The dataset exhibits wide coverage and high diversity. Extensive experiments demonstrate the effectiveness of the dataset in detecting model bias, with all 10 publicly available Chinese large language models exhibiting strong bias in certain categories. Additionally, we observe from our experiments that fine-tuned models could, to a certain extent, heed instructions and avoid generating outputs that are morally harmful in some types, in the way of \"moral self-correction\". Our dataset and results are publicly available at \\href{https://github.com/YFHuangxxxx/CBBQ}{https://github.com/YFHuangxxxx/CBBQ}, offering debiasing research opportunities to a widened community.",
    "authors": [
      "Yufei Huang",
      "Deyi Xiong"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-06-28T14:14:44Z",
    "pdf_url": "https://arxiv.org/pdf/2306.16244v1"
  },
  {
    "arxiv_id": "2306.15887v1",
    "entry_id": "http://arxiv.org/abs/2306.15887v1",
    "title": "Beyond the Hype: Assessing the Performance, Trustworthiness, and Clinical Suitability of GPT3.5",
    "summary": "The use of large language models (LLMs) in healthcare is gaining popularity, but their practicality and safety in clinical settings have not been thoroughly assessed. In high-stakes environments like medical settings, trust and safety are critical issues for LLMs. To address these concerns, we present an approach to evaluate the performance and trustworthiness of a GPT3.5 model for medical image protocol assignment. We compare it with a fine-tuned BERT model and a radiologist. In addition, we have a radiologist review the GPT3.5 output to evaluate its decision-making process. Our evaluation dataset consists of 4,700 physician entries across 11 imaging protocol classes spanning the entire head. Our findings suggest that the GPT3.5 performance falls behind BERT and a radiologist. However, GPT3.5 outperforms BERT in its ability to explain its decision, detect relevant word indicators, and model calibration. Furthermore, by analyzing the explanations of GPT3.5 for misclassifications, we reveal systematic errors that need to be resolved to enhance its safety and suitability for clinical use.",
    "authors": [
      "Salmonn Talebi",
      "Elizabeth Tong",
      "Mohammad R. K. Mofrad"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-06-28T03:03:51Z",
    "pdf_url": "https://arxiv.org/pdf/2306.15887v1"
  },
  {
    "arxiv_id": "2306.13945v1",
    "entry_id": "http://arxiv.org/abs/2306.13945v1",
    "title": "Large Sequence Models for Sequential Decision-Making: A Survey",
    "summary": "Transformer architectures have facilitated the development of large-scale and general-purpose sequence models for prediction tasks in natural language processing and computer vision, e.g., GPT-3 and Swin Transformer. Although originally designed for prediction problems, it is natural to inquire about their suitability for sequential decision-making and reinforcement learning problems, which are typically beset by long-standing issues involving sample efficiency, credit assignment, and partial observability. In recent years, sequence models, especially the Transformer, have attracted increasing interest in the RL communities, spawning numerous approaches with notable effectiveness and generalizability. This survey presents a comprehensive overview of recent works aimed at solving sequential decision-making tasks with sequence models such as the Transformer, by discussing the connection between sequential decision-making and sequence modeling, and categorizing them based on the way they utilize the Transformer. Moreover, this paper puts forth various potential avenues for future research intending to improve the effectiveness of large sequence models for sequential decision-making, encompassing theoretical foundations, network architectures, algorithms, and efficient training systems. As this article has been accepted by the Frontiers of Computer Science, here is an early version, and the most up-to-date version can be found at https://journal.hep.com.cn/fcs/EN/10.1007/s11704-023-2689-5",
    "authors": [
      "Muning Wen",
      "Runji Lin",
      "Hanjing Wang",
      "Yaodong Yang",
      "Ying Wen",
      "Luo Mai",
      "Jun Wang",
      "Haifeng Zhang",
      "Weinan Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.MA"
    ],
    "published": "2023-06-24T12:06:26Z",
    "pdf_url": "https://arxiv.org/pdf/2306.13945v1"
  },
  {
    "arxiv_id": "2306.13805v2",
    "entry_id": "http://arxiv.org/abs/2306.13805v2",
    "title": "Potential Benefits of Employing Large Language Models in Research in Moral Education and Development",
    "summary": "Recently, computer scientists have developed large language models (LLMs) by training prediction models with large-scale language corpora and human reinforcements. The LLMs have become one promising way to implement artificial intelligence with accuracy in various fields. Interestingly, recent LLMs possess emergent functional features that emulate sophisticated human cognition, especially in-context learning and the chain of thought, which were unavailable in previous prediction models. In this paper, I will examine how LLMs might contribute to moral education and development research. To achieve this goal, I will review the most recently published conference papers and ArXiv preprints to overview the novel functional features implemented in LLMs. I also intend to conduct brief experiments with ChatGPT to investigate how LLMs behave while addressing ethical dilemmas and external feedback. The results suggest that LLMs might be capable of solving dilemmas based on reasoning and revising their reasoning process with external input. Furthermore, a preliminary experimental result from the moral exemplar test may demonstrate that exemplary stories can elicit moral elevation in LLMs as do they among human participants. I will discuss the potential implications of LLMs on research on moral education and development with the results.",
    "authors": [
      "Hyemin Han"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-06-23T22:39:05Z",
    "pdf_url": "https://arxiv.org/pdf/2306.13805v2"
  },
  {
    "arxiv_id": "2306.13549v4",
    "entry_id": "http://arxiv.org/abs/2306.13549v4",
    "title": "A Survey on Multimodal Large Language Models",
    "summary": "Recently, Multimodal Large Language Model (MLLM) represented by GPT-4V has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional multimodal methods, suggesting a potential path to artificial general intelligence. To this end, both academia and industry have endeavored to develop MLLMs that can compete with or even better than GPT-4V, pushing the limit of research at a surprising speed. In this paper, we aim to trace and summarize the recent progress of MLLMs. First of all, we present the basic formulation of MLLM and delineate its related concepts, including architecture, training strategy and data, as well as evaluation. Then, we introduce research topics about how MLLMs can be extended to support more granularity, modalities, languages, and scenarios. We continue with multimodal hallucination and extended techniques, including Multimodal ICL (M-ICL), Multimodal CoT (M-CoT), and LLM-Aided Visual Reasoning (LAVR). To conclude the paper, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated GitHub link collecting the latest papers is available at https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.",
    "authors": [
      "Shukang Yin",
      "Chaoyou Fu",
      "Sirui Zhao",
      "Ke Li",
      "Xing Sun",
      "Tong Xu",
      "Enhong Chen"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-06-23T15:21:52Z",
    "pdf_url": "https://arxiv.org/pdf/2306.13549v4"
  },
  {
    "arxiv_id": "2306.13315v1",
    "entry_id": "http://arxiv.org/abs/2306.13315v1",
    "title": "Abstractive Text Summarization for Resumes With Cutting Edge NLP Transformers and LSTM",
    "summary": "Text summarization is a fundamental task in natural language processing that aims to condense large amounts of textual information into concise and coherent summaries. With the exponential growth of content and the need to extract key information efficiently, text summarization has gained significant attention in recent years. In this study, LSTM and pre-trained T5, Pegasus, BART and BART-Large model performances were evaluated on the open source dataset (Xsum, CNN/Daily Mail, Amazon Fine Food Review and News Summary) and the prepared resume dataset. This resume dataset consists of many information such as language, education, experience, personal information, skills, and this data includes 75 resumes. The primary objective of this research was to classify resume text. Various techniques such as LSTM, pre-trained models, and fine-tuned models were assessed using a dataset of resumes. The BART-Large model fine-tuned with the resume dataset gave the best performance.",
    "authors": [
      "Öykü Berfin Mercan",
      "Sena Nur Cavsak",
      "Aysu Deliahmetoglu",
      "Senem Tanberk"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-06-23T06:33:20Z",
    "pdf_url": "https://arxiv.org/pdf/2306.13315v1"
  },
  {
    "arxiv_id": "2306.13298v1",
    "entry_id": "http://arxiv.org/abs/2306.13298v1",
    "title": "Exploring Qualitative Research Using LLMs",
    "summary": "The advent of AI driven large language models (LLMs) have stirred discussions about their role in qualitative research. Some view these as tools to enrich human understanding, while others perceive them as threats to the core values of the discipline. This study aimed to compare and contrast the comprehension capabilities of humans and LLMs. We conducted an experiment with small sample of Alexa app reviews, initially classified by a human analyst. LLMs were then asked to classify these reviews and provide the reasoning behind each classification. We compared the results with human classification and reasoning. The research indicated a significant alignment between human and ChatGPT 3.5 classifications in one third of cases, and a slightly lower alignment with GPT4 in over a quarter of cases. The two AI models showed a higher alignment, observed in more than half of the instances. However, a consensus across all three methods was seen only in about one fifth of the classifications. In the comparison of human and LLMs reasoning, it appears that human analysts lean heavily on their individual experiences. As expected, LLMs, on the other hand, base their reasoning on the specific word choices found in app reviews and the functional components of the app itself. Our results highlight the potential for effective human LLM collaboration, suggesting a synergistic rather than competitive relationship. Researchers must continuously evaluate LLMs role in their work, thereby fostering a future where AI and humans jointly enrich qualitative research.",
    "authors": [
      "Muneera Bano",
      "Didar Zowghi",
      "Jon Whittle"
    ],
    "categories": [
      "cs.SE",
      "cs.AI"
    ],
    "published": "2023-06-23T05:21:36Z",
    "pdf_url": "https://arxiv.org/pdf/2306.13298v1"
  },
  {
    "arxiv_id": "2306.12306v3",
    "entry_id": "http://arxiv.org/abs/2306.12306v3",
    "title": "Beyond Deep Ensembles: A Large-Scale Evaluation of Bayesian Deep Learning under Distribution Shift",
    "summary": "Bayesian deep learning (BDL) is a promising approach to achieve well-calibrated predictions on distribution-shifted data. Nevertheless, there exists no large-scale survey that evaluates recent SOTA methods on diverse, realistic, and challenging benchmark tasks in a systematic manner. To provide a clear picture of the current state of BDL research, we evaluate modern BDL algorithms on real-world datasets from the WILDS collection containing challenging classification and regression tasks, with a focus on generalization capability and calibration under distribution shift. We compare the algorithms on a wide range of large, convolutional and transformer-based neural network architectures. In particular, we investigate a signed version of the expected calibration error that reveals whether the methods are over- or under-confident, providing further insight into the behavior of the methods. Further, we provide the first systematic evaluation of BDL for fine-tuning large pre-trained models, where training from scratch is prohibitively expensive. Finally, given the recent success of Deep Ensembles, we extend popular single-mode posterior approximations to multiple modes by the use of ensembles. While we find that ensembling single-mode approximations generally improves the generalization capability and calibration of the models by a significant margin, we also identify a failure mode of ensembles when finetuning large transformer-based language models. In this setting, variational inference based approaches such as last-layer Bayes By Backprop outperform other methods in terms of accuracy by a large margin, while modern approximate inference algorithms such as SWAG achieve the best calibration.",
    "authors": [
      "Florian Seligmann",
      "Philipp Becker",
      "Michael Volpp",
      "Gerhard Neumann"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-06-21T14:36:03Z",
    "pdf_url": "https://arxiv.org/pdf/2306.12306v3"
  },
  {
    "arxiv_id": "2306.11489v2",
    "entry_id": "http://arxiv.org/abs/2306.11489v2",
    "title": "Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling",
    "summary": "Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities. Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases. However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-trained language models (KGPLMs) as well as their applications. Inspired by existing studies on KGPLM, this paper proposes to enhance LLMs with KGs by developing knowledge graph-enhanced large language models (KGLLMs). KGLLM provides a solution to enhance LLMs' factual reasoning ability, opening up new avenues for LLM research.",
    "authors": [
      "Linyao Yang",
      "Hongyang Chen",
      "Zhao Li",
      "Xiao Ding",
      "Xindong Wu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-06-20T12:21:06Z",
    "pdf_url": "https://arxiv.org/pdf/2306.11489v2"
  },
  {
    "arxiv_id": "2306.10070v2",
    "entry_id": "http://arxiv.org/abs/2306.10070v2",
    "title": "Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health",
    "summary": "ChatGPT has drawn considerable attention from both the general public and domain experts with its remarkable text generation capabilities. This has subsequently led to the emergence of diverse applications in the field of biomedicine and health. In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically we explore the areas of biomedical information retrieval, question answering, medical text summarization, information extraction, and medical education, and investigate whether LLMs possess the transformative power to revolutionize these tasks or whether the distinct complexities of biomedical domain presents unique challenges. Following an extensive literature survey, we find that significant advances have been made in the field of text generation tasks, surpassing the previous state-of-the-art methods. For other applications, the advances have been modest. Overall, LLMs have not yet revolutionized biomedicine, but recent rapid progress indicates that such methods hold great potential to provide valuable means for accelerating discovery and improving health. We also find that the use of LLMs, like ChatGPT, in the fields of biomedicine and health entails various risks and challenges, including fabricated information in its generated responses, as well as legal and privacy concerns associated with sensitive patient data. We believe this survey can provide a comprehensive and timely overview to biomedical researchers and healthcare practitioners on the opportunities and challenges associated with using ChatGPT and other LLMs for transforming biomedicine and health.",
    "authors": [
      "Shubo Tian",
      "Qiao Jin",
      "Lana Yeganova",
      "Po-Ting Lai",
      "Qingqing Zhu",
      "Xiuying Chen",
      "Yifan Yang",
      "Qingyu Chen",
      "Won Kim",
      "Donald C. Comeau",
      "Rezarta Islamaj",
      "Aadit Kapoor",
      "Xin Gao",
      "Zhiyong Lu"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "q-bio.QM"
    ],
    "published": "2023-06-15T20:19:08Z",
    "pdf_url": "https://arxiv.org/pdf/2306.10070v2"
  },
  {
    "arxiv_id": "2306.09445v2",
    "entry_id": "http://arxiv.org/abs/2306.09445v2",
    "title": "Understanding the Application of Utility Theory in Robotics and Artificial Intelligence: A Survey",
    "summary": "As a unifying concept in economics, game theory, and operations research, even in the Robotics and AI field, the utility is used to evaluate the level of individual needs, preferences, and interests. Especially for decision-making and learning in multi-agent/robot systems (MAS/MRS), a suitable utility model can guide agents in choosing reasonable strategies to achieve their current needs and learning to cooperate and organize their behaviors, optimizing the system's utility, building stable and reliable relationships, and guaranteeing each group member's sustainable development, similar to the human society. Although these systems' complex, large-scale, and long-term behaviors are strongly determined by the fundamental characteristics of the underlying relationships, there has been less discussion on the theoretical aspects of mechanisms and the fields of applications in Robotics and AI. This paper introduces a utility-orient needs paradigm to describe and evaluate inter and outer relationships among agents' interactions. Then, we survey existing literature in relevant fields to support it and propose several promising research directions along with some open problems deemed necessary for further investigations.",
    "authors": [
      "Qin Yang",
      "Rui Liu"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.MA",
      "cs.NE",
      "eess.SY"
    ],
    "published": "2023-06-15T18:55:48Z",
    "pdf_url": "https://arxiv.org/pdf/2306.09445v2"
  },
  {
    "arxiv_id": "2306.14905v1",
    "entry_id": "http://arxiv.org/abs/2306.14905v1",
    "title": "PRISMA-DFLLM: An Extension of PRISMA for Systematic Literature Reviews using Domain-specific Finetuned Large Language Models",
    "summary": "With the proliferation of open-sourced Large Language Models (LLMs) and efficient finetuning techniques, we are on the cusp of the emergence of numerous domain-specific LLMs that have been finetuned for expertise across specialized fields and applications for which the current general-purpose LLMs are unsuitable. In academia, this technology has the potential to revolutionize the way we conduct systematic literature reviews (SLRs), access knowledge and generate new insights. This paper proposes an AI-enabled methodological framework that combines the power of LLMs with the rigorous reporting guidelines of the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA). By finetuning LLMs on domain-specific academic papers that have been selected as a result of a rigorous SLR process, the proposed PRISMA-DFLLM (for Domain-specific Finetuned LLMs) reporting guidelines offer the potential to achieve greater efficiency, reusability and scalability, while also opening the potential for conducting incremental living systematic reviews with the aid of LLMs. Additionally, the proposed approach for leveraging LLMs for SLRs enables the dissemination of finetuned models, empowering researchers to accelerate advancements and democratize cutting-edge research. This paper presents the case for the feasibility of finetuned LLMs to support rigorous SLRs and the technical requirements for realizing this. This work then proposes the extended PRISMA-DFLLM checklist of reporting guidelines as well as the advantages, challenges, and potential implications of implementing PRISMA-DFLLM. Finally, a future research roadmap to develop this line of AI-enabled SLRs is presented, paving the way for a new era of evidence synthesis and knowledge discovery.",
    "authors": [
      "Teo Susnjak"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-06-15T02:52:50Z",
    "pdf_url": "https://arxiv.org/pdf/2306.14905v1"
  },
  {
    "arxiv_id": "2306.08641v1",
    "entry_id": "http://arxiv.org/abs/2306.08641v1",
    "title": "Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models",
    "summary": "The AI community has been pursuing algorithms known as artificial general intelligence (AGI) that apply to any kind of real-world problem. Recently, chat systems powered by large language models (LLMs) emerge and rapidly become a promising direction to achieve AGI in natural language processing (NLP), but the path towards AGI in computer vision (CV) remains unclear. One may owe the dilemma to the fact that visual signals are more complex than language signals, yet we are interested in finding concrete reasons, as well as absorbing experiences from GPT and LLMs to solve the problem. In this paper, we start with a conceptual definition of AGI and briefly review how NLP solves a wide range of tasks via a chat system. The analysis inspires us that unification is the next important goal of CV. But, despite various efforts in this direction, CV is still far from a system like GPT that naturally integrates all tasks. We point out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world. We then imagine a pipeline that puts a CV algorithm (i.e., an agent) in world-scale, interactable environments, pre-trains it to predict future frames with respect to its action, and then fine-tunes it with instruction to accomplish various tasks. We expect substantial research and engineering efforts to push the idea forward and scale it up, for which we share our perspectives on future research directions.",
    "authors": [
      "Lingxi Xie",
      "Longhui Wei",
      "Xiaopeng Zhang",
      "Kaifeng Bi",
      "Xiaotao Gu",
      "Jianlong Chang",
      "Qi Tian"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-06-14T17:15:01Z",
    "pdf_url": "https://arxiv.org/pdf/2306.08641v1"
  },
  {
    "arxiv_id": "2306.08302v3",
    "entry_id": "http://arxiv.org/abs/2306.08302v3",
    "title": "Unifying Large Language Models and Knowledge Graphs: A Roadmap",
    "summary": "Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorporate KGs during the pre-training and inference phases of LLMs, or for the purpose of enhancing understanding of the knowledge learned by LLMs; 2) LLM-augmented KGs, that leverage LLMs for different KG tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized LLMs + KGs, in which LLMs and KGs play equal roles and work in a mutually beneficial way to enhance both LLMs and KGs for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.",
    "authors": [
      "Shirui Pan",
      "Linhao Luo",
      "Yufei Wang",
      "Chen Chen",
      "Jiapu Wang",
      "Xindong Wu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-06-14T07:15:26Z",
    "pdf_url": "https://arxiv.org/pdf/2306.08302v3"
  },
  {
    "arxiv_id": "2306.08107v3",
    "entry_id": "http://arxiv.org/abs/2306.08107v3",
    "title": "AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks",
    "summary": "The fields of both Natural Language Processing (NLP) and Automated Machine Learning (AutoML) have achieved remarkable results over the past years. In NLP, especially Large Language Models (LLMs) have experienced a rapid series of breakthroughs very recently. We envision that the two fields can radically push the boundaries of each other through tight integration. To showcase this vision, we explore the potential of a symbiotic relationship between AutoML and LLMs, shedding light on how they can benefit each other. In particular, we investigate both the opportunities to enhance AutoML approaches with LLMs from different perspectives and the challenges of leveraging AutoML to further improve LLMs. To this end, we survey existing work, and we critically assess risks. We strongly believe that the integration of the two fields has the potential to disrupt both fields, NLP and AutoML. By highlighting conceivable synergies, but also risks, we aim to foster further exploration at the intersection of AutoML and LLMs.",
    "authors": [
      "Alexander Tornede",
      "Difan Deng",
      "Theresa Eimer",
      "Joseph Giovanelli",
      "Aditya Mohan",
      "Tim Ruhkopf",
      "Sarah Segel",
      "Daphne Theodorakopoulos",
      "Tanja Tornede",
      "Henning Wachsmuth",
      "Marius Lindauer"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2023-06-13T19:51:22Z",
    "pdf_url": "https://arxiv.org/pdf/2306.08107v3"
  },
  {
    "arxiv_id": "2306.05817v6",
    "entry_id": "http://arxiv.org/abs/2306.05817v6",
    "title": "How Can Recommender Systems Benefit from Large Language Models: A Survey",
    "summary": "With the rapid development of online services, recommender systems (RS) have become increasingly indispensable for mitigating information overload. Despite remarkable progress, conventional recommendation models (CRM) still have some limitations, e.g., lacking open-world knowledge, and difficulties in comprehending users' underlying preferences and motivations. Meanwhile, large language models (LLM) have shown impressive general intelligence and human-like capabilities, which mainly stem from their extensive open-world knowledge, reasoning ability, as well as their comprehension of human culture and society. Consequently, the emergence of LLM is inspiring the design of recommender systems and pointing out a promising research direction, i.e., whether we can incorporate LLM and benefit from their knowledge and capabilities to compensate for the limitations of CRM. In this paper, we conduct a comprehensive survey on this research direction from the perspective of the whole pipeline in real-world recommender systems. Specifically, we summarize existing works from two orthogonal aspects: where and how to adapt LLM to RS. For the WHERE question, we discuss the roles that LLM could play in different stages of the recommendation pipeline, i.e., feature engineering, feature encoder, scoring/ranking function, user interaction, and pipeline controller. For the HOW question, we investigate the training and inference strategies, resulting in two fine-grained taxonomy criteria, i.e., whether to tune LLM or not, and whether to involve conventional recommendation models for inference. Then, we highlight key challenges in adapting LLM to RS from three aspects, i.e., efficiency, effectiveness, and ethics. Finally, we summarize the survey and discuss the future prospects. We actively maintain a GitHub repository for papers and other related resources: https://github.com/CHIANGEL/Awesome-LLM-for-RecSys/.",
    "authors": [
      "Jianghao Lin",
      "Xinyi Dai",
      "Yunjia Xi",
      "Weiwen Liu",
      "Bo Chen",
      "Hao Zhang",
      "Yong Liu",
      "Chuhan Wu",
      "Xiangyang Li",
      "Chenxu Zhu",
      "Huifeng Guo",
      "Yong Yu",
      "Ruiming Tang",
      "Weinan Zhang"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2023-06-09T11:31:50Z",
    "pdf_url": "https://arxiv.org/pdf/2306.05817v6"
  },
  {
    "arxiv_id": "2306.05480v4",
    "entry_id": "http://arxiv.org/abs/2306.05480v4",
    "title": "Artificial General Intelligence for Medical Imaging Analysis",
    "summary": "Large-scale Artificial General Intelligence (AGI) models, including Large Language Models (LLMs) such as ChatGPT/GPT-4, have achieved unprecedented success in a variety of general domain tasks. Yet, when applied directly to specialized domains like medical imaging, which require in-depth expertise, these models face notable challenges arising from the medical field's inherent complexities and unique characteristics. In this review, we delve into the potential applications of AGI models in medical imaging and healthcare, with a primary focus on LLMs, Large Vision Models, and Large Multimodal Models. We provide a thorough overview of the key features and enabling techniques of LLMs and AGI, and further examine the roadmaps guiding the evolution and implementation of AGI models in the medical sector, summarizing their present applications, potentialities, and associated challenges. In addition, we highlight potential future research directions, offering a holistic view on upcoming ventures. This comprehensive review aims to offer insights into the future implications of AGI in medical imaging, healthcare, and beyond.",
    "authors": [
      "Xiang Li",
      "Lin Zhao",
      "Lu Zhang",
      "Zihao Wu",
      "Zhengliang Liu",
      "Hanqi Jiang",
      "Chao Cao",
      "Shaochen Xu",
      "Yiwei Li",
      "Haixing Dai",
      "Yixuan Yuan",
      "Jun Liu",
      "Gang Li",
      "Dajiang Zhu",
      "Pingkun Yan",
      "Quanzheng Li",
      "Wei Liu",
      "Tianming Liu",
      "Dinggang Shen"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-06-08T18:04:13Z",
    "pdf_url": "https://arxiv.org/pdf/2306.05480v4"
  },
  {
    "arxiv_id": "2306.04802v5",
    "entry_id": "http://arxiv.org/abs/2306.04802v5",
    "title": "A Review on Knowledge Graphs for Healthcare: Resources, Applications, and Promises",
    "summary": "This comprehensive review aims to provide an overview of the current state of Healthcare Knowledge Graphs (HKGs), including their construction, utilization models, and applications across various healthcare and biomedical research domains. We thoroughly analyzed existing literature on HKGs, covering their construction methodologies, utilization techniques, and applications in basic science research, pharmaceutical research and development, clinical decision support, and public health. The review encompasses both model-free and model-based utilization approaches and the integration of HKGs with large language models (LLMs). We searched Google Scholar for relevant papers on HKGs and classified them into the following topics: HKG construction, HKG utilization, and their downstream applications in various domains. We also discussed their special challenges and the promise for future work. The review highlights the potential of HKGs to significantly impact biomedical research and clinical practice by integrating vast amounts of biomedical knowledge from multiple domains. The synergy between HKGs and LLMs offers promising opportunities for constructing more comprehensive knowledge graphs and improving the accuracy of healthcare applications. HKGs have emerged as a powerful tool for structuring medical knowledge, with broad applications across biomedical research, clinical decision-making, and public health. This survey serves as a roadmap for future research and development in the field of HKGs, highlighting the potential of combining knowledge graphs with advanced machine learning models for healthcare transformation.",
    "authors": [
      "Hejie Cui",
      "Jiaying Lu",
      "Ran Xu",
      "Shiyu Wang",
      "Wenjing Ma",
      "Yue Yu",
      "Shaojun Yu",
      "Xuan Kan",
      "Chen Ling",
      "Liang Zhao",
      "Zhaohui S. Qin",
      "Joyce C. Ho",
      "Tianfan Fu",
      "Jing Ma",
      "Mengdi Huai",
      "Fei Wang",
      "Carl Yang"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG",
      "cs.SI"
    ],
    "published": "2023-06-07T21:51:56Z",
    "pdf_url": "https://arxiv.org/pdf/2306.04802v5"
  },
  {
    "arxiv_id": "2306.02051v3",
    "entry_id": "http://arxiv.org/abs/2306.02051v3",
    "title": "A Comprehensive Survey on Relation Extraction: Recent Advances and New Frontiers",
    "summary": "Relation extraction (RE) involves identifying the relations between entities from underlying content. RE serves as the foundation for many natural language processing (NLP) and information retrieval applications, such as knowledge graph completion and question answering. In recent years, deep neural networks have dominated the field of RE and made noticeable progress. Subsequently, the large pre-trained language models have taken the state-of-the-art RE to a new level. This survey provides a comprehensive review of existing deep learning techniques for RE. First, we introduce RE resources, including datasets and evaluation metrics. Second, we propose a new taxonomy to categorize existing works from three perspectives, i.e., text representation, context encoding, and triplet prediction. Third, we discuss several important challenges faced by RE and summarize potential techniques to tackle these challenges. Finally, we outline some promising future directions and prospects in this field. This survey is expected to facilitate researchers' collaborative efforts to address the challenges of real-world RE systems.",
    "authors": [
      "Xiaoyan Zhao",
      "Yang Deng",
      "Min Yang",
      "Lingzhi Wang",
      "Rui Zhang",
      "Hong Cheng",
      "Wai Lam",
      "Ying Shen",
      "Ruifeng Xu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-06-03T08:39:25Z",
    "pdf_url": "https://arxiv.org/pdf/2306.02051v3"
  },
  {
    "arxiv_id": "2306.01475v1",
    "entry_id": "http://arxiv.org/abs/2306.01475v1",
    "title": "Prompt Tuning Large Language Models on Personalized Aspect Extraction for Recommendations",
    "summary": "Existing aspect extraction methods mostly rely on explicit or ground truth aspect information, or using data mining or machine learning approaches to extract aspects from implicit user feedback such as user reviews. It however remains under-explored how the extracted aspects can help generate more meaningful recommendations to the users. Meanwhile, existing research on aspect-based recommendations often relies on separate aspect extraction models or assumes the aspects are given, without accounting for the fact the optimal set of aspects could be dependent on the recommendation task at hand.\n  In this work, we propose to combine aspect extraction together with aspect-based recommendations in an end-to-end manner, achieving the two goals together in a single framework. For the aspect extraction component, we leverage the recent advances in large language models and design a new prompt learning mechanism to generate aspects for the end recommendation task. For the aspect-based recommendation component, the extracted aspects are concatenated with the usual user and item features used by the recommendation model. The recommendation task mediates the learning of the user embeddings and item embeddings, which are used as soft prompts to generate aspects. Therefore, the extracted aspects are personalized and contextualized by the recommendation task. We showcase the effectiveness of our proposed method through extensive experiments on three industrial datasets, where our proposed framework significantly outperforms state-of-the-art baselines in both the personalized aspect extraction and aspect-based recommendation tasks. In particular, we demonstrate that it is necessary and beneficial to combine the learning of aspect extraction and aspect-based recommendation together. We also conduct extensive ablation studies to understand the contribution of each design component in our framework.",
    "authors": [
      "Pan Li",
      "Yuyan Wang",
      "Ed H. Chi",
      "Minmin Chen"
    ],
    "categories": [
      "cs.IR",
      "cs.LG"
    ],
    "published": "2023-06-02T12:00:03Z",
    "pdf_url": "https://arxiv.org/pdf/2306.01475v1"
  },
  {
    "arxiv_id": "2306.00622v1",
    "entry_id": "http://arxiv.org/abs/2306.00622v1",
    "title": "ReviewerGPT? An Exploratory Study on Using Large Language Models for Paper Reviewing",
    "summary": "Given the rapid ascent of large language models (LLMs), we study the question: (How) can large language models help in reviewing of scientific papers or proposals? We first conduct some pilot studies where we find that (i) GPT-4 outperforms other LLMs (Bard, Vicuna, Koala, Alpaca, LLaMa, Dolly, OpenAssistant, StableLM), and (ii) prompting with a specific question (e.g., to identify errors) outperforms prompting to simply write a review. With these insights, we study the use of LLMs (specifically, GPT-4) for three tasks:\n  1. Identifying errors: We construct 13 short computer science papers each with a deliberately inserted error, and ask the LLM to check for the correctness of these papers. We observe that the LLM finds errors in 7 of them, spanning both mathematical and conceptual errors.\n  2. Verifying checklists: We task the LLM to verify 16 closed-ended checklist questions in the respective sections of 15 NeurIPS 2022 papers. We find that across 119 {checklist question, paper} pairs, the LLM had an 86.6% accuracy.\n  3. Choosing the \"better\" paper: We generate 10 pairs of abstracts, deliberately designing each pair in such a way that one abstract was clearly superior than the other. The LLM, however, struggled to discern these relatively straightforward distinctions accurately, committing errors in its evaluations for 6 out of the 10 pairs.\n  Based on these experiments, we think that LLMs have a promising use as reviewing assistants for specific reviewing tasks, but not (yet) for complete evaluations of papers or proposals.",
    "authors": [
      "Ryan Liu",
      "Nihar B. Shah"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.DL"
    ],
    "published": "2023-06-01T12:45:53Z",
    "pdf_url": "https://arxiv.org/pdf/2306.00622v1"
  },
  {
    "arxiv_id": "2306.00393v3",
    "entry_id": "http://arxiv.org/abs/2306.00393v3",
    "title": "Teacher Agent: A Knowledge Distillation-Free Framework for Rehearsal-based Video Incremental Learning",
    "summary": "Rehearsal-based video incremental learning often employs knowledge distillation to mitigate catastrophic forgetting of previously learned data. However, this method faces two major challenges for video task: substantial computing resources from loading teacher model and limited replay capability from performance-limited teacher model. To address these problems, we first propose a knowledge distillation-free framework for rehearsal-based video incremental learning called \\textit{Teacher Agent}. Instead of loading parameter-heavy teacher networks, we introduce an agent generator that is either parameter-free or uses only a few parameters to obtain accurate and reliable soft labels. This method not only greatly reduces the computing requirement but also circumvents the problem of knowledge misleading caused by inaccurate predictions of the teacher model. Moreover, we put forward a self-correction loss which provides an effective regularization signal for the review of old knowledge, which in turn alleviates the problem of catastrophic forgetting. Further, to ensure that the samples in the memory buffer are memory-efficient and representative, we introduce a unified sampler for rehearsal-based video incremental learning to mine fixed-length key video frames. Interestingly, based on the proposed strategies, the network exhibits a high level of robustness against spatial resolution reduction when compared to the baseline. Extensive experiments demonstrate the advantages of our method, yielding significant performance improvements while utilizing only half the spatial resolution of video clips as network inputs in the incremental phases.",
    "authors": [
      "Shengqin Jiang",
      "Yaoyu Fang",
      "Haokui Zhang",
      "Qingshan Liu",
      "Yuankai Qi",
      "Yang Yang",
      "Peng Wang"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-06-01T06:54:56Z",
    "pdf_url": "https://arxiv.org/pdf/2306.00393v3"
  },
  {
    "arxiv_id": "2305.20076v3",
    "entry_id": "http://arxiv.org/abs/2305.20076v3",
    "title": "Decision-Oriented Dialogue for Human-AI Collaboration",
    "summary": "We describe a class of tasks called decision-oriented dialogues, in which AI assistants such as large language models (LMs) must collaborate with one or more humans via natural language to help them make complex decisions. We formalize three domains in which users face everyday decisions: (1) choosing an assignment of reviewers to conference papers, (2) planning a multi-step itinerary in a city, and (3) negotiating travel plans for a group of friends. In each of these settings, AI assistants and users have disparate abilities that they must combine to arrive at the best decision: assistants can access and process large amounts of information, while users have preferences and constraints external to the system. For each task, we build a dialogue environment where agents receive a reward based on the quality of the final decision they reach. We evaluate LMs in self-play and in collaboration with humans and find that they fall short compared to human assistants, achieving much lower rewards despite engaging in longer dialogues. We highlight a number of challenges models face in decision-oriented dialogues, ranging from goal-directed behavior to reasoning and optimization, and release our environments as a testbed for future work.",
    "authors": [
      "Jessy Lin",
      "Nicholas Tomlin",
      "Jacob Andreas",
      "Jason Eisner"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-31T17:50:02Z",
    "pdf_url": "https://arxiv.org/pdf/2305.20076v3"
  },
  {
    "arxiv_id": "2305.19860v5",
    "entry_id": "http://arxiv.org/abs/2305.19860v5",
    "title": "A Survey on Large Language Models for Recommendation",
    "summary": "Large Language Models (LLMs) have emerged as powerful tools in the field of Natural Language Processing (NLP) and have recently gained significant attention in the domain of Recommendation Systems (RS). These models, trained on massive amounts of data using self-supervised learning, have demonstrated remarkable success in learning universal representations and have the potential to enhance various aspects of recommendation systems by some effective transfer techniques such as fine-tuning and prompt tuning, and so on. The crucial aspect of harnessing the power of language models in enhancing recommendation quality is the utilization of their high-quality representations of textual features and their extensive coverage of external knowledge to establish correlations between items and users. To provide a comprehensive understanding of the existing LLM-based recommendation systems, this survey presents a taxonomy that categorizes these models into two major paradigms, respectively Discriminative LLM for Recommendation (DLLM4Rec) and Generative LLM for Recommendation (GLLM4Rec), with the latter being systematically sorted out for the first time. Furthermore, we systematically review and analyze existing LLM-based recommendation systems within each paradigm, providing insights into their methodologies, techniques, and performance. Additionally, we identify key challenges and several valuable findings to provide researchers and practitioners with inspiration. We have also created a GitHub repository to index relevant papers on LLMs for recommendation, https://github.com/WLiK/LLM4Rec.",
    "authors": [
      "Likang Wu",
      "Zhi Zheng",
      "Zhaopeng Qiu",
      "Hao Wang",
      "Hongchao Gu",
      "Tingjia Shen",
      "Chuan Qin",
      "Chen Zhu",
      "Hengshu Zhu",
      "Qi Liu",
      "Hui Xiong",
      "Enhong Chen"
    ],
    "categories": [
      "cs.IR",
      "cs.AI"
    ],
    "published": "2023-05-31T13:51:26Z",
    "pdf_url": "https://arxiv.org/pdf/2305.19860v5"
  },
  {
    "arxiv_id": "2305.18997v1",
    "entry_id": "http://arxiv.org/abs/2305.18997v1",
    "title": "GPT Models in Construction Industry: Opportunities, Limitations, and a Use Case Validation",
    "summary": "Large Language Models(LLMs) trained on large data sets came into prominence in 2018 after Google introduced BERT. Subsequently, different LLMs such as GPT models from OpenAI have been released. These models perform well on diverse tasks and have been gaining widespread applications in fields such as business and education. However, little is known about the opportunities and challenges of using LLMs in the construction industry. Thus, this study aims to assess GPT models in the construction industry. A critical review, expert discussion and case study validation are employed to achieve the study objectives. The findings revealed opportunities for GPT models throughout the project lifecycle. The challenges of leveraging GPT models are highlighted and a use case prototype is developed for materials selection and optimization. The findings of the study would be of benefit to researchers, practitioners and stakeholders, as it presents research vistas for LLMs in the construction industry.",
    "authors": [
      "Abdullahi Saka",
      "Ridwan Taiwo",
      "Nurudeen Saka",
      "Babatunde Salami",
      "Saheed Ajayi",
      "Kabiru Akande",
      "Hadi Kazemi"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-05-30T12:50:51Z",
    "pdf_url": "https://arxiv.org/pdf/2305.18997v1"
  },
  {
    "arxiv_id": "2305.18703v7",
    "entry_id": "http://arxiv.org/abs/2305.18703v7",
    "title": "Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey",
    "summary": "Large language models (LLMs) have significantly advanced the field of natural language processing (NLP), providing a highly useful, task-agnostic foundation for a wide range of applications. However, directly applying LLMs to solve sophisticated problems in specific domains meets many hurdles, caused by the heterogeneity of domain data, the sophistication of domain knowledge, the uniqueness of domain objectives, and the diversity of the constraints (e.g., various social norms, cultural conformity, religious beliefs, and ethical standards in the domain applications). Domain specification techniques are key to make large language models disruptive in many applications. Specifically, to solve these hurdles, there has been a notable increase in research and practices conducted in recent years on the domain specialization of LLMs. This emerging field of study, with its substantial potential for impact, necessitates a comprehensive and systematic review to better summarize and guide ongoing work in this area. In this article, we present a comprehensive survey on domain specification techniques for large language models, an emerging direction critical for large language model applications. First, we propose a systematic taxonomy that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other. Second, we present an extensive taxonomy of critical application domains that can benefit dramatically from specialized LLMs, discussing their practical significance and open challenges. Last, we offer our insights into the current research status and future trends in this area.",
    "authors": [
      "Chen Ling",
      "Xujiang Zhao",
      "Jiaying Lu",
      "Chengyuan Deng",
      "Can Zheng",
      "Junxiang Wang",
      "Tanmoy Chowdhury",
      "Yun Li",
      "Hejie Cui",
      "Xuchao Zhang",
      "Tianjiao Zhao",
      "Amit Panalkar",
      "Dhagash Mehta",
      "Stefano Pasquali",
      "Wei Cheng",
      "Haoyu Wang",
      "Yanchi Liu",
      "Zhengzhang Chen",
      "Haifeng Chen",
      "Chris White",
      "Quanquan Gu",
      "Jian Pei",
      "Carl Yang",
      "Liang Zhao"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-30T03:00:30Z",
    "pdf_url": "https://arxiv.org/pdf/2305.18703v7"
  },
  {
    "arxiv_id": "2305.18149v4",
    "entry_id": "http://arxiv.org/abs/2305.18149v4",
    "title": "Multiscale Positive-Unlabeled Detection of AI-Generated Texts",
    "summary": "Recent releases of Large Language Models (LLMs), e.g. ChatGPT, are astonishing at generating human-like texts, but they may impact the authenticity of texts. Previous works proposed methods to detect these AI-generated texts, including simple ML classifiers, pretrained-model-based zero-shot methods, and finetuned language classification models. However, mainstream detectors always fail on short texts, like SMSes, Tweets, and reviews. In this paper, a Multiscale Positive-Unlabeled (MPU) training framework is proposed to address the difficulty of short-text detection without sacrificing long-texts. Firstly, we acknowledge the human-resemblance property of short machine texts, and rephrase AI text detection as a partial Positive-Unlabeled (PU) problem by regarding these short machine texts as partially ``unlabeled\". Then in this PU context, we propose the length-sensitive Multiscale PU Loss, where a recurrent model in abstraction is used to estimate positive priors of scale-variant corpora. Additionally, we introduce a Text Multiscaling module to enrich training corpora. Experiments show that our MPU method augments detection performance on long AI-generated texts, and significantly improves short-text detection of language model detectors. Language Models trained with MPU could outcompete existing detectors on various short-text and long-text detection benchmarks. The codes are available at https://github.com/mindspore-lab/mindone/tree/master/examples/detect_chatgpt and https://github.com/YuchuanTian/AIGC_text_detector.",
    "authors": [
      "Yuchuan Tian",
      "Hanting Chen",
      "Xutao Wang",
      "Zheyuan Bai",
      "Qinghua Zhang",
      "Ruifeng Li",
      "Chao Xu",
      "Yunhe Wang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-29T15:25:00Z",
    "pdf_url": "https://arxiv.org/pdf/2305.18149v4"
  },
  {
    "arxiv_id": "2305.17680v4",
    "entry_id": "http://arxiv.org/abs/2305.17680v4",
    "title": "Evaluating GPT-3 Generated Explanations for Hateful Content Moderation",
    "summary": "Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-generated explanations as high quality in terms of linguistic fluency, informativeness, persuasiveness, and logical soundness, (2) the persuasive nature of these explanations, however, varied depending on the prompting strategy employed, and (3) this persuasiveness may result in incorrect judgments about the hatefulness of the content. Our study underscores the need for caution in applying LLM-generated explanations for content moderation. Code and results are available at https://github.com/Social-AI-Studio/GPT3-HateEval.",
    "authors": [
      "Han Wang",
      "Ming Shan Hee",
      "Md Rabiul Awal",
      "Kenny Tsu Wei Choo",
      "Roy Ka-Wei Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-28T10:05:13Z",
    "pdf_url": "https://arxiv.org/pdf/2305.17680v4"
  },
  {
    "arxiv_id": "2305.17116v2",
    "entry_id": "http://arxiv.org/abs/2305.17116v2",
    "title": "Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model",
    "summary": "Large language models (LLMs) have made significant advancements in natural language processing (NLP). Broad corpora capture diverse patterns but can introduce irrelevance, while focused corpora enhance reliability by reducing misleading information. Training LLMs on focused corpora poses computational challenges. An alternative approach is to use a retrieval-augmentation (RetA) method tested in a specific domain.\n  To evaluate LLM performance, OpenAI's GPT-3, GPT-4, Bing's Prometheus, and a custom RetA model were compared using 19 questions on diffuse large B-cell lymphoma (DLBCL) disease. Eight independent reviewers assessed responses based on accuracy, relevance, and readability (rated 1-3).\n  The RetA model performed best in accuracy (12/19 3-point scores, total=47) and relevance (13/19, 50), followed by GPT-4 (8/19, 43; 11/19, 49). GPT-4 received the highest readability scores (17/19, 55), followed by GPT-3 (15/19, 53) and the RetA model (11/19, 47). Prometheus underperformed in accuracy (34), relevance (32), and readability (38).\n  Both GPT-3.5 and GPT-4 had more hallucinations in all 19 responses compared to the RetA model and Prometheus. Hallucinations were mostly associated with non-existent references or fabricated efficacy data.\n  These findings suggest that RetA models, supplemented with domain-specific corpora, may outperform general-purpose LLMs in accuracy and relevance within specific domains. However, this evaluation was limited to specific questions and metrics and may not capture challenges in semantic search and other NLP tasks. Further research will explore different LLM architectures, RetA methodologies, and evaluation methods to assess strengths and limitations more comprehensively.",
    "authors": [
      "David Soong",
      "Sriram Sridhar",
      "Han Si",
      "Jan-Samuel Wagner",
      "Ana Caroline Costa Sá",
      "Christina Y Yu",
      "Kubra Karagoz",
      "Meijian Guan",
      "Hisham Hamadeh",
      "Brandon W Higgs"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-26T17:33:05Z",
    "pdf_url": "https://arxiv.org/pdf/2305.17116v2"
  },
  {
    "arxiv_id": "2305.16837v1",
    "entry_id": "http://arxiv.org/abs/2305.16837v1",
    "title": "ChatGPT: A Study on its Utility for Ubiquitous Software Engineering Tasks",
    "summary": "ChatGPT (Chat Generative Pre-trained Transformer) is a chatbot launched by OpenAI on November 30, 2022. OpenAI's GPT-3 family of large language models serve as the foundation for ChatGPT. ChatGPT is fine-tuned with both supervised and reinforcement learning techniques and has received widespread attention for its articulate responses across diverse domains of knowledge. In this study, we explore how ChatGPT can be used to help with common software engineering tasks. Many of the ubiquitous tasks covering the breadth of software engineering such as ambiguity resolution in software requirements, method name suggestion, test case prioritization, code review, log summarization can potentially be performed using ChatGPT. In this study, we explore fifteen common software engineering tasks using ChatGPT. We juxtapose and analyze ChatGPT's answers with the respective state of the art outputs (where available) and/or human expert ground truth. Our experiments suggest that for many tasks, ChatGPT does perform credibly and the response from it is detailed and often better than the human expert output or the state of the art output. However, for a few other tasks, ChatGPT in its present form provides incorrect answers and hence is not suited for such tasks.",
    "authors": [
      "Giriprasad Sridhara",
      "Ranjani H. G.",
      "Sourav Mazumdar"
    ],
    "categories": [
      "cs.SE",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-26T11:29:06Z",
    "pdf_url": "https://arxiv.org/pdf/2305.16837v1"
  },
  {
    "arxiv_id": "2305.16556v3",
    "entry_id": "http://arxiv.org/abs/2305.16556v3",
    "title": "LANISTR: Multimodal Learning from Structured and Unstructured Data",
    "summary": "Multimodal large-scale pretraining has shown impressive performance for unstructured data such as language and image. However, a prevalent real-world scenario involves structured data types, tabular and time-series, along with unstructured data. Such scenarios have been understudied. To bridge this gap, we propose LANISTR, an attention-based framework to learn from LANguage, Image, and STRuctured data. The core of LANISTR's methodology is rooted in \\textit{masking-based} training applied across both unimodal and multimodal levels. In particular, we introduce a new similarity-based multimodal masking loss that enables it to learn cross-modal relations from large-scale multimodal data with missing modalities. On two real-world datasets, MIMIC-IV (from healthcare) and Amazon Product Review (from retail), LANISTR demonstrates remarkable improvements, 6.6\\% (in AUROC) and 14\\% (in accuracy) when fine-tuned with 0.1\\% and 0.01\\% of labeled data, respectively, compared to the state-of-the-art alternatives. Notably, these improvements are observed even with very high ratio of samples (35.7\\% and 99.8\\% respectively) not containing all modalities, underlining the robustness of LANISTR to practical missing modality challenge. Our code and models will be available at https://github.com/google-research/lanistr",
    "authors": [
      "Sayna Ebrahimi",
      "Sercan O. Arik",
      "Yihe Dong",
      "Tomas Pfister"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-05-26T00:50:09Z",
    "pdf_url": "https://arxiv.org/pdf/2305.16556v3"
  },
  {
    "arxiv_id": "2305.15299v4",
    "entry_id": "http://arxiv.org/abs/2305.15299v4",
    "title": "Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond",
    "summary": "Large language models of artificial intelligence (AI), such as ChatGPT, find remarkable but controversial applicability in science and research. This paper reviews epistemological challenges, ethical and integrity risks in science conduct in the advent of generative AI. This is with the aim to lay new timely foundations for a high-quality research ethics review. The role of AI language models as a research instrument and subject is scrutinized along with ethical implications for scientists, participants and reviewers. New emerging practices for research ethics review are discussed, concluding with ten recommendations that shape a response for a more responsible research conduct in the era of AI.",
    "authors": [
      "Evangelos Pournaras"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-05-24T16:23:46Z",
    "pdf_url": "https://arxiv.org/pdf/2305.15299v4"
  },
  {
    "arxiv_id": "2305.15186v1",
    "entry_id": "http://arxiv.org/abs/2305.15186v1",
    "title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
    "summary": "Automatic literature review generation is one of the most challenging tasks in natural language processing. Although large language models have tackled literature review generation, the absence of large-scale datasets has been a stumbling block to the progress. We release SciReviewGen, consisting of over 10,000 literature reviews and 690,000 papers cited in the reviews. Based on the dataset, we evaluate recent transformer-based summarization models on the literature review generation task, including Fusion-in-Decoder extended for literature review generation. Human evaluation results show that some machine-generated summaries are comparable to human-written reviews, while revealing the challenges of automatic literature review generation such as hallucinations and a lack of detailed information. Our dataset and code are available at https://github.com/tetsu9923/SciReviewGen.",
    "authors": [
      "Tetsu Kasanishi",
      "Masaru Isonuma",
      "Junichiro Mori",
      "Ichiro Sakata"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-24T14:26:30Z",
    "pdf_url": "https://arxiv.org/pdf/2305.15186v1"
  },
  {
    "arxiv_id": "2305.14485v1",
    "entry_id": "http://arxiv.org/abs/2305.14485v1",
    "title": "Knowledge Graphs Querying",
    "summary": "Knowledge graphs (KGs) such as DBpedia, Freebase, YAGO, Wikidata, and NELL were constructed to store large-scale, real-world facts as (subject, predicate, object) triples -- that can also be modeled as a graph, where a node (a subject or an object) represents an entity with attributes, and a directed edge (a predicate) is a relationship between two entities. Querying KGs is critical in web search, question answering (QA), semantic search, personal assistants, fact checking, and recommendation. While significant progress has been made on KG construction and curation, thanks to deep learning recently we have seen a surge of research on KG querying and QA. The objectives of our survey are two-fold. First, research on KG querying has been conducted by several communities, such as databases, data mining, semantic web, machine learning, information retrieval, and natural language processing (NLP), with different focus and terminologies; and also in diverse topics ranging from graph databases, query languages, join algorithms, graph patterns matching, to more sophisticated KG embedding and natural language questions (NLQs). We aim at uniting different interdisciplinary topics and concepts that have been developed for KG querying. Second, many recent advances on KG and query embedding, multimodal KG, and KG-QA come from deep learning, IR, NLP, and computer vision domains. We identify important challenges of KG querying that received less attention by graph databases, and by the DB community in general, e.g., incomplete KG, semantic matching, multimodal data, and NLQs. We conclude by discussing interesting opportunities for the data management community, for instance, KG as a unified data model and vector-based query processing.",
    "authors": [
      "Arijit Khan"
    ],
    "categories": [
      "cs.DB",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2023-05-23T19:32:42Z",
    "pdf_url": "https://arxiv.org/pdf/2305.14485v1"
  },
  {
    "arxiv_id": "2305.14196v3",
    "entry_id": "http://arxiv.org/abs/2305.14196v3",
    "title": "ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding",
    "summary": "We introduce ZeroSCROLLS, a zero-shot benchmark for natural language understanding over long texts, which contains only test and small validation sets, without training data. We adapt six tasks from the SCROLLS benchmark, and add four new datasets, including two novel information fusing tasks, such as aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a comprehensive evaluation of both open-source and closed large language models, finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest average score. However, there is still room for improvement on multiple open challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to pass the naive baseline. As the state of the art is a moving target, we invite researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.",
    "authors": [
      "Uri Shaham",
      "Maor Ivgi",
      "Avia Efrat",
      "Jonathan Berant",
      "Omer Levy"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG",
      "stat.ML"
    ],
    "published": "2023-05-23T16:15:31Z",
    "pdf_url": "https://arxiv.org/pdf/2305.14196v3"
  },
  {
    "arxiv_id": "2305.13954v3",
    "entry_id": "http://arxiv.org/abs/2305.13954v3",
    "title": "Robust Prompt Optimization for Large Language Models Against Distribution Shifts",
    "summary": "Large Language Model (LLM) has demonstrated significant ability in various Natural Language Processing tasks. However, their effectiveness is highly dependent on the phrasing of the task prompt, leading to research on automatic prompt optimization using labeled task data. We reveal that these prompt optimization techniques are vulnerable to distribution shifts such as subpopulation shifts, which are common for LLMs in real-world scenarios such as customer reviews analysis. In this light, we propose a new problem of robust prompt optimization for LLMs against distribution shifts, which requires the prompt optimized over the labeled source group can simultaneously generalize to an unlabeled target group. To solve this problem, we propose Generalized Prompt Optimization framework, which incorporates the unlabeled data from the target group into prompt optimization. Extensive experimental results demonstrate the effectiveness of the proposed framework with significant performance improvement on the target group and comparable performance on the source group.",
    "authors": [
      "Moxin Li",
      "Wenjie Wang",
      "Fuli Feng",
      "Yixin Cao",
      "Jizhi Zhang",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-23T11:30:43Z",
    "pdf_url": "https://arxiv.org/pdf/2305.13954v3"
  },
  {
    "arxiv_id": "2305.13246v1",
    "entry_id": "http://arxiv.org/abs/2305.13246v1",
    "title": "Interactive Natural Language Processing",
    "summary": "Interactive Natural Language Processing (iNLP) has emerged as a novel paradigm within the field of NLP, aimed at addressing limitations in existing frameworks while aligning with the ultimate goals of artificial intelligence. This paradigm considers language models as agents capable of observing, acting, and receiving feedback iteratively from external entities. Specifically, language models in this context can: (1) interact with humans for better understanding and addressing user needs, personalizing responses, aligning with human values, and improving the overall user experience; (2) interact with knowledge bases for enriching language representations with factual knowledge, enhancing the contextual relevance of responses, and dynamically leveraging external information to generate more accurate and informed responses; (3) interact with models and tools for effectively decomposing and addressing complex tasks, leveraging specialized expertise for specific subtasks, and fostering the simulation of social behaviors; and (4) interact with environments for learning grounded representations of language, and effectively tackling embodied tasks such as reasoning, planning, and decision-making in response to environmental observations. This paper offers a comprehensive survey of iNLP, starting by proposing a unified definition and framework of the concept. We then provide a systematic classification of iNLP, dissecting its various components, including interactive objects, interaction interfaces, and interaction methods. We proceed to delve into the evaluation methodologies used in the field, explore its diverse applications, scrutinize its ethical and safety issues, and discuss prospective research directions. This survey serves as an entry point for researchers who are interested in this rapidly evolving area and offers a broad view of the current landscape and future trajectory of iNLP.",
    "authors": [
      "Zekun Wang",
      "Ge Zhang",
      "Kexin Yang",
      "Ning Shi",
      "Wangchunshu Zhou",
      "Shaochun Hao",
      "Guangzheng Xiong",
      "Yizhi Li",
      "Mong Yuan Sim",
      "Xiuying Chen",
      "Qingqing Zhu",
      "Zhenzhu Yang",
      "Adam Nik",
      "Qi Liu",
      "Chenghua Lin",
      "Shi Wang",
      "Ruibo Liu",
      "Wenhu Chen",
      "Ke Xu",
      "Dayiheng Liu",
      "Yike Guo",
      "Jie Fu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-22T17:18:29Z",
    "pdf_url": "https://arxiv.org/pdf/2305.13246v1"
  },
  {
    "arxiv_id": "2305.13342v1",
    "entry_id": "http://arxiv.org/abs/2305.13342v1",
    "title": "On the Limitations of Simulating Active Learning",
    "summary": "Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question ``why do active learning algorithms sometimes fail to outperform random sampling?''. We argue that evaluating AL algorithms on available labeled datasets might provide a lower bound as to their effectiveness in real data. We believe it is essential to collectively shape the best practices for AL research, particularly as engineering advancements in LLMs push the research focus towards data-driven approaches (e.g., data efficiency, alignment, fairness). In light of this, we have developed guidelines for future work. Our aim is to draw attention to these limitations within the community, in the hope of finding ways to address them.",
    "authors": [
      "Katerina Margatina",
      "Nikolaos Aletras"
    ],
    "categories": [
      "cs.LG",
      "cs.CL"
    ],
    "published": "2023-05-21T22:52:13Z",
    "pdf_url": "https://arxiv.org/pdf/2305.13342v1"
  },
  {
    "arxiv_id": "2305.12519v3",
    "entry_id": "http://arxiv.org/abs/2305.12519v3",
    "title": "DPIC: Decoupling Prompt and Intrinsic Characteristics for LLM Generated Text Detection",
    "summary": "Large language models (LLMs) have the potential to generate texts that pose risks of misuse, such as plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Consequently, detecting whether a text is generated by LLMs has become increasingly important. Existing high-quality detection methods usually require access to the interior of the model to extract the intrinsic characteristics. However, since we do not have access to the interior of the black-box model, we must resort to surrogate models, which impacts detection quality. In order to achieve high-quality detection of black-box models, we would like to extract deep intrinsic characteristics of the black-box model generated texts. We view the generation process as a coupled process of prompt and intrinsic characteristics of the generative model. Based on this insight, we propose to decouple prompt and intrinsic characteristics (DPIC) for LLM-generated text detection method. Specifically, given a candidate text, DPIC employs an auxiliary LLM to reconstruct the prompt corresponding to the candidate text, then uses the prompt to regenerate text by the auxiliary LLM, which makes the candidate text and the regenerated text align with their prompts, respectively. Then, the similarity between the candidate text and the regenerated text is used as a detection feature, thus eliminating the prompt in the detection process, which allows the detector to focus on the intrinsic characteristics of the generative model. Compared to the baselines, DPIC has achieved an average improvement of 6.76\\% and 2.91\\% in detecting texts from different domains generated by GPT4 and Claude3, respectively.",
    "authors": [
      "Xiao Yu",
      "Yuang Qi",
      "Kejiang Chen",
      "Guoqiang Chen",
      "Xi Yang",
      "Pengyuan Zhu",
      "Xiuwei Shang",
      "Weiming Zhang",
      "Nenghai Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-21T17:26:16Z",
    "pdf_url": "https://arxiv.org/pdf/2305.12519v3"
  },
  {
    "arxiv_id": "2305.11828v3",
    "entry_id": "http://arxiv.org/abs/2305.11828v3",
    "title": "Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews",
    "summary": "Medical systematic reviews play a vital role in healthcare decision making and policy. However, their production is time-consuming, limiting the availability of high-quality and up-to-date evidence summaries. Recent advancements in large language models (LLMs) offer the potential to automatically generate literature reviews on demand, addressing this issue. However, LLMs sometimes generate inaccurate (and potentially misleading) texts by hallucination or omission. In healthcare, this can make LLMs unusable at best and dangerous at worst. We conducted 16 interviews with international systematic review experts to characterize the perceived utility and risks of LLMs in the specific context of medical evidence reviews. Experts indicated that LLMs can assist in the writing process by drafting summaries, generating templates, distilling information, and crosschecking information. They also raised concerns regarding confidently composed but inaccurate LLM outputs and other potential downstream harms, including decreased accountability and proliferation of low-quality reviews. Informed by this qualitative analysis, we identify criteria for rigorous evaluation of biomedical LLMs aligned with domain expert views.",
    "authors": [
      "Hye Sun Yun",
      "Iain J. Marshall",
      "Thomas A. Trikalinos",
      "Byron C. Wallace"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.HC"
    ],
    "published": "2023-05-19T17:09:19Z",
    "pdf_url": "https://arxiv.org/pdf/2305.11828v3"
  },
  {
    "arxiv_id": "2305.11391v2",
    "entry_id": "http://arxiv.org/abs/2305.11391v2",
    "title": "A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation",
    "summary": "Large Language Models (LLMs) have exploded a new heatwave of AI for their ability to engage end-users in human-level conversations with detailed and articulate answers across many knowledge domains. In response to their fast adoption in many industrial applications, this survey concerns their safety and trustworthiness. First, we review known vulnerabilities and limitations of the LLMs, categorising them into inherent issues, attacks, and unintended bugs. Then, we consider if and how the Verification and Validation (V&V) techniques, which have been widely developed for traditional software and deep learning models such as convolutional neural networks as independent processes to check the alignment of their implementations against the specifications, can be integrated and further extended throughout the lifecycle of the LLMs to provide rigorous analysis to the safety and trustworthiness of LLMs and their applications. Specifically, we consider four complementary techniques: falsification and evaluation, verification, runtime monitoring, and regulations and ethical use. In total, 370+ references are considered to support the quick understanding of the safety and trustworthiness issues from the perspective of V&V. While intensive research has been conducted to identify the safety and trustworthiness issues, rigorous yet practical methods are called for to ensure the alignment of LLMs with safety and trustworthiness requirements.",
    "authors": [
      "Xiaowei Huang",
      "Wenjie Ruan",
      "Wei Huang",
      "Gaojie Jin",
      "Yi Dong",
      "Changshun Wu",
      "Saddek Bensalem",
      "Ronghui Mu",
      "Yi Qi",
      "Xingyu Zhao",
      "Kaiwen Cai",
      "Yanghao Zhang",
      "Sihao Wu",
      "Peipei Xu",
      "Dengyu Wu",
      "Andre Freitas",
      "Mustafa A. Mustafa"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-19T02:41:12Z",
    "pdf_url": "https://arxiv.org/pdf/2305.11391v2"
  },
  {
    "arxiv_id": "2305.10716v2",
    "entry_id": "http://arxiv.org/abs/2305.10716v2",
    "title": "A Survey on Time-Series Pre-Trained Models",
    "summary": "Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, pre-trained models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments involving 27 methods, 434 datasets, and 679 transfer learning scenarios are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work.",
    "authors": [
      "Qianli Ma",
      "Zhen Liu",
      "Zhenjing Zheng",
      "Ziyang Huang",
      "Siying Zhu",
      "Zhongzhong Yu",
      "James T. Kwok"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-05-18T05:27:46Z",
    "pdf_url": "https://arxiv.org/pdf/2305.10716v2"
  },
  {
    "arxiv_id": "2305.10473v2",
    "entry_id": "http://arxiv.org/abs/2305.10473v2",
    "title": "Predicting Side Effect of Drug Molecules using Recurrent Neural Networks",
    "summary": "Identification and verification of molecular properties such as side effects is one of the most important and time-consuming steps in the process of molecule synthesis. For example, failure to identify side effects before submission to regulatory groups can cost millions of dollars and months of additional research to the companies. Failure to identify side effects during the regulatory review can also cost lives. The complexity and expense of this task have made it a candidate for a machine learning-based solution. Prior approaches rely on complex model designs and excessive parameter counts for side effect predictions. We believe reliance on complex models only shifts the difficulty away from chemists rather than alleviating the issue. Implementing large models is also expensive without prior access to high-performance computers. We propose a heuristic approach that allows for the utilization of simple neural networks, specifically the recurrent neural network, with a 98+% reduction in the number of required parameters compared to available large language models while still obtaining near identical results as top-performing models.",
    "authors": [
      "Collin Beaudoin",
      "Koustubh Phalak",
      "Swaroop Ghosh"
    ],
    "categories": [
      "q-bio.QM",
      "cs.LG"
    ],
    "published": "2023-05-17T16:56:19Z",
    "pdf_url": "https://arxiv.org/pdf/2305.10473v2"
  },
  {
    "arxiv_id": "2305.10196v1",
    "entry_id": "http://arxiv.org/abs/2305.10196v1",
    "title": "A Survey on Zero Pronoun Translation",
    "summary": "Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution, so that researchers can recognise the current state and future directions of this field. We provide an organisation of the literature based on evolution, dataset, method and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use; 4) general-purpose metrics are not reliable on nuances and complexities of ZPT, emphasizing the necessity of targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of gender bias.",
    "authors": [
      "Longyue Wang",
      "Siyou Liu",
      "Mingzhou Xu",
      "Linfeng Song",
      "Shuming Shi",
      "Zhaopeng Tu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-17T13:19:01Z",
    "pdf_url": "https://arxiv.org/pdf/2305.10196v1"
  },
  {
    "arxiv_id": "2305.10167v1",
    "entry_id": "http://arxiv.org/abs/2305.10167v1",
    "title": "Pragmatic Reasoning in Structured Signaling Games",
    "summary": "In this work we introduce a structured signaling game, an extension of the classical signaling game with a similarity structure between meanings in the context, along with a variant of the Rational Speech Act (RSA) framework which we call structured-RSA (sRSA) for pragmatic reasoning in structured domains. We explore the behavior of the sRSA in the domain of color and show that pragmatic agents using sRSA on top of semantic representations, derived from the World Color Survey, attain efficiency very close to the information theoretic limit after only 1 or 2 levels of recursion. We also explore the interaction between pragmatic reasoning and learning in multi-agent reinforcement learning framework. Our results illustrate that artificial agents using sRSA develop communication closer to the information theoretic frontier compared to agents using RSA and just reinforcement learning. We also find that the ambiguity of the semantic representation increases as the pragmatic agents are allowed to perform deeper reasoning about each other during learning.",
    "authors": [
      "Emil Carlsson",
      "Devdatt Dubhashi"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-05-17T12:43:29Z",
    "pdf_url": "https://arxiv.org/pdf/2305.10167v1"
  },
  {
    "arxiv_id": "2305.10091v1",
    "entry_id": "http://arxiv.org/abs/2305.10091v1",
    "title": "Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges",
    "summary": "Multi-agent reinforcement learning (MARL) is a widely used Artificial Intelligence (AI) technique. However, current studies and applications need to address its scalability, non-stationarity, and trustworthiness. This paper aims to review methods and applications and point out research trends and visionary prospects for the next decade. First, this paper summarizes the basic methods and application scenarios of MARL. Second, this paper outlines the corresponding research methods and their limitations on safety, robustness, generalization, and ethical constraints that need to be addressed in the practical applications of MARL. In particular, we believe that trustworthy MARL will become a hot research topic in the next decade. In addition, we suggest that considering human interaction is essential for the practical application of MARL in various societies. Therefore, this paper also analyzes the challenges while MARL is applied to human-machine interaction.",
    "authors": [
      "Ziyuan Zhou",
      "Guanjun Liu",
      "Ying Tang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-05-17T09:53:13Z",
    "pdf_url": "https://arxiv.org/pdf/2305.10091v1"
  },
  {
    "arxiv_id": "2305.09802v3",
    "entry_id": "http://arxiv.org/abs/2305.09802v3",
    "title": "Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models",
    "summary": "Smart home assistants function best when user commands are direct and well-specified (e.g., \"turn on the kitchen light\"), or when a hard-coded routine specifies the response. In more natural communication, however, human speech is unconstrained, often describing goals (e.g., \"make it cozy in here\" or \"help me save energy\") rather than indicating specific target devices and actions to take on those devices. Current systems fail to understand these under-specified commands since they cannot reason about devices and settings as they relate to human situations. We introduce large language models (LLMs) to this problem space, exploring their use for controlling devices and creating automation routines in response to under-specified user commands in smart homes. We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness. We address these gaps with Sasha, a smarter smart home assistant. Sasha responds to loosely-constrained commands like \"make it cozy\" or \"help me sleep better\" by executing plans to achieve user goals, e.g., setting a mood with available devices, or devising automation routines. We implement and evaluate Sasha in a hands-on user study, showing the capabilities and limitations of LLM-driven smart homes when faced with unconstrained user-generated scenarios.",
    "authors": [
      "Evan King",
      "Haoxiang Yu",
      "Sangsu Lee",
      "Christine Julien"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-05-16T20:52:04Z",
    "pdf_url": "https://arxiv.org/pdf/2305.09802v3"
  },
  {
    "arxiv_id": "2305.09620v3",
    "entry_id": "http://arxiv.org/abs/2305.09620v3",
    "title": "AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction",
    "summary": "Large language models (LLMs) that produce human-like responses have begun to revolutionize research practices in the social sciences. We develop a novel methodological framework that fine-tunes LLMs with repeated cross-sectional surveys to incorporate the meaning of survey questions, individual beliefs, and temporal contexts for opinion prediction. We introduce two new emerging applications of the AI-augmented survey: retrodiction (i.e., predict year-level missing responses) and unasked opinion prediction (i.e., predict entirely missing responses). Among 3,110 binarized opinions from 68,846 Americans in the General Social Survey from 1972 to 2021, our models based on Alpaca-7b excel in retrodiction (AUC = 0.86 for personal opinion prediction, $ρ$ = 0.98 for public opinion prediction). These remarkable prediction capabilities allow us to fill in missing trends with high confidence and pinpoint when public attitudes changed, such as the rising support for same-sex marriage. On the other hand, our fine-tuned Alpaca-7b models show modest success in unasked opinion prediction (AUC = 0.73, $ρ$ = 0.67). We discuss practical constraints and ethical concerns regarding individual autonomy and privacy when using LLMs for opinion prediction. Our study demonstrates that LLMs and surveys can mutually enhance each other's capabilities: LLMs can broaden survey potential, while surveys can improve the alignment of LLMs.",
    "authors": [
      "Junsol Kim",
      "Byungkyu Lee"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-16T17:13:07Z",
    "pdf_url": "https://arxiv.org/pdf/2305.09620v3"
  },
  {
    "arxiv_id": "2305.08112v1",
    "entry_id": "http://arxiv.org/abs/2305.08112v1",
    "title": "Quantum Operation of Affective Artificial Intelligence",
    "summary": "The review analyzes the fundamental principles which Artificial Intelligence should be based on in order to imitate the realistic process of taking decisions by humans experiencing emotions. Two approaches are compared, one based on quantum theory and the other employing classical terms. Both these approaches have a number of similarities, being principally probabilistic. The analogies between quantum measurements under intrinsic noise and affective decision making are elucidated. It is shown that cognitive processes have many features that are formally similar to quantum measurements. This, however, in no way means that for the imitation of human decision making Affective Artificial Intelligence has necessarily to rely on the functioning of quantum systems. Appreciating the common features between quantum measurements and decision making helps for the formulation of an axiomatic approach employing only classical notions. Artificial Intelligence, following this approach, operates similarly to humans, by taking into account the utility of the considered alternatives as well as their emotional attractiveness. Affective Artificial Intelligence, whose operation takes account of the cognition-emotion duality, avoids numerous behavioural paradoxes of traditional decision making. A society of intelligent agents, interacting through the repeated multistep exchange of information, forms a network accomplishing dynamic decision making. The considered intelligent networks can characterize the operation of either a human society of affective decision makers, or the brain composed of neurons, or a typical probabilistic network of an artificial intelligence.",
    "authors": [
      "V. I. Yukalov"
    ],
    "categories": [
      "cs.AI",
      "q-bio.NC",
      "quant-ph"
    ],
    "published": "2023-05-14T09:40:13Z",
    "pdf_url": "https://arxiv.org/pdf/2305.08112v1"
  },
  {
    "arxiv_id": "2305.07605v3",
    "entry_id": "http://arxiv.org/abs/2305.07605v3",
    "title": "Generative AI: Implications and Applications for Education",
    "summary": "The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.",
    "authors": [
      "Anastasia Olga",
      "Tzirides",
      "Akash Saini",
      "Gabriela Zapata",
      "Duane Searsmith",
      "Bill Cope",
      "Mary Kalantzis",
      "Vania Castro",
      "Theodora Kourkoulou",
      "John Jones",
      "Rodrigo Abrantes da Silva",
      "Jen Whiting",
      "Nikoleta Polyxeni Kastania"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-05-12T16:52:38Z",
    "pdf_url": "https://arxiv.org/pdf/2305.07605v3"
  },
  {
    "arxiv_id": "2305.06472v2",
    "entry_id": "http://arxiv.org/abs/2305.06472v2",
    "title": "ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps",
    "summary": "Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT represents a landmark achievement in this research paradigm, offering hope for general artificial intelligence due to its highly intelligent natural language understanding ability. However, the PHM field lacks a consensus on how to respond to this significant change in the AI field, and a systematic review and roadmap is required to elucidate future development directions. To fill this gap, this paper systematically expounds on the key components and latest developments of LSF-Models. Then, we systematically answered how to build the LSF-Model applicable to PHM tasks and outlined the challenges and future development roadmaps for this research paradigm.",
    "authors": [
      "Yan-Fu Li",
      "Huan Wang",
      "Muxia Sun"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-05-10T21:37:44Z",
    "pdf_url": "https://arxiv.org/pdf/2305.06472v2"
  },
  {
    "arxiv_id": "2305.06055v1",
    "entry_id": "http://arxiv.org/abs/2305.06055v1",
    "title": "A Classification of Feedback Loops and Their Relation to Biases in Automated Decision-Making Systems",
    "summary": "Prediction-based decision-making systems are becoming increasingly prevalent in various domains. Previous studies have demonstrated that such systems are vulnerable to runaway feedback loops, e.g., when police are repeatedly sent back to the same neighborhoods regardless of the actual rate of criminal activity, which exacerbate existing biases. In practice, the automated decisions have dynamic feedback effects on the system itself that can perpetuate over time, making it difficult for short-sighted design choices to control the system's evolution. While researchers started proposing longer-term solutions to prevent adverse outcomes (such as bias towards certain groups), these interventions largely depend on ad hoc modeling assumptions and a rigorous theoretical understanding of the feedback dynamics in ML-based decision-making systems is currently missing. In this paper, we use the language of dynamical systems theory, a branch of applied mathematics that deals with the analysis of the interconnection of systems with dynamic behaviors, to rigorously classify the different types of feedback loops in the ML-based decision-making pipeline. By reviewing existing scholarly work, we show that this classification covers many examples discussed in the algorithmic fairness community, thereby providing a unifying and principled framework to study feedback loops. By qualitative analysis, and through a simulation example of recommender systems, we show which specific types of ML biases are affected by each type of feedback loop. We find that the existence of feedback loops in the ML-based decision-making pipeline can perpetuate, reinforce, or even reduce ML biases.",
    "authors": [
      "Nicolò Pagan",
      "Joachim Baumann",
      "Ezzat Elokda",
      "Giulia De Pasquale",
      "Saverio Bolognani",
      "Anikó Hannák"
    ],
    "categories": [
      "cs.CY",
      "cs.LG"
    ],
    "published": "2023-05-10T11:15:22Z",
    "pdf_url": "https://arxiv.org/pdf/2305.06055v1"
  },
  {
    "arxiv_id": "2305.05726v2",
    "entry_id": "http://arxiv.org/abs/2305.05726v2",
    "title": "Vision-Language Models in Remote Sensing: Current Progress and Future Trends",
    "summary": "The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of interest and research in the field of large language models for Artificial General Intelligence (AGI). These models provide intelligent solutions close to human thinking, enabling us to use general artificial intelligence to solve problems in various applications. However, in remote sensing (RS), the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research in remote sensing primarily focuses on visual understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-language models excel, as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. Vision-language models can go beyond visual recognition of RS images, model semantic relationships, and generate natural language descriptions of the image. This makes them better suited for tasks requiring visual and textual understanding, such as image captioning, and visual question answering. This paper provides a comprehensive review of the research on vision-language models in remote sensing, summarizing the latest progress, highlighting challenges, and identifying potential research opportunities.",
    "authors": [
      "Xiang Li",
      "Congcong Wen",
      "Yuan Hu",
      "Zhenghang Yuan",
      "Xiao Xiang Zhu"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-05-09T19:17:07Z",
    "pdf_url": "https://arxiv.org/pdf/2305.05726v2"
  },
  {
    "arxiv_id": "2305.05176v1",
    "entry_id": "http://arxiv.org/abs/2305.05176v1",
    "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
    "summary": "There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost. The ideas and findings presented here lay a foundation for using LLMs sustainably and efficiently.",
    "authors": [
      "Lingjiao Chen",
      "Matei Zaharia",
      "James Zou"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CL",
      "cs.SE"
    ],
    "published": "2023-05-09T05:11:02Z",
    "pdf_url": "https://arxiv.org/pdf/2305.05176v1"
  },
  {
    "arxiv_id": "2305.04417v1",
    "entry_id": "http://arxiv.org/abs/2305.04417v1",
    "title": "Unlocking Practical Applications in Legal Domain: Evaluation of GPT for Zero-Shot Semantic Annotation of Legal Texts",
    "summary": "We evaluated the capability of a state-of-the-art generative pre-trained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) coming from legal documents of various types. Discussions of potential uses (e.g., document drafting, summarization) of this emerging technology in legal domain have intensified, but to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings. Yet, this particular type of use could unlock many practical applications (e.g., in contract review) and research opportunities (e.g., in empirical legal studies). We fill the gap with this study. We examined if and how successfully the model can semantically annotate small batches of short text snippets (10-50) based exclusively on concise definitions of the semantic types. We found that the GPT model performs surprisingly well in zero-shot settings on diverse types of documents (F1=.73 on a task involving court opinions, .86 for contracts, and .54 for statutes and regulations). These findings can be leveraged by legal scholars and practicing lawyers alike to guide their decisions in integrating LLMs in wide range of workflows involving semantic annotation of legal texts.",
    "authors": [
      "Jaromir Savelka"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-08T01:55:53Z",
    "pdf_url": "https://arxiv.org/pdf/2305.04417v1"
  },
  {
    "arxiv_id": "2305.13934v1",
    "entry_id": "http://arxiv.org/abs/2305.13934v1",
    "title": "Perception, performance, and detectability of conversational artificial intelligence across 32 university courses",
    "summary": "The emergence of large language models has led to the development of powerful tools such as ChatGPT that can produce text indistinguishable from human-generated work. With the increasing accessibility of such technology, students across the globe may utilize it to help with their school work -- a possibility that has sparked discussions on the integrity of student evaluations in the age of artificial intelligence (AI). To date, it is unclear how such tools perform compared to students on university-level courses. Further, students' perspectives regarding the use of such tools, and educators' perspectives on treating their use as plagiarism, remain unknown. Here, we compare the performance of ChatGPT against students on 32 university-level courses. We also assess the degree to which its use can be detected by two classifiers designed specifically for this purpose. Additionally, we conduct a survey across five countries, as well as a more in-depth survey at the authors' institution, to discern students' and educators' perceptions of ChatGPT's use. We find that ChatGPT's performance is comparable, if not superior, to that of students in many courses. Moreover, current AI-text classifiers cannot reliably detect ChatGPT's use in school work, due to their propensity to classify human-written answers as AI-generated, as well as the ease with which AI-generated text can be edited to evade detection. Finally, we find an emerging consensus among students to use the tool, and among educators to treat this as plagiarism. Our findings offer insights that could guide policy discussions addressing the integration of AI into educational frameworks.",
    "authors": [
      "Hazem Ibrahim",
      "Fengyuan Liu",
      "Rohail Asim",
      "Balaraju Battu",
      "Sidahmed Benabderrahmane",
      "Bashar Alhafni",
      "Wifag Adnan",
      "Tuka Alhanai",
      "Bedoor AlShebli",
      "Riyadh Baghdadi",
      "Jocelyn J. Bélanger",
      "Elena Beretta",
      "Kemal Celik",
      "Moumena Chaqfeh",
      "Mohammed F. Daqaq",
      "Zaynab El Bernoussi",
      "Daryl Fougnie",
      "Borja Garcia de Soto",
      "Alberto Gandolfi",
      "Andras Gyorgy",
      "Nizar Habash",
      "J. Andrew Harris",
      "Aaron Kaufman",
      "Lefteris Kirousis",
      "Korhan Kocak",
      "Kangsan Lee",
      "Seungah S. Lee",
      "Samreen Malik",
      "Michail Maniatakos",
      "David Melcher",
      "Azzam Mourad",
      "Minsu Park",
      "Mahmoud Rasras",
      "Alicja Reuben",
      "Dania Zantout",
      "Nancy W. Gleason",
      "Kinga Makovi",
      "Talal Rahwan",
      "Yasir Zaki"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-05-07T10:37:51Z",
    "pdf_url": "https://arxiv.org/pdf/2305.13934v1"
  },
  {
    "arxiv_id": "2305.05377v1",
    "entry_id": "http://arxiv.org/abs/2305.05377v1",
    "title": "Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models",
    "summary": "The research creates a professional certification survey to test large language models and evaluate their employable skills. It compares the performance of two AI models, GPT-3 and Turbo-GPT3.5, on a benchmark dataset of 1149 professional certifications, emphasizing vocational readiness rather than academic performance. GPT-3 achieved a passing score (>70% correct) in 39% of the professional certifications without fine-tuning or exam preparation. The models demonstrated qualifications in various computer-related fields, such as cloud and virtualization, business analytics, cybersecurity, network setup and repair, and data analytics. Turbo-GPT3.5 scored 100% on the valuable Offensive Security Certified Professional (OSCP) exam. The models also displayed competence in other professional domains, including nursing, licensed counseling, pharmacy, and teaching. Turbo-GPT3.5 passed the Financial Industry Regulatory Authority (FINRA) Series 6 exam with a 70% grade without preparation. Interestingly, Turbo-GPT3.5 performed well on customer service tasks, suggesting potential applications in human augmentation for chatbots in call centers and routine advice services. The models also score well on sensory and experience-based tests such as wine sommelier, beer taster, emotional quotient, and body language reader. The OpenAI model improvement from Babbage to Turbo resulted in a median 60% better-graded performance in less than a few years. This progress suggests that focusing on the latest model's shortcomings could lead to a highly performant AI capable of mastering the most demanding professional certifications. We open-source the benchmark to expand the range of testable professional skills as the models improve or gain emergent capabilities.",
    "authors": [
      "David Noever",
      "Matt Ciolino"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-07T00:56:58Z",
    "pdf_url": "https://arxiv.org/pdf/2305.05377v1"
  },
  {
    "arxiv_id": "2305.03263v1",
    "entry_id": "http://arxiv.org/abs/2305.03263v1",
    "title": "Bayesian Reinforcement Learning with Limited Cognitive Load",
    "summary": "All biological and artificial agents must learn and make decisions given limits on their ability to process information. As such, a general theory of adaptive behavior should be able to account for the complex interactions between an agent's learning history, decisions, and capacity constraints. Recent work in computer science has begun to clarify the principles that shape these dynamics by bridging ideas from reinforcement learning, Bayesian decision-making, and rate-distortion theory. This body of work provides an account of capacity-limited Bayesian reinforcement learning, a unifying normative framework for modeling the effect of processing constraints on learning and action selection. Here, we provide an accessible review of recent algorithms and theoretical results in this setting, paying special attention to how these ideas can be applied to studying questions in the cognitive and behavioral sciences.",
    "authors": [
      "Dilip Arumugam",
      "Mark K. Ho",
      "Noah D. Goodman",
      "Benjamin Van Roy"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-05-05T03:29:34Z",
    "pdf_url": "https://arxiv.org/pdf/2305.03263v1"
  },
  {
    "arxiv_id": "2305.02750v2",
    "entry_id": "http://arxiv.org/abs/2305.02750v2",
    "title": "A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects",
    "summary": "Proactive dialogue systems, related to a wide range of real-world conversational applications, equip the conversational agent with the capability of leading the conversation direction towards achieving pre-defined targets or fulfilling certain goals from the system side. It is empowered by advanced techniques to progress to more complicated tasks that require strategical and motivational interactions. In this survey, we provide a comprehensive overview of the prominent problems and advanced designs for conversational agent's proactivity in different types of dialogues. Furthermore, we discuss challenges that meet the real-world application needs but require a greater research focus in the future. We hope that this first survey of proactive dialogue systems can provide the community with a quick access and an overall picture to this practical problem, and stimulate more progresses on conversational AI to the next level.",
    "authors": [
      "Yang Deng",
      "Wenqiang Lei",
      "Wai Lam",
      "Tat-Seng Chua"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-04T11:38:49Z",
    "pdf_url": "https://arxiv.org/pdf/2305.02750v2"
  },
  {
    "arxiv_id": "2305.02531v6",
    "entry_id": "http://arxiv.org/abs/2305.02531v6",
    "title": "Can LLMs Capture Human Preferences?",
    "summary": "We explore the viability of Large Language Models (LLMs), specifically OpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting preferences, with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking, we examine responses from LLMs across various languages and compare them to human responses, exploring preferences between smaller, sooner, and larger, later rewards. Our findings reveal that both GPT models demonstrate less patience than humans, with GPT-3.5 exhibiting a lexicographic preference for earlier rewards, unlike human decision-makers. Though GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans. Interestingly, GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions, a procedure we term \"chain-of-thought conjoint,\" can mitigate, but does not eliminate, discrepancies between LLM and human responses. While directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings of preferences. Chain-of-thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.",
    "authors": [
      "Ali Goli",
      "Amandeep Singh"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-04T03:51:31Z",
    "pdf_url": "https://arxiv.org/pdf/2305.02531v6"
  },
  {
    "arxiv_id": "2305.02251v2",
    "entry_id": "http://arxiv.org/abs/2305.02251v2",
    "title": "Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems",
    "summary": "The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a \"big picture\" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality scientific discoveries highly autonomously at a level comparable, and possibly superior, to the best human scientists by 2050.",
    "authors": [
      "Stefan Kramer",
      "Mattia Cerrato",
      "Jannis Brugger",
      "Sašo Džeroski",
      "Ross King"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-03T16:35:41Z",
    "pdf_url": "https://arxiv.org/pdf/2305.02251v2"
  },
  {
    "arxiv_id": "2305.01941v1",
    "entry_id": "http://arxiv.org/abs/2305.01941v1",
    "title": "Exploring the Protein Sequence Space with Global Generative Models",
    "summary": "Recent advancements in specialized large-scale architectures for training image and language have profoundly impacted the field of computer vision and natural language processing (NLP). Language models, such as the recent ChatGPT and GPT4 have demonstrated exceptional capabilities in processing, translating, and generating human languages. These breakthroughs have also been reflected in protein research, leading to the rapid development of numerous new methods in a short time, with unprecedented performance. Language models, in particular, have seen widespread use in protein research, as they have been utilized to embed proteins, generate novel ones, and predict tertiary structures. In this book chapter, we provide an overview of the use of protein generative models, reviewing 1) language models for the design of novel artificial proteins, 2) works that use non-Transformer architectures, and 3) applications in directed evolution approaches.",
    "authors": [
      "Sergio Romero-Romero",
      "Sebastian Lindner",
      "Noelia Ferruz"
    ],
    "categories": [
      "q-bio.BM",
      "cs.LG"
    ],
    "published": "2023-05-03T07:45:29Z",
    "pdf_url": "https://arxiv.org/pdf/2305.01941v1"
  },
  {
    "arxiv_id": "2305.01263v2",
    "entry_id": "http://arxiv.org/abs/2305.01263v2",
    "title": "How Simulation Helps Autonomous Driving:A Survey of Sim2real, Digital Twins, and Parallel Intelligence",
    "summary": "Safety and cost are two important concerns for the development of autonomous driving technologies. From the academic research to commercial applications of autonomous driving vehicles, sufficient simulation and real world testing are required. In general, a large scale of testing in simulation environment is conducted and then the learned driving knowledge is transferred to the real world, so how to adapt driving knowledge learned in simulation to reality becomes a critical issue. However, the virtual simulation world differs from the real world in many aspects such as lighting, textures, vehicle dynamics, and agents' behaviors, etc., which makes it difficult to bridge the gap between the virtual and real worlds. This gap is commonly referred to as the reality gap (RG). In recent years, researchers have explored various approaches to address the reality gap issue, which can be broadly classified into three categories: transferring knowledge from simulation to reality (sim2real), learning in digital twins (DTs), and learning by parallel intelligence (PI) technologies. In this paper, we consider the solutions through the sim2real, DTs, and PI technologies, and review important applications and innovations in the field of autonomous driving. Meanwhile, we show the state-of-the-arts from the views of algorithms, models, and simulators, and elaborate the development process from sim2real to DTs and PI. The presentation also illustrates the far-reaching effects and challenges in the development of sim2real, DTs, and PI in autonomous driving.",
    "authors": [
      "Xuemin Hu",
      "Shen Li",
      "Tingyu Huang",
      "Bo Tang",
      "Rouxing Huai",
      "Long Chen"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2023-05-02T09:00:32Z",
    "pdf_url": "https://arxiv.org/pdf/2305.01263v2"
  },
  {
    "arxiv_id": "2305.00955v2",
    "entry_id": "http://arxiv.org/abs/2305.00955v2",
    "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation",
    "summary": "Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.",
    "authors": [
      "Patrick Fernandes",
      "Aman Madaan",
      "Emmy Liu",
      "António Farinhas",
      "Pedro Henrique Martins",
      "Amanda Bertsch",
      "José G. C. de Souza",
      "Shuyan Zhou",
      "Tongshuang Wu",
      "Graham Neubig",
      "André F. T. Martins"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-05-01T17:36:06Z",
    "pdf_url": "https://arxiv.org/pdf/2305.00955v2"
  },
  {
    "arxiv_id": "2305.00844v1",
    "entry_id": "http://arxiv.org/abs/2305.00844v1",
    "title": "Automated Paper Screening for Clinical Reviews Using Large Language Models",
    "summary": "Objective: To assess the performance of the OpenAI GPT API in accurately and efficiently identifying relevant titles and abstracts from real-world clinical review datasets and compare its performance against ground truth labelling by two independent human reviewers.\n  Methods: We introduce a novel workflow using the OpenAI GPT API for screening titles and abstracts in clinical reviews. A Python script was created to make calls to the GPT API with the screening criteria in natural language and a corpus of title and abstract datasets that have been filtered by a minimum of two human reviewers. We compared the performance of our model against human-reviewed papers across six review papers, screening over 24,000 titles and abstracts.\n  Results: Our results show an accuracy of 0.91, a sensitivity of excluded papers of 0.91, and a sensitivity of included papers of 0.76. On a randomly selected subset of papers, the GPT API demonstrated the ability to provide reasoning for its decisions and corrected its initial decision upon being asked to explain its reasoning for a subset of incorrect classifications.\n  Conclusion: The GPT API has the potential to streamline the clinical review process, save valuable time and effort for researchers, and contribute to the overall quality of clinical reviews. By prioritizing the workflow and acting as an aid rather than a replacement for researchers and reviewers, the GPT API can enhance efficiency and lead to more accurate and reliable conclusions in medical research.",
    "authors": [
      "Eddie Guo",
      "Mehul Gupta",
      "Jiawen Deng",
      "Ye-Jean Park",
      "Mike Paget",
      "Christopher Naugler"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-05-01T14:16:37Z",
    "pdf_url": "https://arxiv.org/pdf/2305.00844v1"
  },
  {
    "arxiv_id": "2304.14670v2",
    "entry_id": "http://arxiv.org/abs/2304.14670v2",
    "title": "Prompt Engineering for Healthcare: Methodologies and Applications",
    "summary": "Prompt engineering is a critical technique in the field of natural language processing that involves designing and optimizing the prompts used to input information into models, aiming to enhance their performance on specific tasks. With the recent advancements in large language models, prompt engineering has shown significant superiority across various domains and has become increasingly important in the healthcare domain. However, there is a lack of comprehensive reviews specifically focusing on prompt engineering in the medical field. This review will introduce the latest advances in prompt engineering in the field of natural language processing for the medical field. First, we will provide the development of prompt engineering and emphasize its significant contributions to healthcare natural language processing applications such as question-answering systems, text summarization, and machine translation. With the continuous improvement of general large language models, the importance of prompt engineering in the healthcare domain is becoming increasingly prominent. The aim of this article is to provide useful resources and bridges for healthcare natural language processing researchers to better explore the application of prompt engineering in this field. We hope that this review can provide new ideas and inspire for research and application in medical natural language processing.",
    "authors": [
      "Jiaqi Wang",
      "Enze Shi",
      "Sigang Yu",
      "Zihao Wu",
      "Chong Ma",
      "Haixing Dai",
      "Qiushi Yang",
      "Yanqing Kang",
      "Jinru Wu",
      "Huawen Hu",
      "Chenxi Yue",
      "Haiyang Zhang",
      "Yiheng Liu",
      "Yi Pan",
      "Zhengliang Liu",
      "Lichao Sun",
      "Xiang Li",
      "Bao Ge",
      "Xi Jiang",
      "Dajiang Zhu",
      "Yixuan Yuan",
      "Dinggang Shen",
      "Tianming Liu",
      "Shu Zhang"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-04-28T08:03:42Z",
    "pdf_url": "https://arxiv.org/pdf/2304.14670v2"
  },
  {
    "arxiv_id": "2304.13712v2",
    "entry_id": "http://arxiv.org/abs/2304.13712v2",
    "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
    "summary": "This paper presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream natural language processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. Firstly, we offer an introduction and brief summary of current GPT- and BERT-style LLMs. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, natural language generation tasks, emergent abilities, and considerations for specific tasks.We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at \\url{https://github.com/Mooler0410/LLMsPracticalGuide}.",
    "authors": [
      "Jingfeng Yang",
      "Hongye Jin",
      "Ruixiang Tang",
      "Xiaotian Han",
      "Qizhang Feng",
      "Haoming Jiang",
      "Bing Yin",
      "Xia Hu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-04-26T17:52:30Z",
    "pdf_url": "https://arxiv.org/pdf/2304.13712v2"
  },
  {
    "arxiv_id": "2304.13191v1",
    "entry_id": "http://arxiv.org/abs/2304.13191v1",
    "title": "Towards Explainable and Safe Conversational Agents for Mental Health: A Survey",
    "summary": "Virtual Mental Health Assistants (VMHAs) are seeing continual advancements to support the overburdened global healthcare system that gets 60 million primary care visits, and 6 million Emergency Room (ER) visits annually. These systems are built by clinical psychologists, psychiatrists, and Artificial Intelligence (AI) researchers for Cognitive Behavioral Therapy (CBT). At present, the role of VMHAs is to provide emotional support through information, focusing less on developing a reflective conversation with the patient. A more comprehensive, safe and explainable approach is required to build responsible VMHAs to ask follow-up questions or provide a well-informed response. This survey offers a systematic critical review of the existing conversational agents in mental health, followed by new insights into the improvements of VMHAs with contextual knowledge, datasets, and their emerging role in clinical decision support. We also provide new directions toward enriching the user experience of VMHAs with explainability, safety, and wholesome trustworthiness. Finally, we provide evaluation metrics and practical considerations for VMHAs beyond the current literature to build trust between VMHAs and patients in active communications.",
    "authors": [
      "Surjodeep Sarkar",
      "Manas Gaur",
      "L. Chen",
      "Muskan Garg",
      "Biplav Srivastava",
      "Bhaktee Dongaonkar"
    ],
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-04-25T23:12:13Z",
    "pdf_url": "https://arxiv.org/pdf/2304.13191v1"
  },
  {
    "arxiv_id": "2304.12667v1",
    "entry_id": "http://arxiv.org/abs/2304.12667v1",
    "title": "Disagreement amongst counterfactual explanations: How transparency can be deceptive",
    "summary": "Counterfactual explanations are increasingly used as an Explainable Artificial Intelligence (XAI) technique to provide stakeholders of complex machine learning algorithms with explanations for data-driven decisions. The popularity of counterfactual explanations resulted in a boom in the algorithms generating them. However, not every algorithm creates uniform explanations for the same instance. Even though in some contexts multiple possible explanations are beneficial, there are circumstances where diversity amongst counterfactual explanations results in a potential disagreement problem among stakeholders. Ethical issues arise when for example, malicious agents use this diversity to fairwash an unfair machine learning model by hiding sensitive features. As legislators worldwide tend to start including the right to explanations for data-driven, high-stakes decisions in their policies, these ethical issues should be understood and addressed. Our literature review on the disagreement problem in XAI reveals that this problem has never been empirically assessed for counterfactual explanations. Therefore, in this work, we conduct a large-scale empirical analysis, on 40 datasets, using 12 explanation-generating methods, for two black-box models, yielding over 192.0000 explanations. Our study finds alarmingly high disagreement levels between the methods tested. A malicious user is able to both exclude and include desired features when multiple counterfactual explanations are available. This disagreement seems to be driven mainly by the dataset characteristics and the type of counterfactual algorithm. XAI centers on the transparency of algorithmic decision-making, but our analysis advocates for transparency about this self-proclaimed transparency",
    "authors": [
      "Dieter Brughmans",
      "Lissa Melis",
      "David Martens"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-04-25T09:15:37Z",
    "pdf_url": "https://arxiv.org/pdf/2304.12667v1"
  },
  {
    "arxiv_id": "2304.12479v5",
    "entry_id": "http://arxiv.org/abs/2304.12479v5",
    "title": "AGI: Artificial General Intelligence for Education",
    "summary": "Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This position paper reviews AGI's key concepts, capabilities, scope, and potential within future education, including achieving future educational goals, designing pedagogy and curriculum, and performing assessments. It highlights that AGI can significantly improve intelligent tutoring systems, educational assessment, and evaluation procedures. AGI systems can adapt to individual student needs, offering tailored learning experiences. They can also provide comprehensive feedback on student performance and dynamically adjust teaching methods based on student progress. The paper emphasizes that AGI's capabilities extend to understanding human emotions and social interactions, which are critical in educational settings. The paper discusses that ethical issues in education with AGI include data bias, fairness, and privacy and emphasizes the need for codes of conduct to ensure responsible AGI use in academic settings like homework, teaching, and recruitment. We also conclude that the development of AGI necessitates interdisciplinary collaborations between educators and AI engineers to advance research and application efforts.",
    "authors": [
      "Ehsan Latif",
      "Gengchen Mai",
      "Matthew Nyaaba",
      "Xuansheng Wu",
      "Ninghao Liu",
      "Guoyu Lu",
      "Sheng Li",
      "Tianming Liu",
      "Xiaoming Zhai"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-04-24T22:31:59Z",
    "pdf_url": "https://arxiv.org/pdf/2304.12479v5"
  },
  {
    "arxiv_id": "2304.12284v2",
    "entry_id": "http://arxiv.org/abs/2304.12284v2",
    "title": "Synthpop++: A Hybrid Framework for Generating A Country-scale Synthetic Population",
    "summary": "Population censuses are vital to public policy decision-making. They provide insight into human resources, demography, culture, and economic structure at local, regional, and national levels. However, such surveys are very expensive (especially for low and middle-income countries with high populations, such as India), time-consuming, and may also raise privacy concerns, depending upon the kinds of data collected.\n  In light of these issues, we introduce SynthPop++, a novel hybrid framework, which can combine data from multiple real-world surveys (with different, partially overlapping sets of attributes) to produce a real-scale synthetic population of humans. Critically, our population maintains family structures comprising individuals with demographic, socioeconomic, health, and geolocation attributes: this means that our ``fake'' people live in realistic locations, have realistic families, etc. Such data can be used for a variety of purposes: we explore one such use case, Agent-based modelling of infectious disease in India.\n  To gauge the quality of our synthetic population, we use both machine learning and statistical metrics. Our experimental results show that synthetic population can realistically simulate the population for various administrative units of India, producing real-scale, detailed data at the desired level of zoom -- from cities, to districts, to states, eventually combining to form a country-scale synthetic population.",
    "authors": [
      "Bhavesh Neekhra",
      "Kshitij Kapoor",
      "Debayan Gupta"
    ],
    "categories": [
      "cs.LG",
      "cs.CY"
    ],
    "published": "2023-04-24T17:27:56Z",
    "pdf_url": "https://arxiv.org/pdf/2304.12284v2"
  },
  {
    "arxiv_id": "2304.11257v1",
    "entry_id": "http://arxiv.org/abs/2304.11257v1",
    "title": "Who's the Best Detective? LLMs vs. MLs in Detecting Incoherent Fourth Grade Math Answers",
    "summary": "Written answers to open-ended questions can have a higher long-term effect on learning than multiple-choice questions. However, it is critical that teachers immediately review the answers, and ask to redo those that are incoherent. This can be a difficult task and can be time-consuming for teachers. A possible solution is to automate the detection of incoherent answers. One option is to automate the review with Large Language Models (LLM). In this paper, we analyze the responses of fourth graders in mathematics using three LLMs: GPT-3, BLOOM, and YOU. We used them with zero, one, two, three and four shots. We compared their performance with the results of various classifiers trained with Machine Learning (ML). We found that LLMs perform worse than MLs in detecting incoherent answers. The difficulty seems to reside in recursive questions that contain both questions and answers, and in responses from students with typical fourth-grader misspellings. Upon closer examination, we have found that the ChatGPT model faces the same challenges.",
    "authors": [
      "Felipe Urrutia",
      "Roberto Araya"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-04-21T21:25:30Z",
    "pdf_url": "https://arxiv.org/pdf/2304.11257v1"
  },
  {
    "arxiv_id": "2304.10691v2",
    "entry_id": "http://arxiv.org/abs/2304.10691v2",
    "title": "SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model",
    "summary": "Skin and subcutaneous diseases rank high among the leading contributors to the global burden of nonfatal diseases, impacting a considerable portion of the population. Nonetheless, the field of dermatology diagnosis faces three significant hurdles. Firstly, there is a shortage of dermatologists accessible to diagnose patients, particularly in rural regions. Secondly, accurately interpreting skin disease images poses a considerable challenge. Lastly, generating patient-friendly diagnostic reports is usually a time-consuming and labor-intensive task for dermatologists. To tackle these challenges, we present SkinGPT-4, which is the world's first interactive dermatology diagnostic system powered by an advanced visual large language model. SkinGPT-4 leverages a fine-tuned version of MiniGPT-4, trained on an extensive collection of skin disease images (comprising 52,929 publicly available and proprietary images) along with clinical concepts and doctors' notes. We designed a two-step training process to allow SkinGPT to express medical features in skin disease images with natural language and make accurate diagnoses of the types of skin diseases. With SkinGPT-4, users could upload their own skin photos for diagnosis, and the system could autonomously evaluate the images, identifies the characteristics and categories of the skin conditions, performs in-depth analysis, and provides interactive treatment recommendations. Meanwhile, SkinGPT-4's local deployment capability and commitment to user privacy also render it an appealing choice for patients in search of a dependable and precise diagnosis of their skin ailments. To demonstrate the robustness of SkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, which were independently reviewed by certified dermatologists, and showed that SkinGPT-4 could provide accurate diagnoses of skin diseases.",
    "authors": [
      "Juexiao Zhou",
      "Xiaonan He",
      "Liyuan Sun",
      "Jiannan Xu",
      "Xiuying Chen",
      "Yuetan Chu",
      "Longxi Zhou",
      "Xingyu Liao",
      "Bin Zhang",
      "Xin Gao"
    ],
    "categories": [
      "eess.IV",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-04-21T01:17:09Z",
    "pdf_url": "https://arxiv.org/pdf/2304.10691v2"
  },
  {
    "arxiv_id": "2304.09948v1",
    "entry_id": "http://arxiv.org/abs/2304.09948v1",
    "title": "Catch Me If You Can: Identifying Fraudulent Physician Reviews with Large Language Models Using Generative Pre-Trained Transformers",
    "summary": "The proliferation of fake reviews of doctors has potentially detrimental consequences for patient well-being and has prompted concern among consumer protection groups and regulatory bodies. Yet despite significant advancements in the fields of machine learning and natural language processing, there remains limited comprehension of the characteristics differentiating fraudulent from authentic reviews. This study utilizes a novel pre-labeled dataset of 38048 physician reviews to establish the effectiveness of large language models in classifying reviews. Specifically, we compare the performance of traditional ML models, such as logistic regression and support vector machines, to generative pre-trained transformer models. Furthermore, we use GPT4, the newest model in the GPT family, to uncover the key dimensions along which fake and genuine physician reviews differ. Our findings reveal significantly superior performance of GPT-3 over traditional ML models in this context. Additionally, our analysis suggests that GPT3 requires a smaller training sample than traditional models, suggesting its appropriateness for tasks with scarce training data. Moreover, the superiority of GPT3 performance increases in the cold start context i.e., when there are no prior reviews of a doctor. Finally, we employ GPT4 to reveal the crucial dimensions that distinguish fake physician reviews. In sharp contrast to previous findings in the literature that were obtained using simulated data, our findings from a real-world dataset show that fake reviews are generally more clinically detailed, more reserved in sentiment, and have better structure and grammar than authentic ones.",
    "authors": [
      "Aishwarya Deep Shukla",
      "Laksh Agarwal",
      "Jie Mein",
      "Goh",
      "Guodong",
      "Gao",
      "Ritu Agarwal"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-04-19T19:59:26Z",
    "pdf_url": "https://arxiv.org/pdf/2304.09948v1"
  },
  {
    "arxiv_id": "2304.09803v1",
    "entry_id": "http://arxiv.org/abs/2304.09803v1",
    "title": "On the Perception of Difficulty: Differences between Humans and AI",
    "summary": "With the increased adoption of artificial intelligence (AI) in industry and society, effective human-AI interaction systems are becoming increasingly important. A central challenge in the interaction of humans with AI is the estimation of difficulty for human and AI agents for single task instances.These estimations are crucial to evaluate each agent's capabilities and, thus, required to facilitate effective collaboration. So far, research in the field of human-AI interaction estimates the perceived difficulty of humans and AI independently from each other. However, the effective interaction of human and AI agents depends on metrics that accurately reflect each agent's perceived difficulty in achieving valuable outcomes. Research to date has not yet adequately examined the differences in the perceived difficulty of humans and AI. Thus, this work reviews recent research on the perceived difficulty in human-AI interaction and contributing factors to consistently compare each agent's perceived difficulty, e.g., creating the same prerequisites. Furthermore, we present an experimental design to thoroughly examine the perceived difficulty of both agents and contribute to a better understanding of the design of such systems.",
    "authors": [
      "Philipp Spitzer",
      "Joshua Holstein",
      "Michael Vössing",
      "Niklas Kühl"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-04-19T16:42:54Z",
    "pdf_url": "https://arxiv.org/pdf/2304.09803v1"
  },
  {
    "arxiv_id": "2304.09064v1",
    "entry_id": "http://arxiv.org/abs/2304.09064v1",
    "title": "LLM-based Interaction for Content Generation: A Case Study on the Perception of Employees in an IT department",
    "summary": "In the past years, AI has seen many advances in the field of NLP. This has led to the emergence of LLMs, such as the now famous GPT-3.5, which revolutionise the way humans can access or generate content. Current studies on LLM-based generative tools are mainly interested in the performance of such tools in generating relevant content (code, text or image). However, ethical concerns related to the design and use of generative tools seem to be growing, impacting the public acceptability for specific tasks. This paper presents a questionnaire survey to identify the intention to use generative tools by employees of an IT company in the context of their work. This survey is based on empirical models measuring intention to use (TAM by Davis, 1989, and UTAUT2 by Venkatesh and al., 2008). Our results indicate a rather average acceptability of generative tools, although the more useful the tool is perceived to be, the higher the intention to use seems to be. Furthermore, our analyses suggest that the frequency of use of generative tools is likely to be a key factor in understanding how employees perceive these tools in the context of their work. Following on from this work, we plan to investigate the nature of the requests that may be made to these tools by specific audiences.",
    "authors": [
      "Alexandre Agossah",
      "Frédérique Krupa",
      "Matthieu Perreira Da Silva",
      "Patrick Le Callet"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-04-18T15:35:43Z",
    "pdf_url": "https://arxiv.org/pdf/2304.09064v1"
  },
  {
    "arxiv_id": "2304.07840v2",
    "entry_id": "http://arxiv.org/abs/2304.07840v2",
    "title": "Enhancing Automated Program Repair through Fine-tuning and Prompt Engineering",
    "summary": "Sequence-to-sequence models have been used to transform erroneous programs into correct ones when trained with a large enough dataset. Some recent studies also demonstrated strong empirical evidence that code review could improve the program repair further. Large language models, trained with Natural Language (NL) and Programming Language (PL), can contain inherent knowledge of both. In this study, we investigate if this inherent knowledge of PL and NL can be utilized to improve automated program repair. We applied PLBART and CodeT5, two state-of-the-art language models that are pre-trained with both PL and NL, on two such natural language-based program repair datasets and found that the pre-trained language models fine-tuned with datasets containing both code review and subsequent code changes notably outperformed each of the previous models. With the advent of code generative models like Codex and GPT-3.5-Turbo, we also performed zero-shot and few-shots learning-based prompt engineering to assess their performance on these datasets. However, the practical application of using LLMs in the context of automated program repair is still a long way off based on our manual analysis of the generated repaired codes by the learning models.",
    "authors": [
      "Rishov Paul",
      "Md. Mohib Hossain",
      "Mohammed Latif Siddiq",
      "Masum Hasan",
      "Anindya Iqbal",
      "Joanna C. S. Santos"
    ],
    "categories": [
      "cs.LG",
      "cs.SE"
    ],
    "published": "2023-04-16T17:29:51Z",
    "pdf_url": "https://arxiv.org/pdf/2304.07840v2"
  },
  {
    "arxiv_id": "2305.03123v4",
    "entry_id": "http://arxiv.org/abs/2305.03123v4",
    "title": "ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review",
    "summary": "ChatGPT is another large language model (LLM) vastly available for the consumers on their devices but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail the issues and concerns raised over chatGPT in line with aforementioned characteristics. We also discuss the recent EU AI Act briefly in accordance with the SPADE evaluation. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also suggest some policies and recommendations for EU AI policy act concerning ethics, digital divide, and sustainability",
    "authors": [
      "Sunder Ali Khowaja",
      "Parus Khuwaja",
      "Kapal Dev",
      "Weizheng Wang",
      "Lewis Nkenyereye"
    ],
    "categories": [
      "cs.CY",
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-04-13T16:01:28Z",
    "pdf_url": "https://arxiv.org/pdf/2305.03123v4"
  },
  {
    "arxiv_id": "2304.03589v1",
    "entry_id": "http://arxiv.org/abs/2304.03589v1",
    "title": "On Efficient Training of Large-Scale Deep Learning Models: A Literature Review",
    "summary": "The field of deep learning has witnessed significant progress, particularly in computer vision (CV), natural language processing (NLP), and speech. The use of large-scale models trained on vast amounts of data holds immense promise for practical applications, enhancing industrial productivity and facilitating social development. With the increasing demands on computational capacity, though numerous studies have explored the efficient training, a comprehensive summarization on acceleration techniques of training deep learning models is still much anticipated. In this survey, we present a detailed review for training acceleration. We consider the fundamental update formulation and split its basic components into five main perspectives: (1) data-centric: including dataset regularization, data sampling, and data-centric curriculum learning techniques, which can significantly reduce the computational complexity of the data samples; (2) model-centric, including acceleration of basic modules, compression training, model initialization and model-centric curriculum learning techniques, which focus on accelerating the training via reducing the calculations on parameters; (3) optimization-centric, including the selection of learning rate, the employment of large batchsize, the designs of efficient objectives, and model average techniques, which pay attention to the training policy and improving the generality for the large-scale models; (4) budgeted training, including some distinctive acceleration methods on source-constrained situations; (5) system-centric, including some efficient open-source distributed libraries/systems which provide adequate hardware support for the implementation of acceleration algorithms. By presenting this comprehensive taxonomy, our survey presents a comprehensive review to understand the general mechanisms within each component and their joint interaction.",
    "authors": [
      "Li Shen",
      "Yan Sun",
      "Zhiyuan Yu",
      "Liang Ding",
      "Xinmei Tian",
      "Dacheng Tao"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.DC"
    ],
    "published": "2023-04-07T11:13:23Z",
    "pdf_url": "https://arxiv.org/pdf/2304.03589v1"
  },
  {
    "arxiv_id": "2304.03394v2",
    "entry_id": "http://arxiv.org/abs/2304.03394v2",
    "title": "Deep Learning for Opinion Mining and Topic Classification of Course Reviews",
    "summary": "Student opinions for a course are important to educators and administrators, regardless of the type of the course or the institution. Reading and manually analyzing open-ended feedback becomes infeasible for massive volumes of comments at institution level or online forums. In this paper, we collected and pre-processed a large number of course reviews publicly available online. We applied machine learning techniques with the goal to gain insight into student sentiments and topics. Specifically, we utilized current Natural Language Processing (NLP) techniques, such as word embeddings and deep neural networks, and state-of-the-art BERT (Bidirectional Encoder Representations from Transformers), RoBERTa (Robustly optimized BERT approach) and XLNet (Generalized Auto-regression Pre-training). We performed extensive experimentation to compare these techniques versus traditional approaches. This comparative study demonstrates how to apply modern machine learning approaches for sentiment polarity extraction and topic-based classification utilizing course feedback. For sentiment polarity, the top model was RoBERTa with 95.5% accuracy and 84.7% F1-macro, while for topic classification, an SVM (Support Vector Machine) was the top classifier with 79.8% accuracy and 80.6% F1-macro. We also provided an in-depth exploration of the effect of certain hyperparameters on the model performance and discussed our observations. These findings can be used by institutions and course providers as a guide for analyzing their own course feedback using NLP models towards self-evaluation and improvement.",
    "authors": [
      "Anna Koufakou"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-04-06T21:48:29Z",
    "pdf_url": "https://arxiv.org/pdf/2304.03394v2"
  },
  {
    "arxiv_id": "2304.08497v1",
    "entry_id": "http://arxiv.org/abs/2304.08497v1",
    "title": "Agent-Based Modeling and its Tradeoffs: An Introduction & Examples",
    "summary": "Agent-based modeling is a computational dynamic modeling technique that may be less familiar to some readers. Agent-based modeling seeks to understand the behaviour of complex systems by situating agents in an environment and studying the emergent outcomes of agent-agent and agent-environment interactions. In comparison with compartmental models, agent-based models offer simpler, more scalable and flexible representation of heterogeneity, the ability to capture dynamic and static network and spatial context, and the ability to consider history of individuals within the model. In contrast, compartmental models offer faster development time with less programming required, lower computational requirements that do not scale with population, and the option for concise mathematical formulation with ordinary, delay or stochastic differential equations supporting derivation of properties of the system behaviour. In this chapter, basic characteristics of agent-based models are introduced, advantages and disadvantages of agent-based models, as compared with compartmental models, are discussed, and two example agent-based infectious disease models are reviewed.",
    "authors": [
      "G. Wade McDonald",
      "Nathaniel D. Osgood"
    ],
    "categories": [
      "cs.MA",
      "cs.CE"
    ],
    "published": "2023-04-06T15:51:15Z",
    "pdf_url": "https://arxiv.org/pdf/2304.08497v1"
  },
  {
    "arxiv_id": "2304.02697v1",
    "entry_id": "http://arxiv.org/abs/2304.02697v1",
    "title": "Revolutionizing Single Cell Analysis: The Power of Large Language Models for Cell Type Annotation",
    "summary": "In recent years, single cell RNA sequencing has become a widely used technique to study cellular diversity and function. However, accurately annotating cell types from single cell data has been a challenging task, as it requires extensive knowledge of cell biology and gene function. The emergence of large language models such as ChatGPT and New Bing in 2023 has revolutionized this process by integrating the scientific literature and providing accurate annotations of cell types. This breakthrough enables researchers to conduct literature reviews more efficiently and accurately, and can potentially uncover new insights into cell type annotation. By using ChatGPT to annotate single cell data, we can relate rare cell type to their function and reveal specific differentiation trajectories of cell subtypes that were previously overlooked. This can have important applications in understanding cancer progression, mammalian development, and stem cell differentiation, and can potentially lead to the discovery of key cells that interrupt the differentiation pathway and solve key problems in the life sciences. Overall, the future of cell type annotation in single cell data looks promising and the Large Language model will be an important milestone in the history of single cell analysis.",
    "authors": [
      "Zehua Zeng",
      "Hongwu Du"
    ],
    "categories": [
      "q-bio.GN",
      "cs.LG"
    ],
    "published": "2023-04-05T18:45:54Z",
    "pdf_url": "https://arxiv.org/pdf/2304.02697v1"
  },
  {
    "arxiv_id": "2304.02213v5",
    "entry_id": "http://arxiv.org/abs/2304.02213v5",
    "title": "Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT",
    "summary": "The amount of data has growing significance in exploring cutting-edge materials and a number of datasets have been generated either by hand or automated approaches. However, the materials science field struggles to effectively utilize the abundance of data, especially in applied disciplines where materials are evaluated based on device performance rather than their properties. This article presents a new natural language processing (NLP) task called structured information inference (SII) to address the complexities of information extraction at the device level in materials science. We accomplished this task by tuning GPT-3 on an existing perovskite solar cell FAIR (Findable, Accessible, Interoperable, Reusable) dataset with 91.8% F1-score and extended the dataset with data published since its release. The produced data is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature empowers materials scientists to develop models by selecting high-quality review articles within their domain. Additionally, we designed experiments to predict the electrical performance of solar cells and design materials or devices with targeted parameters using large language models (LLMs). Our results demonstrate comparable performance to traditional machine learning methods without feature selection, highlighting the potential of LLMs to acquire scientific knowledge and design new materials akin to materials scientists.",
    "authors": [
      "Tong Xie",
      "Yuwei Wan",
      "Wei Huang",
      "Yufei Zhou",
      "Yixuan Liu",
      "Qingyuan Linghu",
      "Shaozhou Wang",
      "Chunyu Kit",
      "Clara Grazian",
      "Wenjie Zhang",
      "Bram Hoex"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-04-05T04:01:52Z",
    "pdf_url": "https://arxiv.org/pdf/2304.02213v5"
  },
  {
    "arxiv_id": "2304.00612v1",
    "entry_id": "http://arxiv.org/abs/2304.00612v1",
    "title": "Eight Things to Know about Large Language Models",
    "summary": "The widespread public deployment of large language models (LLMs) in recent months has prompted a wave of new attention and engagement from advocates, policymakers, and scholars from many fields. This attention is a timely response to the many urgent questions that this technology raises, but it can sometimes miss important considerations. This paper surveys the evidence for eight potentially surprising such points:\n  1. LLMs predictably get more capable with increasing investment, even without targeted innovation.\n  2. Many important LLM behaviors emerge unpredictably as a byproduct of increasing investment.\n  3. LLMs often appear to learn and use representations of the outside world.\n  4. There are no reliable techniques for steering the behavior of LLMs.\n  5. Experts are not yet able to interpret the inner workings of LLMs.\n  6. Human performance on a task isn't an upper bound on LLM performance.\n  7. LLMs need not express the values of their creators nor the values encoded in web text.\n  8. Brief interactions with LLMs are often misleading.",
    "authors": [
      "Samuel R. Bowman"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-04-02T20:03:27Z",
    "pdf_url": "https://arxiv.org/pdf/2304.00612v1"
  },
  {
    "arxiv_id": "2304.00399v1",
    "entry_id": "http://arxiv.org/abs/2304.00399v1",
    "title": "From Zero to Hero: Convincing with Extremely Complicated Math",
    "summary": "Becoming a (super) hero is almost every kid's dream. During their sheltered childhood, they do whatever it takes to grow up to be one. Work hard, play hard -- all day long. But as they're getting older, distractions are more and more likely to occur. They're getting off track. They start discovering what is feared as simple math. Finally, they end up as a researcher, writing boring, non-impressive papers all day long because they only rely on simple mathematics. No top-tier conferences, no respect, no groupies. Life's over.\n  To finally put an end to this tragedy, we propose a fundamentally new algorithm, dubbed zero2hero, that turns every research paper into a scientific masterpiece. Given a LaTeX document containing ridiculously simple math, based on next-generation large language models, our system automatically over-complicates every single equation so that no one, including yourself, is able to understand what the hell is going on. Future reviewers will be blown away by the complexity of your equations, immediately leading to acceptance. zero2hero gets you back on track, because you deserve to be a hero$^{\\text{TM}}$. Code leaked at \\url{https://github.com/mweiherer/zero2hero}.",
    "authors": [
      "Maximilian Weiherer",
      "Bernhard Egger"
    ],
    "categories": [
      "cs.CV",
      "cs.AI"
    ],
    "published": "2023-04-01T22:09:35Z",
    "pdf_url": "https://arxiv.org/pdf/2304.00399v1"
  },
  {
    "arxiv_id": "2303.18223v16",
    "entry_id": "http://arxiv.org/abs/2303.18223v16",
    "title": "A Survey of Large Language Models",
    "summary": "Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.",
    "authors": [
      "Wayne Xin Zhao",
      "Kun Zhou",
      "Junyi Li",
      "Tianyi Tang",
      "Xiaolei Wang",
      "Yupeng Hou",
      "Yingqian Min",
      "Beichen Zhang",
      "Junjie Zhang",
      "Zican Dong",
      "Yifan Du",
      "Chen Yang",
      "Yushuo Chen",
      "Zhipeng Chen",
      "Jinhao Jiang",
      "Ruiyang Ren",
      "Yifan Li",
      "Xinyu Tang",
      "Zikang Liu",
      "Peiyu Liu",
      "Jian-Yun Nie",
      "Ji-Rong Wen"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-03-31T17:28:46Z",
    "pdf_url": "https://arxiv.org/pdf/2303.18223v16"
  },
  {
    "arxiv_id": "2303.14143v1",
    "entry_id": "http://arxiv.org/abs/2303.14143v1",
    "title": "\"Get ready for a party\": Exploring smarter smart spaces with help from large language models",
    "summary": "The right response to someone who says \"get ready for a party\" is deeply influenced by meaning and context. For a smart home assistant (e.g., Google Home), the ideal response might be to survey the available devices in the home and change their state to create a festive atmosphere. Current practical systems cannot service such requests since they require the ability to (1) infer meaning behind an abstract statement and (2) map that inference to a concrete course of action appropriate for the context (e.g., changing the settings of specific devices). In this paper, we leverage the observation that recent task-agnostic large language models (LLMs) like GPT-3 embody a vast amount of cross-domain, sometimes unpredictable contextual knowledge that existing rule-based home assistant systems lack, which can make them powerful tools for inferring user intent and generating appropriate context-dependent responses during smart home interactions. We first explore the feasibility of a system that places an LLM at the center of command inference and action planning, showing that LLMs have the capacity to infer intent behind vague, context-dependent commands like \"get ready for a party\" and respond with concrete, machine-parseable instructions that can be used to control smart devices. We furthermore demonstrate a proof-of-concept implementation that puts an LLM in control of real devices, showing its ability to infer intent and change device state appropriately with no fine-tuning or task-specific training. Our work hints at the promise of LLM-driven systems for context-awareness in smart environments, motivating future research in this area.",
    "authors": [
      "Evan King",
      "Haoxiang Yu",
      "Sangsu Lee",
      "Christine Julien"
    ],
    "categories": [
      "cs.HC",
      "cs.AI"
    ],
    "published": "2023-03-24T16:51:08Z",
    "pdf_url": "https://arxiv.org/pdf/2303.14143v1"
  },
  {
    "arxiv_id": "2303.13988v6",
    "entry_id": "http://arxiv.org/abs/2303.13988v6",
    "title": "Machine Psychology",
    "summary": "Large language models (LLMs) show increasingly advanced emergent capabilities and are being incorporated across various societal domains. Understanding their behavior and reasoning abilities therefore holds significant importance. We argue that a fruitful direction for research is engaging LLMs in behavioral experiments inspired by psychology that have traditionally been aimed at understanding human cognition and behavior. In this article, we highlight and summarize theoretical perspectives, experimental paradigms, and computational analysis techniques that this approach brings to the table. It paves the way for a \"machine psychology\" for generative artificial intelligence (AI) that goes beyond performance benchmarks and focuses instead on computational insights that move us toward a better understanding and discovery of emergent abilities and behavioral patterns in LLMs. We review existing work taking this approach, synthesize best practices, and highlight promising future directions. We also highlight the important caveats of applying methodologies designed for understanding humans to machines. We posit that leveraging tools from experimental psychology to study AI will become increasingly valuable as models evolve to be more powerful, opaque, multi-modal, and integrated into complex real-world settings.",
    "authors": [
      "Thilo Hagendorff",
      "Ishita Dasgupta",
      "Marcel Binz",
      "Stephanie C. Y. Chan",
      "Andrew Lampinen",
      "Jane X. Wang",
      "Zeynep Akata",
      "Eric Schulz"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-03-24T13:24:41Z",
    "pdf_url": "https://arxiv.org/pdf/2303.13988v6"
  },
  {
    "arxiv_id": "2303.13489v2",
    "entry_id": "http://arxiv.org/abs/2303.13489v2",
    "title": "Boosting Reinforcement Learning and Planning with Demonstrations: A Survey",
    "summary": "Although reinforcement learning has seen tremendous success recently, this kind of trial-and-error learning can be impractical or inefficient in complex environments. The use of demonstrations, on the other hand, enables agents to benefit from expert knowledge rather than having to discover the best action to take through exploration. In this survey, we discuss the advantages of using demonstrations in sequential decision making, various ways to apply demonstrations in learning-based decision making paradigms (for example, reinforcement learning and planning in the learned models), and how to collect the demonstrations in various scenarios. Additionally, we exemplify a practical pipeline for generating and utilizing demonstrations in the recently proposed ManiSkill robot learning benchmark.",
    "authors": [
      "Tongzhou Mu",
      "Hao Su"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.RO"
    ],
    "published": "2023-03-23T17:53:44Z",
    "pdf_url": "https://arxiv.org/pdf/2303.13489v2"
  },
  {
    "arxiv_id": "2303.16750v2",
    "entry_id": "http://arxiv.org/abs/2303.16750v2",
    "title": "A Gold Standard Dataset for the Reviewer Assignment Problem",
    "summary": "Many peer-review venues are using algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the \"similarity score\" -- a numerical estimate of the expertise of a reviewer in reviewing a paper -- and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of publicly available gold-standard data. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously.\n  Using our dataset, we compare several widely used similarity algorithms and offer key insights. First, all algorithms exhibit significant error, with misranking rates between 12%-30% in easier cases and 36%-43% in harder ones. Second, most specialized algorithms are designed to work with titles and abstracts of papers, and in this regime the SPECTER2 algorithm performs best. Interestingly, classical TF-IDF matches SPECTER2 in accuracy when given access to full submission texts. In contrast, off-the-shelf LLMs lag behind specialized approaches.",
    "authors": [
      "Ivan Stelmakh",
      "John Wieting",
      "Sarina Xi",
      "Graham Neubig",
      "Nihar B. Shah"
    ],
    "categories": [
      "cs.IR",
      "cs.DL",
      "cs.LG"
    ],
    "published": "2023-03-23T16:15:03Z",
    "pdf_url": "https://arxiv.org/pdf/2303.16750v2"
  },
  {
    "arxiv_id": "2303.12961v2",
    "entry_id": "http://arxiv.org/abs/2303.12961v2",
    "title": "The Shaky Foundations of Clinical Foundation Models: A Survey of Large Language Models and Foundation Models for EMRs",
    "summary": "The successes of foundation models such as ChatGPT and AlphaFold have spurred significant interest in building similar models for electronic medical records (EMRs) to improve patient care and hospital operations. However, recent hype has obscured critical gaps in our understanding of these models' capabilities. We review over 80 foundation models trained on non-imaging EMR data (i.e. clinical text and/or structured data) and create a taxonomy delineating their architectures, training data, and potential use cases. We find that most models are trained on small, narrowly-scoped clinical datasets (e.g. MIMIC-III) or broad, public biomedical corpora (e.g. PubMed) and are evaluated on tasks that do not provide meaningful insights on their usefulness to health systems. In light of these findings, we propose an improved evaluation framework for measuring the benefits of clinical foundation models that is more closely grounded to metrics that matter in healthcare.",
    "authors": [
      "Michael Wornow",
      "Yizhe Xu",
      "Rahul Thapa",
      "Birju Patel",
      "Ethan Steinberg",
      "Scott Fleming",
      "Michael A. Pfeffer",
      "Jason Fries",
      "Nigam H. Shah"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-03-22T23:54:14Z",
    "pdf_url": "https://arxiv.org/pdf/2303.12961v2"
  },
  {
    "arxiv_id": "2303.12132v1",
    "entry_id": "http://arxiv.org/abs/2303.12132v1",
    "title": "Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense",
    "summary": "Generative Language Models gained significant attention in late 2022 / early 2023, notably with the introduction of models refined to act consistently with users' expectations of interactions with AI (conversational models). Arguably the focal point of public attention has been such a refinement of the GPT3 model -- the ChatGPT and its subsequent integration with auxiliary capabilities, including search as part of Microsoft Bing. Despite extensive prior research invested in their development, their performance and applicability to a range of daily tasks remained unclear and niche. However, their wider utilization without a requirement for technical expertise, made in large part possible through conversational fine-tuning, revealed the extent of their true capabilities in a real-world environment. This has garnered both public excitement for their potential applications and concerns about their capabilities and potential malicious uses. This review aims to provide a brief overview of the history, state of the art, and implications of Generative Language Models in terms of their principles, abilities, limitations, and future prospects -- especially in the context of cyber-defense, with a focus on the Swiss operational environment.",
    "authors": [
      "Andrei Kucharavy",
      "Zachary Schillaci",
      "Loïc Maréchal",
      "Maxime Würsch",
      "Ljiljana Dolamic",
      "Remi Sabonnadiere",
      "Dimitri Percia David",
      "Alain Mermoud",
      "Vincent Lenders"
    ],
    "categories": [
      "cs.CL",
      "cs.CR",
      "cs.LG"
    ],
    "published": "2023-03-21T18:45:09Z",
    "pdf_url": "https://arxiv.org/pdf/2303.12132v1"
  },
  {
    "arxiv_id": "2303.12040v7",
    "entry_id": "http://arxiv.org/abs/2303.12040v7",
    "title": "Roots and Requirements for Collaborative AIs",
    "summary": "The vision of AI collaborators is a staple of mythology and science fiction, where artificial agents with special talents assist human partners and teams. In this dream, sophisticated AIs understand nuances of collaboration and human communication. The AI as collaborator dream is different from computer tools that augment human intelligence (IA) or intermediate human collaboration. Those tools have their roots in the 1960s and helped to drive an information technology revolution. They can be useful but they are not intelligent and do not collaborate as effectively as skilled people. With the increase of hybrid and remote work since the COVID pandemic, the benefits and requirements for better coordination, collaboration, and communication are becoming hot topics in the workplace. Employers and workers face choices and trade-offs as they negotiate the options for working from home versus working at the office. Many factors such as the high costs of homes near employers are impeding a mass return to the office. Government advisory groups and leaders in AI have advocated for years that AIs should be transparent and effective collaborators. Nonetheless, robust AIs that collaborate like talented people remain out of reach. Are AI teammates part of a solution? How artificially intelligent (AI) could and should they be? This position paper reviews the arc of technology and public calls for human-machine teaming. It draws on earlier research in psychology and the social sciences about what human-like collaboration requires. This paper sets a context for a second science-driven paper that advocates a radical shift in technology and methodology for creating resilient, intelligent, and human-compatible AIs (Stefik & Price, 2023). The aspirational goal is that such AIs would learn, share what they learn, and collaborate to achieve high capabilities.",
    "authors": [
      "Mark Stefik"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-03-21T17:27:38Z",
    "pdf_url": "https://arxiv.org/pdf/2303.12040v7"
  },
  {
    "arxiv_id": "2303.12023v2",
    "entry_id": "http://arxiv.org/abs/2303.12023v2",
    "title": "Logical Reasoning over Natural Language as Knowledge Representation: A Survey",
    "summary": "Logical reasoning is central to human cognition and intelligence. It includes deductive, inductive, and abductive reasoning. Past research of logical reasoning within AI uses formal language as knowledge representation and symbolic reasoners. However, reasoning with formal language has proved challenging (e.g., brittleness and knowledge-acquisition bottleneck). This paper provides a comprehensive overview on a new paradigm of logical reasoning, which uses natural language as knowledge representation and pretrained language models as reasoners, including philosophical definition and categorization of logical reasoning, advantages of the new paradigm, benchmarks and methods, challenges of the new paradigm, possible future directions, and relation to related NLP fields. This new paradigm is promising since it not only alleviates many challenges of formal representation but also has advantages over end-to-end neural methods. This survey focus on transformer-based LLMs explicitly working on deductive, inductive, and abductive reasoning over English representation.",
    "authors": [
      "Zonglin Yang",
      "Xinya Du",
      "Rui Mao",
      "Jinjie Ni",
      "Erik Cambria"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-03-21T16:56:05Z",
    "pdf_url": "https://arxiv.org/pdf/2303.12023v2"
  },
  {
    "arxiv_id": "2303.11191v1",
    "entry_id": "http://arxiv.org/abs/2303.11191v1",
    "title": "A Survey of Demonstration Learning",
    "summary": "With the fast improvement of machine learning, reinforcement learning (RL) has been used to automate human tasks in different areas. However, training such agents is difficult and restricted to expert users. Moreover, it is mostly limited to simulation environments due to the high cost and safety concerns of interactions in the real world. Demonstration Learning is a paradigm in which an agent learns to perform a task by imitating the behavior of an expert shown in demonstrations. It is a relatively recent area in machine learning, but it is gaining significant traction due to having tremendous potential for learning complex behaviors from demonstrations. Learning from demonstration accelerates the learning process by improving sample efficiency, while also reducing the effort of the programmer. Due to learning without interacting with the environment, demonstration learning would allow the automation of a wide range of real world applications such as robotics and healthcare. This paper provides a survey of demonstration learning, where we formally introduce the demonstration problem along with its main challenges and provide a comprehensive overview of the process of learning from demonstrations from the creation of the demonstration data set, to learning methods from demonstrations, and optimization by combining demonstration learning with different machine learning methods. We also review the existing benchmarks and identify their strengths and limitations. Additionally, we discuss the advantages and disadvantages of the paradigm as well as its main applications. Lastly, we discuss our perspective on open problems and research directions for this rapidly growing field.",
    "authors": [
      "André Correia",
      "Luís A. Alexandre"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-03-20T15:22:10Z",
    "pdf_url": "https://arxiv.org/pdf/2303.11191v1"
  },
  {
    "arxiv_id": "2303.10854v1",
    "entry_id": "http://arxiv.org/abs/2303.10854v1",
    "title": "Dynamic Documentation for AI Systems",
    "summary": "AI documentation is a rapidly-growing channel for coordinating the design of AI technologies with policies for transparency and accessibility. Calls to standardize and enact documentation of algorithmic harms and impacts are now commonplace. However, documentation standards for AI remain inchoate, and fail to match the capabilities and social effects of increasingly impactful architectures such as Large Language Models (LLMs). In this paper, we show the limits of present documentation protocols, and argue for dynamic documentation as a new paradigm for understanding and evaluating AI systems. We first review canonical approaches to system documentation outside the context of AI, focusing on the complex history of Environmental Impact Statements (EISs). We next compare critical elements of the EIS framework to present challenges with algorithmic documentation, which have inherited the limitations of EISs without incorporating their strengths. These challenges are specifically illustrated through the growing popularity of Model Cards and two case studies of algorithmic impact assessment in China and Canada. Finally, we evaluate more recent proposals, including Reward Reports, as potential components of fully dynamic AI documentation protocols.",
    "authors": [
      "Soham Mehta",
      "Anderson Rogers",
      "Thomas Krendl Gilbert"
    ],
    "categories": [
      "cs.CY",
      "cs.AI"
    ],
    "published": "2023-03-20T04:23:07Z",
    "pdf_url": "https://arxiv.org/pdf/2303.10854v1"
  },
  {
    "arxiv_id": "2303.13379v2",
    "entry_id": "http://arxiv.org/abs/2303.13379v2",
    "title": "Practical and Ethical Challenges of Large Language Models in Education: A Systematic Scoping Review",
    "summary": "Educational technology innovations leveraging large language models (LLMs) have shown the potential to automate the laborious process of generating and analysing textual content. While various innovations have been developed to automate a range of educational tasks (e.g., question generation, feedback provision, and essay grading), there are concerns regarding the practicality and ethicality of these innovations. Such concerns may hinder future research and the adoption of LLMs-based innovations in authentic educational contexts. To address this, we conducted a systematic scoping review of 118 peer-reviewed papers published since 2017 to pinpoint the current state of research on using LLMs to automate and support educational tasks. The findings revealed 53 use cases for LLMs in automating education tasks, categorised into nine main categories: profiling/labelling, detection, grading, teaching support, prediction, knowledge representation, feedback, content generation, and recommendation. Additionally, we also identified several practical and ethical challenges, including low technological readiness, lack of replicability and transparency, and insufficient privacy and beneficence considerations. The findings were summarised into three recommendations for future studies, including updating existing innovations with state-of-the-art models (e.g., GPT-3/4), embracing the initiative of open-sourcing models/systems, and adopting a human-centred approach throughout the developmental process. As the intersection of AI and education is continuously evolving, the findings of this study can serve as an essential reference point for researchers, allowing them to leverage the strengths, learn from the limitations, and uncover potential research opportunities enabled by ChatGPT and other generative AI models.",
    "authors": [
      "Lixiang Yan",
      "Lele Sha",
      "Linxuan Zhao",
      "Yuheng Li",
      "Roberto Martinez-Maldonado",
      "Guanliang Chen",
      "Xinyu Li",
      "Yueqiao Jin",
      "Dragan Gašević"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY"
    ],
    "published": "2023-03-17T18:14:46Z",
    "pdf_url": "https://arxiv.org/pdf/2303.13379v2"
  },
  {
    "arxiv_id": "2303.08140v1",
    "entry_id": "http://arxiv.org/abs/2303.08140v1",
    "title": "Digital staining in optical microscopy using deep learning -- a review",
    "summary": "Until recently, conventional biochemical staining had the undisputed status as well-established benchmark for most biomedical problems related to clinical diagnostics, fundamental research and biotechnology. Despite this role as gold-standard, staining protocols face several challenges, such as a need for extensive, manual processing of samples, substantial time delays, altered tissue homeostasis, limited choice of contrast agents for a given sample, 2D imaging instead of 3D tomography and many more. Label-free optical technologies, on the other hand, do not rely on exogenous and artificial markers, by exploiting intrinsic optical contrast mechanisms, where the specificity is typically less obvious to the human observer. Over the past few years, digital staining has emerged as a promising concept to use modern deep learning for the translation from optical contrast to established biochemical contrast of actual stainings. In this review article, we provide an in-depth analysis of the current state-of-the-art in this field, suggest methods of good practice, identify pitfalls and challenges and postulate promising advances towards potential future implementations and applications.",
    "authors": [
      "Lucas Kreiss",
      "Shaowei Jiang",
      "Xiang Li",
      "Shiqi Xu",
      "Kevin C. Zhou",
      "Alexander Mühlberg",
      "Kyung Chul Lee",
      "Kanghyun Kim",
      "Amey Chaware",
      "Michael Ando",
      "Laura Barisoni",
      "Seung Ah Lee",
      "Guoan Zheng",
      "Kyle Lafata",
      "Oliver Friedrich",
      "Roarke Horstmeyer"
    ],
    "categories": [
      "eess.IV",
      "cs.LG",
      "physics.bio-ph"
    ],
    "published": "2023-03-14T15:23:48Z",
    "pdf_url": "https://arxiv.org/pdf/2303.08140v1"
  },
  {
    "arxiv_id": "2303.05344v2",
    "entry_id": "http://arxiv.org/abs/2303.05344v2",
    "title": "Recent Advances of Deep Robotic Affordance Learning: A Reinforcement Learning Perspective",
    "summary": "As a popular concept proposed in the field of psychology, affordance has been regarded as one of the important abilities that enable humans to understand and interact with the environment. Briefly, it captures the possibilities and effects of the actions of an agent applied to a specific object or, more generally, a part of the environment. This paper provides a short review of the recent developments of deep robotic affordance learning (DRAL), which aims to develop data-driven methods that use the concept of affordance to aid in robotic tasks. We first classify these papers from a reinforcement learning (RL) perspective, and draw connections between RL and affordances. The technical details of each category are discussed and their limitations identified. We further summarise them and identify future challenges from the aspects of observations, actions, affordance representation, data-collection and real-world deployment. A final remark is given at the end to propose a promising future direction of the RL-based affordance definition to include the predictions of arbitrary action consequences.",
    "authors": [
      "Xintong Yang",
      "Ze Ji",
      "Jing Wu",
      "Yu-kun Lai"
    ],
    "categories": [
      "cs.RO",
      "cs.AI"
    ],
    "published": "2023-03-09T15:42:01Z",
    "pdf_url": "https://arxiv.org/pdf/2303.05344v2"
  },
  {
    "arxiv_id": "2303.04926v1",
    "entry_id": "http://arxiv.org/abs/2303.04926v1",
    "title": "Automated Cyber Defence: A Review",
    "summary": "Within recent times, cybercriminals have curated a variety of organised and resolute cyber attacks within a range of cyber systems, leading to consequential ramifications to private and governmental institutions. Current security-based automation and orchestrations focus on automating fixed purpose and hard-coded solutions, which are easily surpassed by modern-day cyber attacks. Research within Automated Cyber Defence will allow the development and enabling intelligence response by autonomously defending networked systems through sequential decision-making agents. This article comprehensively elaborates the developments within Automated Cyber Defence through a requirement analysis divided into two sub-areas, namely, automated defence and attack agents and Autonomous Cyber Operation (ACO) Gyms. The requirement analysis allows the comparison of automated agents and highlights the importance of ACO Gyms for their continual development. The requirement analysis is also used to critique ACO Gyms with an overall aim to develop them for deploying automated agents within real-world networked systems. Relevant future challenges were addressed from the overall analysis to accelerate development within the area of Automated Cyber Defence.",
    "authors": [
      "Sanyam Vyas",
      "John Hannay",
      "Andrew Bolton",
      "Professor Pete Burnap"
    ],
    "categories": [
      "cs.CR",
      "cs.AI"
    ],
    "published": "2023-03-08T22:37:50Z",
    "pdf_url": "https://arxiv.org/pdf/2303.04926v1"
  },
  {
    "arxiv_id": "2303.04544v1",
    "entry_id": "http://arxiv.org/abs/2303.04544v1",
    "title": "Models of symbol emergence in communication: a conceptual review and a guide for avoiding local minima",
    "summary": "Computational simulations are a popular method for testing hypotheses about the emergence of communication. This kind of research is performed in a variety of traditions including language evolution, developmental psychology, cognitive science, machine learning, robotics, etc. The motivations for the models are different, but the operationalizations and methods used are often similar. We identify the assumptions and explanatory targets of several most representative models and summarise the known results. We claim that some of the assumptions -- such as portraying meaning in terms of mapping, focusing on the descriptive function of communication, modelling signals with amodal tokens -- may hinder the success of modelling. Relaxing these assumptions and foregrounding the interactions of embodied and situated agents allows one to systematise the multiplicity of pressures under which symbolic systems evolve. In line with this perspective, we sketch the road towards modelling the emergence of meaningful symbolic communication, where symbols are simultaneously grounded in action and perception and form an abstract system.",
    "authors": [
      "Julian Zubek",
      "Tomasz Korbak",
      "Joanna Rączaszek-Leonardi"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.MA"
    ],
    "published": "2023-03-08T12:53:03Z",
    "pdf_url": "https://arxiv.org/pdf/2303.04544v1"
  },
  {
    "arxiv_id": "2303.04280v1",
    "entry_id": "http://arxiv.org/abs/2303.04280v1",
    "title": "Solving Vehicle Routing Problem for unmanned heterogeneous vehicle systems using Asynchronous Multi-Agent Architecture (A-teams)",
    "summary": "Fast moving but power hungry unmanned aerial vehicles (UAVs) can recharge on slow-moving unmanned ground vehicles (UGVs) to survey large areas in an effective and efficient manner. In order to solve this computationally challenging problem in a reasonable time, we created a two-level optimization heuristics. At the outer level, the UGV route is parameterized by few free parameters and at the inner level, the UAV route is solved by formulating and solving a vehicle routing problem with capacity constraints, time windows, and dropped visits. The UGV free parameters need to be optimized judiciously in order to create high quality solutions. We explore two methods for tuning the free UGV parameters: (1) a genetic algorithm, and (2) Asynchronous Multi-agent architecture (Ateams). The A-teams uses multiple agents to create, improve, and destroy solutions. The parallel asynchronous architecture enables A-teams to quickly optimize the parameters. Our results on test cases show that the A-teams produces similar solutions as genetic algorithm but with a speed up of 2-3 times.",
    "authors": [
      "Subramanian Ramasamy",
      "Md Safwan Mondal",
      "Pranav A. Bhounsule"
    ],
    "categories": [
      "cs.RO",
      "cs.DC",
      "cs.MA"
    ],
    "published": "2023-03-07T22:57:59Z",
    "pdf_url": "https://arxiv.org/pdf/2303.04280v1"
  },
  {
    "arxiv_id": "2303.05279v2",
    "entry_id": "http://arxiv.org/abs/2303.05279v2",
    "title": "Can large language models build causal graphs?",
    "summary": "Building causal graphs can be a laborious process. To ensure all relevant causal pathways have been captured, researchers often have to discuss with clinicians and experts while also reviewing extensive relevant medical literature. By encoding common and medical knowledge, large language models (LLMs) represent an opportunity to ease this process by automatically scoring edges (i.e., connections between two variables) in potential graphs. LLMs however have been shown to be brittle to the choice of probing words, context, and prompts that the user employs. In this work, we evaluate if LLMs can be a useful tool in complementing causal graph development.",
    "authors": [
      "Stephanie Long",
      "Tibor Schuster",
      "Alexandre Piché"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-03-07T22:05:31Z",
    "pdf_url": "https://arxiv.org/pdf/2303.05279v2"
  },
  {
    "arxiv_id": "2303.04226v1",
    "entry_id": "http://arxiv.org/abs/2303.04226v1",
    "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
    "summary": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant attention from society. As a result, many individuals have become interested in related resources and are seeking to uncover the background and secrets behind its impressive performance. In fact, ChatGPT and other Generative AI (GAI) techniques belong to the category of Artificial Intelligence Generated Content (AIGC), which involves the creation of digital content, such as images, music, and natural language, through AI models. The goal of AIGC is to make the content creation process more efficient and accessible, allowing for the production of high-quality content at a faster pace. AIGC is achieved by extracting and understanding intent information from instructions provided by human, and generating the content according to its knowledge and the intent information. In recent years, large-scale models have become increasingly important in AIGC as they provide better intent extraction and thus, improved generation results. With the growth of data and the size of the models, the distribution that the model can learn becomes more comprehensive and closer to reality, leading to more realistic and high-quality content generation. This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimodal interaction and multimodal interaction. From the perspective of unimodality, we introduce the generation tasks and relative models of text and image. From the perspective of multimodality, we introduce the cross-application between the modalities mentioned above. Finally, we discuss the existing open problems and future challenges in AIGC.",
    "authors": [
      "Yihan Cao",
      "Siyu Li",
      "Yixin Liu",
      "Zhiling Yan",
      "Yutong Dai",
      "Philip S. Yu",
      "Lichao Sun"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-03-07T20:36:13Z",
    "pdf_url": "https://arxiv.org/pdf/2303.04226v1"
  },
  {
    "arxiv_id": "2303.04129v1",
    "entry_id": "http://arxiv.org/abs/2303.04129v1",
    "title": "Foundation Models for Decision Making: Problems, Methods, and Opportunities",
    "summary": "Foundation models pretrained on diverse data at scale have demonstrated extraordinary capabilities in a wide range of vision and language tasks. When such models are deployed in real world environments, they inevitably interface with other entities and agents. For example, language models are often used to interact with human beings through dialogue, and visual perception models are used to autonomously navigate neighborhood streets. In response to these developments, new paradigms are emerging for training foundation models to interact with other agents and perform long-term reasoning. These paradigms leverage the existence of ever-larger datasets curated for multimodal, multitask, and generalist interaction. Research at the intersection of foundation models and decision making holds tremendous promise for creating powerful new systems that can interact effectively across a diverse range of applications such as dialogue, autonomous driving, healthcare, education, and robotics. In this manuscript, we examine the scope of foundation models for decision making, and provide conceptual tools and technical background for understanding the problem space and exploring new research directions. We review recent approaches that ground foundation models in practical decision making applications through a variety of methods such as prompting, conditional generative modeling, planning, optimal control, and reinforcement learning, and discuss common challenges and open problems in the field.",
    "authors": [
      "Sherry Yang",
      "Ofir Nachum",
      "Yilun Du",
      "Jason Wei",
      "Pieter Abbeel",
      "Dale Schuurmans"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-03-07T18:44:07Z",
    "pdf_url": "https://arxiv.org/pdf/2303.04129v1"
  },
  {
    "arxiv_id": "2303.04150v4",
    "entry_id": "http://arxiv.org/abs/2303.04150v4",
    "title": "Evolutionary Reinforcement Learning: A Survey",
    "summary": "Reinforcement learning (RL) is a machine learning approach that trains agents to maximize cumulative rewards through interactions with environments. The integration of RL with deep learning has recently resulted in impressive achievements in a wide range of challenging tasks, including board games, arcade games, and robot control. Despite these successes, there remain several crucial challenges, including brittle convergence properties caused by sensitive hyperparameters, difficulties in temporal credit assignment with long time horizons and sparse rewards, a lack of diverse exploration, especially in continuous search space scenarios, difficulties in credit assignment in multi-agent reinforcement learning, and conflicting objectives for rewards. Evolutionary computation (EC), which maintains a population of learning agents, has demonstrated promising performance in addressing these limitations. This article presents a comprehensive survey of state-of-the-art methods for integrating EC into RL, referred to as evolutionary reinforcement learning (EvoRL). We categorize EvoRL methods according to key research fields in RL, including hyperparameter optimization, policy search, exploration, reward shaping, meta-RL, and multi-objective RL. We then discuss future research directions in terms of efficient methods, benchmarks, and scalable platforms. This survey serves as a resource for researchers and practitioners interested in the field of EvoRL, highlighting the important challenges and opportunities for future research. With the help of this survey, researchers and practitioners can develop more efficient methods and tailored benchmarks for EvoRL, further advancing this promising cross-disciplinary research field.",
    "authors": [
      "Hui Bai",
      "Ran Cheng",
      "Yaochu Jin"
    ],
    "categories": [
      "cs.NE",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-03-07T01:38:42Z",
    "pdf_url": "https://arxiv.org/pdf/2303.04150v4"
  },
  {
    "arxiv_id": "2303.03542v1",
    "entry_id": "http://arxiv.org/abs/2303.03542v1",
    "title": "Multi-resolution Interpretation and Diagnostics Tool for Natural Language Classifiers",
    "summary": "Developing explainability methods for Natural Language Processing (NLP) models is a challenging task, for two main reasons. First, the high dimensionality of the data (large number of tokens) results in low coverage and in turn small contributions for the top tokens, compared to the overall model performance. Second, owing to their textual nature, the input variables, after appropriate transformations, are effectively binary (presence or absence of a token in an observation), making the input-output relationship difficult to understand. Common NLP interpretation techniques do not have flexibility in resolution, because they usually operate at word-level and provide fully local (message level) or fully global (over all messages) summaries. The goal of this paper is to create more flexible model explainability summaries by segments of observation or clusters of words that are semantically related to each other. In addition, we introduce a root cause analysis method for NLP models, by analyzing representative False Positive and False Negative examples from different segments. At the end, we illustrate, using a Yelp review data set with three segments (Restaurant, Hotel, and Beauty), that exploiting group/cluster structures in words and/or messages can aid in the interpretation of decisions made by NLP models and can be utilized to assess the model's sensitivity or bias towards gender, syntax, and word meanings.",
    "authors": [
      "Peyman Jalali",
      "Nengfeng Zhou",
      "Yufei Yu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-03-06T22:59:02Z",
    "pdf_url": "https://arxiv.org/pdf/2303.03542v1"
  },
  {
    "arxiv_id": "2303.02411v1",
    "entry_id": "http://arxiv.org/abs/2303.02411v1",
    "title": "The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges",
    "summary": "Recent advancements in visiolinguistic (VL) learning have allowed the development of multiple models and techniques that offer several impressive implementations, able to currently resolve a variety of tasks that require the collaboration of vision and language. Current datasets used for VL pre-training only contain a limited amount of visual and linguistic knowledge, thus significantly limiting the generalization capabilities of many VL models. External knowledge sources such as knowledge graphs (KGs) and Large Language Models (LLMs) are able to cover such generalization gaps by filling in missing knowledge, resulting in the emergence of hybrid architectures. In the current survey, we analyze tasks that have benefited from such hybrid approaches. Moreover, we categorize existing knowledge sources and types, proceeding to discussion regarding the KG vs LLM dilemma and its potential impact to future hybrid approaches.",
    "authors": [
      "Maria Lymperaiou",
      "Giorgos Stamou"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CV"
    ],
    "published": "2023-03-04T13:12:18Z",
    "pdf_url": "https://arxiv.org/pdf/2303.02411v1"
  },
  {
    "arxiv_id": "2303.01618v2",
    "entry_id": "http://arxiv.org/abs/2303.01618v2",
    "title": "Deconstructing deep active inference",
    "summary": "Active inference is a theory of perception, learning and decision making, which can be applied to neuroscience, robotics, and machine learning. Recently, reasearch has been taking place to scale up this framework using Monte-Carlo tree search and deep learning. The goal of this activity is to solve more complicated tasks using deep active inference. First, we review the existing literature, then, we progresively build a deep active inference agent. For two agents, we have experimented with five definitions of the expected free energy and three different action selection strategies. According to our experiments, the models able to solve the dSprites environment are the ones that maximise rewards. Finally, we compare the similarity of the representation learned by the layers of various agents using centered kernel alignment. Importantly, the agent maximising reward and the agent minimising expected free energy learn very similar representations except for the last layer of the critic network (reflecting the difference in learning objective), and the variance layers of the transition and encoder networks. We found that the reward maximising agent is a lot more certain than the agent minimising expected free energy. This is because the agent minimising expected free energy always picks the action down, and does not gather enough data for the other actions. In contrast, the agent maximising reward, keeps on selecting the actions left and right, enabling it to successfully solve the task. The only difference between those two agents is the epistemic value, which aims to make the outputs of the transition and encoder networks as close as possible. Thus, the agent minimising expected free energy picks a single action (down), and becomes an expert at predicting the future when selecting this action. This makes the KL divergence between the output of the transition and encoder networks small.",
    "authors": [
      "Théophile Champion",
      "Marek Grześ",
      "Lisa Bonheme",
      "Howard Bowman"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-03-02T22:39:56Z",
    "pdf_url": "https://arxiv.org/pdf/2303.01618v2"
  },
  {
    "arxiv_id": "2303.01173v1",
    "entry_id": "http://arxiv.org/abs/2303.01173v1",
    "title": "Resource-Constrained Station-Keeping for Helium Balloons using Reinforcement Learning",
    "summary": "High altitude balloons have proved useful for ecological aerial surveys, atmospheric monitoring, and communication relays. However, due to weight and power constraints, there is a need to investigate alternate modes of propulsion to navigate in the stratosphere. Very recently, reinforcement learning has been proposed as a control scheme to maintain the balloon in the region of a fixed location, facilitated through diverse opposing wind-fields at different altitudes. Although air-pump based station keeping has been explored, there is no research on the control problem for venting and ballasting actuated balloons, which is commonly used as a low-cost alternative. We show how reinforcement learning can be used for this type of balloon. Specifically, we use the soft actor-critic algorithm, which on average is able to station-keep within 50\\;km for 25\\% of the flight, consistent with state-of-the-art. Furthermore, we show that the proposed controller effectively minimises the consumption of resources, thereby supporting long duration flights. We frame the controller as a continuous control reinforcement learning problem, which allows for a more diverse range of trajectories, as opposed to current state-of-the-art work, which uses discrete action spaces. Furthermore, through continuous control, we can make use of larger ascent rates which are not possible using air-pumps. The desired ascent-rate is decoupled into desired altitude and time-factor to provide a more transparent policy, compared to low-level control commands used in previous works. Finally, by applying the equations of motion, we establish appropriate thresholds for venting and ballasting to prevent the agent from exploiting the environment. More specifically, we ensure actions are physically feasible by enforcing constraints on venting and ballasting.",
    "authors": [
      "Jack Saunders",
      "Loïc Prenevost",
      "Özgür Şimşek",
      "Alan Hunter",
      "Wenbin Li"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-03-02T11:35:59Z",
    "pdf_url": "https://arxiv.org/pdf/2303.01173v1"
  },
  {
    "arxiv_id": "2302.14233v2",
    "entry_id": "http://arxiv.org/abs/2302.14233v2",
    "title": "Goal Driven Discovery of Distributional Differences via Language Descriptions",
    "summary": "Mining large corpora can generate useful discoveries but is time-consuming for humans. We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. The task input is a problem comprising a research goal \"$\\textit{comparing the side effects of drug A and drug B}$\" and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). The output is a language description (discovery) of how these corpora differ (patients taking drug A \"$\\textit{mention feelings of paranoia}$\" more often). We build a D5 system, and to quantitatively measure its performance, we 1) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health, and 2) propose a set of unified evaluation metrics: validity, relevance, novelty, and significance. With the dataset and the unified metrics, we confirm that language models can use the goals to propose more relevant, novel, and significant candidate discoveries. Finally, our system produces discoveries previously unknown to the authors on a wide range of applications in OpenD5, including temporal and demographic differences in discussion topics, political stances and stereotypes in speech, insights in commercial reviews, and error patterns in NLP models.",
    "authors": [
      "Ruiqi Zhong",
      "Peter Zhang",
      "Steve Li",
      "Jinwoo Ahn",
      "Dan Klein",
      "Jacob Steinhardt"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-02-28T01:32:32Z",
    "pdf_url": "https://arxiv.org/pdf/2302.14233v2"
  },
  {
    "arxiv_id": "2302.11089v3",
    "entry_id": "http://arxiv.org/abs/2302.11089v3",
    "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
    "summary": "This review article is an attempt to survey all recent AI based techniques used to deal with major functions in This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.",
    "authors": [
      "Arman Asgharpoor Golroudbari",
      "Mohammad Hossein Sabour"
    ],
    "categories": [
      "cs.RO",
      "cs.AI",
      "eess.SP",
      "eess.SY"
    ],
    "published": "2023-02-22T01:42:49Z",
    "pdf_url": "https://arxiv.org/pdf/2302.11089v3"
  },
  {
    "arxiv_id": "2302.10463v2",
    "entry_id": "http://arxiv.org/abs/2302.10463v2",
    "title": "Vision-based Multi-future Trajectory Prediction: A Survey",
    "summary": "Vision-based trajectory prediction is an important task that supports safe and intelligent behaviours in autonomous systems. Many advanced approaches have been proposed over the years with improved spatial and temporal feature extraction. However, human behaviour is naturally diverse and uncertain. Given the past trajectory and surrounding environment information, an agent can have multiple plausible trajectories in the future. To tackle this problem, an essential task named multi-future trajectory prediction (MTP) has recently been studied. This task aims to generate a diverse, acceptable and explainable distribution of future predictions for each agent. In this paper, we present the first survey for MTP with our unique taxonomies and a comprehensive analysis of frameworks, datasets and evaluation metrics. We also compare models on existing MTP datasets and conduct experiments on the ForkingPath dataset. Finally, we discuss multiple future directions that can help researchers develop novel multi-future trajectory prediction systems and other diverse learning tasks similar to MTP.",
    "authors": [
      "Renhao Huang",
      "Hao Xue",
      "Maurice Pagnucco",
      "Flora Salim",
      "Yang Song"
    ],
    "categories": [
      "cs.RO",
      "cs.CV",
      "cs.LG"
    ],
    "published": "2023-02-21T06:11:08Z",
    "pdf_url": "https://arxiv.org/pdf/2302.10463v2"
  },
  {
    "arxiv_id": "2302.10035v3",
    "entry_id": "http://arxiv.org/abs/2302.10035v3",
    "title": "Large-scale Multi-Modal Pre-trained Models: A Comprehensive Survey",
    "summary": "With the urgent demand for generalized deep models, many pre-trained big models are proposed, such as BERT, ViT, GPT, etc. Inspired by the success of these models in single domains (like computer vision and natural language processing), the multi-modal pre-trained big models have also drawn more and more attention in recent years. In this work, we give a comprehensive survey of these models and hope this paper could provide new insights and helps fresh researchers to track the most cutting-edge works. Specifically, we firstly introduce the background of multi-modal pre-training by reviewing the conventional deep learning, pre-training works in natural language process, computer vision, and speech. Then, we introduce the task definition, key challenges, and advantages of multi-modal pre-training models (MM-PTMs), and discuss the MM-PTMs with a focus on data, objectives, network architectures, and knowledge enhanced pre-training. After that, we introduce the downstream tasks used for the validation of large-scale MM-PTMs, including generative, classification, and regression tasks. We also give visualization and analysis of the model parameters and results on representative downstream tasks. Finally, we point out possible research directions for this topic that may benefit future works. In addition, we maintain a continuously updated paper list for large-scale pre-trained multi-modal big models: https://github.com/wangxiao5791509/MultiModal_BigModels_Survey. This paper has been published by the journal Machine Intelligence Research (MIR), https://link.springer.com/article/10.1007/s11633-022-1410-8, DOI: 10.1007/s11633-022-1410-8, vol. 20, no. 4, pp. 447-482, 2023.",
    "authors": [
      "Xiao Wang",
      "Guangyao Chen",
      "Guangwu Qian",
      "Pengcheng Gao",
      "Xiao-Yong Wei",
      "Yaowei Wang",
      "Yonghong Tian",
      "Wen Gao"
    ],
    "categories": [
      "cs.CV",
      "cs.AI",
      "cs.MM"
    ],
    "published": "2023-02-20T15:34:03Z",
    "pdf_url": "https://arxiv.org/pdf/2302.10035v3"
  },
  {
    "arxiv_id": "2302.09419v3",
    "entry_id": "http://arxiv.org/abs/2302.09419v3",
    "title": "A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT",
    "summary": "Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.",
    "authors": [
      "Ce Zhou",
      "Qian Li",
      "Chen Li",
      "Jun Yu",
      "Yixin Liu",
      "Guangjing Wang",
      "Kai Zhang",
      "Cheng Ji",
      "Qiben Yan",
      "Lifang He",
      "Hao Peng",
      "Jianxin Li",
      "Jia Wu",
      "Ziwei Liu",
      "Pengtao Xie",
      "Caiming Xiong",
      "Jian Pei",
      "Philip S. Yu",
      "Lichao Sun"
    ],
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-02-18T20:51:09Z",
    "pdf_url": "https://arxiv.org/pdf/2302.09419v3"
  },
  {
    "arxiv_id": "2302.09193v3",
    "entry_id": "http://arxiv.org/abs/2302.09193v3",
    "title": "Copula-based transferable models for synthetic population generation",
    "summary": "Population synthesis involves generating synthetic yet realistic representations of a target population of micro-agents for behavioral modeling and simulation. Traditional methods, often reliant on target population samples, such as census data or travel surveys, face limitations due to high costs and small sample sizes, particularly at smaller geographical scales. We propose a novel framework based on copulas to generate synthetic data for target populations where only empirical marginal distributions are known. This method utilizes samples from different populations with similar marginal dependencies, introduces a spatial component into population synthesis, and considers various information sources for more realistic generators. Concretely, the process involves normalizing the data and treating it as realizations of a given copula, and then training a generative model before incorporating the information on the marginals of the target population. Utilizing American Community Survey data, we assess our framework's performance through standardized root mean squared error (SRMSE) and so-called sampled zeros. We focus on its capacity to transfer a model learned from one population to another. Our experiments include transfer tests between regions at the same geographical level as well as to lower geographical levels, hence evaluating the framework's adaptability in varied spatial contexts. We compare Bayesian Networks, Variational Autoencoders, and Generative Adversarial Networks, both individually and combined with our copula framework. Results show that the copula enhances machine learning methods in matching the marginals of the reference data. Furthermore, it consistently surpasses Iterative Proportional Fitting in terms of SRMSE in the transferability experiments, while introducing unique observations not found in the original training sample.",
    "authors": [
      "Pascal Jutras-Dubé",
      "Mohammad B. Al-Khasawneh",
      "Zhichao Yang",
      "Javier Bas",
      "Fabian Bastin",
      "Cinzia Cirillo"
    ],
    "categories": [
      "stat.ML",
      "cs.LG"
    ],
    "published": "2023-02-17T23:58:14Z",
    "pdf_url": "https://arxiv.org/pdf/2302.09193v3"
  },
  {
    "arxiv_id": "2302.09051v5",
    "entry_id": "http://arxiv.org/abs/2302.09051v5",
    "title": "Complex QA and language models hybrid architectures, Survey",
    "summary": "This paper reviews the state-of-the-art of large language models (LLM) architectures and strategies for \"complex\" question-answering with a focus on hybrid architectures. LLM based chatbot services have allowed anyone to grasp the potential of LLM to solve many common problems, but soon discovered their limitations for complex questions. Addressing more specific, complex questions (e.g., \"What is the best mix of power-generation methods to reduce climate change ?\") often requires specialized architectures, domain knowledge, new skills, decomposition and multi-step resolution, deep reasoning, sensitive data protection, explainability, and human-in-the-loop processes. Therefore, we review: (1) necessary skills and tasks for handling complex questions and common LLM limits to overcome; (2) dataset, cost functions and evaluation metrics for measuring and improving (e.g. accuracy, explainability, fairness, robustness, groundedness, faithfulness, toxicity...); (3) family of solutions to overcome LLM limitations by (a) training and reinforcement (b) hybridization, (c) prompting, (d) agentic-architectures (agents, tools) and extended reasoning.",
    "authors": [
      "Xavier Daull",
      "Patrice Bellot",
      "Emmanuel Bruno",
      "Vincent Martin",
      "Elisabeth Murisasco"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.IR",
      "cs.LG"
    ],
    "published": "2023-02-17T18:31:31Z",
    "pdf_url": "https://arxiv.org/pdf/2302.09051v5"
  },
  {
    "arxiv_id": "2302.07926v1",
    "entry_id": "http://arxiv.org/abs/2302.07926v1",
    "title": "Commonsense Reasoning for Conversational AI: A Survey of the State of the Art",
    "summary": "Large, transformer-based pretrained language models like BERT, GPT, and T5 have demonstrated a deep understanding of contextual semantics and language syntax. Their success has enabled significant advances in conversational AI, including the development of open-dialogue systems capable of coherent, salient conversations which can answer questions, chat casually, and complete tasks. However, state-of-the-art models still struggle with tasks that involve higher levels of reasoning - including commonsense reasoning that humans find trivial. This paper presents a survey of recent conversational AI research focused on commonsense reasoning. The paper lists relevant training datasets and describes the primary approaches to include commonsense in conversational AI. The paper also discusses benchmarks used for evaluating commonsense in conversational AI problems. Finally, the paper presents preliminary observations of the limited commonsense capabilities of two state-of-the-art open dialogue models, BlenderBot3 and LaMDA, and its negative effect on natural interactions. These observations further motivate research on commonsense reasoning in conversational AI.",
    "authors": [
      "Christopher Richardson",
      "Larry Heck"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-02-15T19:55:57Z",
    "pdf_url": "https://arxiv.org/pdf/2302.07926v1"
  },
  {
    "arxiv_id": "2302.07425v5",
    "entry_id": "http://arxiv.org/abs/2302.07425v5",
    "title": "Bandit Social Learning: Exploration under Myopic Behavior",
    "summary": "We study social learning dynamics motivated by reviews on online platforms. The agents collectively follow a simple multi-armed bandit protocol, but each agent acts myopically, without regards to exploration. We allow the greedy (exploitation-only) algorithm, as well as a wide range of behavioral biases. Specifically, we allow myopic behaviors that are consistent with (parameterized) confidence intervals for the arms' expected rewards. We derive stark learning failures for any such behavior, and provide matching positive results. The learning-failure results extend to Bayesian agents and Bayesian bandit environments.\n  In particular, we obtain general, quantitatively strong results on failure of the greedy bandit algorithm, both for ``frequentist\" and ``Bayesian\" versions. Failure results known previously are quantitatively weak, and either trivial or very specialized. Thus, we provide a theoretical foundation for designing non-trivial bandit algorithms, \\ie algorithms that intentionally explore, which has been missing from the literature.\n  Our general behavioral model can be interpreted as agents' optimism or pessimism. The matching positive results entail a maximal allowed amount of optimism. Moreover, we find that no amount of pessimism helps against the learning failures, whereas even a small-but-constant fraction of extreme optimists avoids the failures and leads to near-optimal regret rates.",
    "authors": [
      "Kiarash Banihashem",
      "MohammadTaghi Hajiaghayi",
      "Suho Shin",
      "Aleksandrs Slivkins"
    ],
    "categories": [
      "cs.GT",
      "cs.DS",
      "cs.LG"
    ],
    "published": "2023-02-15T01:57:57Z",
    "pdf_url": "https://arxiv.org/pdf/2302.07425v5"
  },
  {
    "arxiv_id": "2302.07267v6",
    "entry_id": "http://arxiv.org/abs/2302.07267v6",
    "title": "Diminished Diversity-of-Thought in a Standard Large Language Model",
    "summary": "We test whether Large Language Models (LLMs) can be used to simulate human participants in social-science studies. To do this, we run replications of 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT3.5. Based on our pre-registered analyses, we find that among the eight studies we could analyse, our GPT sample replicated 37.5% of the original results and 37.5% of the Many Labs 2 results. However, we were unable to analyse the remaining six studies due to an unexpected phenomenon we call the \"correct answer\" effect. Different runs of GPT3.5 answered nuanced questions probing political orientation, economic preference, judgement, and moral philosophy with zero or near-zero variation in responses: with the supposedly \"correct answer.\" In one exploratory follow-up study, we found that a \"correct answer\" was robust to changing the demographic details that precede the prompt. In another, we found that most but not all \"correct answers\" were robust to changing the order of answer choices. One of our most striking findings occurred in our replication of the Moral Foundations Theory survey results, where we found GPT3.5 identifying as a political conservative in 99.6% of the cases, and as a liberal in 99.3% of the cases in the reverse-order condition. However, both self-reported 'GPT conservatives' and 'GPT liberals' showed right-leaning moral foundations. Our results cast doubts on the validity of using LLMs as a general replacement for human participants in the social sciences. Our results also raise concerns that a hypothetical AI-led future may be subject to a diminished diversity-of-thought.",
    "authors": [
      "Peter S. Park",
      "Philipp Schoenegger",
      "Chongyang Zhu"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-02-13T17:57:50Z",
    "pdf_url": "https://arxiv.org/pdf/2302.07267v6"
  },
  {
    "arxiv_id": "2302.05508v2",
    "entry_id": "http://arxiv.org/abs/2302.05508v2",
    "title": "FairPy: A Toolkit for Evaluation of Prediction Biases and their Mitigation in Large Language Models",
    "summary": "Recent studies have demonstrated that large pretrained language models (LLMs) such as BERT and GPT-2 exhibit biases in token prediction, often inherited from the data distributions present in their training corpora. In response, a number of mathematical frameworks have been proposed to quantify, identify, and mitigate these the likelihood of biased token predictions. In this paper, we present a comprehensive survey of such techniques tailored towards widely used LLMs such as BERT, GPT-2, etc. We additionally introduce Fairpy, a modular and extensible toolkit that provides plug-and-play interfaces for integrating these mathematical tools, enabling users to evaluate both pretrained and custom language models. Fairpy supports the implementation of existing debiasing algorithms. The toolkit is open-source and publicly available at: \\href{https://github.com/HrishikeshVish/Fairpy}{https://github.com/HrishikeshVish/Fairpy}",
    "authors": [
      "Hrishikesh Viswanath",
      "Tianyi Zhang"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-02-10T20:54:10Z",
    "pdf_url": "https://arxiv.org/pdf/2302.05508v2"
  },
  {
    "arxiv_id": "2302.04343v1",
    "entry_id": "http://arxiv.org/abs/2302.04343v1",
    "title": "CRL+: A Novel Semi-Supervised Deep Active Contrastive Representation Learning-Based Text Classification Model for Insurance Data",
    "summary": "Financial sector and especially the insurance industry collect vast volumes of text on a daily basis and through multiple channels (their agents, customer care centers, emails, social networks, and web in general). The information collected includes policies, expert and health reports, claims and complaints, results of surveys, and relevant social media posts. It is difficult to effectively extract label, classify, and interpret the essential information from such varied and unstructured material. Therefore, the Insurance Industry is among the ones that can benefit from applying technologies for the intelligent analysis of free text through Natural Language Processing (NLP).\n  In this paper, CRL+, a novel text classification model combining Contrastive Representation Learning (CRL) and Active Learning is proposed to handle the challenge of using semi-supervised learning for text classification. In this method, supervised (CRL) is used to train a RoBERTa transformer model to encode the textual data into a contrastive representation space and then classify using a classification layer. This (CRL)-based transformer model is used as the base model in the proposed Active Learning mechanism to classify all the data in an iterative manner. The proposed model is evaluated using unstructured obituary data with objective to determine the cause of the death from the data. This model is compared with the CRL model and an Active Learning model with the RoBERTa base model. The experiment shows that the proposed method can outperform both methods for this specific task.",
    "authors": [
      "Amir Namavar Jahromi",
      "Ebrahim Pourjafari",
      "Hadis Karimipour",
      "Amit Satpathy",
      "Lovell Hodge"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-02-08T21:23:52Z",
    "pdf_url": "https://arxiv.org/pdf/2302.04343v1"
  },
  {
    "arxiv_id": "2303.07205v3",
    "entry_id": "http://arxiv.org/abs/2303.07205v3",
    "title": "The Science of Detecting LLM-Generated Texts",
    "summary": "The emergence of large language models (LLMs) has resulted in the production of LLM-generated texts that is highly sophisticated and almost indistinguishable from texts written by humans. However, this has also sparked concerns about the potential misuse of such texts, such as spreading misinformation and causing disruptions in the education system. Although many detection approaches have been proposed, a comprehensive understanding of the achievements and challenges is still lacking. This survey aims to provide an overview of existing LLM-generated text detection techniques and enhance the control and regulation of language generation models. Furthermore, we emphasize crucial considerations for future research, including the development of comprehensive evaluation metrics and the threat posed by open-source LLMs, to drive progress in the area of LLM-generated text detection.",
    "authors": [
      "Ruixiang Tang",
      "Yu-Neng Chuang",
      "Xia Hu"
    ],
    "categories": [
      "cs.CL",
      "cs.AI"
    ],
    "published": "2023-02-04T04:49:17Z",
    "pdf_url": "https://arxiv.org/pdf/2303.07205v3"
  },
  {
    "arxiv_id": "2302.05448v1",
    "entry_id": "http://arxiv.org/abs/2302.05448v1",
    "title": "The Construction of Reality in an AI: A Review",
    "summary": "AI constructivism as inspired by Jean Piaget, described and surveyed by Frank Guerin, and representatively implemented by Gary Drescher seeks to create algorithms and knowledge structures that enable agents to acquire, maintain, and apply a deep understanding of the environment through sensorimotor interactions. This paper aims to increase awareness of constructivist AI implementations to encourage greater progress toward enabling lifelong learning by machines. It builds on Guerin's 2008 \"Learning Like a Baby: A Survey of AI approaches.\" After briefly recapitulating that survey, it summarizes subsequent progress by the Guerin referents, numerous works not covered by Guerin (or found in other surveys), and relevant efforts in related areas. The focus is on knowledge representations and learning algorithms that have been used in practice viewed through lenses of Piaget's schemas, adaptation processes, and staged development. The paper concludes with a preview of a simple framework for constructive AI being developed by the author that parses concepts from sensory input and stores them in a semantic memory network linked to episodic data. Extensive references are provided.",
    "authors": [
      "Jeffrey W. Johnston"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-02-03T22:52:17Z",
    "pdf_url": "https://arxiv.org/pdf/2302.05448v1"
  },
  {
    "arxiv_id": "2302.00560v1",
    "entry_id": "http://arxiv.org/abs/2302.00560v1",
    "title": "Co-Writing with Opinionated Language Models Affects Users' Views",
    "summary": "If large language models like GPT-3 preferably produce a particular point of view, they may influence people's opinions on an unknown scale. This study investigates whether a language-model-powered writing assistant that generates some opinions more often than others impacts what users write - and what they think. In an online experiment, we asked participants (N=1,506) to write a post discussing whether social media is good for society. Treatment group participants used a language-model-powered writing assistant configured to argue that social media is good or bad for society. Participants then completed a social media attitude survey, and independent judges (N=500) evaluated the opinions expressed in their writing. Using the opinionated language model affected the opinions expressed in participants' writing and shifted their opinions in the subsequent attitude survey. We discuss the wider implications of our results and argue that the opinions built into AI language technologies need to be monitored and engineered more carefully.",
    "authors": [
      "Maurice Jakesch",
      "Advait Bhat",
      "Daniel Buschek",
      "Lior Zalmanson",
      "Mor Naaman"
    ],
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.CL"
    ],
    "published": "2023-02-01T16:26:32Z",
    "pdf_url": "https://arxiv.org/pdf/2302.00560v1"
  },
  {
    "arxiv_id": "2301.13126v3",
    "entry_id": "http://arxiv.org/abs/2301.13126v3",
    "title": "LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain",
    "summary": "Lately, propelled by the phenomenal advances around the transformer architecture, the legal NLP field has enjoyed spectacular growth. To measure progress, well curated and challenging benchmarks are crucial. However, most benchmarks are English only and in legal NLP specifically there is no multilingual benchmark available yet. Additionally, many benchmarks are saturated, with the best models clearly outperforming the best humans and achieving near perfect scores. We survey the legal NLP literature and select 11 datasets covering 24 languages, creating LEXTREME. To provide a fair comparison, we propose two aggregate scores, one based on the datasets and one on the languages. The best baseline (XLM-R large) achieves both a dataset aggregate score a language aggregate score of 61.3. This indicates that LEXTREME is still very challenging and leaves ample room for improvement. To make it easy for researchers and practitioners to use, we release LEXTREME on huggingface together with all the code required to evaluate models and a public Weights and Biases project with all the runs.",
    "authors": [
      "Joel Niklaus",
      "Veton Matoshi",
      "Pooja Rani",
      "Andrea Galassi",
      "Matthias Stürmer",
      "Ilias Chalkidis"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-01-30T18:05:08Z",
    "pdf_url": "https://arxiv.org/pdf/2301.13126v3"
  },
  {
    "arxiv_id": "2301.12351v4",
    "entry_id": "http://arxiv.org/abs/2301.12351v4",
    "title": "Emerging Synergies in Causality and Deep Generative Models: A Survey",
    "summary": "In the field of artificial intelligence (AI), the quest to understand and model data-generating processes (DGPs) is of paramount importance. Deep generative models (DGMs) have proven adept in capturing complex data distributions but often fall short in generalization and interpretability. On the other hand, causality offers a structured lens to comprehend the mechanisms driving data generation and highlights the causal-effect dynamics inherent in these processes. While causality excels in interpretability and the ability to extrapolate, it grapples with intricacies of high-dimensional spaces. Recognizing the synergistic potential, we delve into the confluence of causality and DGMs. We elucidate the integration of causal principles within DGMs, investigate causal identification using DGMs, and navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs). We offer insights into methodologies, highlight open challenges, and suggest future directions, positioning our comprehensive review as an essential guide in this swiftly emerging and evolving area.",
    "authors": [
      "Guanglin Zhou",
      "Shaoan Xie",
      "Guang-Yuan Hao",
      "Shiming Chen",
      "Biwei Huang",
      "Xiwei Xu",
      "Chen Wang",
      "Liming Zhu",
      "Lina Yao",
      "Kun Zhang"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-01-29T04:10:12Z",
    "pdf_url": "https://arxiv.org/pdf/2301.12351v4"
  },
  {
    "arxiv_id": "2301.10079v2",
    "entry_id": "http://arxiv.org/abs/2301.10079v2",
    "title": "Reformulation Techniques for Automated Planning: A Systematic Review",
    "summary": "Automated planning is a prominent area of Artificial Intelligence, and an important component for intelligent autonomous agents. A cornerstone of domain-independent planning is the separation between planning logic, i.e. the automated reasoning side, and the knowledge model, that encodes a formal representation of domain knowledge needed to reason upon a given problem to synthesise a solution plan. Such a separation enables the use of reformulation techniques, which transform how a model is represented in order to improve the efficiency of plan generation. Over the past decades, significant research effort has been devoted to the design of reformulation techniques. In this paper, we present a systematic review of the large body of work on reformulation techniques for classical planning, aiming to provide a holistic view of the field and to foster future research in the area. As a tangible outcome, we provide a qualitative comparison of the existing classes of techniques, that can help researchers gain an overview of their strengths and weaknesses.",
    "authors": [
      "Diaeddin Alarnaouti",
      "George Baryannis",
      "Mauro Vallati"
    ],
    "categories": [
      "cs.AI"
    ],
    "published": "2023-01-24T15:33:37Z",
    "pdf_url": "https://arxiv.org/pdf/2301.10079v2"
  },
  {
    "arxiv_id": "2301.10067v1",
    "entry_id": "http://arxiv.org/abs/2301.10067v1",
    "title": "Intrinsic Motivation in Model-based Reinforcement Learning: A Brief Review",
    "summary": "The reinforcement learning research area contains a wide range of methods for solving the problems of intelligent agent control. Despite the progress that has been made, the task of creating a highly autonomous agent is still a significant challenge. One potential solution to this problem is intrinsic motivation, a concept derived from developmental psychology. This review considers the existing methods for determining intrinsic motivation based on the world model obtained by the agent. We propose a systematic approach to current research in this field, which consists of three categories of methods, distinguished by the way they utilize a world model in the agent's components: complementary intrinsic reward, exploration policy, and intrinsically motivated goals. The proposed unified framework describes the architecture of agents using a world model and intrinsic motivation to improve learning. The potential for developing new techniques in this area of research is also examined.",
    "authors": [
      "Artem Latyshev",
      "Aleksandr I. Panov"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-01-24T15:13:02Z",
    "pdf_url": "https://arxiv.org/pdf/2301.10067v1"
  },
  {
    "arxiv_id": "2301.09937v1",
    "entry_id": "http://arxiv.org/abs/2301.09937v1",
    "title": "Explainable Deep Reinforcement Learning: State of the Art and Challenges",
    "summary": "Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. While the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article we aim to provide a review of state of the art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - i.e., of those that take the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state of the art methods, categorizing them in classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes identifying open questions and important challenges.",
    "authors": [
      "George A. Vouros"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-01-24T11:41:25Z",
    "pdf_url": "https://arxiv.org/pdf/2301.09937v1"
  },
  {
    "arxiv_id": "2302.09019v3",
    "entry_id": "http://arxiv.org/abs/2302.09019v3",
    "title": "Tensor Networks Meet Neural Networks: A Survey and Future Perspectives",
    "summary": "Tensor networks (TNs) and neural networks (NNs) are two fundamental data modeling approaches. TNs were introduced to solve the curse of dimensionality in large-scale tensors by converting an exponential number of dimensions to polynomial complexity. As a result, they have attracted significant attention in the fields of quantum physics and machine learning. Meanwhile, NNs have displayed exceptional performance in various applications, e.g., computer vision, natural language processing, and robotics research. Interestingly, although these two types of networks originate from different observations, they are inherently linked through the typical multilinearity structure underlying both TNs and NNs, thereby motivating a significant number of developments regarding combinations of TNs and NNs. In this paper, we refer to these combinations as tensorial neural networks~(TNNs) and present an introduction to TNNs from both data processing and model architecture perspectives. From the data perspective, we explore the capabilities of TNNs in multi-source fusion, multimodal pooling, data compression, multi-task training, and quantum data processing. From the model perspective, we examine TNNs' integration with various architectures, including Convolutional Neural Networks, Recurrent Neural Networks, Graph Neural Networks, Transformers, Large Language Models, and Quantum Neural Networks. Furthermore, this survey also explores methods for improving TNNs, examines flexible toolboxes for implementing TNNs, and documents TNN development while highlighting potential future directions. To the best of our knowledge, this is the first comprehensive survey that bridges the connections among NNs and TNs. We provide a curated list of TNNs at https://github.com/tnbar/awesome-tensorial-neural-networks.",
    "authors": [
      "Maolin Wang",
      "Yu Pan",
      "Zenglin Xu",
      "Guangxi Li",
      "Xiangli Yang",
      "Danilo Mandic",
      "Andrzej Cichocki"
    ],
    "categories": [
      "cs.LG"
    ],
    "published": "2023-01-22T17:35:56Z",
    "pdf_url": "https://arxiv.org/pdf/2302.09019v3"
  },
  {
    "arxiv_id": "2301.05402v1",
    "entry_id": "http://arxiv.org/abs/2301.05402v1",
    "title": "In BLOOM: Creativity and Affinity in Artificial Lyrics and Art",
    "summary": "We apply a large multilingual language model (BLOOM-176B) in open-ended generation of Chinese song lyrics, and evaluate the resulting lyrics for coherence and creativity using human reviewers. We find that current computational metrics for evaluating large language model outputs (MAUVE) have limitations in evaluation of creative writing. We note that the human concept of creativity requires lyrics to be both comprehensible and distinctive -- and that humans assess certain types of machine-generated lyrics to score more highly than real lyrics by popular artists. Inspired by the inherently multimodal nature of album releases, we leverage a Chinese-language stable diffusion model to produce high-quality lyric-guided album art, demonstrating a creative approach for an artist seeking inspiration for an album or single. Finally, we introduce the MojimLyrics dataset, a Chinese-language dataset of popular song lyrics for future research.",
    "authors": [
      "Evan Crothers",
      "Herna Viktor",
      "Nathalie Japkowicz"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-01-13T06:22:22Z",
    "pdf_url": "https://arxiv.org/pdf/2301.05402v1"
  },
  {
    "arxiv_id": "2301.04761v2",
    "entry_id": "http://arxiv.org/abs/2301.04761v2",
    "title": "NarrowBERT: Accelerating Masked Language Model Pretraining and Inference",
    "summary": "Large-scale language model pretraining is a very successful form of self-supervised learning in natural language processing, but it is increasingly expensive to perform as the models and pretraining corpora have become larger over time. We propose NarrowBERT, a modified transformer encoder that increases the throughput for masked language model pretraining by more than $2\\times$. NarrowBERT sparsifies the transformer model such that the self-attention queries and feedforward layers only operate on the masked tokens of each sentence during pretraining, rather than all of the tokens as with the usual transformer encoder. We also show that NarrowBERT increases the throughput at inference time by as much as $3.5\\times$ with minimal (or no) performance degradation on sentence encoding tasks like MNLI. Finally, we examine the performance of NarrowBERT on the IMDB and Amazon reviews classification and CoNLL NER tasks and show that it is also comparable to standard BERT performance.",
    "authors": [
      "Haoxin Li",
      "Phillip Keung",
      "Daniel Cheng",
      "Jungo Kasai",
      "Noah A. Smith"
    ],
    "categories": [
      "cs.CL",
      "cs.LG"
    ],
    "published": "2023-01-11T23:45:50Z",
    "pdf_url": "https://arxiv.org/pdf/2301.04761v2"
  },
  {
    "arxiv_id": "2301.04299v1",
    "entry_id": "http://arxiv.org/abs/2301.04299v1",
    "title": "SoK: Adversarial Machine Learning Attacks and Defences in Multi-Agent Reinforcement Learning",
    "summary": "Multi-Agent Reinforcement Learning (MARL) is vulnerable to Adversarial Machine Learning (AML) attacks and needs adequate defences before it can be used in real world applications. We have conducted a survey into the use of execution-time AML attacks against MARL and the defences against those attacks. We surveyed related work in the application of AML in Deep Reinforcement Learning (DRL) and Multi-Agent Learning (MAL) to inform our analysis of AML for MARL. We propose a novel perspective to understand the manner of perpetrating an AML attack, by defining Attack Vectors. We develop two new frameworks to address a gap in current modelling frameworks, focusing on the means and tempo of an AML attack against MARL, and identify knowledge gaps and future avenues of research.",
    "authors": [
      "Maxwell Standen",
      "Junae Kim",
      "Claudia Szabo"
    ],
    "categories": [
      "cs.LG",
      "cs.AI",
      "cs.CR"
    ],
    "published": "2023-01-11T04:25:00Z",
    "pdf_url": "https://arxiv.org/pdf/2301.04299v1"
  },
  {
    "arxiv_id": "2301.03056v1",
    "entry_id": "http://arxiv.org/abs/2301.03056v1",
    "title": "The State of Human-centered NLP Technology for Fact-checking",
    "summary": "Misinformation threatens modern society by promoting distrust in science, changing narratives in public health, heightening social polarization, and disrupting democratic elections and financial markets, among a myriad of other societal harms. To address this, a growing cadre of professional fact-checkers and journalists provide high-quality investigations into purported facts. However, these largely manual efforts have struggled to match the enormous scale of the problem. In response, a growing body of Natural Language Processing (NLP) technologies have been proposed for more scalable fact-checking. Despite tremendous growth in such research, however, practical adoption of NLP technologies for fact-checking still remains in its infancy today.\n  In this work, we review the capabilities and limitations of the current NLP technologies for fact-checking. Our particular focus is to further chart the design space for how these technologies can be harnessed and refined in order to better meet the needs of human fact-checkers. To do so, we review key aspects of NLP-based fact-checking: task formulation, dataset construction, modeling, and human-centered strategies, such as explainable models and human-in-the-loop approaches. Next, we review the efficacy of applying NLP-based fact-checking tools to assist human fact-checkers. We recommend that future research include collaboration with fact-checker stakeholders early on in NLP research, as well as incorporation of human-centered design practices in model development, in order to further guide technology development for human use and practical adoption. Finally, we advocate for more research on benchmark development supporting extrinsic evaluation of human-centered fact-checking technologies.",
    "authors": [
      "Anubrata Das",
      "Houjiang Liu",
      "Venelin Kovatchev",
      "Matthew Lease"
    ],
    "categories": [
      "cs.CL",
      "cs.AI",
      "cs.CY",
      "cs.HC"
    ],
    "published": "2023-01-08T15:13:13Z",
    "pdf_url": "https://arxiv.org/pdf/2301.03056v1"
  },
  {
    "arxiv_id": "2301.02691v1",
    "entry_id": "http://arxiv.org/abs/2301.02691v1",
    "title": "Systems for Parallel and Distributed Large-Model Deep Learning Training",
    "summary": "Deep learning (DL) has transformed applications in a variety of domains, including computer vision, natural language processing, and tabular data analysis. The search for improved DL model accuracy has led practitioners to explore increasingly large neural architectures, with some recent Transformer models spanning hundreds of billions of learnable parameters. These designs have introduced new scale-driven systems challenges for the DL space, such as memory bottlenecks, poor runtime efficiency, and high costs of model development. Efforts to address these issues have explored techniques such as parallelization of neural architectures, spilling data across the memory hierarchy, and memory-efficient data representations. This survey will explore the large-model training systems landscape, highlighting key challenges and the various techniques that have been used to address them.",
    "authors": [
      "Kabir Nagrecha"
    ],
    "categories": [
      "cs.DC",
      "cs.LG"
    ],
    "published": "2023-01-06T19:17:29Z",
    "pdf_url": "https://arxiv.org/pdf/2301.02691v1"
  },
  {
    "arxiv_id": "2301.01379v1",
    "entry_id": "http://arxiv.org/abs/2301.01379v1",
    "title": "A Succinct Summary of Reinforcement Learning",
    "summary": "This document is a concise summary of many key results in single-agent reinforcement learning (RL). The intended audience are those who already have some familiarity with RL and are looking to review, reference and/or remind themselves of important ideas in the field.",
    "authors": [
      "Sanjeevan Ahilan"
    ],
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published": "2023-01-03T22:17:55Z",
    "pdf_url": "https://arxiv.org/pdf/2301.01379v1"
  },
  {
    "arxiv_id": "2301.00912v1",
    "entry_id": "http://arxiv.org/abs/2301.00912v1",
    "title": "Distributed Machine Learning for UAV Swarms: Computing, Sensing, and Semantics",
    "summary": "Unmanned aerial vehicle (UAV) swarms are considered as a promising technique for next-generation communication networks due to their flexibility, mobility, low cost, and the ability to collaboratively and autonomously provide services. Distributed learning (DL) enables UAV swarms to intelligently provide communication services, multi-directional remote surveillance, and target tracking. In this survey, we first introduce several popular DL algorithms such as federated learning (FL), multi-agent Reinforcement Learning (MARL), distributed inference, and split learning, and present a comprehensive overview of their applications for UAV swarms, such as trajectory design, power control, wireless resource allocation, user assignment, perception, and satellite communications. Then, we present several state-of-the-art applications of UAV swarms in wireless communication systems, such us reconfigurable intelligent surface (RIS), virtual reality (VR), semantic communications, and discuss the problems and challenges that DL-enabled UAV swarms can solve in these applications. Finally, we describe open problems of using DL in UAV swarms and future research directions of DL enabled UAV swarms. In summary, this survey provides a comprehensive survey of various DL applications for UAV swarms in extensive scenarios.",
    "authors": [
      "Yahao Ding",
      "Zhaohui Yang",
      "Quoc-Viet Pham",
      "Zhaoyang Zhang",
      "Mohammad Shikh-Bahaei"
    ],
    "categories": [
      "cs.LG",
      "cs.AI"
    ],
    "published": "2023-01-03T01:05:18Z",
    "pdf_url": "https://arxiv.org/pdf/2301.00912v1"
  }
]